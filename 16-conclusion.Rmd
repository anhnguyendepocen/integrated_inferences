# Final Words

The central idea of this book is that we can learn about the world by combining new evidence with causal models that represent  prior  knowledge about causal relations in the domain of interest. 

<!-- many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account for how and under what conditions causal relations play out.  -->

The growth of randomized experiments and other design-based approaches over the last two decades has made it possible to dispense with or diminish the role of background assumptions for some research questions and contexts. This is a remarkable achievement that has put the testing of some hypotheses and estimation of some causal quantities on a firmer footing. At the same time, there are limits to model-free social science. In particular, design-based inference relies on (as-good-as) random assignment by the researcher or by nature, placing bounds on the kinds of causes and contexts we can investigate. The approach is also generally limited to estimating a single causal quantity: the average causal effect.

Building on pioneering work by scholars in computer science, statistics, and philosophy, we have outlined a principled approach to, and provided software tools for, mobilizing prior knowledge to learn from new data in situations where randomization is unavailable and to answer questions for which randomization is unhelpful. In this approach, causal models are *guides* to research design, *machines* for inference, and *objects* of inquiry. As guides, the models yield expectations about the learning that can be derived from a given case or set of cases and from a given type of evidence, conditional on the question being asked. As inferential machines, models allow updating on that query once the data are in hand. Finally, when we confront a model with data, we learn about the parameters of the model itself, which can be used to answer a range of other causal questions and allowing cumulation of knowledge across studies.  

##Rethinking model-based inquiry

Intellectual currents in empirical social science have been rushing against model-based inquiry for the last decade or more. Profound uneasiness with the assumptions required to draw causal inferences from observational regression estimates has prompted a flight to the epistemic safety of random assignment. With a minimal set of assumptions, a randomized experiment allows for an unbiased estimate of an important quantity, the average treatment effect. Randomized experiments offer answers that are easy to defend. Model-based analysis offers answers that will always be model-dependent. To the extent that what we our after is confidence in our conclusions, the move from models to strong designs represents unquestionable progress.

What we hope that readers will take from this book, however, is that it is a fallacy to think of explicitly model-based inquiry as a second-best alternative to model-free experimentation. It is  a different way of learning about the world altogether. Whether our data are generated from an experiment or arise observationally, organizing inquiry around a causal model has a number of distinct advantages:

**Many questions.** When we update a causal model, we do not estimate a single causal quantity of interest: we learn about *the model*. Most concretely, when we encounter new data, we update our beliefs about *all* parameters in the model at the same time. We can then use the updated parameters to answer very broad classes of causal questions, well beyond the population-level average effect. These include case-level questions (*Does $X$ explain $Y$ in this case?*), process questions  (*Through which channel does $X$ affect  $Y$?*), and transportability questions (*What are the implications of results derived in one context for processes and effects in other contexts?*). 

**Common answer strategy.** Strikingly, these diverse types of questions are all asked and answered in this approach using the same procedure: forming, updating, and querying a causal model. Likewise, once we update a model given a set of data, we can then pose the full range of causal queries to the updated model. In this respect, the causal models approach differs markedly from common statistical frameworks in which distinct estimators are constructed to estimate particular estimands.

**Answers without identification.** The approach can be used to generate answers even when queries are not *identified.* The ability to "identify" causal effects has been a central pursuit of much social science research in recent years. But identification is in some ways a curious goal. A causal quantity is identified, if, with infinite data, the correct value can be ascertained with certainty---informally, the distribution that will emerge is consistent with only one parameter value. Oddly, however knowing that a model, or quantity, is identified in this way does not tell you that estimation with finite data is any good [@maclaren2019can]. Nor is estimation of a non-identified model with finite data necessarily bad. While there is a tendency to discount models for which quantities of interest are not identified, in fact it is relatively easy to see that conisderable learning is possible even without identification, using the same procedure of updating and querying models. Updating non-identified models can lead to a tightening of posteriors, even if some quantities can never be distinguished from each other.  


<!-- Using causal models also provides a clear *procedure* for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw.  -->

**Integration** Embedding inference within an explicit causal model brings about an integration across forms of data and beliefs that may otherwise develop in isolation from one another. For one thing, the approach allows us to combine arbitrary mixes of forms of evidence, including data on causes and outcomes and evidence on causal processes (whether from the same or different sets of cases). Further, the causal-model approach ensures that our findings about *cases* (given evidence about those cases) are informed by what we know about the *population* to which those cases belong, and vice versa. And, as we discuss further below, approach generates integration between inputs and outputs into the inferential process: it ensures that the way in which we update from the data is logically consistent with our prior beliefs about the world. 

**A framework for knowledge cumulation.** Closely related to integration is cumulation: a causal-model framework provides a ready-made apparatus for combining information across studies. Thinking in meta-analytic terms, the framework provides a mechanism for combining the evidence from multiple, independent studies. Thinking sequentially, the model updated from one set of data can become the starting point for the next study of the same causal domain. 

Yet the approach allows for But compared with most prevailing approaches to observational inference---where the background model is typically left implicit or conveyed informally or incompletely---the approach ensures *transparency* about the beliefs on which inferences rest. This allows us then to assess the degree of sensitivity of conclusions to our prior beliefs. 

<!-- While we have outlined a set of strategies for validating and selecting our models, the model-contingency of conclusions is an important and inescapable limitation of the framework.  -->

As we have developed the approach, our thinking about qualitative, quantitative, and mixed-method inference has shifted. In using causal models and seeing their benefits, we have have also developed a keener sense of the risks they entail. We outline these lessons and these risks next.   

- Yes, model-dependent
- But can then learn about the model
- Fully integrative
- Agenda-setting




## Lessons learned along the way

We note three ways in which our thinking about inference evolved in the course of this project, from our original article on mixing methods (@humphreys2015mixing) to the completion of this book.

**"Within" vs. "Between" Case Evidence.** We embarked on this project motivated by an interest in how qualitative and quantitative data could be formally combined to draw case- and population-level causal inferences. In @humphreys2015mixing, we drew on a common operationalization of "quantitative" and "qualitative" data as akin to "dataset" and "causal process" observations, respectively, as defined by @collier2010sources; this is a distinction that roughly maps onto somewhat older notions of "cross-case" and "within-case" forms of analysis (@mahoney2000strategies). In a typical mixed-method setup, we might think of combining a "quantitative" dataset as containing $X$ and $Y$ (and covariate) observations for many cases with "qualitative" observations on causal processes, such as a mediator $M$, for a subset of these cases. 

In fact, as we came to realize, the apparent distinction here between forms of data has no meaning in the formal setup and analysis of models. There is no need to think of $X$ and $Y$ observations as being tied to a large-$N$ analysis or to observations of mediating or other processes as being tied to small-$N$ analysis. One could, for instance, have data on $M$ for a large set of cases but data on $Y$ or $X$ for only a small number. Updating the model to learn about the causal query of interest will proceed in the same basic manner. The cross-case/within-case dichotomy plays no role in the way inferences are drawn: given any pattern of data we observe in the cases at hand, we are always assessing the likelihood of that data pattern under different values of the model's parameters. In this framework, what we have conventionally thought of as qualitative and quantitative inference strategies are not just integrated; the distinction between them breaks down completely.

**A single system.** Our initial thinking about integrating inferences was not particularly well integrated. In @humphreys2015mixing, we posited a set of prior beliefs with which the researcher would begin, including beliefs about the values of estimands ($\lambda$ values) and beliefs about the informativeness of within-case (clue) information ($\phi$ values). These sets of beliefs were essentially independent of one another, as they are in many accounts of process tracing, including Bayesian approaches (@FairfieldBayes2015, @BennettAppendix). In the standard Bayesian approach, one articulates a degree of confidence in some hypothesis about the world and one articulates a belief about the value of the evidence one goes looking for (e.g, that the search for the evidence constitutes a "hoop test" for the hypothesis). And these two sets of beliefs can be arbitrarily combined.

What we came to realize, however, was that *both* sets of beliefs --- about the hypothesis being examined and about the probative value of the data --- represent substantive probabilistic claims about the world, and in particular about *causal relationships* in the domain under investigation. They, thus, cannot not be treated as generally independent of one another: our beliefs about causal relations *imply* our beliefs about the probative value of the evidence. These implications flow naturally in a causal-model framework. When both sets of beliefs are themselves derived from an underlying model representing prior knowledge about the domain of interest, then the same conjectures that inform our beliefs about the hypotheses also inform our beliefs about the informativeness of additional data.

**Pretending to have priors.** As we explored the world of causal models, we started out thinking that, in providing priors over causal relations, one is directly stating beliefs about how the world works. For instance, one might believe that either $X$ caused $Y$ or that it did not; and one might believe either that $M=1$ should be observed in the event that $X$ caused $Y$ or it should not be. These statements are, in fact, clearly model-dependent. Beyond the model required to describe events in such crisp terms, the statements involve counterfactuals on counterfactuals---models of causal processes. Once a model involves assertions of conditional independence, we are clearly in the business of dealing in simplifications. Our priors become less statements of how we believe the world works and become statements about what set of models are least bad within a class of tractable abstractions.

<!-- AJ: There's a clear tension between the above point and the ways in which we talk about models as reflecting "prior beliefs" or "prior knowledge" about the world. Need to resolve this. -->


## Rules of thumb for reasoning about learning

1. Learning requires uncertainty. And expected learning goes up as you become more uncertain about what you’ll find.  If your causal model puts a very high probability on X having a positive effect on M, and you already know X’s value, you should expect to learn very little from observing M since you’re very likely to see exactly the M value you expect given X. (Currently in Chap. 12)((And we want to make research design choices based on expected learning, not based on the mere possibility of learning: yes, our beliefs will shift if we look for M and find the unexpected value. But because that data-realization is highly unlikely, we expect the learning from observing M to be minimal. 

2. Pure within-case (or n=1) learning requires informative priors about the nodes to be observed. For instance, in a chain model, where we want to go and observe M, it’s not enough to have an informative prior about the X->Y relationship. We need an informative prior about the X->M or M->Y link in order to learn from M. For instance, are positive X->M effects more common than negative ones?

3. Learning is possible in the absence of informative priors if we have n>1 because it is sometimes possible to draw causal information from correlations. For instance, even if we have no idea what the distribution of X->M effects are, observing some correlation (or no correlation) between M and X across multiple cases provides information about the likelihood of an X->Y effect (even if it doesn’t tell us about the direction of the effect).

4. If there are different ways a query can be satisfied, evidence against one of those ways is evidence against the query as a whole. Say we have a two-path model — with one direct and one indirect path — and we want to know if X affects Y. We observe a mediator, M, along the indirect path in a set of cases. If the M data pattern is inconsistent with an indirect effect, then this is also evidence against an overall effect. In general, finding evidence against one way the effect can happen reduces our confidence in the effect happening at all. 

5. Prior beliefs structure the learning from new data. When confronted with new data, a prior will condition updating as though the prior were “trying” to preserve itself.  (There are elements of this point in Chap 13 — you learn more about heterogeneity if ATE is known.)((Say, we’re working with a chain model, suppose we have a lot of prior X,Y data consistent with a positive ATE of X on Y. We then process trace an X=1, Y=1 case and an X=0, Y=0 case, observing M=1 in both, so M is uncorrelated with X. We’ve just found evidence against an effect of X on Y, so this will reduce our posterior on the ATE. But the ATE prior also has a way to “preserve” itself: it can force a downward updating of our beliefs about the prevalence negative effects. If we haven’t observed any cases in which negative effects might operate, there is little to constrain that downward updating. The overall result will be updating on positive effects (downwardly) together a some mix of updating on the ATE (downwardly) and updating on negative effects (downwardly).((A corollary is that learning about a kind of case that we directly observe can get “transmitted” to a kind of case that we don’t directly observe via a constraint on our beliefs imposed by the model or priors. This is just a special instance of priors generating probative value: our priors on the ATE makes evidence about positive effects probative about negative effects. If we had flat priors on the ATE, learning about positive effects would have no impact on our beliefs about negative effects.

A parallel example arises with a two-path model and observation of a mediator, M, along the indirect path in a set of cases. Suppose we find an M data pattern inconsistent with any kind of indirect effect: what happens to our beliefs about the ATE? In general, finding evidence against one way the effect can happen should reduce our confidence in the effect happening at all. However, if we have started out with a strong prior on the ATE but flat priors over whether the effect is a direct or indirect one, then our updating will tend to conserve our ATE beliefs by updating our beliefs about direct effects. So evidence against the indirect effect will function as evidence for direct effects as well as evidence against a total effect. 

A further implication for process tracing is that there will generally be sharp limits to what we can learn about overall effects if we study mediators along only *some* of the theorized pathways, especially if we already have some prior information about effects. The difficulty is that whatever we learn from the mediators we do observe, about the pathways they lie along, will get offset by shifts in our beliefs about other pathways, generated by the constraint in our prior knowledge about the overall effect. Suppose you already have some belief that economic development makes democracy more likely, and you think there may be two mechanisms: one running through a rising middle class and one operating through a more robust and organized working class. Suppose then that I show you that the organization of the working class does not vary with per capita GDP. Rationally, you should then reduce your confidence in the working-class pathway. However, you should also *increase* your confidence in the operation of the middle-class pathway --- because (a) you have prior reason to believe that the overall development $\rightarrow$ democracy effect exists and (b) we have not observed a mediator along the middle-class pathway. On balance, then, learning about just one pathway will not have a large impact on beliefs about the overall effect of GDP on democratization. The larger lesson here is that, if our process tracing strategy involves the examination of mediators to learn about total $X \rightarrow Y$ effects, then how much we stand to learn depends on whether we are collecting diagnostic evidence along *all* plausible pathways connecting $X$ to $Y$.

To be clear, we do not need to collect mediator clues on all *possible* pathways. If we have strong priors that one or more possible pathways are very unlikley, then we might safely be able to avoid collecting observations along those pathways without substantially reducing the prospects for learning. Also, the point that we are making here applies to using mediator data to answer queries about the effect of $X$ on $Y$. If instead we want to know whether a *particular pathway* is operating, then collecting evidence just on that pathway might be highly informative.

6. Stronger priors impose a stronger constraint on learning.  If we’ve observed only a small amount of X,Y data in the above chain-model example, then observing the M data in the on-the-regression-line cases will have a weaker impact on our beliefs about negative effects, and a bigger impact on our beliefs about ATE. Ditto for the 2-path model example. However, where prior data/beliefs on the ATE are strong, we’ll learn less about the ATE, and more about negative effects (or the direct path). 

7. If there are different ways in which a query can be satisfied, evidence against the likelier way is stronger evidence against the query than is evidence against an unlikelier way. In the 2-path model, if we started out thinking that the indirect effect was more likely than the direct effect, then evidence against the direct effect will have a bigger impact on our beliefs about the overall model. 

8. It is difficult to get empirical leverage on very unlikely queries. And queries may be unlikelier than they appear. Suppose we start with the 2-path model, and want to know if X has a positive effect on Y that rests on a chain of positive effects via M. And suppose, importantly, that we begin with flat priors over all nodal types. Our intuitions likely tell us that this is exactly the kind of question for which an observation of M is the perfect empirical strategy. And that intuition is, in a sense, correct: we can indeed learn about the query by observing M. Seeing M=1 in an X=Y=1 case, for instance. would be evidence consistent with the query while M=0 would be inconsistent. Fine. ((But we will only learn a little from this observation. The reason is that the query itself has a very low prior probability. It may actually not be obvious at first glance just how unlikely our query is to be true. (After all, the model has two causal paths, and we’re asking if positive effects run through one of them, right? Not quite.) Seeing this requires us to think about the joint probabilities implied by the query. First, the query requires X to have a positive effect on M, which we think there’s only a 25% chance of. In addition, the query puts a very narrow constraint on Y’s possible nodal types:  Y has to have a nodal type in which M has a positive effect on Y when X does not change, and in which X does not have a positive effect on Y unless M changes from 0 to 1. This pair of conditions is met by only 2 of Y’s 16 nodal types, implying a 12.5% chance. The prior on the query is thus 0.25 x 0.125 = 0.03125. Thus, while observing M=0 takes the probability of the query down to 0%, we started out very close to 0%! And observing M=1 results in only a small uptick, to about 6% because there remain many type combinations consistent with M=1 but that do not fit through the needle-eye of this query.((((



## Worries about what you have to put in

While we have found the syntax of Directed Acyclic Graphs to provide a flexible framework for setting up causal models, we have also become more keenly aware of some of the limitations of DAGs in representing causal processes. We discuss a few of these here.

**Well defined nodes?** A DAG presupposes a set of well-defined nodes that come with location and time stamps. [Point to be elaborated.] This involves a discretization of the world that may not align with how qualitative researchers study the world. For instance, qualitative researchers do not just observe that (a) domino 1 fell and (b) domino 2 fell. They observe that domino 2 fell *just* as domino 1 hit it. 

<!-- AJ: I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->

**Acyclic, really?** DAGs are by definition acyclic. And it is not hard to argue that, since cause precedes effect, causal relations *should* be acyclic for any well-defined nodes. In practice, however, our variables often come with coarse periodizations: there was or was not mobilization in the 1990s; there was or was not democratization in the 1990s. We cannot extract the direction of arrows from the definition of nodes this coarse. 

<!-- AJ: Here too, I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->

**Coherent underlying causal accounts.** The approach we describe is one in which researchers are asked to provide a coherent model---albeit with uncertainty---regarding the ways in which nodes are causally related to each other.  For instance, a researcher interested in using information on $K$ to ascertain whether $X$ caused $Y$ is expected to have a theory of whether $K$ acts as a moderator or a mediator for $X$, and whether it is realized before or after $Y$. Yet it is possible that a researcher has well formed beliefs about the informativeness of $K$ *without* an underlying model of how $K$ is causally related to $X$ or $Y$. Granted, one might wonder where these beliefs come from or how they can be defended. We nonetheless note that one limitation of the approach we have described is that one cannot make use of an observation without a coherent account of that observation's causal position relative to other variables and relationships of interest. 

## Limits on what you can get out

**Complexity.** To maintain simplicity, we have largely focused in this book on models with binary nodes. At first blush, this class of causal models indeed appears very simple. Yet even with binary nodes, complexity rises rapidly as the number of nodes and connections among them increases. As a node goes from having 1 parent to 2 parents to 3 parents, for instance, the number of nodal types --- at that node alone --- goes from 4 to 16 to 64, with knock-on effects for the number of possible causal types (combinations of nodal types across the model). A move in the direction of continuous variables --- say, from binary nodes to nodes with 3 ordinal values --- would also involve a dramatic increase in the complexity to the type-space. A potential solution is to move away from a fully non-parametric setting and impose structure on permissible function forms. [To expand on this point]

<!-- AJ: These next two points seem more like advantages to me. They are fundamental features of causal inference we can see more clearly if we use models, not limitations of the approach itself. No? Maybe we should have a section on that sort of thing: limits to causal inference uncovered by the approach. (I've added the model-dependence point.)-->


**Limits of qualitative data under ignorable assignments.** A key payoff deploying causal models is the prospect of combining in-depth observations of a small number of cases with less-intensive investigation of a larger number of cases. Yet one of the lessons of the foregoing analysis is that the gains to such mixing may be limited. For instance, where the causal effect of $X$ on $Y$ can be identified (assignment of $X$ is as-good-as random), one will learn little about this effect from process observations in a small number of cases. Put differently, case studies simply cannot add much to experimental estimates of the ATE, though there may be other queries of interest on which small-$N$ process evidence can be informative. [To be expanded]

**Model-dependence of conclusions** One striking finding of some of the analyses presented here is see how sensitive conclusions can be to what would seem to be quite modest changes to models. We see two ways of thinking about the implications of this fact for a causal-models framework. 

One lesson to draw would be that there are tight limits to building inference upon causal models. If results in this approach depend heavily on prior beliefs, which could be wrong, then we might doubt the utility of the framework. Perhaps all that we have done in this book is to make the case for pure design-based inference.

An alternative lesson also offers itself, however. To the extent that our inferences depend on our background causal beliefs, a transparent and systematic engagement with models becomes all the more important. If inferences are not built explicitly on models, we have no way of knowing how fragile they are, how they would change under an alternative set of premises, or what kind of learning we need to undertake if we want to generate more secure conclusions. 

We do not see causal models as the only way forward or as a panacea, and we are conscious of the limitations and complexities of the approach we have outlined, as well as the need for extension and elaboration along numerous fronts. Yet we think there is value in further development of forms of empirical social science that can operate with analytic transparency outside the safe inferential confines of random assignment.

## Know what you impose when you impose structure

e.g. changige expectations of trematne effect
learning in a confound model with M unobserved

beliefs about share a in a X->Y model fom reg data are different to learning from the saem data in a X->M->Y model wiht M unobserved. 

If you specify X->M->Y <-W and W is a smoking gun then it must be smokoning gund for M and for Y


## Looking ahead

Approach is intellectually well developed, but more work needed to make practical.

- How to generate models -- e.g., elicitation from experts. Madigan.
- Levels of measurement: Beyond binary nodes. Currently working on this.
- Computational issues
