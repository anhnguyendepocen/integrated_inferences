# Mixing models {#mm}

***

We provide three examples of situations in which, by combining models, researchers learn more than they could from any single model.

***


```{r packagesused15, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
library(DeclareDesign)
```

In the previous two chapters, we described one form of integration that structural causal models can enable: the systematic combination of (what we typically think of as) qualitative and quantitative evidence for the purposes of drawing population- and case-level causal inferences. One feature of the analyses we have been considering so far is that the integration is essentially "nested." We are, for instance, integrating quantitative evidence for a large set of cases with qualitative evidence for a *subset* of those cases. We are, moreover, drawing inferences from the set of cases we observe to a population *within which* that sample of cases is situated. 

In this chapter, we examine how we can use structural causal models to integrate across studies or settings that are, in a sense, more disjointed from one another: across studies that examine different causal relationships altogether; study designs that require different assumptions about exogeneity; and contexts across which the causal quantities of interest may vary.

1. **Integrating across a model** Often, individual studies in a substantive domain examine distinct segments of a broader web of causal relationships. For instance, while one study might examine the effect of $X$ on $Y$, another might examine the effect of $Z$ on $Y$, and yet another might examine the effect of $Z$ on $K$. We show in this chapter how we can integrate across such studies in ways that yield learning that we could not achieve by taking each study on its own terms. 

2. **Integrating between experimental and observational studies** One form of multi-method research that has become increasingly common is the use of both observational and experimental methods to study the same basic causal relationships. While an experiment can offer causal identification in a usually local or highly controlled setting, an observational analysis can often shed light on how the same relationships operate "in the wild," if with greater risk of confounding. Usually, observational and experimental results are presented in parallel, as separate sources of support for a causal claim. We show how, in a causal model setup, we can use experimental and observational data *jointly* to address questions that cannot be answered when the designs are considered separately.

3. **Transporting knowledge across contexts** Researchers are sometimes in a situation in which they can identify causal quantities in a particular setting --- say, from a randomized controlled trial implemented in a specific local context --- but want to know how those inferences travel to other settings. Would the intervention work differently in other countries or regions? As we will explain, with an appropriately specified causal model and the right data from the original context, we can draw inferences about causal relationships in other contexts.

Before delving into the details of these strategies, we make one key qualification explicit: each of these approaches requires us to believe that setting-, or study-, specific  causal model can be nested within a lower level, "encompssing," model that operates across the multiple settings that we are learning from and want to draw inferences about. Encompassing models, of course, can specifically take heterogeneity across settings into account, for instance by including in the model moderators that condition the effects of interest. But we have to believe that we have indeed captured in the model any ways in which relationships vary across the set of contexts across which we are integrating evidence or transporting inferences. 

Put differently, and perhaps more positively, we see social scientists commonly seeking to transport knowledge or combine information informally across studies and settings. Often such efforts are motivated, sometimes implicitly, by an interest in or reliance on general theoretical propositions. The approaches that we describe below force the researcher to be *explicit* about the underlying causal beliefs that warrant that integration while also ensuring that the integration proceeds in a way that is logically consistent with stated beliefs.


## A jigsaw puzzle: Integrating across a model

Generating knowledge about a causal domain often involves cumulating learning across studies that each focus in on some specific part of the domain. For instance, scholars interested in the political economy and democratization might undertake studies focused on the relationship between inequality and mass protests; studies on the role of mass mobilization in generating regime change; pathways other than mass mobilization through which inequality might affect democratization; studies of the role of international sanctions on the likelihood that autocracies will democratize; and studies of the effects of democratization on other things, such as growth or the distribution of resources. 

We can think of these studies as each analyzing data on a particular part of a broader, more encompassing causal model. In an informal way, *if* findings "hold together" in a reasonably intuitive way, we might be able to piece together an impression of the overall relations among variables in this domain.  Yet an informal approach becomes more difficult for complex models or data patterns and, more importantly, will leave opportunities for learning unexaploited.

Consider this simple DAG, in which both $X$ and $Z$ are causes of $Y$, and $Z$ also causes $K$. Now imagine three studies, all conducted in contexts in which we believe this model to hold: 

```{r jigsaw, eval = TRUE, echo = FALSE}
model <- make_model("X -> Y <- Z -> K") %>%

          set_parameters(
            statement = list("(Y[X=1, Z = 1] > Y[X=0, Z = 1])",  
                             "(K[Z = 1] > K[Z = 0])"),
            node = c("Y","K"), 
            parameters = c(.24,.85))

plot(model)

```


1. Study 1 is an RCT in which $X$ is randomized, with data collected on both $Y$ and $K$. $K$ is collected. $Z$ is not observed.
1. Study 2 is a factorial experiment, in which $X$ and $Z$ are independently randomized, allowing an examination of the joint effects of $X$ and $Z$ on $Y$. $K$ is not observed.
2. Study 3 is an experiment randomizing $Z$, with only $K$ observed as an outcome. $X$ and $Y$ are not observed.

Now, let's say that our primary interest is in the relationship between $X$ and $Y$. Obviously, Study 1 will, with a sufficiently large sample, perform just fine in estimaing the average treatment effect of $X$ on $Y$. However, what if we are interested in a case-oriented query, such as the probability of causation: the probability, say, $X=1$ caused $Y=1$ in a given $X=1, Y=1$ case? 

We know that within-case, process-tracing clues can sometimes provide probative value on case-level estimands like the probability of causation, and we have observed $K$ in the Study 3 cases. So what if we combine the $X$, $Y$, and $K$ data? 

```{r, echo = FALSE}

df <- make_data(model, 300, using = "parameters") %>%
  
      mutate(study = rep(1:3, each = 100),
             Z = ifelse(study == 1, NA, Z),
             K = ifelse(study == 2, NA, K),
             X = ifelse(study == 3, NA, X),
             Y = ifelse(study == 3, NA, Y)
             )

```

```{r, echo = FALSE}
if(do_diagnosis){

updated1 <- update_model(model, filter(df, study == 1))
updated2 <- update_model(model, filter(df, study == 2))
updated3 <- update_model(model, filter(df, study == 3))
updated_all <- update_model(model, df)

subs <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0")
subs2 <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0",
              "X == 1 & Y == 1 & K == 1 & Z == 1",
              "X == 1 & Y == 1 & K == 0 & Z == 1",
              "X == 1 & Y == 1 & K == 1 & Z == 0",
              "X == 1 & Y == 1 & K == 0 & Z == 0")

# If updating done using case data only
result1 <- query_model(updated1, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result2 <- query_model(updated2, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result3 <- query_model(updated3, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result4 <- query_model(updated_all, queries = "Y[X=0] == 0", given = subs2, using = "posteriors")

write_rds(list(result1, result2, result3, result4), "saved/10a_frankenstein.rds")
}

```


<!-- AJ: Suggest we flip the study ordering to the X-Y experiment is STudy 1. Makes a more natural progression with the prose. -->

A simple analysis of the graph tells us that $K$ cannot help us learn about $Y$'s potential outcomes since $K$ and $Y$ are $d$-separated by $Z$, and we have not observed $Z$ in Study 3. We see this confirmed in Table \@ref(tab:frank1). 

In the first pair of rows, we  show the results of analyses in which we have simulated data from the whole model, then updated using the Study 1 observations. We give here the posterior mean on the probability of causation for an $X=Y=1$ case, conditional on each possible value that $K$ might take on. As we can see, our beliefs about the estimand remain unaffected by $K$'s value, meaning that it contains no information about $X$'s effect in the case.

<!-- AJ: When we say data on K is not available in S1, do we mean we haven't used K to *update* the model? But we are then simulating what would we would infer if we *did* collect $K$ for a case in S1? This is my read of the code. Or have I misread? Are we imagining a situation in which we simply don't have K at all, so by definition can't learn from K? I've written the text below on the first assumption, so will need changing if that's wrong. -->

We see that the same thing is true for each of the other studies. In study 2, we have not used $K$ to update the model, and so have not learned anything form the data about $K$'s relationship to the other variables. Thus, we have no foundation on which to ground probative value fo $K$. In study 3, we understand the $Z,K$ relationship well, but know nothing quantitatively about how $Z$ and $X$ relate to $Y$. Thus, we have learned nothing from Study 3 about what observing $K$ might tell us about the effect of $X$ on $Y$.
 

```{r frank1, echo = FALSE}

frank <- read_rds("saved/10a_frankenstein.rds")

kable(
  cbind(Study = c(1, NA, 2, NA, 3, NA),
        rbind(
    frank[[1]][,-c(1,3)],
    frank[[2]][,-c(1,3)], 
    frank[[3]][,-c(1,3)])), 
  caption = "The clue $K$ uninformative in all three studies")
```


We can do much better, however, if we combine the data and update *jointly* across all model paramaters. The results are shown in Table \@ref(tab:frank4). Updating simultaneously across the studies allows us, in a sense, to bridge across inferences. In particular, inferences from Study 2 make $Z$ informative about $Y$'s potential outcomes under different values of $X$. Meanwhile, inferences from the data in Study 3 allow us to use information on $K$ to update on values for $Z$. As we now see in rows 1 and 2, having updated the model in an integrated fashion, $K$ now *is* informative about the probability of causation, with our posterior mean on this query changing substantially depending on the value of $K$ that we observe in a case.  

Rows 3-4 highlight that the updating works through inferences on $Z$: we see that if $Z$ is already known (we show this for $Z=1$, but it holds for $Z=0$ as well), then there are no additional gains from knowledge of $K$. 


```{r frank4, echo = FALSE}
kable(frank[[4]][1:4,-c(1,3)], caption = "Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$", digits = 2)
```


<!-- In sum, the collection of studies collectively provides a justification for learning from $K$ when assessing a case level effect of $X$ on $Z$ in study 1.  -->

<!-- AJ: Is something wrong with the study numberings in this next paragraph? -->

<!-- I find the reference to justifying a model confusing here. It's not going to be self-evident to readers how that's what we've just done. It seems like we've *started* with a model and used it. But I'm not 100% clear on what you want to say here. Can you clarify? -->

We devote Chapter 15 to a discussion of how we justify a model. However, we note already that in this example we have an instance in which a researcher (examining a case in study 3) might wish to draw inferences using $K$, but she does not have anything in study 1 that justifies using $K$ for inference. However with access to studies 2 and 3, and conditional on the overall model, she has a justification for process tracing strategy. The general principle is that weaker commitments to lower level theories ---here the causal structure---can justify more fully inferences from more fully specified higher-level theories.  

<!-- AJ: Also finding the last sentence above confusing. -->

## Multilevel models, meta-analysis

A key idea in Bayesian meta-analysis is that when you analyze multiple studies together you learn not only about common processes that give rise to the different results seen in different sites, but you also learn more about each study from seeing the other studies.

A classic setup is provided in GELMAN, which we have access to estimates of effects and uncertainty in eight sites (schools), $(b_j, se_j)_{j \in \{1,2,\dots,8\}}$. We assume that each $b_j$ is a draw from distribution $N(\beta_j, se_j)$ and that each  $\beta_j$ is a draw from distribution $N(\beta, \sigma)$. In that setup we want to learn not just about the superpopulation parameters  $\beta, \sigma$, but also about the study level effects $(\beta_j)_{j \in \{1,2,\dots,8\}}$.

In a similar way here we define a model in which "setting" is a node. We observe an experiment that takes pace in setting 0 and in setting 1. We then conside the different conclusions we draw for the effect of $X$ on 

```{r, echo=FALSE}


if(do_diagnosis){
  
  model_1 <- make_model("X -> Y") %>% 
    set_parameters(node  = "Y", parameters = c(.1, .1, .7, .1))
  model_2 <- make_model("X -> Y") %>% 
    set_parameters(node  = "Y", parameters = c(.1, .2, .6, .1))
  model_3 <- make_model("X -> Y <- Setting") 
  model_4 <- model_3 %>% set_priors(2) %>% 
    set_priors(statement = interacts("Setting", "X", "Y"), alphas  = .1)

  df_1 <- simulate_data(model_1, 100) %>% mutate(Setting = 0)
  df_2 <- simulate_data(model_2, 100) %>% mutate(Setting = 1)
  
  model_1 <- update_model(model_1, df_1)
  model_2 <- update_model(model_2, df_2)
  model_3 <- update_model(model_3, rbind(df_1, df_2), iter = 8000)
  model_4 <- update_model(model_4, rbind(df_1, df_2), iter = 8000)
  
  q1 <- query_model(model_1, list(`Setting 0` = te("X","Y")), using = "posteriors")
  q2 <- query_model(model_2, list(`Setting 1` = te("X","Y")), using = "posteriors")
  
  q3_1 <- query_model(model_3, list(`Integrated (flat priors)` = te("X","Y")), 
                   given = c(TRUE, "Setting==0", "Setting==1"), 
                   expand_grid = TRUE, using = "posteriors")

  q3_2 <- query_model(model_3, interacts("Setting", "X", "Y"), 
                   using = c("priors", "posteriors"),
                   expand_grid = TRUE) 
    
  q4_1 <- query_model(model_4, list(`Integrated (weak heterogeneity)` = te("X","Y")), 
                   given = c(TRUE, "Setting==0", "Setting==1"), 
                   expand_grid = TRUE, using = "posteriors")

  q4_2 <- query_model(model_4, interacts("Setting", "X", "Y"), 
                   using = c("priors", "posteriors"),
                   expand_grid = TRUE) 
  
  write_rds(list(q1, q2, q3_1, q3_2, q4_1, q4_2), "saved/10a_multilevel.rds")
  
}

  multilevel <- read_rds("saved/10a_multilevel.rds")

    rbind(
    multilevel[[1]],  multilevel[[2]],
  multilevel[[3]], 
  multilevel[[5]]) %>% kable(caption = "Inferences from separate analyses and from integrated analysis (meta analysis) given (a) flat priors and (b) expectation of similar effects across studies", digits =2)
```
We see in both cases a drop in our estimates for effects in Setting 1 in both cases, relative to the single study case. Where weak heterogeneity is assumed we also see a rise in estimates for Setting 2. 

We can also update on the amount of heterogeneity. Here the estimand is the share of units that would respond differently to treatment if they were in a different setting. We see a dropin expectations of heterogeneitz in the analysis with flat priors, but a rise relative if we start thinking heterogeneity is unlikley.

```{r, echo = FALSE}
    multilevel[[4]] %>% kable(caption = "Interaction | Flat priors")
  multilevel[[6]] %>% kable(caption = "Interaction | Expected homogeneity")
  
```

## Combining observational and experimental data

Experimental studies are often understood as the "gold standard" for causal inference. This is, in particular, because of the ability of a randomized trial (given certain assumptions, such as "no spillovers") to eliminate sources of confounding. By design, an experiment removes from the situation processes that, in nature, would generate a correlation between selection into treatment and potential outcomes. An experiment thereby allows for an unbiased estimate of the average causal effect of the treatment on the outcome. 

At the same time, an interesting weakness of experimental studies is that, by dealing so effectively with selection into treatment, they limit our ability to learn about selection and its implications in the real world. Often, however, we want to know what causal effects would be specifically for units that *would* in fact take up a treatment in a real-world, non-experimental settings. This kind of problem is studied for example by @knox2019design.

Consider, for instance, a policy that would make schooling subsidies available to parents, with the aim of improving educational outcomes for children. How would we know if the policy was effective? A source of confounding in an observational setting might be that those parents who apply for and take up the subsidy might also be those who are investing more in their children's education in other ways as compared to those parents who do not apply for the subsidy. To eliminate this problem, we might design an experiment in which parents are randomly assigned to receive (or not receive) the subsidy and compare outcomes between children in the treatment and control groups. With a no-spillovers assumption, we can extract the $ATE$ of the receipt of subsidies. 

What this experiment cannot tell us, however, is how much the policy will boost educational outcomes outside the experiment. That is because the causal quantity of interest, for answering that question, is *not* the $ATE$: it is the average treatment effect for the *treated* ($ATT$), given real-world selection effects. That is, the policymaker wants to know what the effect of the subsidy will be for the children of parents who *select into* treatment. One could imagine the real-world $ATT$ being higher than the $ATE$ if, for instance, those parents who are informed and interested enough to take up the subsidy also put the subsidy to more effective use. One could also imagine the $ATT$ being lower than the $ATE$ is, for instance, there are diminishing marginal returns to educational investments and the self-selecting parents are already investing quite a lot. 

Even outside a policy context, we may be interested in the effect of a causal condition *where* that causal condition emerges. To return to our inequality and democracy example, we may want to know what would have happened to autocracies with low inequality *if* they had had high inequality -- the standard average-treatment effect question. But we might also be interested in knowing how much of a difference high inequality makes *in the kinds of cases* where high inequality tends to be occur -- where the effect could be very different. 

With such questions, we are in a sort of bind. The experiment cannot tell us *who* would naturally select into treatment and what the effects would be for them. Yet an observational study faces the challenge of ruling out confounding. Ideally, we would like to be able to combine the best features of both: use an experiment to deal with confounding and use observational data to learn about those whom nature assigns to treatment.

We can achieve this form of integration with a causal model. We do so by creating a model in which random assignment is nested within a broader set of assignment processes. We plot the model in Figure \@ref(fig:appcombexpob)

At the substantive core of this model is the $X \rightarrow Y$ relationship. However, we give $X$ a parent that is fully exogenous, $Z$, to capture a random-assignment process. We give $X$ a second parent, $O$, that is confounded with $Y$: $O$ here represents the observational scenario. Finally, we include a "switch" variable, $R$, that determines whether $X$ is randomly assigned or not. So when $R=1$, $X$ is determined solely by $Z$, with $X=Z$. When $R=0$, we are in an observational setting, and $X$ is determined solely by the confounded $O$, with $X=O$.

A few notes on the parameter space. Parameters allow for complete confounding between $O$ and $Y$, but $Z$ and $Y$ are unconfounded. $X$ has only one causal type since its job is to operate as a conveyor belt, simply inheriting the value of $Z$ or $O$, depending on $R$. 

Note also that this model assumes the exclusion restriction that entering the experimental sample ($R$) is not related to $Y$ other than through assignment of $X$. 


```{r copobsetup, message = FALSE, warning = FALSE, echo = FALSE}
model <- make_model("R -> X -> Y; O -> X <- Z; O <-> Y") %>%
  
	set_restrictions("(X[R=1, Z=0]!=0) | (X[R=1, Z=1]!=1) | (X[R=0, O=0]!=0) | (X[R=0, O=1]!=1)")

```

```{r appcombexpob, message = FALSE, warning = FALSE, echo = FALSE, fig.cap="A model that nests an observational and an experimental study. The treatment $X$ either takes on the observational value $O$, or the assigned values $Z$, depending on whether or not the case has been randomized, $R$."}

plot(model)
```

```{r, echo = FALSE, include = FALSE}
P <- get_parameter_matrix(model)
kable(P[,1:4])
```


Now, let us imagine true parameter values such that $X$ has a $0.2$ average effect on $Y$. However, the effect is different for those who are selected into treatment in an observational setting: it is positive ($0.6$) for cases in which $X=1$ under observational assignment, but negative ($-0.2$) for cases in which $X=0$ under observational assignment. (See appendix for complete specification.) 

When we use the model to analyze the data, we will start with flat priors on the causal types. 

```{r, echo = FALSE}

model <- model %>%
	set_parameters(node = "Y", confound = "O==0", parameters = c(.8, .2,  0,  0)) %>%
	set_parameters(node = "Y", confound = "O==1", parameters = c( 0,  0, .6, .4))

```

The implied true values for the estimands of interest, and our priors on those estimands, are displayed in Table \@ref(tab:fusionestimands).

<!-- AJ: I think the talk of parameters and priors is less intuitive than it could be in a table and passage like this, though I realize it maps onto CQ argument names. By "parameters" we mean *true* values in the simulation, right? Can we convert to "truth" in the "Using" column? Truth vs. priors seems nicely intuitive. -->

```{r fusionestimands, echo = FALSE}

if(do_diagnosis){
result <- query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = c("parameters", "priors"), 
    expand_grid = TRUE)
write_rds(result, "saved/10a_fusionestimands.rds")
}

read_rds("saved/10a_fusionestimands.rds") %>% kable(caption = "Estimands in different sites", digits = 2)

```


```{r, echo = FALSE}
data <- make_data(model, n = 800)
```


Now, we generate data from the model, using the posited "true" parameter values, and then update the model using these data. We begin by analyzing just the observational data (cases where $R=0$) and display the results in Table \@ref(tab:fusiondim). The true effect is $0.2$ but naive analysis on the observational data would yield a strongly upwardly biased estimate. Table \@ref(tab:fusiondim) shows differences-in-means estimates using data on observational units only drawn from this model.


```{r fusiondim, echo  = FALSE, warning = FALSE, message = FALSE}
x <- estimatr::difference_in_means(Y~X, data = filter(data, R==0))
kable(summary(x)[[1]], digits = 3, caption = "Inferences on the ATE from differences in means")
```

The estimates from updating on the full model, shown in Table \@ref(tab:fusionCQ), are much better.

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, data), "saved/10a_exp_obs.rds")
  }
updated <- read_rds("saved/10a_exp_obs.rds")
```

```{r fusionCQ, echo = FALSE}
result <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result, caption = "Estimates on the ATE for observational ($R=0$) and experimental ($R=1$) set.")
```




```{r, eval = FALSE, echo = FALSE}
updated_no_O <- update_model(model, dplyr::filter(data, R==1))
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, dplyr::filter(data, R==1)), "saved/10a_exp_obs_2.rds")
  }
updated_no_O <- read_rds("saved/10a_exp_obs_2.rds")
```

```{r appcombexpopp8, echo = FALSE, include = FALSE}
result <- query_model(
    updated_no_O, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result)
```

Since the model used both the experimental and the observational data, it is interesting to ask whether the observational data improved the estimates of the treatment effect or does inference draw only from  the experimental data? We answer the question in the appendix by  updating using experimental data only.
 We find there that we do indeed get a tightening of posterior variance and a more accurate result when we use the observational data but that the gains are relatively small: the experimental data alone is quite powerful. The gains would be smaller still if we had more data, in which case inferences from the experimental data would be more accurate still. 

However we do learn something of particular interest from this model. A key feature here is that there is heterogeneity between those that are in treatment and those that are in control *in the observational* sample. We learn nothing about this heterogeneity from the experimental data alone but we learn a lot from the mixed model, picking up the strong self selection into treatment in the observational group: 

```{r, echo = FALSE}
result2 <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list("R==1 & X==0", "R==1 & X==1", "R==0 & X==0", "R==0 & X==1"),
    using = "posteriors")

kable(result2, caption = "Effects of $X$ conditional on $X$ for units that were randomly assigned or not.  Effects of $X$ do not depend on $X$ in the experimental group, but they do in the observational group becuase of seld selection. ")
```

In essence by mixing the experimental and observational data we can learn what the effects for those that self select into treatment *and* what they would be for those that self select into control. The results here relate to the  LATE theorem [@angrist1995identification] in the following way. If we imagine using data only on (a) the experimental group in control and (b) the observational group, some of whom are in treatment, we can conceptualize our design as one in which the observational group are "encouraged" to take up treatment and we figure out the effect for the compliers in this group (those that self select into treatment). At the same time if we imagine using data only on (a) the experimental group in treatment and (b) the observational group, some of whom are in control, we can conceptualize our design as one in which the observational group are "encouraged" to take teh control condition and we figure out the effect for the compliers in this group (those that self select into control). 


## Transportation of findings across contexts

Say we study the effect of $X$ on $Y$ in case 0 (a country, for instance) and want to make inferences to case 1 (another country). Our problem however is that effects are heterogeneous and features  that differ across units may be related both to treatment assignment, outcomes, and selection into the sample. This is the problem studied by @pearl2014external. In particular  @pearl2014external show for which nodes data is needed in order to  "licence" external claims, given a model. 

We illustrate with a simple model in which a confounder has a different distribution in a study site and a target site.

```{r extval, echo = FALSE, fig.cap= "Extrapolation when confounders have different distributions across cases."}

model <- make_model("Case -> W  -> X -> Y <- W") %>%
  set_restrictions("W[Case = 1] < W[Case = 0]") %>%
  set_parameters(node = "X", statement = "X[W=1]>X[W=0]", parameters = 1/2)%>%
  set_parameters(node = "Y", statement = complements("W", "X", "Y"), parameters = .17) %>%
  set_parameters(node = "Y", statement = decreasing("X", "Y"), parameters = 0) 

plot(model)


```



```{r appev2, echo = FALSE}
if(do_diagnosis){
appev2 <-
  query_model(model,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Case==0", "Case==1"),
            using = c("priors", "parameters"), expand_grid = TRUE) 
write_rds(appev2, "saved/10a_appev2.rds")
}

read_rds("saved/10a_appev2.rds")  %>% 
  kable(caption = "Priors and true values (parameters) for three estimand: the frequency of $W$, the effect of $X$ on $Y$, and the effect conditional on $W=1$")

```

Priors and estimands (parameters) are show in Table \@ref(tab:appev2).
We see that the incidence of $W$ as well as the ATE of $X$ on $Y$ is larger in case 1 than in case 0 (in parameters, though not in priors). However the effect of $X$ on $Y$ conditional on $W$ is the same in both places. 

We now update the model *using data on $X$ and $Y$ only from one case* (case 1) and data on *W* from both and check inferences on the other.


```{r, echo = FALSE}
if(do_diagnosis){

  data <- make_data(model, n = 10000, 
                  vars = list(c("Case", "W"), c("X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Case == 0"))

  transport <- update_model(model, data)

  write_rds(query_model(transport,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Case==0", "Case==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport.rds")
}

q <- read_rds("saved/10a_transport.rds")

kable(q, caption = "Extrapolation when two sites differ on $W$ and $W$ is observable in both sites")

```

We do well in recovering the (different) effects both in the location we study and the one in which we do not. In essence querying the model for the out of sample case requests a type of post stratification. We get the right answer, though as always this depends on  the model being correct.

Had we attempted to make the extrapolation without data on $W$ in country 1 we would get it wrong. In that case however we would also report greater posterior variance. The posterior variance here captures the fact that we know things could be different in country 1, but we don't know in what way they are different. Note that we get the CATE right since in the model this is assumed to be the same across cases.


```{r, echo = FALSE}
if(do_diagnosis){

  data2 <- make_data(model, n = 10000, 
                  vars = list(c("Case"), c("W", "X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Case == 0"))

  transport2 <- update_model(model, data2)

  write_rds(query_model(transport2,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Case==0", "Case==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport2.rds")
}

q2 <- read_rds("saved/10a_transport2.rds")

kable(q2, caption = "Extrapolation when two sites differ on $W$ and $W$ is not observable in target country.")

```


<!-- FLAG: ADD  A SITUATION WITH AN ARROW FROM Case to Y and WHO THAT WE DO NOT HAVE IDENTIFICATION -->
