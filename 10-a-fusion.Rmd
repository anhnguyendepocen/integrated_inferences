# Mixing models {#mm}

***

We provide three examples of situations in which by combining models, researchers learn more than they could from any single model.

***


```{r packagesused15, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
library(DeclareDesign)
```


## A jigsaw puzzle: Learning from multiple models

Consider a situation in which we believe the same model holds in multiple sites but in which learning about the model requires combining data about different parts of the model from multiple studies. The graph is not too interesting in this example, what is more interesting is the data structure

```{r jigsaw, eval = TRUE, echo = FALSE}
model <- make_model("X -> Y <- Z -> K") %>%

          set_parameters(
            statement = list("(Y[X=1, Z = 1] > Y[X=0, Z = 1])",  
                             "(K[Z = 1] > K[Z = 0])"),
            node = c("Y","K"), 
            parameters = c(.24,.85))

plot(model)

```

We imagine we have access to three types of data;

1. Study 1 is a factorial study examining the joint effects of $X$ and $Z$ on $Y$, $K$ is not observed
2. Study 2 is an RCT looking at the relation between $Z$ and $K$. $X$ and $Y$ are not observed. 
3. Study 3 is an RCT looking at the effects of $X$ on $Y$, ancillary data on context, $K$ is collected, but $Z$ is not observed

```{r, echo = FALSE}

df <- make_data(model, 300, using = "parameters") %>%
  
      mutate(study = rep(1:3, each = 100),
             K = ifelse(study == 1, NA, K),
             X = ifelse(study == 2, NA, X),
             Y = ifelse(study == 2, NA, Y),
             Z = ifelse(study == 3, NA, Z)
             )

```

```{r, echo = FALSE}
if(do_diagnosis){

updated1 <- update_model(model, filter(df, study == 1))
updated2 <- update_model(model, filter(df, study == 2))
updated3 <- update_model(model, filter(df, study == 3))
updated_all <- update_model(model, df)

subs <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0")
subs2 <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0",
              "X == 1 & Y == 1 & K == 1 & Z == 1",
              "X == 1 & Y == 1 & K == 0 & Z == 1",
              "X == 1 & Y == 1 & K == 1 & Z == 0",
              "X == 1 & Y == 1 & K == 0 & Z == 0")

# If updating done using case data only
result1 <- query_model(updated1, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result2 <- query_model(updated2, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result3 <- query_model(updated3, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result4 <- query_model(updated_all, queries = "Y[X=0] == 0", given = subs2, using = "posteriors")

write_rds(list(result1, result2, result3, result4), "saved/10a_frankenstein.rds")
}

```



For each study we can ask: what is the probability that $X$ caused $Y$ in $X=Y=1$ cases conditional on $K$.  Table \@ref(tab:frank1) gives the answers. In each case the conclusion does not depend on $K$---though part of the causal model, and available in data in 2 cases, the individual studies don't have enough  information about the full model to be able to make use of $K$. In study 1 data on $K$ is not available, in study 2 it is available but researchers do not know, quantitatively, how it relates to  $Z$. In the third study the $Z,K$ relationship is well understood but the joint relation between  $Z,X$, and $Y$ is not understood.
 

```{r frank1, echo = FALSE}

frank <- read_rds("saved/10a_frankenstein.rds")

kable(
  cbind(Study = c(1, NA, 2, NA, 3, NA),
        rbind(
    frank[[1]][,-c(1,3)],
    frank[[2]][,-c(1,3)], 
    frank[[3]][,-c(1,3)])), 
  caption = "The clue $K$ uninformative in all three studies")
```


Table \@ref(tab:frank4) shows the inferences when the data are combined with joint updating across all parameters. In this case fuller understanding of the model lets researchers use information on $K$ to update on values for $Z$ and in turn update on the likely effects of $X$ on $Y$. Rows 3-4 highlight that the updating works through inferences on $Z$ and there if $Z$ is known (whether it is equal to 0 or 1), as in Study 2, there are no additional gains from knowledge of $K$. 


```{r frank4, echo = FALSE}
kable(frank[[4]][1:4,-c(1,3)], caption = "Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$", digits = 2)
```


Thus the collection of studies collectively allow for inferences that are not possible from any one study. 

In Chapter 15 we will discuss justifications for model based inference, but we note already that in this example we have an instance in which a researcher (examining a case in study 3) might wish to draw inferences using $K$, but she does not have anything in study 1 that justifies using $K$ for inference. However with access to studies 2 and 3, and conditional on the overall model, she has a justification for process tracing strategy. The general principle is that weaker commitments to lower level theories ---here the causal structure---can justify more fully inferences from more fully specifie higher level theories.  


## Combining observational and experimental data

Experimental studies are sometimes referred to as gold standard. But an interesting weakness of experimental studies is that, by dealing so effectively with self selection into treatment, they limit our ability to learn about self selection. Often however we want to know what causal effects would be specifically for people that would take up a treatment in non experimental settings. This kind of problem is studied for example by @knox2019design.

A causal model can encompass both experimental and observational data and let you answer this kind of question. To illustrate, imagine that node $R$ indicates whether a unit was assigned to be randomly assigned to treatment ($X=Z$ if $R=1$) or took on its observational value ($X=O$ if $R=0$). We assume the exclusion restriction that entering the experimental sample is not related to $Y$ other than through assignment of $X$. We plot the model in Figure \@ref(fig:appcombexpob).


```{r copobsetup, message = FALSE, warning = FALSE, echo = FALSE}
model <- make_model("R -> X -> Y; O -> X <- Z; O <-> Y") %>%
  
	set_restrictions("(X[R=1, Z=0]!=0) | (X[R=1, Z=1]!=1) | (X[R=0, O=0]!=0) | (X[R=0, O=1]!=1)")

```

```{r appcombexpob, message = FALSE, warning = FALSE, echo = FALSE, fig.cap="A model that nests an observational and an experimental study. The treatment $X$ either takes on the observational value $O$, or the assigned values $Z$, depending on whether or not the case has been randomized, $R$."}

plot(model)
```

```{r, echo = FALSE, include = FALSE}
P <- get_parameter_matrix(model)
kable(P[,1:4])
```


In this model, $X$ has only one causal type since its job is to operate as a kind of switch, inheriting the value of $Z$ or $O$ depending on $R$. Parameters allow for complete confounding between $O$ and $Y$ but $Z$ and $Y$ are unconfounded.

We imagine parameter values in which there is a true .2 effect of $X$ on $Y$. However the effect is positive (.6) for cases in which $X=1$ under observational assignment but negative (-.2) for cases in which $X=0$ under observational assignment. (See appendix for complete specification.)

```{r, echo = FALSE}

model <- model %>%
	set_parameters(node = "Y", confound = "O==0", parameters = c(.8, .2,  0,  0)) %>%
	set_parameters(node = "Y", confound = "O==1", parameters = c( 0,  0, .6, .4))

```

The implied estimands and priors are as in Table \@ref(tab:fusionestimands).

```{r fusionestimands, echo = FALSE}

if(do_diagnosis){
result <- query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = c("parameters", "priors"), 
    expand_grid = TRUE)
write_rds(result, "saved/10a_fusionestimands.rds")
}

read_rds("saved/10a_fusionestimands.rds") %>% kable(caption = "Estimands in different sites", digits = 2)

```


```{r, echo = FALSE}
data <- make_data(model, n = 800)
```


The true effect is .2 but naive analysis on the observational data would yield a strongly upwardly biased estimate. Table \@ref(tab:fusiondim) shows differences-in-means estimates using data on observational units only drawn from this model.


```{r fusiondim, echo  = FALSE, warning = FALSE, message = FALSE}
x <- estimatr::difference_in_means(Y~X, data = filter(data, R==0))
kable(summary(x)[[1]], digits = 3, caption = "Inferences on the ATE from differences in means")
```

The estimates from updating on the full model, shown in Table \@ref(tab:fusionCQ), are much better.

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, data), "saved/10a_exp_obs.rds")
  }
updated <- read_rds("saved/10a_exp_obs.rds")
```

```{r fusionCQ, echo = FALSE}
result <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result, caption = "Estimates on the ATE for observational ($R=0$) and experimental ($R=1$) set.")
```




```{r, eval = FALSE, echo = FALSE}
updated_no_O <- update_model(model, dplyr::filter(data, R==1))
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, dplyr::filter(data, R==1)), "saved/10a_exp_obs_2.rds")
  }
updated_no_O <- read_rds("saved/10a_exp_obs_2.rds")
```

```{r appcombexpopp8, echo = FALSE, include = FALSE}
result <- query_model(
    updated_no_O, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result)
```

Since the model used both the experimental and the observational data, it is interesting to ask whether the observational data improved the estimates of the treatment effect or does inference draw only from  the experimental data? We answer the question in the appendix by  updating using experimental data only.
 We find there that we do indeed get a tightening of posterior variance and a more accurate result when we use the observational data but that the gains are relatively small: the experimental data alone is quite powerful. The gains would be smaller still if we had more data, in which case inferences from the experimental data would be more accurate still. 

However we do learn something of particular interest from this model. A key feature here is that there is heterogeneity between those that are in treatment and those that are in control *in the observational* sample. We learn nothing about this heterogeneity from the experimental data alone but we learn a lot from the mixed model, picking up the strong self selection into treatment in the observational group: 

```{r, echo = FALSE}
result2 <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list("R==1 & X==0", "R==1 & X==1", "R==0 & X==0", "R==0 & X==1"),
    using = "posteriors")

kable(result2, caption = "Effects of $X$ conditional on $X$ for units that were randomly assigned or not.  Effects of $X$ do not depend on $X$ in the experimental group, but they do in the observational group becuase of seld selection. ")
```

In essence by mixing the experimental and observational data we can learn what the effects for those that self select into treatment *and* what they would be for those that self select into control. The results here relate to the  LATE theorem [@angrist1995identification] in the following way. If we imagine using data only on (a) the experimental group in control and (b) the observational group, some of whom are in treatment, we can conceptualize our design as one in which the observational group are "encouraged" to take up treatment and we figure out the effect for the compliers in this group (those that self select into treatment). At the same time if we imagine using data only on (a) the experimental group in treatment and (b) the observational group, some of whom are in control, we can conceptualize our design as one in which the observational group are "encouraged" to take teh control condition and we figure out the effect for the compliers in this group (those that self select into control). 


## Transportation of findings across contexts

Say we study the effect of $X$ on $Y$ in case 0 (a country, for instance) and want to make inferences to case 1 (another country). Our problem however is that effects are heterogeneous and features  that differ across units may be related both to treatment assignment, outcomes, and selection into the sample. This is the problem studied by @pearl2014external. In particular  @pearl2014external show for which nodes data is needed in order to  "licence" external claims, given a model. 

We illustrate with a simple model in which a confounder has a different distribution in a study site and a target site.

```{r extval, echo = FALSE, fig.cap= "Extrapolation when confounders have different distributions across cases."}

model <- make_model("Case -> W  -> X -> Y <- W") %>%
  set_restrictions("W[Case = 1] < W[Case = 0]") %>%
  set_parameters(node = "X", statement = "X[W=1]>X[W=0]", parameters = 1/2)%>%
  set_parameters(node = "Y", statement = complements("W", "X", "Y"), parameters = .17) %>%
  set_parameters(node = "Y", statement = decreasing("X", "Y"), parameters = 0) 

plot(model)


```



```{r appev2, echo = FALSE}
if(do_diagnosis){
appev2 <-
  query_model(model,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Case==0", "Case==1"),
            using = c("priors", "parameters"), expand_grid = TRUE) 
write_rds(appev2, "saved/10a_appev2.rds")
}

read_rds("saved/10a_appev2.rds")  %>% 
  kable(caption = "Priors and true values (parameters) for three estimand: the frequency of $W$, the effect of $X$ on $Y$, and the effect conditional on $W=1$")

```

Priors and estimands (parameters) are show in Table \@ref(tab:appev2).
We see that the incidence of $W$ as well as the ATE of $X$ on $Y$ is larger in case 1 than in case 0 (in parameters, though not in priors). However the effect of $X$ on $Y$ conditional on $W$ is the same in both places. 

We now update the model *using data on $X$ and $Y$ only from one case* (case 1) and data on *W* from both and check inferences on the other.


```{r, echo = FALSE}
if(do_diagnosis){

  data <- make_data(model, n = 10000, 
                  vars = list(c("Case", "W"), c("X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Case == 0"))

  transport <- update_model(model, data)

  write_rds(query_model(transport,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Case==0", "Case==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport.rds")
}

q <- read_rds("saved/10a_transport.rds")

kable(q, caption = "Extrapolation when two sites differ on $W$ and $W$ is observable in both sites")

```

We do well in recovering the (different) effects both in the location we study and the one in which we do not. In essence querying the model for the out of sample case requests a type of post stratification. We get the right answer, though as always this depends on  the model being correct.

Had we attempted to make the extrapolation without data on $W$ in country 1 we would get it wrong. In that case however we would also report greater posterior variance. The posterior variance here captures the fact that we know things could be different in country 1, but we don't know in what way they are different. Note that we get the CATE right since in the model this is assumed to be the same across cases.


```{r, echo = FALSE}
if(do_diagnosis){

  data2 <- make_data(model, n = 10000, 
                  vars = list(c("Case"), c("W", "X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Case == 0"))

  transport2 <- update_model(model, data2)

  write_rds(query_model(transport2,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Case==0", "Case==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport2.rds")
}

q2 <- read_rds("saved/10a_transport2.rds")

kable(q2, caption = "Extrapolation when two sites differ on $W$ and $W$ is not observable in target country.")

```


<!-- FLAG: ADD  A SITUATION WITH AN ARROW FROM Case to Y and WHO THAT WE DO NOT HAVE IDENTIFICATION -->
