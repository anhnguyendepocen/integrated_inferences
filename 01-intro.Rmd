# Introduction  {#intro}

<!--
:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We describe the book's general approach, preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences, and provide a roadmap for the rest of the book.
::::
-->

<br>

```{r packagesused01, include = FALSE}
source("_packages_used.R")
```

Here is the key idea of this book. 

Quantitative social scientists spend a lot of time trying to understand causal relations between variables by looking across large numbers of cases to see how outcomes differ when potential causes differ. This strategy relies on variation in causal conditions across units of analysis, and the quality of the resulting inferences depends in large part on what forces give rise to that variation.  

Qualitative social scientists, like historians, spend a lot of time looking at a smaller set of cases and seek to learn about causal relations by examining evidence of causal processes in operation within these cases. Qualitative scholars rely on theories of how things work, theories that specify what should be observable within a case if indeed an outcome were generated by a particular cause.  

These two approaches seem to differ in what they seek to explain---individual-level or population-level outcomes; in the forms of evidence they require---cross-case variation or within-case detail; and in what they need to assume---knowledge of assignment processes or knowledge of causal processes. 

The central theme of this book is that this distinction, though culturally real (@goertz2012tale), is neither epistemologically deep nor analytically helpful. Social scientists can work with causal models that simultaneously exploit cross-case variation and within-case detail, that address both case-level and population-level questions, and that both depend on, and contribute to developing, theories of how things work.

We describe an approach to doing this in which researchers *form* causal models, *update* those models using data, and then *query* the models to get answers to particular causal questions. This framework is very different from standard statistical approaches in which researchers focus on selecting the best estimator to estimate a particular estimand of interest. In a causal models framework, the model itself gets updated, not the estimate: we begin by learning about processes, and only then draw inferences about particular causal relations of interest, either at the case level or at the population level.

We do not claim that a causal-model-based approach is the best or only strategy suited to addressing causal questions. There are plenty of settings in which other approaches would likely work better. But we do think that the approach holds considerable promise --- allowing researchers to combine disparate data in a principled way to ask a vast range of causal questions, helping integrate theory and empirics in a compelling way, and providing coherent guidance on research design --- and that it should have a place in the applied researcher's toolkit. 

Our goals in this book are to motivate this approach; provide an introduction to the theory of structural causal models; provide practical guidance for setting up, updating, and querying causal models; and show how the approach can inform key research-design choices, especially case-selection and data-collection strategies.

## The Case for Causal Models

There are three closely related motivations for embracing a causal models approach.
One is is a concern over the limits of design-based inference. A second is an interest in integrating qualitative knowledge with quantitative approaches.  A third is an interest in better connecting empirical strategies to theory. 


### The limits to design-based inference 

To caricature positions a bit, consider the difference between an engineer and a skeptic. The engineer tackles problems of causal inference using models: theories of how the world works, generated from past experiences and applied to the situation at hand. They come with prior beliefs about a set of mechanisms operating in the world and, in a given situation, will ask whether the conditions are in place for a known mechanism to operate effectively. The skeptic, on the other hand, maintains a critical position, resisting the importation of beliefs that are not supported by evidence in the case at hand. 

The engineer's approach echoes what was until recently a dominant orientation among social scientists. At the turn of the current century, much analysis---both empirical and theoretical---took the form of modelling processes ("data generating processes") and then interrogating those models.  

Over the last two decades, however, skeptics have raised a set of compelling concerns about the assumption-laden nature of standard regression analysis, while also clarifying how valid inferences can be made with limited resort to models in certain research situations. The result has been a growth in the use of design-based inference techniques that, in principle, allow for model-free estimation of causal effects (see @dunning2012natural, @GerGreKap04, @druckman2011experimentation, @palfrey2009laboratory among others). These include lab, survey, and field experiments and natural-experimental methods exploiting either true or "as-if" randomization by nature. With the turn to experimental and natural-experimental methods has come a broader conceptual shift, with a growing reliance on the "potential outcomes" framework which provide a clear language for thinking about causation (see @Rubin1974, @splawa1990application among others) without having to invoke fully specfied models of data-generating processes.

The ability to estimate average effects and to characterize uncertainty---for instance  calculating $p$-values and standard errors---without resort to models is an extraordinary development. In @fisher1935design's terms, with these tools, randomization processes provide a "reasoned basis for inference," placing empirical claims on a powerful footing.

Excitement about the strengths of these approaches has been mixed with various concerns regarding how the approach shapes inquiry. We highlight two. 

The first concern---raised by many in recent years (e.g., @thelen2015comparative)---is about design-based inference's scope of application. While experimentation and natural experiments represent powerful tools, the range of research situations in which model-free inference is possible is inevitably limited. For a wide range of causal conditions of interest both to social scientists and to society, controlled experimentation is impossible, and true or "as-if" randomization is absent. Moreover, limiting our focus to those questions for which, or situations in which, exogeneity can be established "by design" would represent a dramatic narrowing of social science's ken.  To be clear, this is not an argument against experimentation or design-based inference when these can be used; rather it is an argument for why social science needs a broader set of tools.

The second concern is more subtle. The great advantage of design-based inference is that it liberates researchers from the need to rely on models to make claims about causal effects. The risk is that, in operating model-free, researchers end up learning about effect sizes but not about models. Yet often the model is the thing we  want to learn about. Our goal as social scientists is to come to grips with how the world works, not simply to collect propositions about the effects that different causes have had on different outcomes in different times and places. It is through models that we derive an understanding of how things might work in contexts and for processes and variables that we have not yet studied. Thus, our interest in models is intrinsic, not instrumental. By taking models out of the equation, as it were, we limit the potential for learning about the world. 

### Qualitative and mixed-method inference

Recent years have seen the elucidation of the inferential logic behind "process tracing" procedures used in qualitative political science and other disciplines. On our read of this literature, the logic of process tracing in these accounts depends on a particular form of model-based inference.^[As we describe in @humphreys2015mixing, the term "qualitative research" means many different things to different scholars, and there are multiple approaches to mixing qualitative and quantitative methods. There we distinguish between approaches that suggest that qualitative and quantitative approaches address distinct, if complementary, questions; those that suggest that they involve distinct measurement strategies; and those that suggest that they employ distinct inferential logics. The approach that we employ in @humphreys2015mixing connects most with the third family of approaches. Most closely related, in political science, is work in  @GlynnQuinn2011, in which researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. In the present book, however, we move toward a position in which fundamental differences between qualitative and quantitative inference tend to dissolve, with all inference drawing on what might be considered a "qualitative" logic in which the researcher's task is to confront a pattern of evidence with a theoretical logic.] While process tracing as a method has been around for more than three decades (e.g., @george1985case), its logic has been most fully laid out by qualitative methodologists in political science and sociology over the last 15 years (e.g., @bennett2014process, @george2005case, @brady2010rethinking, @Hall2003aligning, @mahoney2010after). Whereas @king1994designing sought to derive qualitative principles of causal inference within a correlational framework, qualitative methodologists writing in the wake of "KKV" have emphasized and clarified process-tracing's "within-case" inferential logic: in process tracing, explanatory hypotheses are tested based on observations of what happened within a case, rather than on observation of covariation of causes and effects across cases. 

The process-tracing literature has also advanced increasingly elaborate conceptualizations of the different kinds of probative value that within-case evidence can yield. For instance, qualitative methodologists have explicated the logic of different test types ("hoop tests", "smoking gun tests", etc.) involving varying degrees of specificity and sensitivity (@collier2011understanding, @Mahony:Logic:2012).^[A smoking-gun test is a test that seeks information that is only plausibly present if a hypothesis is true (thus, generating strong evidence for the hypothesis if passed); a hoop test seeks data that should certainly be present if a proposition is true (thus generating strong evidence against the hypothesis if failed); and a doubly decisive test is both smoking-gun and hoop (for an expanded typology, see also @rohlfing2013comparative).] Other scholars have expressed the leverage provided by process-tracing evidence in Bayesian terms, moving from a set of discrete test types to a more continuous notion of probative value (@fairfield2017explicit, @BennettAppendix, @humphreys2015mixing).^[In @humphreys2015mixing, we use a fully Bayesian structure to generalize Van Evera's four test types in two ways: first, by allowing the probative values of clues to be continuous; and, second, by allowing for researcher uncertainty (and, in turn, updating) over these values. In the Bayesian formulation, use of process-tracing information is not formally used to conduct tests that are either "passed" or "failed", but rather to update beliefs about different propositions.]

<!-- ^[Note that these statements are statements about likelihood functions and do not require a specifically Bayesian mode of inference.] -->

Yet, conceptualizing the different ways in which probative value might operate leaves a fundamental question unanswered: what gives within-case evidence its probative value with respect to causal relations? We do not see a clear answer to this question in the current process-tracing literature. Implicitly---but worth rendering explicit---*the probative value of process-tracing evidence depends on researcher beliefs that come from outside of the analysis in question.* We enter a research situation with a model of how the world works, and we use this model to make inferences given observed patterns in the data --- while at the same time updating those models based on the data. 

A key aim of this book is to demonstrate the role that models can --- and, in our view, must --- play in drawing case-level causal inferences and to clarify conditions under which these models can be defended. To do so we draw on an approach to specifying causal models developed originally in computer science and that predates this work in qualitative methodology. The broad approach, described in @cowell1999probabilistic and @pearl2009causality, is consistent with the potential outcomes framework, and provides rules for updating on population and case level causal queries from different types of data.

In addition to clarifying the logic of qualitative inference, we will argue that such causal models can also enable the systematic integration of qualitative and quantitative forms of evidence. Social scientists are increasingly developing mixed-method research designs, research strategies that combine quantitative with qualitative forms of evidence. A typical mixed-methods study includes the estimation of causal effects using data from many cases as well as a detailed examination of the processes taking place in a few cases. Now-classic examples of this approach include Lieberman's study of racial and regional dynamics in tax policy (@lieberman2003race); Swank's analysis of globalization and the welfare state (@swank2002global); and Stokes' study of neoliberal reform in Latin America (@stokes2001mandates). Major recent methodological texts provide intellectual justification of this trend toward mixing, characterizing  small-$n$  and large-$n$ analysis as drawing on a single logic of inference and/or as serving complementary functions (@king1994designing; @collier2004sources). The American Political Science Association now has an organized section devoted in part to the promotion of multi-method investigations, and the emphasis on multiple strategies of inference research is now embedded in guidelines from many research funding agencies [@CRESWELL2008]. 

However, while scholars frequently point to the benefits of mixing correlational and process-based inquiry (e.g., @collier2010sources, p.~181), and have sometimes mapped out broad strategies of multi-method research design (@Lieberman2005nested, @SeawrightGerring2008), they have rarely provided specific guidance on how the integration of inferential leverage should unfold. In particular, the literature has not supplied specific principles for aggregating findings---whether mutually reinforcing or contradictory---across different modes of analysis.^[A small number of exceptions stand out. In the approach suggested by @gordon2004quantitative, for instance, available expert (possibly imperfect) knowledge regarding the operative causal mechanisms for a small number of cases can be used to anchor the statistical estimation procedure in a large-N study. @WesternJackman1994 propose a Bayesian approach in which qualitative information shapes subjective priors which in turn affect inferences from quantitative data. Relatedly,  in @GlynnQuinn2011, researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. @seawrightbook presents an informal framework in which case studies are used to test the assumptions underlying statistical inferences, such as the assumption of no-confounding or the stable-unit treatment value assumption (SUTVA).] 
<!-- What we still lack, however, is a comprehensive framework that allows us to enter qualitative and quantitative forms of information into an integrated analysis for the purposes of answering the wide range of causal questions that are of interests to social scientists, including questions about case-level explanations and causal effects, average causal effects, and causal pathways.  -->
As we aim to demonstrate in this book, however, grounding inference in causal models provides a very natural way of combining information of the $X,Y$ variety with information about the causal processes connecting $X$ and $Y$. The approach that we develop here can be readily addressed both to the  case-oriented questions that tend to be of interest to qualitative scholars and to the population-oriented questions that tend to motivate quantitative inquiry. 

As will become clear, when we structure our inquiry in terms of causal models, the conceptual distinction between qualitative and quantitative inference becomes hard to sustain. Notably, this is not because all causal inference depends fundamentally on covariation but because in a causal-model-based inference, what matters for the informativeness of a piece of evidence is how that evidence alters beliefs about a model, and in turn, a query. While the apparatus that we present is formal, the approach---in asking how pieces of evidence drawn from different parts of a process map on to a base of theoretical knowledge---is arguably most closely connected to process tracing in its core logic.


### Connecting theory and empirics

The relationship between theory and empirics has been a surprisingly uncomfortable one in political science. In a recent intervention, for instance, @clarke2012model draw attention to and critique political scientists'  widespread reliance on the "hypothetico-deductive" (H-D) framework, in which a theory or model is elaborated, empirical predictions derived, and data sought to test these predictions and the model from which they derive. Clarke and Primo draw on decades of scholarship in the philosophy of science pointing to deep problems with the H-D framework, including with the idea that the truth of a model logically derived from first principles can be *tested* against evidence. 

In fact the relationship between theory and evidence in social inquiry is often surprisingly unclear both in qualitative and quantitative work. We can perhaps illustrate it best, however, by reference to qualitative work, where the centrality of theory to inference has been most emphasized. In process tracing, theory is what justifies inferences. In their classic text on case study approaches, @george2005case describe process tracing as the search for evidence of "the causal process that a theory hypothesizes or implies" (6). Similarly, @Hall2003aligning conceptualizes the approach as testing for the causal-process-related observable implications of a theory; @mahoney2010after indicates that the events for which process tracers go looking are those posited by theory (128); and @gerring2006case describes theory as a source of predictions that the case-study analyst tests (116). Theory, in these accounts, is supposed to help us figure out where to look for discriminating evidence. 

What is not clear, however, is a how researchers can derive within-case empirical predictions from theory and how exactly doing so provides leverage on a causal question. From what elements of a theory can scholars derive informative within-case observations? <!-- Given a set of possible things to be observed in a case, how can theory help us distinguish more from less informative observations?  -->
Of the many possible observations suggested by a theory, how can we determine which would add probative value to the evidence already at hand? How do the evidentiary requisites for drawing a causal inference, given a theory, depend on the particular causal question of interest---on whether, for instance, we are interested in identifying the cause of an outcome in a case, estimating an average causal effect, or identifying the pathway through which an effect is generated? Perhaps most confusingly, if the theory tells us what to look for to draw an inference, can the inferences be about the theory itself or are we constrained to make theory dependent inferences? 
In short, how exactly can we ground causal inferences from within-case evidence in background knowledge about how the world works?

Much quantitative work in political science features a similarly weak integration between theory and research design. The modal inferential approach in quantitative work, both observational and experimental, involves looking for correlations between causes and outcomes, with less regard for intervening or surrounding causal relationships.^[There are of course many exceptions, including work that uses structural equation modeling, and research that focuses specifically on understanding heterogeneity and mediation processes.] If a theory suggests a *set* of relations, it is common to examine these separately---does $A$ cause $B$ does $B$ cause $C$? are relations stronger or weaker here or there?---without standard procedures for bringing the disparate pieces of evidence together to form theoretical conclusions. More attention has been paid to empirical implications of theoretical models than to theoretical implications of empirical models.  

In this book, we seek to show how scholars can simultaneously make fuller and more explicit use of theoretical knowledge in designing their research projects and analyzing data and make use of data to update on theoretical models. Like Clarke and Primo, we treat models not as veridical accounts of the world but as maps: maps, based on prior theoretical knowledge, about causal relations in a domain of interest. Also, as in Clarke and Primo's approach, we do not write down a model in order to test its veracity (though, in later chapters, we do discuss ways of justifying and evaluating models). Rather, our focus is on how we can systematically *use* causal models --- in the sense of *mobilizing background knowledge of the world* --- to guide our empirical strategies and inform our inferences. Grounding our empirical strategy in a model allows us, in turn, to learn about features of the model itself as we encounter the data.


## Key contributions

This book draws on methods developed in the study of Bayesian networks, a field pioneered by scholars in computer science, statistics, and philosophy (see especially @pearl2009causality). Bayesian networks, a form of causal model, have had limited traction to date in political science. Yet the literature on Bayesian networks and their graphical counterparts, directed acyclic graphs (DAGs), is a body of work that addresses very directly the kinds of problems with which qualitative and quantitative scholars routinely grapple.^[For application to quantitative analysis strategies in  political science, @glynn2007non give a clear introduction to how these methods  can be used to motivate strategies for conditioning and adjusting for causal inference. @garcia2015graphical demonstrate how these methods can be used to assess claims of external validity. With a focus on qualitative methods, @Waldner2015completeness uses causal diagrams to lay out a "completeness standard" for good process tracing. @weller2014finding employ graphs to conceptualize the different possible pathways between causal and outcome variables among which qualitative researchers may want to distinguish. Generally, in discussions of qualitative methodology, graphs are used to capture core features of theoretical accounts, but  are not developed specifically to ensure a representation of the kind of independence relations implied by structural causal models (notably what is called in the literature the "Markov condition"). Moreover, efforts to tie these causal graphs to probative observations, as in @Waldner2015completeness, are generally limited to identifying steps in a causal chain that the researcher should seek to observe.] 

Drawing on this work, we show in the chapters that follow how a theory can be formalized as a causal model represented by a causal graph and a set of structural equations. Engaging in this modest degree of formalization yields enormous benefits. It allows us, for a wide range of causal questions, to specify causal questions clearly and assess what inferences to make about queries from new data.

For students engaging in process tracing, the benefits of this approach are multiple. In particular, the framework that we develop in this book provides:

- A grounding for assessing the "probative value" of evidence drawn from different parts of any causal network. The approach yields a principled and transparent approach to answering the question: how should the observation of a given piece of data affect my causal beliefs about a case?

- A transparent, replicable method of aggregating inferences from observations drawn from different locations in a causal network.  Having collected multiple pieces of evidence from different parts of a causal process or case context, what should I end up believing about the causal question of interest?

- A common approach for assessing a wide variety of queries (estimands). We can use the same apparatus to learn *simultaneously* about different case-level causal questions, such as "What caused the outcome in this case?" and "Through what pathway did this cause exert its effect?"

<!-- - Consistency of probative value, priors, and therefore inferences with how you think the world works.  -->

<!-- - Transparency, allowing for evaluation -->

- Guidance for research design. Given finite resources, researchers must make choices about where to look for evidence. A causal model framework can help researchers assess, a priori, the relative expected informativenss of different evidentiary and case-selection strategies, conditional on how they think the world works and the question they want to answer.


The approach also offers a range of distinctive benefits to researchers seeking to engage in mixed-method inference and to learn about general causal relations, as well as about individual cases. The framework's central payoff for multi-method research is the systematic integration of qualitative and quantitative information to answer any given causal query. We note that the form of integration that we pursue here differs from that offered in other accounts of multi-method research. In @seawrightbook's approach, for instance, one form of data --- quantitative *or* qualitative --- is always used to draw causal inferences, while the other form of data is used to test assumptions or improve measures employed in that primary inferential strategy. In the approach that we develop in this book, in contrast, we are always using *all* information available to update on causal quantities of interest. In fact, within the causal models framework, there is no fundamental difference between quantitative and qualitative data, as both enter as values of nodes in a causal graph. This formalization --- this reductive move --- may well discomfit some readers. And we acknowledge that our approach undeniably involves a loss of some of what makes qualitative research distinct and valuable. Yet, this translation of qualitative and quantitative observations into a common, causal model framework offers major advantages. Beyond the integration of different forms of information, these advantages include:

- Transparency. The framework makes manifest precisely how each form of evidence enters into the analysis and shapes conclusions.

<!-- AJ: I don't understand this one. - A way to justify the background assumptions you've used -->

- Learning across levels of analysis. In a causal model approach, we use case-level information to learn about populations and general theory. At the same time, we use what we have learned about populations to sharpen our inferences about causal relations within individual cases.

- Cumulation of knowledge. A causal model framework provides a straightforward, principled mechanism for building on what we have already learned. As we see data, we update our model; and then our updated model can inform the inferences we draw from the next set of observations. Models can, likewise, provide an explicit framework for positing and learning about the generalizability and portability of findings across research contexts.

- Guidance for research design. With a causal model in hand, we can formally assess key multi-method design choices, including the balance we should strike between breadth (the number of cases) and depth (intensiveness of analysis in individual cases) and the choice of cases for intensive analysis. 


<!-- * make systematic use of theory to figure out what kinds of evidence have probative value for our causal queries and, in turn, to design maximally informative empirical strategies; -->

<!-- * draw inferences from the evidence in a manner disciplined by our theoretical knowledge of how the world works; -->

<!-- * integrate in this analysis information conventionally considered "qualitative" (e.g., evidence about causal mechanisms) with information conventionally considered "quantitative" (e.g., evidence about causal and outcome variables); and -->

<!-- * learn about the models that we bring to the table, which allows for the cumulation of knowledge through the updating of the theoretical knowledge base on which future inferences can draw. -->

Using causal models also has substantial implications for common methodological intuitions, advice, and practice. To touch on just a few of these implications: 

  * Our elaboration and application of model-based process tracing shows that, given plausible causal models, process tracing's common focus on intervening causal chains may be much less productive than other empirical strategies, such as examining moderating conditions. 

  * Our examination of model-based case-selection indicates that for many common purposes there is nothing particularly especially about "on the regression line" cases or those in which the outcome occurred, and there is nothing necessarily damning about selecting on the dependent variables. Rather optimal case selection depends on factors that have to date received little attention, such as the population distribution of cases and the probative value of the available evidence. 
  
  * Our analysis of clue-selection as a decision problem shows that the probative value of a piece evidence cannot be assessed in isolation, but hinges critically on what we have already observed.  

<!-- **MORE TO COME HERE ELABORATING IMPLICATIONS, COMPARING TO ADVICE AND PRACTICE IN LITERATURE** -->

The basic analytical apparatus that we employ here is not new. Rather, we see the book's goals as being of three kinds. First, we aim to import insights: to introduce political scientists to an approach that has received little attention in the discipline but that can be useful for addressing the sorts of causal questions with which political scientists are commonly preoccupied. As a model-based approach, it is a framework especially well suited to a field of inquiry in which exogeneity frequently cannot be assumed by design---that is, in which we often have no choice but to be engineers. 

Second, we draw connections between the Bayesian networks approach and key concerns and challenges with which students in social sciences routinely grapple. Working with causal models and DAGs most naturally connects to concerns about confounding and identification that have been central to much quantitative methodological development. Yet we also show how causal models can address issues central to process tracing, such as how to select cases for examination, how to think about the probative value of causal process observations, and how to structure our search for evidence, given finite resources. 

Third, we provide a set of usable tools for implementing the approach. We provide intuition and software, the `CausalQueries` package, that researchers can use to make research design choices and draw inferences from the data.


## The Road Ahead

The book is divided into four main parts. 

In the first part of the book, we set out the basics. In Chapter \@ref(models), following a review of the common potential-outcomes approach to causality, we introduce the concept and key components of a causal model. Chapter \@ref(illustratemodels) illustrates how we can represent of causal beliefs in the form causal models by translating the arguments of a several prominent works of political science into causal models.  In Chapter \@ref(questions), we set out a range of causal questions that researchers might want to address --- including questions about case-level causal effects, population-level effects, and mechanisms --- and define these queries within a causal model framework. Chapter \@ref(bayeschapter) offers a primer on the key ideas in Bayesian inference that we will mobilize in later sections of the book. In Chapter \@ref(theory), we map between causal models and theories, showing how we can think of any causal model as situated within a hierarchy of complexity: within this hierarchy, any causal model can be justified by references to a "lower level", more detailed model that offers a theory of why things work the way do that the higher level. This conceptualization is crucial insofar as we use more detailed (lower-level) models to generate empirical leverage on relationships represented in simpler, higher-level models. 


<!-- art off by  describing the kinds of causal estimands of interest. The main goal here is to introduce the key ideas in the study of Bayesian nets and to argue for a focus of interest away from  average treatment effects as go-to estimands of interest and towards a focus on causal structures as the key quantity of interest. The next chapter introduces key Bayesian ideas; what Bayes' rule is and how to use it. The third chapter connects the study of Bayesian networks to theoretical claims. The key argument here is that causal models should be thought of as theories which are themselves supportable by lower-level models (theories).  -->

The second part of the book shows how we can use causal models to undertake process-tracing and mixed method inference. Chapter \@ref(pt) lays out the logic of case-level inference from causal models: the central idea here is that what we learn from evidence is always conditional on the prior beliefs embedded in our model. In Chapter \@ref(ptapp), we illustrate model-based process-tracing with an application to the substantive issue of economic inequality's effects on democratization. Chapter \@ref(mixing) moves to mixed data problems: situations in which a researcher wants to use "quantitative" (broadly, $X,Y$) data on a large set of cases and more detailed ("qualitative") data on some subset of these cases. We show how we can use any arbitrary mix of observations across a sample of any size (greater than 1) to update on all causal parameters in a model, and then use the updated model to address the full range of general and case-level queries of interest. In Chapter \@ref(mixingapp), we illustrate this integrative approach by revisiting the problem of inequality and democracy introduced in Chapter \@ref(ptapp). Finally, in Chapter \@ref(mm), we take the project of integration a step further by showing how we can use models to integrate findings across *studies* and across *settings*. We show, for instance, how we can learn jointly from the data generated by an observational study and an experimental study of the same causal domain and how models can help us reason in principled ways about the transportability of findings across contexts. 

The third part of the book unpacks what causal models can contribute to research design. Across Chapters \@ref(elements), \@ref(clue), and \@ref(caseselection), we demonstrate how researchers can mobilize their models, as well as prior observations, to determine what kind of new evidence is likely to be most informative about the query of interest, how to strike the balance between extensiveness and intensiveness of analysis, and which cases to select for in-depth process tracing. 

<!-- In this framework the problem of case selection is equivalent to the kind of problem of clue selection discussed in Chapter 6. For a canonical multicase model however we use simulation approaches to provide guidance for how cases should be selected. The broad conclusion here is that researchers should go where the probative value lies, and all else equal, should select cases approximately proportional to the size of $XY$ strata---whether or not these are "on the regression line."  -->

The fourth and final part of the book steps back to put the model-based approach into question. Until this point, we have been advocating an embrace of models to aid inference. But the dangers of doing this are demonstrably large. The key problem is that with model-based inference, the inferences are only as good as the model. In the end, while we advocate a focus on models, we know that skeptics are right to distrust them. This final part approaches this problem from two perspectives. In Chapter \@ref(justifying), we demonstrate the *possibility* of justifying models from external evidence, though we do not pretend that the conditions for doing so will arise commonly. In Chapter \@ref(evaluation), drawing on common practice in Bayesian statistics, we present a set of strategies that researchers can use to evaluate and compare the validity of models, and to investigate the degree to which findings hinge on model assumptions. 

In the concluding chapter we summarize what we see as the main advantages of a causal-model-based approach to inference, draw out a set of key concerns and limitations of the framework, and identify what we see as the key avenues for progress in model-based inference.

Here we go.

