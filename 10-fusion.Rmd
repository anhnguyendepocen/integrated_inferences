# Mixing models {#mm}


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We provide four examples of situations in which, by combining models, researchers learn more than they could from any single model.
::::
<br>




```{r packagesused15, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
library(DeclareDesign)
```

In the previous two chapters, we described one form of integration that structural causal models can enable: the systematic combination of (what we typically think of as) qualitative and quantitative evidence for the purposes of drawing population- and case-level causal inferences. One feature of the analyses we have been considering so far is that the integration is essentially "nested." We are, for instance, integrating quantitative evidence for a large set of cases with qualitative evidence for a *subset* of those cases. We are, moreover, drawing inferences from the set of cases we observe to a population *within which* that sample of cases is situated. 

In this chapter, we examine how we can use structural causal models to integrate across studies or settings that are, in a sense, more disjointed from one another: across studies that examine different causal relationships altogether; study designs that require different assumptions about exogeneity; and contexts across which the causal quantities of interest may vary.

1. **Integrating across a model** Often, individual studies in a substantive domain examine distinct segments of a broader web of causal relationships. For instance, while one study might examine the effect of $X$ on $Y$, another might examine the effect of $Z$ on $Y$, and yet another might examine the effect of $Z$ on $K$. We show in this chapter how we can integrate across such studies in ways that yield learning that we could not achieve by taking each study on its own terms. 

2. **Integrating between experimental and observational studies** One form of multi-method research that has become increasingly common is the use of both observational and experimental methods to study the same basic causal relationships. While an experiment can offer causal identification in a usually local or highly controlled setting, an observational analysis can often shed light on how the same relationships operate "in the wild," if with greater risk of confounding. Usually, observational and experimental results are presented in parallel, as separate sources of support for a causal claim. We show how, in a causal model setup, we can use experimental and observational data *jointly* to address questions that cannot be answered when the designs are considered separately.

3. **Transporting knowledge across contexts** Researchers are sometimes in a situation in which they can identify causal quantities in a particular setting --- say, from a randomized controlled trial implemented in a specific local context --- but want to know how those inferences travel to other settings. Would the intervention work differently in other countries or regions? As we will explain, with an appropriately specified causal model and the right data from the original context, we can draw inferences about causal relationships in other contexts.

4. **Models in hierarchies.** Sometimes researchers learn about the same types of processes in different settings. By thinking of the processes in each setting as deriving from a family of processes, researchers can learn from observations in one setting about causal processes in another and also learn about the nature of heterogeneity between settings. 


Before delving into the details of these strategies, we make one key qualification explicit: each of these approaches requires us to believe that setting-, or study-, specific  causal model can be nested within a lower level, "encompassing," model that operates across the multiple settings that we are learning from and want to draw inferences about. Encompassing models, of course, can specifically take heterogeneity across settings into account, for instance by including in the model moderators that condition the effects of interest. But we have to believe that we have indeed captured in the model any ways in which relationships vary across the set of contexts across which we are integrating evidence or transporting inferences. 

Put differently, and perhaps more positively, we see social scientists commonly seeking to transport knowledge or combine information informally across studies and settings. Often such efforts are motivated, sometimes implicitly, by an interest in or reliance on general theoretical propositions. The approaches that we describe below force the researcher to be *explicit* about the underlying causal beliefs that warrant that integration while also ensuring that the integration proceeds in a way that is logically consistent with stated beliefs.


## A jigsaw puzzle: Integrating across a model

Generating knowledge about a causal domain often involves cumulating learning across studies that each focus in on some specific part of the domain. For instance, scholars interested in the political economy and democratization might undertake studies focused on the relationship between inequality and mass protests; studies on the role of mass mobilization in generating regime change; pathways other than mass mobilization through which inequality might affect democratization; studies of the role of international sanctions on the likelihood that autocracies will democratize; and studies of the effects of democratization on other things, such as growth or the distribution of resources. 

We can think of these studies as each analyzing data on a particular part of a broader, more encompassing causal model. In an informal way, *if* findings "hold together" in a reasonably intuitive way, we might be able to piece together an impression of the overall relations among variables in this domain.  Yet an informal approach becomes more difficult for complex models or data patterns and, more importantly, will leave opportunities for learning unexaploited.

Consider this simple DAG, in which both $X$ and $Z$ are causes of $Y$, and $Z$ also causes $K$. Now imagine three studies, all conducted in contexts in which we believe this model to hold: 

```{r jigsaw, eval = TRUE, echo = FALSE}
model <- make_model("X -> Y <- Z -> K") %>%

          set_parameters(
            statement = list("(Y[X=1, Z = 1] > Y[X=0, Z = 1])",  
                             "(K[Z = 1] > K[Z = 0])"),
            node = c("Y","K"), 
            parameters = c(.24,.85))

plot(model)

```


1. Study 1 is an RCT in which $X$ is randomized, with data collected on both $Y$ and $K$. $K$ is collected. $Z$ is not observed.
1. Study 2 is a factorial experiment, in which $X$ and $Z$ are independently randomized, allowing an examination of the joint effects of $X$ and $Z$ on $Y$. $K$ is not observed.
2. Study 3 is an experiment randomizing $Z$, with only $K$ observed as an outcome. $X$ and $Y$ are not observed.

Now, let's say that our primary interest is in the relationship between $X$ and $Y$. Obviously, Study 1 will, with a sufficiently large sample, perform just fine in estimaing the average treatment effect of $X$ on $Y$. However, what if we are interested in a case-oriented query, such as the probability of causation: the probability, say, $X=1$ caused $Y=1$ in a given $X=1, Y=1$ case? 

We know that within-case, process-tracing clues can sometimes provide probative value on case-level estimands like the probability of causation, and we have observed $K$ in the Study 3 cases. So what if we combine the $X$, $Y$, and $K$ data? 

```{r, echo = FALSE}

df <- make_data(model, 300, using = "parameters") %>%
  
      mutate(study = rep(1:3, each = 100),
             Z = ifelse(study == 1, NA, Z),
             K = ifelse(study == 2, NA, K),
             X = ifelse(study == 3, NA, X),
             Y = ifelse(study == 3, NA, Y)
             )

```

```{r, echo = FALSE}
if(do_diagnosis){

updated1 <- update_model(model, filter(df, study == 1))
updated2 <- update_model(model, filter(df, study == 2))
updated3 <- update_model(model, filter(df, study == 3))
updated_all <- update_model(model, df)

subs <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0")
subs2 <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0",
              "X == 1 & Y == 1 & K == 1 & Z == 1",
              "X == 1 & Y == 1 & K == 0 & Z == 1",
              "X == 1 & Y == 1 & K == 1 & Z == 0",
              "X == 1 & Y == 1 & K == 0 & Z == 0")

# If updating done using case data only
result1 <- query_model(updated1, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result2 <- query_model(updated2, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result3 <- query_model(updated3, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result4 <- query_model(updated_all, queries = "Y[X=0] == 0", given = subs2, using = "posteriors")

write_rds(list(result1, result2, result3, result4), "saved/10a_frankenstein.rds")
}

```


<!-- AJ: Suggest we flip the study ordering to the X-Y experiment is STudy 1. Makes a more natural progression with the prose. -->

A simple analysis of the graph tells us that $K$ cannot help us learn about $Y$'s potential outcomes since $K$ and $Y$ are $d$-separated by $Z$, and we have not observed $Z$ in Study 3. We see this confirmed in Table \@ref(tab:frank1). 

In the first pair of rows, we  show the results of analyses in which we have simulated data from the whole model, then updated using the Study 1 observations. We give here the posterior mean on the probability of causation for an $X=Y=1$ case, conditional on each possible value that $K$ might take on. As we can see, our beliefs about the estimand remain unaffected by $K$'s value, meaning that it contains no information about $X$'s effect in the case.

<!-- AJ: When we say data on K is not available in S1, do we mean we haven't used K to *update* the model? But we are then simulating what would we would infer if we *did* collect $K$ for a case in S1? This is my read of the code. Or have I misread? Are we imagining a situation in which we simply don't have K at all, so by definition can't learn from K? I've written the text below on the first assumption, so will need changing if that's wrong. -->

We see that the same thing is true for each of the other studies. In study 2, we have not used $K$ to update the model, and so have not learned anything form the data about $K$'s relationship to the other variables. Thus, we have no foundation on which to ground probative value fo $K$. In study 3, we understand the $Z,K$ relationship well, but know nothing quantitatively about how $Z$ and $X$ relate to $Y$. Thus, we have learned nothing from Study 3 about what observing $K$ might tell us about the effect of $X$ on $Y$.
 

```{r frank1, echo = FALSE}

frank <- read_rds("saved/10a_frankenstein.rds")

kable(
  cbind(Study = c(1, NA, 2, NA, 3, NA),
        rbind(
    frank[[1]][,-c(1,3)],
    frank[[2]][,-c(1,3)], 
    frank[[3]][,-c(1,3)])), 
  caption = "The clue $K$ uninformative in all three studies")
```


We can do much better, however, if we combine the data and update *jointly* across all model paramaters. The results are shown in Table \@ref(tab:frank4). Updating simultaneously across the studies allows us, in a sense, to bridge across inferences. In particular, inferences from Study 2 make $Z$ informative about $Y$'s potential outcomes under different values of $X$. Meanwhile, inferences from the data in Study 3 allow us to use information on $K$ to update on values for $Z$. As we now see in rows 1 and 2, having updated the model in an integrated fashion, $K$ now *is* informative about the probability of causation, with our posterior mean on this query changing substantially depending on the value of $K$ that we observe in a case.  

Rows 3-4 highlight that the updating works through inferences on $Z$: we see that if $Z$ is already known (we show this for $Z=1$, but it holds for $Z=0$ as well), then there are no additional gains from knowledge of $K$. 


```{r frank4, echo = FALSE}
kable(frank[[4]][1:4,-c(1,3)], caption = "Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$", digits = 2)
```


<!-- In sum, the collection of studies collectively provides a justification for learning from $K$ when assessing a case level effect of $X$ on $Z$ in study 1.  -->

<!-- AJ: Is something wrong with the study numberings in this next paragraph? -->

<!-- I find the reference to justifying a model confusing here. It's not going to be self-evident to readers how that's what we've just done. It seems like we've *started* with a model and used it. But I'm not 100% clear on what you want to say here. Can you clarify? -->

We devote Chapter 15 to a discussion of how we justify a model. However, we note already that in this example we have an instance in which a researcher (examining a case in study 3) might wish to draw inferences using $K$, but she does not have anything in study 1 that justifies using $K$ for inference. However with access to studies 2 and 3, and conditional on the overall model, she has a justification for process tracing strategy. The general principle is that weaker commitments to lower level theories ---here the causal structure---can justify more fully inferences from more fully specified higher-level theories.  

<!-- AJ: Also finding the last sentence above confusing. -->




## Combining observational and experimental data

Experimental studies are often understood as the "gold standard" for causal inference. This is, in particular, because of the ability of a randomized trial (given certain assumptions, such as "no spillovers") to eliminate sources of confounding. By design, an experiment removes from the situation processes that, in nature, would generate a correlation between selection into treatment and potential outcomes. An experiment thereby allows for an unbiased estimate of the average causal effect of the treatment on the outcome. 

At the same time, an interesting weakness of experimental studies is that, by dealing so effectively with selection into treatment, they limit our ability to learn about selection and its implications in the real world. Often, however, we want to know what causal effects would be specifically for units that *would* in fact take up a treatment in a real-world, non-experimental settings. This kind of problem is studied for example by @knox2019design.

Consider, for instance, a policy that would make schooling subsidies available to parents, with the aim of improving educational outcomes for children. How would we know if the policy was effective? A source of confounding in an observational setting might be that those parents who apply for and take up the subsidy might also be those who are investing more in their children's education in other ways as compared to those parents who do not apply for the subsidy. To eliminate this problem, we might design an experiment in which parents are randomly assigned to receive (or not receive) the subsidy and compare outcomes between children in the treatment and control groups. With a no-spillovers assumption, we can extract the $ATE$ of the receipt of subsidies. 

What this experiment cannot tell us, however, is how much the policy will boost educational outcomes outside the experiment. That is because the causal quantity of interest, for answering that question, is *not* the $ATE$: it is the average treatment effect for the *treated* ($ATT$), given real-world selection effects. That is, the policymaker wants to know what the effect of the subsidy will be for the children of parents who *select into* treatment. One could imagine the real-world $ATT$ being higher than the $ATE$ if, for instance, those parents who are informed and interested enough to take up the subsidy also put the subsidy to more effective use. One could also imagine the $ATT$ being lower than the $ATE$ is, for instance, there are diminishing marginal returns to educational investments and the self-selecting parents are already investing quite a lot. 

Even outside a policy context, we may be interested in the effect of a causal condition *where* that causal condition emerges. To return to our inequality and democracy example, we may want to know what would have happened to autocracies with low inequality *if* they had had high inequality -- the standard average-treatment effect question. But we might also be interested in knowing how much of a difference high inequality makes *in the kinds of cases* where high inequality tends to be occur -- where the effect could be very different. 

With such questions, we are in a sort of bind. The experiment cannot tell us *who* would naturally select into treatment and what the effects would be for them. Yet an observational study faces the challenge of ruling out confounding. Ideally, we would like to be able to combine the best features of both: use an experiment to deal with confounding and use observational data to learn about those whom nature assigns to treatment.

We can achieve this form of integration with a causal model. We do so by creating a model in which random assignment is nested within a broader set of assignment processes. We plot the model in Figure \@ref(fig:appcombexpob)

At the substantive core of this model is the $X \rightarrow Y$ relationship. However, we give $X$ a parent that is fully exogenous, $Z$, to capture a random-assignment process. We give $X$ a second parent, $O$, that is confounded with $Y$: $O$ here represents the observational scenario. Finally, we include a "switch" variable, $R$, that determines whether $X$ is randomly assigned or not. So when $R=1$, $X$ is determined solely by $Z$, with $X=Z$. When $R=0$, we are in an observational setting, and $X$ is determined solely by the confounded $O$, with $X=O$.

A few notes on the parameter space. Parameters allow for complete confounding between $O$ and $Y$, but $Z$ and $Y$ are unconfounded. $X$ has only one causal type since its job is to operate as a conveyor belt, simply inheriting the value of $Z$ or $O$, depending on $R$. 

Note also that this model assumes the exclusion restriction that entering the experimental sample ($R$) is not related to $Y$ other than through assignment of $X$. 


```{r copobsetup, message = FALSE, warning = FALSE, echo = FALSE}
model <- make_model("R -> X -> Y; O -> X <- Z; O <-> Y") %>%
  
	set_restrictions("(X[R=1, Z=0]!=0) | (X[R=1, Z=1]!=1) | (X[R=0, O=0]!=0) | (X[R=0, O=1]!=1)")

```

```{r appcombexpob, message = FALSE, warning = FALSE, echo = FALSE, fig.cap="A model that nests an observational and an experimental study. The treatment $X$ either takes on the observational value $O$, or the assigned values $Z$, depending on whether or not the case has been randomized, $R$."}

plot(model)
```

```{r, echo = FALSE, include = FALSE}
P <- get_parameter_matrix(model)
kable(P[,1:4])
```


Now, let us imagine true parameter values such that $X$ has a $0.2$ average effect on $Y$. However, the effect is different for those who are selected into treatment in an observational setting: it is positive ($0.6$) for cases in which $X=1$ under observational assignment, but negative ($-0.2$) for cases in which $X=0$ under observational assignment. (See appendix for complete specification.) 

When we use the model to analyze the data, we will start with flat priors on the causal types. 

```{r, echo = FALSE}

model <- model %>%
	set_parameters(node = "Y", confound = "O==0", parameters = c(.8, .2,  0,  0)) %>%
	set_parameters(node = "Y", confound = "O==1", parameters = c( 0,  0, .6, .4))

```

The implied true values for the estimands of interest, and our priors on those estimands, are displayed in Table \@ref(tab:fusionestimands).

<!-- AJ: I think the talk of parameters and priors is less intuitive than it could be in a table and passage like this, though I realize it maps onto CQ argument names. By "parameters" we mean *true* values in the simulation, right? Can we convert to "truth" in the "Using" column? Truth vs. priors seems nicely intuitive. -->

```{r fusionestimands, echo = FALSE}

if(do_diagnosis){
result <- query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = c("parameters", "priors"), 
    expand_grid = TRUE)
write_rds(result, "saved/10a_fusionestimands.rds")
}

read_rds("saved/10a_fusionestimands.rds") %>% kable(caption = "Estimands in different sites", digits = 2)

```


```{r, echo = FALSE}
data <- make_data(model, n = 800)
```


Now, we generate data from the model, using the posited "true" parameter values, and then update the model using these data. 

We begin by analyzing just the observational data (cases where $R=0$) and display the results in Table \@ref(tab:fusiondim). Recall that the true average effect of $X$ on $Y$ is $0.2$. Naive analysis of the observational data, taking a simple difference in means between the $X=0$ and $X=1$ cases, yields a strongly upwardly biased estimate of that effect, of 0.0806. 

```{r fusiondim, echo  = FALSE, warning = FALSE, message = FALSE}
x <- estimatr::difference_in_means(Y~X, data = filter(data, R==0))
kable(summary(x)[[1]], digits = 3, caption = "Inferences on the ATE from differences in means")
```

In contrast, when we use CausalQueries to update on the full causal model and use both the experimental and observational data, we get the much more accurate results shown in Table \@ref(tab:fusionCQ). Moving down the rows, we show here the estimate of the unconditional $ATE$, the estimate for the observational context ($R=0$), and the estimate for the experimental context ($R=1$). Unsurprisingly, the estimates are identical across all three settings since, in the model, $R$ is $d$-separated from $Y$ by $X$, which is observed. And, as we see, the posterior means are very close to the right answer of $0.2$.

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, data), "saved/10a_exp_obs.rds")
  }
updated <- read_rds("saved/10a_exp_obs.rds")
```

```{r fusionCQ, echo = FALSE}
result <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result, caption = "Estimates on the ATE for observational ($R=0$) and experimental ($R=1$) set.")
```




```{r, eval = FALSE, echo = FALSE}
updated_no_O <- update_model(model, dplyr::filter(data, R==1))
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, dplyr::filter(data, R==1)), "saved/10a_exp_obs_2.rds")
  }
updated_no_O <- read_rds("saved/10a_exp_obs_2.rds")
```

```{r appcombexpopp8, echo = FALSE, include = FALSE}
result <- query_model(
    updated_no_O, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result)
```

Since the model used both the experimental and the observational data, we might wonder from where the leverage derived: did the observational data improve our estimates of the average treatment effect, or do our inferences emerge strictly from the experimental data? In the appendix, we show results when we updating using experimental data only.
 Comparing the two sets of results, we find there that we do indeed get a tightening of posterior variance and a more accurate result when we use both the observational and experimental data, but the experimental data alone are quite powerful, as we should expect for an estimate of the $ATE$. The observational data do not add a great deal to an $ATE$ estimate, and the gains from observational data would be smaller still (and the experimental results even more accurate) if the experimental sample were larger. 

<!-- FLAG: Need to create above table in appendix -->

However, what we can learn about uniquely from this model and the combined observational and experimental data is *heterogeneity* in effects between those that are in treatment and those that are in control *in the observational* setting. In Table \@ref(combexpobsattatc), we display the results of $ATT$ and $ATC$ queries of the updated model. In the first two rows, we see that, in the experimental setting, the average effect of $X$ on $Y$ is the same on both the treated and control groups, exactly as we would expect under random assignment. In the third row, we see the estimate of $X$'s average effect for those assigned "by nature" to the control group in the observational setting, extracting a result close to the "true" value of $-0.2$. The final row shows our estimate of the treatment effect for those who are selected into treatment in the observational setting, again getting close to the answer implied by the underlying data-generating process ($0.6$). 

```{r combexpobsattatc, echo = FALSE}
result2 <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list("R==1 & X==0", "R==1 & X==1", "R==0 & X==0", "R==0 & X==1"),
    using = "posteriors")

kable(result2, caption = "Effects of $X$ conditional on $X$ for units that were randomly assigned or not.  Effects of $X$ do not depend on $X$ in the experimental group, but they do in the observational group becuase of seld selection. ")
```

We can learn nothing about the observational $ATT$ or $ATC$ from the experimental data alone, where there $ATT$ and $ATC$ are the same quantity. And in the observational data alone, we are hobbled by confounding of unknown direction and size. What the mixed model and data, in effect, are able to do is (a) learn about the $ATE$ experimental data, (b) use inferences on the $ATE$ to separate true effects from confounding in the observational data and thus learn about the direction and size of the confounding in those data, and (c) estimate the treatment effect for the $X=0$ group and for the $X=1$ group, respectively, in the observational data *using* knowledge about confounding in these data. By mixing the experimental and observational data, we can learn about how the treatment has affected those units that, in the "real" world of the observational setting, selected into treatment *and* about how the treatment *would* affect those that selected into control. 

The numbers in our toy example, while purely notional, can help us see why the observational $ATT$ and $ATC$ might be of great interest to decision makers where strong causal heterogeneity is a possibility. Based on the experimental data alone, we might conclude that the policy that makes $X=1$ available is a good bet, given its positive $ATE$ (assuming, of course, that $Y=1$ is a valued outcome). And, of course, the observational data alone would not allow us to confidently conclude otherwise. What the integrated analysis reveals, however, is that $X$ in fact has a *negative* mean effect on those who would be most likely to take up the treatment. The strong positive effect for the control strongly shapes the experimental results but will go unrealized in the real world. 
In a similar vein, these estimates can aid causal explanation. Seeing the positive $ATE$ might lead us to infer that most of the $X=1, Y=1$ cases we observe in the world are ones in which $X=1$ caused $Y=1$. The observational $ATT$ estimates point in a very different direction, however, indicating that these are cases in which $X$ is least likely to have a positive effect and, thus, where $Y=1$ was most likely generated by some other cause.

We note that the results here relate to the LATE theorem [@angrist1995identification]. Imagine using data only on (a) the experimental group in control and (b) the observational group, some of whom are in treatment. We can conceptualize our design as one in which the observational group are "encouraged" to take up treatment, allowing us to estimate the effect for the "compliers" in the observational setting: those that self-select into treatment. Conversely, we could use data only on (a) the experimental group in treatment and (b) the observational group, some of whom are in control. This is a design in which the observational group are "encouraged" to take the control condition, allowing us to estimate the effect for the "compliers" in this group (those that self select into control). 

<!-- AJ: Can't we instead frame the second estimate as the estimate for the never takers? -->

## Transportation of findings across contexts

In some circumstances, we study the effect of $X$ on $Y$ in one context (a country, region, or time period, for instance) and then want to make inferences about these effects in another context (say, another country, region, or time period). We may face the challenge that effects are heterogeneous, and that conditions that vary across contexts may be related to treatment assignment, to outcomes, and to selection into the sample. For example, we might study the relationship between inequality and democratization in low-income countries and then want to know how those effects travel to middle-income settings. However, the level of income may have implications jointly for the level of inequality and for how likely inequality is to generate regime change, meaning that causal effects uncovered in the first context cannot be assumed to operate in the second context.

This is the problem studied by @pearl2014external. In particular,  @pearl2014external show for which nodes data are needed in order to "licence" external claims, given a model. 

We illustrate with a simple model in which an observable confounder has a different distribution across contexts. In the model drawn in Figure \@ref(extval), $Context$ determines the distribution of the confounder, $W$. We set a restriction such that the value of $W$ in Context 1 is never less than the value of $W$ in Context 0; our priors are otherwise flat over the remaining nodal types in the model.  


<!-- AJ: Macartan, can you interpret the parameter values for the sentence above? Not sure I know how to read the complements and decreasing statements correctly. -->

<!-- AJ: Note I've changed Case to Context everwhere, which I think is better ,more general. -->

```{r extval, echo = FALSE, fig.cap= "Extrapolation when confounders have different distributions across cases."}

model <- make_model("Context -> W  -> X -> Y <- W") %>%
  set_restrictions("W[Context = 1] < W[Context = 0]") %>%
  set_parameters(node = "X", statement = "X[W=1]>X[W=0]", parameters = 1/2)%>%
  set_parameters(node = "Y", statement = complements("W", "X", "Y"), parameters = .17) %>%
  set_parameters(node = "Y", statement = decreasing("X", "Y"), parameters = 0) 

plot(model)


```


```{r appev2, echo = FALSE}
if(do_diagnosis){
appev2 <-
  query_model(model,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("priors", "parameters"), expand_grid = TRUE) 
write_rds(appev2, "saved/10a_appev2.rds")
}

read_rds("saved/10a_appev2.rds")  %>% 
  kable(caption = "Priors and true values (parameters) for three estimand: the frequency of $W$, the effect of $X$ on $Y$, and the effect conditional on $W=1$")

```

<!-- AJ: Incidence is a confusing label. Can we make Incidence_W or something like that? -->

We show priors and true values for the estimands (drawn from the parameters) in Table \@ref(tab:appev2). We see that the incidence of $W=1$ is higher in Context 1 than in Context 0, both in our priors and in the "truth" posited by the assigned parameter values. The "true" $ATE$ of $X$ on $Y$ is also higher in Context 1, though this is not reflected in our priors. The average treatment effect conditional on $W$ is the same in both contexts, whether we work from priors or assigned parameter values, as it must be given the model. That is, in this model the $ATE$ varies conditional on $W$ --- and it varies conditional *only* on $W$.  

<!-- incidence in as well as the ATE of $X$ on $Y$ is larger in case 1 than in case 0 (in parameters, though not in priors). However the effect of $X$ on $Y$ conditional on $W$ is the same in both places.  -->

We now update the model using data from one context and then see if we can transport those findings to the other context. Specifically, we update using data on $X, Y,$ and $W$ from Context 0. We then use the updated beliefs to draw inferences about Context 1, using data *only* on $W$ from Context 1. In Table \@ref(appev3), we show our posteriors on the queries of interest as compared to the truth, given the parameter values. 




```{r appev3, echo = FALSE}
if(do_diagnosis){

  data <- make_data(model, n = 10000, 
                  vars = list(c("Context", "W"), c("X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Context == 0"))

  transport <- update_model(model, data)

  write_rds(query_model(transport,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport.rds")
}

q <- read_rds("saved/10a_transport.rds")

kable(q, caption = "Extrapolation when two sites differ on $W$ and $W$ is observable in both contexts")

```

<!-- AJ: Not sure we need the Incidence and CATE rows for this table. -->

By comparing the $ATE$ estimates using our posteriors and the estimates using the assigned parameter values, we see that we have done well in recovering the effects, *both* for the context we studied (i.e., in which we observed $X$ and $Y$) and for the context we did not study.  We can think of the learning here as akin to post-stratification. We have learned from observing $X, Y$, and $W$ in Context 0 how $X$'s effect depends on $W$. Then we use those updated beliefs when confronted with a new value of $W$ in Context 1 to form a belief about $X$'s effect in this second context. Of course, getting the right answer from this procedure depends, as always, on starting with the correct model.

We can also see, in Table \@ref(appev4), what would have happened if we had attempted to make the extrapolation to Context 1 without data on $W$ in that context.  We would get the wrong answer for Context 1, though we would also report greater posterior variance. The higher posterior variance here captures the fact that we know things could be different in Context 1, but we don't know in what way they are different. 

<!-- AJ: This last set of results seems wrong. The estimates using posteriors seem again to be very close to those using parameters, and variances are not higher than in previous table. Seems we're still using info on W in code below?  -->

<!-- Note that we get the CATE right since in the model this is assumed to be the same across cases. -->


```{r appev4, echo = FALSE}
if(do_diagnosis){

  data2 <- make_data(model, n = 10000, 
                  vars = list(c("Context"), c("W", "X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Context == 0"))

  transport2 <- update_model(model, data2)

  write_rds(query_model(transport2,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport2.rds")
}

q2 <- read_rds("saved/10a_transport2.rds")

kable(q2, caption = "Extrapolation when two contexts differ on $W$ and $W$ is not observable in target context")

```


<!-- FLAG: ADD  A SITUATION WITH AN ARROW FROM Case to Y and WHO THAT WE DO NOT HAVE IDENTIFICATION -->

## Multilevel models, meta-analysis

<!-- AJ: Need a better section heading. If we have a good name for this approach, we can also use this when we make comparisons across approaches in next section. -->

<!-- AJ: Transition here needs some work, I think.  -->

A key idea in Bayesian meta-analysis is that when you analyze multiple studies together you learn not only about common processes that give rise to the different results seen in different sites, but you also learn more about each study from seeing the other studies.

A classic setup is provided in @gelman2013bayesian, in which we have access to estimates of effects and uncertainty in eight sites (schools), $(b_j, se_j)_{j \in \{1,2,\dots,8\}}$. To integrate learning across these studies we employ a  "hierarchical model"
that treats each $b_j$ as a draw from distribution $N(\beta_j, se_j)$ (and, in turn treats each  $\beta_j$ is a draw from distribution $N(\beta, \sigma)$). In that setup we want to learn about the superpopulation parameters  $\beta, \sigma$, but we also get to learn more about the study level effects $(\beta_j)_{j \in \{1,2,\dots,8\}}$ by studying them jointly.

<!-- AJ: Not clear to me that Gelman here is the right way to set up the analysis in this subsection. Doesn't Gelman's setup work more like our setup in the next section, with settings treated as being drawn stochastically from a common distribution at the superpop level? -->

<!-- We define a model in which $X$ points into $Y$, and in which $Setting$ is a third node that also points into $Y$. Thus $Setting$ can potentially moderate the effect of $X$ on $Y$. We observe an experiment that takes place in $Setting$ 0 and also in $Setting$ 1. Note that, in this setup, any differences in the distribution of effects of $X$ on $Y$ between the two settings can *only* arise because of $Setting$. That is, we are here assuming the same causal effects across settings conditional on $Setting$ itself. -->

<!-- We then consider the different conclusions we draw for the effect of $X$ on $Y$ in each setting, depending on whether we pool data across settings and on our priors over how much $Setting$ matters for $X$'s effects. -->

<!-- We design the simulation so that in $X$'s true causal effect is stronger in $Setting$ 0 than in $Setting$ 1. We then generate data from these true parameters for each setting, 100 cases for each, and run the analysis in different ways, with results displayed in Table \@ref(settingmatters). In the first two rows, we show $ATE$ posteriors from separate analyses of the data for each setting. We see that we recover an $ATE$ that is higher in $Setting 0$ than in $Setting 1$, as expected. -->

<!-- In the third row (Integrated flat priors), we use data from both settings to estimate an overall $ATE$. Moreover, we put equal weight on all possible $Y$ types, i.e., on all possible joint effects of $X$ and $Setting$, meaning that we provide substantial scope for $Setting$ to moderate $X$'s effects. As expected, the $ATE$ looking across both settings lies between the $ATE$ for each in the unpooled analyses.  -->

<!-- <!-- AJ: Why is the ATE in "Integrated, flat priors" not more centered between the ATE in the first two rows? It's much lower than I'd have expected. Does allowing interactions pull the ATE down in some way? --> 

<!-- In rows 4 and 5, we then estimate the $ATE$ in each setting separately, again using flat priors that allow for substantial interactions with setting. Differently from the first two rows, however, we are using data from both settings in estimating effects for each setting. Consider what happens we estimate the $ATE$ for $Setting$ 0: we are using our posterior over $Y$'s nodal type shares to answer this query. Of course, data from $Setting$ 0 itself provides what we might think of as the most context-*specific* information about the causal effect in that particular setitng. But data from $Setting$ 1, where we also observe $X$ and $Y$ values, *also* provides information about those shares, and this additional information drawn from $Setting$ 1 is reflected in the integrated estimate for $Setting$ 0 -- and vice versa for the integrated $Setting$ 1 estimate. With the pooling of data, we see that the $ATE$ for each setting.... Also note that the standard deviation of the posterior has shrunk, reflecting the fact that we have brought more data to bear on the question. -->

<!-- <!-- AJ: I don't know how to finish the above sentence because I don't get the lack of convergence -- the lower ATE for Setting 1. Especially since we *do* get the expected convergence with weak interactions priors. --> 

<!-- Finally, in the last three rows, we set priors such that the moderating effect of $Setting$ is believed to be weak. Here we see that the $ATE$'s for the two settings converge more strongly, reflecting the influence of our low-heterogeneity priors. Moreover, our uncertainty shrinks further here, reflecting the fact that -- if we believe that heterogeneity across settings is low -- then we also believe that data from one setting is *more* informative about the other setting: i.e., we get a bigger boost in statistical power from the pooling under these priors.  -->

<!-- <!-- AJ: Why is the overall $ATE$ higher here than with flat priors? Must be that the interactions themselves depress the ATE. Less interaction then means higher ATE?? --> 



<!-- ```{r settingmatters, echo=FALSE} -->


<!-- if(do_diagnosis){ -->

<!--   model_1 <- make_model("X -> Y") %>%  -->
<!--     set_parameters(node  = "Y", parameters = c(.1, .1, .7, .1)) -->
<!--   model_2 <- make_model("X -> Y") %>%  -->
<!--     set_parameters(node  = "Y", parameters = c(.1, .2, .6, .1)) -->
<!--   model_3 <- make_model("X -> Y <- Setting")  -->
<!--   model_4 <- model_3 %>% set_priors(2) %>%  -->
<!--     set_priors(statement = interacts("Setting", "X", "Y"), alphas  = .1) -->

<!--   df_1 <- simulate_data(model_1, 100) %>% mutate(Setting = 0) -->
<!--   df_2 <- simulate_data(model_2, 100) %>% mutate(Setting = 1) -->

<!--   model_1 <- update_model(model_1, df_1) -->
<!--   model_2 <- update_model(model_2, df_2) -->
<!--   model_3 <- update_model(model_3, rbind(df_1, df_2), iter = 8000) -->
<!--   model_4 <- update_model(model_4, rbind(df_1, df_2), iter = 8000) -->

<!--   q1 <- query_model(model_1, list(`Setting 0` = te("X","Y")), using = "posteriors") -->
<!--   q2 <- query_model(model_2, list(`Setting 1` = te("X","Y")), using = "posteriors") -->

<!--   q3_1 <- query_model(model_3, list(`Integrated (flat priors)` = te("X","Y")),  -->
<!--                    given = c(TRUE, "Setting==0", "Setting==1"),  -->
<!--                    expand_grid = TRUE, using = "posteriors") -->

<!--   q3_2 <- query_model(model_3, interacts("Setting", "X", "Y"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->

<!--   q4_1 <- query_model(model_4, list(`Integrated (low heterogeneity)` = te("X","Y")),  -->
<!--                    given = c(TRUE, "Setting==0", "Setting==1"),  -->
<!--                    expand_grid = TRUE, using = "posteriors") -->

<!--   q4_2 <- query_model(model_4, interacts("Setting", "X", "Y"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->

<!--   write_rds(list(q1, q2, q3_1, q3_2, q4_1, q4_2), "saved/10a_multilevel.rds") -->

<!-- } -->

<!--   multilevel <- read_rds("saved/10a_multilevel.rds") -->

<!--     rbind( -->
<!--     multilevel[[1]],  multilevel[[2]], -->
<!--   multilevel[[3]],  -->
<!--   multilevel[[5]]) %>% kable(caption = "Inferences from separate analyses and from integrated analysis (meta analysis) given (a) flat priors and (b) expectation of similar effects across studies", digits =2) -->
<!-- ``` -->

<!-- <!-- We see in both cases a drop in our estimates for effects in Setting 1 in both cases, relative to the single study case. Where weak heterogeneity is assumed we also see a rise in estimates for Setting 2.  -->

<!-- Further, we can use these same updated models to update specifically on the amount of heterogeneity across settings. We operationalize heterogeneity here as the share of units that *would* respond differently to $X$ if they were in a different setting. In Table XXXX, we compare our prior on this quantity to our posterior. Where we started out with flat priors --- allowing for a great deal of heterogeneity --- we see that the data bring these beliefs downward. This makes sense given that, under the true data-generating process, the effects in the two settings are only moderately different. Conversely, where we start with a very low prior on heterogeneity, the data lead us to believe there is *more* heterogeneity than we had initially believed (though, given the strength of the prior that we have used in this example, we can still see its formidable efect on the posterior). -->

<!-- ```{r, echo = FALSE} -->
<!--     multilevel[[4]] %>% kable(caption = "Interaction | Flat priors") -->
<!--   multilevel[[6]] %>% kable(caption = "Interaction | Expected homogeneity") -->

<!-- ``` -->

<!-- ```{r, include = FALSE} -->
<!-- # Approach 2 -->

<!-- model_5 <- make_model("X -> Y <- Setting -> W -> Y") %>%  -->
<!--   set_restrictions("Y[X=1, W=1] != Y[X=0, W=1]") -->
<!-- length(model_5$nodal_types$Y) -->


<!-- if(do_diagnosis){ -->

<!--   model_5 <- update_model(model_5, rbind(df_1, df_2), iter = 8000) -->

<!--   q5_1 <- query_model(model_5, list(`Integrated (Latent W)` = te("X","Y")),  -->
<!--                    given = c(TRUE, "Setting==0", "Setting==1"),  -->
<!--                    expand_grid = TRUE, using = "posteriors") -->

<!--   q5_2 <- query_model(model_5, interacts("Setting", "X", "Y"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->
<!--   q5_3 <- query_model(model_5, "W==1",  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->
<!--   q5_4 <- query_model(model_5, c("W[Setting=1]-W[Setting=0]"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->

<!--   write_rds(list(q5_1, q5_2, q5_3, q5_4), "saved/10a_multilevel_latent.rds") -->

<!-- } -->


<!-- ``` -->

<!-- ## Real multilevel -->

<!-- AJ: Need a better section heading -->

<!-- In the situations we have considered so far, we are learning from and about the particular contexts that we are studying. We are combining experimental data from one setting, for instance, with observational data from another. Or we are using updating from one context to draw an inference about another setting. Our inferences in these setups are limited strictly to the settings at hand, however.  -->

A hierarchical model like this allows us to think about the populations in our study sites as themselves drawn from a larger population ("superpopulation") of settings. And, crucially, it allows us in turn to use data in the study sites to learn about that broader superpopulation of settings.

<!-- AJ: Is "superpopulation" right here and below? -->

<!-- For instance, we might be studying the effect of an individual's relative location in the income scale on their preferences for democracy. We might collect data in a set of 10 countries and estimate the average causal effect of relative income in each of them. We can readily average inferences across these countries or study country-level moderators of the effect to explain differences in effects across the 10 countries. Yet these data from the 10 countries also contain information of a more general sort: they tell us something about the "superpopulation" of settings from which these 10 countries have been "drawn." -->

Although often used in the context of linear models with parameters for average causal effects, this logic works just as well with the kinds of causal models we have been using in this book. 

Let's review how our analytic setup has worked so far. At each node in a causal model, we conceptualize a given case as having a particular nodal type. The case's nodal type is drawn from a distribution of nodal types in the population of cases from which this case has been drawn. When we do process tracing, we consider that population-level distribution to be a set of fixed shares of nodal types in the population: say, for node $Y$, we might believe that half the cases in the population are $\lambda^Y_{01}$, a quarter are $\lambda^Y_{00}$, and a quarter are $\lambda^Y_{11}$. We then use data from the case to update on the case's nodal types (or on the combination of nodal types that correspond to some case-level query), given the population-level shares. 

When we engage in population-level inference, we begin with *uncertainty* about the population-level shares of types, and we express our prior beliefs about those shares as a Dirichlet *distribution*. So, for instance, our beliefs might be centered around a $\lambda^Y_{01}=0.5, \lambda^Y_{00}=0.25, \lambda^Y_{11}=0.25$ breakdown of shares in the population; and we also express some degree of uncertainty about what the breakdown is. Now, when we analyze data on some number of cases, we can update both on those cases' types and on our beliefs about the distribution of types in the population -- perhaps shifting toward a higher share of $\lambda^Y_{01}$'s (and with a change in the distribution's variance).

We can also, as in the last section, build a model in which there are multiple settings, possibly differing on some population-level characteristic. Fundamentally, however, the setup in the last section still involved population-level inference in that we were assuming that the *type shares* ($\lambda$ values) are the same across settings. The settings might differ in the value of a moderating variable, but they do not differ in the shares of cases that *would* respond in any given way to the moderator (and other causal conditions). The data allow us to update on what those common, cross-setting type proportions are.

When we build a hierarchical model, each case is still understood as being embedded within a population: our cases might be citizens, say, each embedded within a country. The key difference from population-level inference is that we now conceive of there being *multiple* populations -- say, multiple countries -- each drawn from a population of populations, or superpopulation. Now, we think of each population (country) as having its own set of type shares for each node. And we think of each country's type shares as being drawn from a Dirichlet distribution of type shares (for each node) that lives at the superpopulation level. Moreover, we are *uncertain* about what that distribution at the superpopulation level *is*. We uncertain around what type proportions the superpopulation-level distribution is centered, and we are uncertain about how dispersed this distribution is. While the distribution's central tendency will be related to the mean type shares for countries, its variance will determine the degree of *heterogeneity* across countries in their type shares. 

To summarize, in population-level inference, we express uncertainty about the population's type shares with a Dirichlet prior, at the population level, on which we update. In the hierarchical setting, we are uncertain both about the population-level type shares and the superpopulation Dirichlet from which each node's type shares are drawn. We express our uncertainty about each superpopulation Dirichlet by positing a prior distribution over the Dirichlet's alpha parameters.

Now, when we observe data on citizens within countries, we can update our beliefs about types fora the particular citizens we observe, about type shares in the population of citizens within each country that we study, *and* on the parameters of the Dirichlet distribution from which population shares have been drawn. In updating on the last of these, we are learning not just about the countries we observe but also about those we do not directly observe. 

<!-- AJ: Check that all of the above is right! -->

<!-- AJ: Need something here about how the approach to hierarchical models in Gelman et al is different from or related to how we're operationalizing them. -->

We illustrate with a simulation using a simple $X,Y$ model. We imagine that we are studying the $X \rightarrow Y$ relationship in `n` countries. Each country has a parameter distribution drawn from  common Dirichelets. We start off with flat priors over the alpha arguments of the superpopulation Dirichlets. 

<!-- AJ: I am confused by the statement that "Each country has a parameter distribution drawn from  common Dirichelets." I am generally struggling a bit with this bit still, and so may have this point wrong above. I am thinking: we have a set of alphas (Dirichlet parameters) for each *country* -- so we have a prior distribution of shares for each country. And the country-level alphas have been drawn from a common distribution of *alphas* at the superpop level. Is that right? Or is all the randomness at the superpop level, and we're now thinking of each country as having a specific set of shares, and our prior and posterior distributions are ONLY Dirichlet distributions at the superpop level? -->


<!-- MH: No we do not have alphas for each country; each country does and always did have a specific set of shares. This was true before but we represented our prior uncertainty over the specific shares using the Dirichlet. We *still* are uncertain over the shares but now we beliefe there *is* a a Dirichlet distribution from which these are drwan (before the Dirichlet just represented our uncertainty; now it represents the actual data generation process); we just don;t know what the Dirichlet is so we are are uncertain over it (before we did know what the Diriclet is since that just meant we knew what our priors are; now we don;t know what it is because it represents the true dgp). Easiest to think through this focussed only on the distribution of a binary variable X. (a) For each person X is either 0 or 1. (b) in a study the distribution of X is given by p (prob X = 1); in a single country study we want to figure out what p is and we have a prior distribution over possible values of p, using a Beta distribution with parameters a,b.  (c) in a multicountry study we think that there is a different p is every country and we now imagine these are drawn from a beta distribution with parameters a,b; but we don;t know what a and b are. So we put priors over a and b (inverse gamma priors). -->

<!-- AJ: OK, revised text accordingly. -->

We assign a particular true set of superpopulation parameter values that, for the analytic exercise, is treated as unknown and that we would like to recover. In this true world, the probability of assignment to $X=1$ is .4, and the average treatment effect is .1. Using these true parameter values, we simulate $X, Y$ data for $n=8$ countries.

```{r, comment = "", include = FALSE}
model   <- make_model("X->Y")
alpha_X <- c(6, 4)*1
alpha_Y <- c(.5, .4, .6, .5)*5

n <- 8

if(do_diagnosis){
  
  # Draw parameter values for each study
  lambda_X = rdirichlet(n, alpha_X)
  lambda_Y = rdirichlet(n, alpha_Y)
  
  # Draw data for each study
  data_events <- 
    lapply(1:n,
           function(j)
             make_data(model, n = 50,
                       parameters = c(lambda_X[j,], lambda_Y[j,])) %>%
             collapse_data(model))
  
  write_rds(list(lambda_X=lambda_X, lambda_Y=lambda_Y, data = data_events), "saved/10a_truth.rds")
  
  }

# Load
truth <- read_rds("saved/10a_truth.rds")
data_events <- truth$data
data_events

```

<!-- AJ: I think you'll need to do some elaboration for this step, Macartan. -->


```{r, include = FALSE}

# Updating

#We prepare `stan` data and update:


# Stan data is the same as for CausalQueries except we provide a data matrix instead of a vector
data_multilevel   <- CausalQueries:::prep_stan_data(model = model, data = data_events[[1]])
data_multilevel$Y <- sapply(data_events, function(j) j$count)
data_multilevel$n_studies <- ncol(data_multilevel$Y)


if(do_diagnosis)
  rstan::stan(file = "multilevel_dirichlet.stan",  data = data_multilevel, iter = 12000) %>%
  write_rds("saved/10a_meta_pooled.rds")

if(do_diagnosis)
  lapply(1:length(data_events), function(s)
    update_model(model, data = data_events[[s]], 
                 data_type = "compact", iter = 4000)$posterior_distribution) %>%
  write_rds("saved/10a_meta_unpooled.rds")

pooled   <-  read_rds("saved/10a_meta_pooled.rds")
unpooled <-  read_rds("saved/10a_meta_unpooled.rds")
```



```{r plotalphas, echo = FALSE, fig.cap="Joint distributions over alpha parameters."}

# Results

# We use a custom summary function to summarize some results from these models and provide results in a graph and table.

alphas <- rstan::extract(pooled, pars = "alpha")$alpha

par(mfrow = c(1,3))
    plot(alphas[,1], alphas[,2] )
    plot(alphas[,3], alphas[,6] )
    plot(alphas[,4], alphas[,5] )

```

```{r, include = FALSE}
my_summary <- function(pooled, unpooled, truth){
  
  meta    <-  rstan::extract(pooled, pars = "alpha")$alpha
  studies <-  rstan::extract(pooled, pars = "lambdas")$lambdas
  
  out1 <- data.frame(
    analysis = c("Pooled", "Pooled", "Pooled"),
    study = c("Meta", "Meta", "Meta"),
    estimand = c("ATE", "Concentration", "Prob(X=1)"),
    mean = c(
      mean((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      mean((meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      mean((meta[,2])/(meta[,1] + meta[,2]))),
   sd = c(
      sd((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      sd((meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      sd((meta[,2])/(meta[,1] + meta[,2]))))
  
  my_sum <- function(df, s=1, label = NA)  
    data.frame(
               analysis = c(label, label),
               study = c(paste(s),paste(s)),
               estimand = c("ATE", "Prob(X=1)"),
               mean =     c(mean(df[,5] - df[,4]), mean(df[,2])),
               sd =       c(sd(df[,5] - df[,4]), sd(df[,2])))
  
  out2 <- lapply(1:dim(studies)[3], function(s) my_sum(studies[,,s], s, label = "Pooled"))
  
  out3 <- lapply(1:length(data_events), function(s) unpooled[[s]] %>% my_sum(s, "Unpooled"))

  out4 <- data.frame(
    analysis = rep("truth", nrow(truth$lambda_X)),
    study = as.character(rep(1:nrow(truth$lambda_X), 2)),
    estimand = rep(c("ATE", "Prob(X=1)"), each = nrow(truth$lambda_X)),
    mean = c(truth$lambda_Y[,3] - truth$lambda_Y[,2], truth$lambda_X[,2]),
    sd = NA)
  
  
  bind_rows(list(out3, out2, out1, out4)) %>%
    arrange(estimand)
  }
```

In Figure \@ref(fig:plotalphas), we graph our posterior beliefs about the superpopulation parameters. We do this by plotting two alpha parameters against each other at a time. In the first panel, we plot the alphas for $X=0$ and $X=1$. In the next panel, we plot the alpha's corresponding to $c$ types against those corresponding to $d$ types. And in the third panel we plot the alpha's corresponding to $a$ types against those corresponding to $b$ types.

As we can see, each distribution falls roughly along a diagonal. Probability mass located further up the diagonal represents worlds in which the superpopulation Dirichlet distribution of type shares is relatively low in variance. Thus, the more that our posterior beliefs are concentrated toward a graph's northeast corner, the lower the heterogeneity we have inferred there to be in the relevant type shares across countries. Meanwhile, the dispersion of probability mass *away* from the diagonal represents greater posterior *uncertainty* about the heterogeneity across countries, arising from greater variance about the posterior distribution of the alphas.

We can think of a concentration parameter here that is operationalized as the sum of the $\alpha^j$ terms for a node, $j$, with a higher value representing lower overall heterogeneity. 

<!-- This would be 4 for a flat distribution ($(1,1,1,1)$ and should be a lot higher with this example.  -->


```{r, warning = FALSE, message = FALSE, echo = FALSE, fig.cap = "Estimates from individual analyses and from meta analysis", include = FALSE}
results <- my_summary(pooled, unpooled, truth)  
```

```{r metaplot, echo = FALSE, fig.cap="Updating on study level parameters from integrated analyses. "}
results %>% filter(estimand == "ATE") %>%
ggplot(aes(mean, study, color = analysis)) + geom_point()

```

In Figure \@ref(fig:metaplot) we turn to the causal query of interest and show a comparison of three $ATE$ estimates for each country: in blue, we show the unpooled estimate, or the estimate we get for each country using only data from that country; in red, we see the pooled estimates, or the estimate we get for each country using data from *all* countries to inform that country's parameter estimates; and in black, we plot the truth as posited for this simulation. As we can see, the pooled estimates are all closer to the center than the unpooled estimates: this is because we are effectively using data from all countries to discount extreme features of the data observed in a given country. Put differently, the pooled data serve the function of a prior when it comes to drawing inferences about a single country: our inference is a compromise between the data from that country and the beliefs we have formed from the pooled data. We can also see that, for most countries, the pooling helps: the regularization provided by the pooling gives us an estimate closer to the truth for most of the settings.
