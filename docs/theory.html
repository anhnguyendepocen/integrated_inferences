<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Theories as causal models | Integrated Inferences</title>
  <meta name="description" content="Bayesian causal models for integrated inferences." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Theories as causal models | Integrated Inferences" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://macartan.github.io/integrated_inferences/" />
  <meta property="og:image" content="https://macartan.github.io/integrated_inferences//images/plot.png" />
  <meta property="og:description" content="Bayesian causal models for integrated inferences." />
  <meta name="github-repo" content="macartan/integrated_inferences" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Theories as causal models | Integrated Inferences" />
  
  <meta name="twitter:description" content="Bayesian causal models for integrated inferences." />
  <meta name="twitter:image" content="https://macartan.github.io/integrated_inferences//images/plot.png" />

<meta name="author" content="Macartan Humphreys and Alan M. Jacobs" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayeschapter.html"/>
<link rel="next" href="pt.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="headers/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Integrated Inferences</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-case-for-causal-models"><i class="fa fa-check"></i><b>1.1</b> The Case for Causal Models</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#the-limits-to-design-based-inference"><i class="fa fa-check"></i><b>1.1.1</b> The limits to design-based inference</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#qualitative-and-mixed-method-inference"><i class="fa fa-check"></i><b>1.1.2</b> Qualitative and mixed-method inference</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#connecting-theory-and-empirics"><i class="fa fa-check"></i><b>1.1.3</b> Connecting theory and empirics</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-contributions"><i class="fa fa-check"></i><b>1.2</b> Key contributions</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-road-ahead"><i class="fa fa-check"></i><b>1.3</b> The Road Ahead</a></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Causal Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="models.html"><a href="models.html#counterfactualmodel"><i class="fa fa-check"></i><b>2.1</b> The counterfactual model</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="models.html"><a href="models.html#potential-outcomes"><i class="fa fa-check"></i><b>2.1.1</b> Potential outcomes</a></li>
<li class="chapter" data-level="2.1.2" data-path="models.html"><a href="models.html#generalization"><i class="fa fa-check"></i><b>2.1.2</b> Generalization</a></li>
<li class="chapter" data-level="2.1.3" data-path="models.html"><a href="models.html#summaries-of-potential-outcomes"><i class="fa fa-check"></i><b>2.1.3</b> Summaries of potential outcomes</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="models.html"><a href="models.html#causal-models-and-directed-acyclic-graphs"><i class="fa fa-check"></i><b>2.2</b> Causal Models and Directed Acyclic Graphs</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="models.html"><a href="models.html#components-of-a-causal-model"><i class="fa fa-check"></i><b>2.2.1</b> Components of a Causal Model</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="models.html"><a href="models.html#graphing-models-and-using-graphs"><i class="fa fa-check"></i><b>2.3</b> Graphing models and using graphs</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="models.html"><a href="models.html#graphing"><i class="fa fa-check"></i><b>2.3.1</b> Rules for graphing causal models</a></li>
<li class="chapter" data-level="2.3.2" data-path="models.html"><a href="models.html#conditional-independence-from-dags"><i class="fa fa-check"></i><b>2.3.2</b> Conditional independence from DAGs</a></li>
<li class="chapter" data-level="2.3.3" data-path="models.html"><a href="models.html#simplifying-models"><i class="fa fa-check"></i><b>2.3.3</b> Simplifying models</a></li>
<li class="chapter" data-level="2.3.4" data-path="models.html"><a href="models.html#retaining-probabilistic-relations"><i class="fa fa-check"></i><b>2.3.4</b> Retaining probabilistic relations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="models.html"><a href="models.html#conc2"><i class="fa fa-check"></i><b>2.4</b> Conclusion</a></li>
<li class="chapter" data-level="2.5" data-path="models.html"><a href="models.html#chapter-appendix"><i class="fa fa-check"></i><b>2.5</b> Chapter Appendix</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="models.html"><a href="models.html#steps-for-constructing-causal-models"><i class="fa fa-check"></i><b>2.5.1</b> Steps for constructing causal models</a></li>
<li class="chapter" data-level="2.5.2" data-path="models.html"><a href="models.html#model-construction-in-code"><i class="fa fa-check"></i><b>2.5.2</b> Model construction in code</a></li>
<li class="chapter" data-level="2.5.3" data-path="models.html"><a href="models.html#rules-for-moving-between-levels"><i class="fa fa-check"></i><b>2.5.3</b> Rules for moving between levels</a></li>
<li class="chapter" data-level="2.5.4" data-path="models.html"><a href="models.html#reading-conditional-independence-from-a-graph"><i class="fa fa-check"></i><b>2.5.4</b> Reading conditional independence from a graph</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="illustratemodels.html"><a href="illustratemodels.html"><i class="fa fa-check"></i><b>3</b> Illustrating Causal Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="illustratemodels.html"><a href="illustratemodels.html#welfare-state-reform"><i class="fa fa-check"></i><b>3.1</b> Welfare state reform</a></li>
<li class="chapter" data-level="3.2" data-path="illustratemodels.html"><a href="illustratemodels.html#military-interventions"><i class="fa fa-check"></i><b>3.2</b> Military Interventions</a></li>
<li class="chapter" data-level="3.3" data-path="illustratemodels.html"><a href="illustratemodels.html#development-and-democratization"><i class="fa fa-check"></i><b>3.3</b> Development and Democratization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i><b>4</b> Causal Queries</a>
<ul>
<li class="chapter" data-level="4.1" data-path="questions.html"><a href="questions.html#case-level-causal-effects"><i class="fa fa-check"></i><b>4.1</b> Case-level causal effects</a></li>
<li class="chapter" data-level="4.2" data-path="questions.html"><a href="questions.html#case-level-causal-attribution"><i class="fa fa-check"></i><b>4.2</b> Case-level causal attribution</a></li>
<li class="chapter" data-level="4.3" data-path="questions.html"><a href="questions.html#actual-causes"><i class="fa fa-check"></i><b>4.3</b> Actual causes</a></li>
<li class="chapter" data-level="4.4" data-path="questions.html"><a href="questions.html#average-causal-effects"><i class="fa fa-check"></i><b>4.4</b> Average causal effects</a></li>
<li class="chapter" data-level="4.5" data-path="questions.html"><a href="questions.html#causal-paths"><i class="fa fa-check"></i><b>4.5</b> Causal Paths</a></li>
<li class="chapter" data-level="4.6" data-path="questions.html"><a href="questions.html#general-procedure"><i class="fa fa-check"></i><b>4.6</b> General procedure</a></li>
<li class="chapter" data-level="4.7" data-path="questions.html"><a href="questions.html#chapter-appendix-1"><i class="fa fa-check"></i><b>4.7</b> Chapter Appendix</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayeschapter.html"><a href="bayeschapter.html"><i class="fa fa-check"></i><b>5</b> Bayesian Answers</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-basics"><i class="fa fa-check"></i><b>5.1</b> Bayes Basics</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-instances"><i class="fa fa-check"></i><b>5.1.1</b> Simple instances</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-rule-for-discrete-hypotheses"><i class="fa fa-check"></i><b>5.1.2</b> Bayes’ Rule for Discrete Hypotheses</a></li>
<li class="chapter" data-level="5.1.3" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-rule-for-continuous-parameters-and-the-dirichlet-family"><i class="fa fa-check"></i><b>5.1.3</b> Bayes’ Rule for Continuous Parameters and The Dirichlet family</a></li>
<li class="chapter" data-level="5.1.4" data-path="bayeschapter.html"><a href="bayeschapter.html#moments"><i class="fa fa-check"></i><b>5.1.4</b> Moments</a></li>
<li class="chapter" data-level="5.1.5" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-estimation-in-practice"><i class="fa fa-check"></i><b>5.1.5</b> Bayes estimation in practice</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-applied"><i class="fa fa-check"></i><b>5.2</b> Bayes applied</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-bayesian-process-tracing"><i class="fa fa-check"></i><b>5.2.1</b> Simple Bayesian Process Tracing</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayeschapter.html"><a href="bayeschapter.html#a-generalization-bayesian-inference-on-queries"><i class="fa fa-check"></i><b>5.2.2</b> A Generalization: Bayesian Inference on Queries</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayeschapter.html"><a href="bayeschapter.html#features-of-bayesian-updating"><i class="fa fa-check"></i><b>5.3</b> Features of Bayesian updating</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="bayeschapter.html"><a href="bayeschapter.html#AppPriors"><i class="fa fa-check"></i><b>5.3.1</b> Priors matter</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simultaneous-joint-updating"><i class="fa fa-check"></i><b>5.3.2</b> Simultaneous, joint updating</a></li>
<li class="chapter" data-level="5.3.3" data-path="bayeschapter.html"><a href="bayeschapter.html#posteriors-are-independent-of-the-ordering-of-data"><i class="fa fa-check"></i><b>5.3.3</b> Posteriors are independent of the ordering of data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>6</b> Theories as causal models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="theory.html"><a href="theory.html#models-as-theories-of"><i class="fa fa-check"></i><b>6.1</b> Models as <em>theories of</em></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="theory.html"><a href="theory.html#implications-of-structural-causal-models"><i class="fa fa-check"></i><b>6.1.1</b> Implications of structural causal models</a></li>
<li class="chapter" data-level="6.1.2" data-path="theory.html"><a href="theory.html#probabilistic-causal-models"><i class="fa fa-check"></i><b>6.1.2</b> Probabilistic causal models</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="theory.html"><a href="theory.html#theorygains"><i class="fa fa-check"></i><b>6.2</b> Gains from theory</a></li>
<li class="chapter" data-level="6.3" data-path="theory.html"><a href="theory.html#formal-theories-and-causal-models"><i class="fa fa-check"></i><b>6.3</b> Formal theories and causal models</a></li>
</ul></li>
<li class="part"><span><b>II Model-Based Causal Inference</b></span></li>
<li class="chapter" data-level="7" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>7</b> Process Tracing with Causal Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="pt.html"><a href="pt.html#process-tracing-and-causal-models"><i class="fa fa-check"></i><b>7.1</b> Process tracing and causal models</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="pt.html"><a href="pt.html#the-intuition"><i class="fa fa-check"></i><b>7.1.1</b> The intuition</a></li>
<li class="chapter" data-level="7.1.2" data-path="pt.html"><a href="pt.html#a-formalization-of-the-general-approach"><i class="fa fa-check"></i><b>7.1.2</b> A formalization of the general approach</a></li>
<li class="chapter" data-level="7.1.3" data-path="pt.html"><a href="pt.html#priors"><i class="fa fa-check"></i><b>7.1.3</b> Priors</a></li>
<li class="chapter" data-level="7.1.4" data-path="pt.html"><a href="pt.html#updating-on-types-given-the-data."><i class="fa fa-check"></i><b>7.1.4</b> Updating on types given the data.</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="pt.html"><a href="pt.html#mapping-from-models-to-classic-qualitative-tests"><i class="fa fa-check"></i><b>7.2</b> Mapping from models to classic qualitative tests</a></li>
<li class="chapter" data-level="7.3" data-path="pt.html"><a href="pt.html#assessing-the-possibility-of-probative-value-from-a-graph"><i class="fa fa-check"></i><b>7.3</b> Assessing the possibility of probative value from a graph</a></li>
<li class="chapter" data-level="7.4" data-path="pt.html"><a href="pt.html#principles-of-learning"><i class="fa fa-check"></i><b>7.4</b> Principles of learning</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="pt.html"><a href="pt.html#a-dag-alone-does-not-guarantee-probative-value-for-a-single-case"><i class="fa fa-check"></i><b>7.4.1</b> A DAG alone does not guarantee probative value for a single case</a></li>
<li class="chapter" data-level="7.4.2" data-path="pt.html"><a href="pt.html#learning-requires-uncertainty"><i class="fa fa-check"></i><b>7.4.2</b> Learning requires uncertainty</a></li>
<li class="chapter" data-level="7.4.3" data-path="pt.html"><a href="pt.html#the-more-specific-the-query-the-more-difficult-it-is-to-gain-leverage"><i class="fa fa-check"></i><b>7.4.3</b> The more specific the query the more difficult it is to gain leverage</a></li>
<li class="chapter" data-level="7.4.4" data-path="pt.html"><a href="pt.html#population-level-uncertainty-does-not-alter-case-level-causal-inference"><i class="fa fa-check"></i><b>7.4.4</b> Population-level uncertainty does not alter case-level causal inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptapp.html"><a href="ptapp.html"><i class="fa fa-check"></i><b>8</b> Process Tracing Application</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ptapp.html"><a href="ptapp.html#inequality-and-democratization-the-debate"><i class="fa fa-check"></i><b>8.1</b> Inequality and Democratization: The Debate</a></li>
<li class="chapter" data-level="8.2" data-path="ptapp.html"><a href="ptapp.html#a-structural-causal-model"><i class="fa fa-check"></i><b>8.2</b> A Structural Causal Model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ptapp.html"><a href="ptapp.html#forming-priors"><i class="fa fa-check"></i><b>8.2.1</b> Forming Priors</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptapp.html"><a href="ptapp.html#results"><i class="fa fa-check"></i><b>8.3</b> Results</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ptapp.html"><a href="ptapp.html#inferences-for-cases-with-observed-democratization"><i class="fa fa-check"></i><b>8.3.1</b> Inferences for cases with observed democratization</a></li>
<li class="chapter" data-level="8.3.2" data-path="ptapp.html"><a href="ptapp.html#cases-with-incomplete-data"><i class="fa fa-check"></i><b>8.3.2</b> Cases with incomplete data</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ptapp.html"><a href="ptapp.html#theory-dependence"><i class="fa fa-check"></i><b>8.4</b> Theory dependence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing.html"><a href="mixing.html"><i class="fa fa-check"></i><b>9</b> Integrated inferences</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mixing.html"><a href="mixing.html#sample-inference"><i class="fa fa-check"></i><b>9.1</b> Sample inference</a></li>
<li class="chapter" data-level="9.2" data-path="mixing.html"><a href="mixing.html#general-queries"><i class="fa fa-check"></i><b>9.2</b> General queries</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="mixing.html"><a href="mixing.html#set-up"><i class="fa fa-check"></i><b>9.2.1</b> Set up</a></li>
<li class="chapter" data-level="9.2.2" data-path="mixing.html"><a href="mixing.html#inference"><i class="fa fa-check"></i><b>9.2.2</b> Inference</a></li>
<li class="chapter" data-level="9.2.3" data-path="mixing.html"><a href="mixing.html#wrinkles"><i class="fa fa-check"></i><b>9.2.3</b> Wrinkles</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="mixing.html"><a href="mixing.html#mixed-methods"><i class="fa fa-check"></i><b>9.3</b> Mixed methods</a></li>
<li class="chapter" data-level="9.4" data-path="mixing.html"><a href="mixing.html#considerations"><i class="fa fa-check"></i><b>9.4</b> Considerations</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="mixing.html"><a href="mixing.html#probative-value-can-be-derived-from-a-causal-structure-plus-data"><i class="fa fa-check"></i><b>9.4.1</b> Probative value can be derived from a causal structure plus data</a></li>
<li class="chapter" data-level="9.4.2" data-path="mixing.html"><a href="mixing.html#learning-without-identification"><i class="fa fa-check"></i><b>9.4.2</b> Learning without identification</a></li>
<li class="chapter" data-level="9.4.3" data-path="mixing.html"><a href="mixing.html#beyond-binary-data"><i class="fa fa-check"></i><b>9.4.3</b> Beyond binary data</a></li>
<li class="chapter" data-level="9.4.4" data-path="mixing.html"><a href="mixing.html#measurement-error"><i class="fa fa-check"></i><b>9.4.4</b> Measurement error</a></li>
<li class="chapter" data-level="9.4.5" data-path="mixing.html"><a href="mixing.html#spillovers"><i class="fa fa-check"></i><b>9.4.5</b> Spillovers</a></li>
<li class="chapter" data-level="9.4.6" data-path="mixing.html"><a href="mixing.html#clustering"><i class="fa fa-check"></i><b>9.4.6</b> Clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mixingapp.html"><a href="mixingapp.html"><i class="fa fa-check"></i><b>10</b> Integrated Inferences Application</a>
<ul>
<li class="chapter" data-level="10.1" data-path="mixingapp.html"><a href="mixingapp.html#a-trained-model"><i class="fa fa-check"></i><b>10.1</b> A trained model</a></li>
<li class="chapter" data-level="10.2" data-path="mixingapp.html"><a href="mixingapp.html#data"><i class="fa fa-check"></i><b>10.2</b> Data</a></li>
<li class="chapter" data-level="10.3" data-path="mixingapp.html"><a href="mixingapp.html#inference-1"><i class="fa fa-check"></i><b>10.3</b> Inference</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-cause-democratization"><i class="fa fa-check"></i><b>10.3.1</b> Did inequality <em>cause</em> democratization?</a></li>
<li class="chapter" data-level="10.3.2" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-prevent-democracy"><i class="fa fa-check"></i><b>10.3.2</b> Did inequality <em>prevent</em> democracy?</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="mixingapp.html"><a href="mixingapp.html#from-cases-to-population"><i class="fa fa-check"></i><b>10.4</b> From cases to population</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="mixingapp.html"><a href="mixingapp.html#contribution-to-case-level-inference"><i class="fa fa-check"></i><b>10.4.1</b> Contribution to case-level inference</a></li>
<li class="chapter" data-level="10.4.2" data-path="mixingapp.html"><a href="mixingapp.html#how-much-do-we-get-from-the-model-vs.-the-data"><i class="fa fa-check"></i><b>10.4.2</b> How much do we get from the model vs. the data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mm.html"><a href="mm.html"><i class="fa fa-check"></i><b>11</b> Mixing models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="mm.html"><a href="mm.html#a-jigsaw-puzzle-integrating-across-a-model"><i class="fa fa-check"></i><b>11.1</b> A jigsaw puzzle: Integrating across a model</a></li>
<li class="chapter" data-level="11.2" data-path="mm.html"><a href="mm.html#combining-observational-and-experimental-data"><i class="fa fa-check"></i><b>11.2</b> Combining observational and experimental data</a></li>
<li class="chapter" data-level="11.3" data-path="mm.html"><a href="mm.html#transportation-of-findings-across-contexts"><i class="fa fa-check"></i><b>11.3</b> Transportation of findings across contexts</a></li>
<li class="chapter" data-level="11.4" data-path="mm.html"><a href="mm.html#multilevel-models-meta-analysis"><i class="fa fa-check"></i><b>11.4</b> Multilevel models, meta-analysis</a></li>
</ul></li>
<li class="part"><span><b>III Design Choices</b></span></li>
<li class="chapter" data-level="12" data-path="clue.html"><a href="clue.html"><i class="fa fa-check"></i><b>12</b> Clue Selection as a Decision Problem</a>
<ul>
<li class="chapter" data-level="12.1" data-path="clue.html"><a href="clue.html#a-model-informed-approach-to-clue-selection"><i class="fa fa-check"></i><b>12.1</b> A model-informed approach to clue-selection</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="clue.html"><a href="clue.html#clue-selection-with-a-simple-example"><i class="fa fa-check"></i><b>12.1.1</b> Clue selection with a simple example</a></li>
<li class="chapter" data-level="12.1.2" data-path="clue.html"><a href="clue.html#dependence-on-prior-beliefs"><i class="fa fa-check"></i><b>12.1.2</b> Dependence on prior beliefs</a></li>
<li class="chapter" data-level="12.1.3" data-path="clue.html"><a href="clue.html#clue-selection-for-the-democratization-model"><i class="fa fa-check"></i><b>12.1.3</b> Clue selection for the democratization model</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="clue.html"><a href="clue.html#dynamic-strategies"><i class="fa fa-check"></i><b>12.2</b> Dynamic Strategies</a></li>
<li class="chapter" data-level="12.3" data-path="clue.html"><a href="clue.html#conclusion"><i class="fa fa-check"></i><b>12.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="caseselection.html"><a href="caseselection.html"><i class="fa fa-check"></i><b>13</b> Case selection for mixed methods inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="caseselection.html"><a href="caseselection.html#case-selection-strategies"><i class="fa fa-check"></i><b>13.1</b> Case selection strategies</a></li>
<li class="chapter" data-level="13.2" data-path="caseselection.html"><a href="caseselection.html#no-general-rules"><i class="fa fa-check"></i><b>13.2</b> No general rules</a></li>
<li class="chapter" data-level="13.3" data-path="caseselection.html"><a href="caseselection.html#specific-case-walk-through"><i class="fa fa-check"></i><b>13.3</b> Specific case walk through</a></li>
<li class="chapter" data-level="13.4" data-path="caseselection.html"><a href="caseselection.html#simulation-results"><i class="fa fa-check"></i><b>13.4</b> Simulation results</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="caseselection.html"><a href="caseselection.html#models-queries-and-strategies"><i class="fa fa-check"></i><b>13.4.1</b> Models, queries, and strategies</a></li>
<li class="chapter" data-level="13.4.2" data-path="caseselection.html"><a href="caseselection.html#results-1"><i class="fa fa-check"></i><b>13.4.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="wideordeep.html"><a href="wideordeep.html"><i class="fa fa-check"></i><b>14</b> Going wide, going deep</a>
<ul>
<li class="chapter" data-level="14.1" data-path="wideordeep.html"><a href="wideordeep.html#walk-through-of-a-simple-comparison"><i class="fa fa-check"></i><b>14.1</b> Walk-through of a simple comparison</a></li>
<li class="chapter" data-level="14.2" data-path="wideordeep.html"><a href="wideordeep.html#simulation-results-1"><i class="fa fa-check"></i><b>14.2</b> Simulation results</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="wideordeep.html"><a href="wideordeep.html#approach"><i class="fa fa-check"></i><b>14.2.1</b> Approach</a></li>
<li class="chapter" data-level="14.2.2" data-path="wideordeep.html"><a href="wideordeep.html#simulation-results-2"><i class="fa fa-check"></i><b>14.2.2</b> Simulation results</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="wideordeep.html"><a href="wideordeep.html#factoring-in-the-cost-of-data"><i class="fa fa-check"></i><b>14.3</b> Factoring in the cost of data</a></li>
</ul></li>
<li class="part"><span><b>IV Models in Question</b></span></li>
<li class="chapter" data-level="15" data-path="justifying.html"><a href="justifying.html"><i class="fa fa-check"></i><b>15</b> Justifying models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="justifying.html"><a href="justifying.html#justifying-probative-value"><i class="fa fa-check"></i><b>15.1</b> Justifying probative value</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="justifying.html"><a href="justifying.html#nothing-from-nothing"><i class="fa fa-check"></i><b>15.1.1</b> Nothing from nothing</a></li>
<li class="chapter" data-level="15.1.2" data-path="justifying.html"><a href="justifying.html#justifying-the-classic-process-tracing-tests"><i class="fa fa-check"></i><b>15.1.2</b> Justifying the classic process-tracing tests</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="justifying.html"><a href="justifying.html#empirical-discovery-of-causal-structure"><i class="fa fa-check"></i><b>15.2</b> Empirical discovery of causal structure</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="evaluation.html"><a href="evaluation.html"><i class="fa fa-check"></i><b>16</b> Evaluating models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="evaluation.html"><a href="evaluation.html#four-strategies"><i class="fa fa-check"></i><b>16.1</b> Four Strategies</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="evaluation.html"><a href="evaluation.html#check-conditional-independence"><i class="fa fa-check"></i><b>16.1.1</b> Check conditional independence</a></li>
<li class="chapter" data-level="16.1.2" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value-are-the-data-unexpected-given-your-model"><i class="fa fa-check"></i><b>16.1.2</b> Bayesian <span class="math inline">\(p-\)</span>value: Are the data unexpected given your model?</a></li>
<li class="chapter" data-level="16.1.3" data-path="evaluation.html"><a href="evaluation.html#leave-one-out-loo-cross-validation"><i class="fa fa-check"></i><b>16.1.3</b> Leave-one-out (LOO) cross-validation</a></li>
<li class="chapter" data-level="16.1.4" data-path="evaluation.html"><a href="evaluation.html#sensitivity"><i class="fa fa-check"></i><b>16.1.4</b> Sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="evaluation.html"><a href="evaluation.html#evaluating-the-democracy-inequality-model"><i class="fa fa-check"></i><b>16.2</b> Evaluating the Democracy-Inequality model</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="evaluation.html"><a href="evaluation.html#check-assumptions-of-conditional-independence"><i class="fa fa-check"></i><b>16.2.1</b> Check assumptions of conditional independence</a></li>
<li class="chapter" data-level="16.2.2" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value"><i class="fa fa-check"></i><b>16.2.2</b> Bayesian <span class="math inline">\(p\)</span>-value</a></li>
<li class="chapter" data-level="16.2.3" data-path="evaluation.html"><a href="evaluation.html#loo-validation"><i class="fa fa-check"></i><b>16.2.3</b> LOO validation</a></li>
<li class="chapter" data-level="16.2.4" data-path="evaluation.html"><a href="evaluation.html#sensitivity-to-priors"><i class="fa fa-check"></i><b>16.2.4</b> Sensitivity to priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="conclusionchapter.html"><a href="conclusionchapter.html"><i class="fa fa-check"></i><b>17</b> Final Words</a>
<ul>
<li class="chapter" data-level="17.1" data-path="conclusionchapter.html"><a href="conclusionchapter.html#the-benefits"><i class="fa fa-check"></i><b>17.1</b> The benefits</a></li>
<li class="chapter" data-level="17.2" data-path="conclusionchapter.html"><a href="conclusionchapter.html#the-worries"><i class="fa fa-check"></i><b>17.2</b> The worries</a></li>
<li class="chapter" data-level="17.3" data-path="conclusionchapter.html"><a href="conclusionchapter.html#the-future"><i class="fa fa-check"></i><b>17.3</b> The future</a></li>
</ul></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li class="chapter" data-level="18" data-path="examplesappendix.html"><a href="examplesappendix.html"><i class="fa fa-check"></i><b>18</b> <code>CausalQueries</code></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/macartan/CausalQueries/" target="blank">Uses CausalQueries</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Integrated Inferences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theory" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Theories as causal models</h1>
<!--
:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
In this short conceptual chapter we describe an approach in which theoretical claims are thought of as model justifications within a  hierarchy of causal models. Lower-level models serve as a theory for a higher level model if the higher level model can be deduced from the lower level model---possibly in combination with data. Consistency requirements determine what new theory emerges from the combination of prior theory and data, but also constrain what prior theories can be invoked to justify a given claim. 
::::
-->
<p><br></p>
<!-- 6.1 Models imply models -->
<!-- 6.1.1 Structure implies structures -->
<!-- 6.1.2 Data implies updated probabilistic causal models -->
<!-- 6.2 Three uses -->
<!-- 6.2.1 Justification -->
<!-- 6.2.2 Explanation -->
<!-- 6.2.3 Integrated inferences -->
<!-- 6.3 The rules -->
<!-- 6.3.1 Not one to one: show many to 1 -->
<!-- 6.3.2 Structural consistency (both ways) -->
<!-- 6.3.3 Distributional consistency -->
<!-- FLAG: AJ GO OVER PROBABILISTIC SECTION TO CONNECT TO OTHER VERSIONS OF THEORY -->
<!-- FLAG: MH CHECK CONNECTION BETWEE EVALUATION AND JUSTIFICATION -->
<p>In Chapter <a href="illustratemodels.html#illustratemodels">3</a>, we described a set of theories and represented them as causal models. But so far we haven’t been very explicit in what we mean by a theory or how theory maps onto a causal-model framework.</p>
<p>In this book, we will think of theory as a type of <em>explanation</em>: a theory provides an account of how or under what conditions a set of causal relationships operate. We generally express both a theory and the claims being theorized as causal models. The theory is then is a model that <em>implies</em> another model—possibly with the help of some data.</p>
<p>To fix ideas: a simple claim might be that “<em>A</em> caused <em>B</em> in case <span class="math inline">\(j\)</span>.” This claim is itself a model, albeit a very simple one. The theory that supports this model might be of the form “<em>A</em> always causes <em>B</em>,” “<em>A</em> always causes <em>B</em> whenever <em>C</em> (and <em>C</em> holds in case <em>j</em>),” or “<em>A</em> invariably causes <em>B</em> and invariably <em>B</em> causes <em>C</em>.” These all have in common that they are arguments that could be provided to support the simple claim; in each case, if you believe the theory you believe the implication.</p>
<!-- We begin by elaborating upon our working definition of "theory," showing how it operates within a conception of models in "levels." This requires thinking through how a more detailed, "lower-level" causal structure can imply, and thus *explain*, a less-detailed "higher-level" structure. We also work through how the nodal and causal types in a more detailed model, and beliefs about those types, map onto types (and beliefs about types) in a simpler model. Further, we outline what kinds of elaborations of a simple model are possible (as we outlined in Chapter \@ref(models) what kinds of simplifications of models are permissible). We then turn to the relationship between theory and evidence in a causal-model framework, as compared to the way that relationship operates in a more conventional theory-testing approach.  -->
<p>The rest of this short chapter builds out this idea and uses it to provide a way of characterizing when a theory is useful or not.
In the first section, we consider multiple senses in which one model might imply, and thus serve as a <em>theory of</em>, another model. For one thing, we consider how one causal structure can imply (theorize) another causal structure, by including additional new nodes and nodal types that explain how or when causal effects in the original model will unfold. Next, we consider how the causal-type <em>ranges</em> of models can relate to one another: one model can imply another model when the former’s causal types constitute a subset of the latter’s. In this situation, the theory represents a more specific, stronger claim about the kinds of causal effects that are operating. We then turn to logical relations between probabilistic models. We show how the distributions over nodal types in a simpler model structure can be underwritten by distributions over nodal types in a more detailed model structure. Here, a claim about the prevalence (or probability) of causal effects in a causal network is justified by claims about the prevalence or probability of causal effects in a more granular rendering of that network. Finally, we show how a probabilistic model plus <em>data</em> can provide a theoretical underpinning for a new, stronger model.</p>
<p>Second, we consider how theories-as-models can be useful. In embedding theorization within the world of causal models, we ultimately have an empirical objective in mind. Theorizing a causal relationship of interest, in our framework, means elaborating our causal beliefs about the world in greater detail. As we show in later chapters, theorizing in the form of a causal model allows us to generate research designs: to identify sources of inferential leverage and to explicitly and systematically link observations of components of a causal system to the causal questions we seek to answer. In the second section of this chapter, however, we provide a high-level conceptualization of the empirical gains from theory.
<!-- We discuss toward the end of the chapter how this definition of theory relates to common understandings of theory in the social sciences.  --></p>
<p>In the chapter’s third and final section, we show how our formalization of theory maps onto <em>formal</em> theory as usually understood, showing how we can generate a causal model from a game-theoretic model.</p>
<div id="models-as-theories-of" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Models as <em>theories of</em></h2>
<p>Let us say that a causal model, <span class="math inline">\(M^\prime\)</span>, is a <em>theory of</em> <span class="math inline">\(M\)</span> if <span class="math inline">\(M\)</span> is implied by <span class="math inline">\(M^\prime\)</span>. It is a theory <em>because</em> it has implications. Otherwise it is a conclusion, an inference, or claim. A theory, <span class="math inline">\(M^\prime\)</span>, might itself sit atop—be supported by—another theory, <span class="math inline">\(M^{\prime\prime}\)</span>, that implies <span class="math inline">\(M^\prime\)</span>. To help fix the idea of theory as “supporting” or “underlying” the model(s) it theorizes, we refer to the theory, <span class="math inline">\(M^\prime\)</span>, as a <em>lower</em>-level model relative to <span class="math inline">\(M\)</span> and refer to <span class="math inline">\(M\)</span> as a <em>higher</em>-level model relative to its theorization, <span class="math inline">\(M^\prime\)</span>.<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></p>
<!-- Higher-level models can be generated from lower-level models in two ways, both of which are consistent with common understandings of what it is for a set of claims to constitute or, conversely, derive from a ''theory.'' -->
<!-- We can distill two ways in which lower-level models can relate to, or support, higher-level models: -->
<!-- ### Disaggregating nodes -->
<p>Both structural models and probabilistic models—possibly in combination with data—imply other models. We discuss each in turn.</p>
<!-- FLAG: DISCUSS UP FRON DIFFERENT WAYS WE USE THE TERM JIMPLICAITON -->
<div id="implications-of-structural-causal-models" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Implications of structural causal models</h3>
<p>Structural models can imply multiple other simpler structural models. Similarly structural models can be implied by multiple more involved models.</p>
<p>We imagine two forms of lower level model, those that involve “type splintering” and those that involve “type reduction.”</p>
<p><strong>Type splintering theorization.</strong> Theorization often involves a refinement of causal types, implemented through the addition of nodes.
Take the very simple model, <span class="math inline">\(M\)</span>, represented in Figure <a href="theory.html#fig:Highlow">6.1</a>(a). The model simply states that <span class="math inline">\(X\)</span> has (or <em>can</em> have) a causal effect on <span class="math inline">\(Y\)</span>.</p>
<p>What theories might justify <span class="math inline">\(M\)</span>? This question can be rephrased as “what models imply model <span class="math inline">\(M\)</span>?” The figure points to two possibilities. Both models <span class="math inline">\(M^\prime\)</span> and <span class="math inline">\(M^{\prime\prime}\)</span> imply model <span class="math inline">\(M\)</span>. They can be thought of as <em>theories</em>, or lower-level model, of <span class="math inline">\(M\)</span>.</p>
<p>Model <span class="math inline">\(M^\prime\)</span> differs by the addition of a node, <span class="math inline">\(K\)</span>, in the causal chain between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We can say that <span class="math inline">\(M^\prime\)</span> is a <em>theory</em> of <span class="math inline">\(M\)</span> for two reasons. First it provides a <em>justification</em>—if you believe <span class="math inline">\(M^\prime\)</span> you should believe <span class="math inline">\(M\)</span>: if <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> through <span class="math inline">\(K\)</span>, then <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>. But as well as a justification it also provides an <em>explanation</em> of <span class="math inline">\(M\)</span>. Suppose we already <em>know</em> that <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> but want to know <em>why</em>. If we ask, “why does <span class="math inline">\(X\)</span> affect <span class="math inline">\(Y\)</span>?” <span class="math inline">\(M^\prime\)</span> provides an answer: <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> <em>because</em> <span class="math inline">\(X\)</span> affects <span class="math inline">\(K\)</span>, and <span class="math inline">\(K\)</span> affects <span class="math inline">\(Y\)</span>.</p>
<p>Model <span class="math inline">\(M^{\prime\prime}\)</span> differs by the addition of a node, <span class="math inline">\(C\)</span>, that moderates the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. <span class="math inline">\(M^{\prime\prime}\)</span> justifies <span class="math inline">\(M\)</span> in the sense that if you believe <span class="math inline">\(M^{\prime\prime}\)</span> you should believe <span class="math inline">\(M\)</span>. It provides an explanation of a kind also: if you believe model <span class="math inline">\(M^{\prime\prime}\)</span> you likely believe that the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is what it is because of <span class="math inline">\(C\)</span>. Had <span class="math inline">\(C\)</span> been different the causal relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> might have been also.</p>
<!--
:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
**Two kinds of type-splintering theories.**

Theories are “lower-level” causal models that explain or provide an account of a “higher-level”, simpler model. Two common forms of theorization involve articulating  *mediating* and *moderating* relationships:

  * *Mediation*: A mediator, $M$, can be introduced between $X$ and $Y$, thus splintering $\theta^Y$ into $\theta^M$ and $\theta^{Y_\text{lower}}$. The mediation theory thus explains the $X \rightarrow Y$ relationship.
  * *Moderation*: A component of $\theta^Y$ can be extracted and specified as a substantive variable. This variable is now a substantively conceptualized moderator of the $X \rightarrow Y$ relationship. The moderation theory thus provides a fuller explanation of why $X$ has different effects on $Y$ in different contexts.

::::
-->
<p><br></p>
<p>A key idea is that both <span class="math inline">\(M&#39;\)</span> and <span class="math inline">\(M&#39;&#39;\)</span> involve a redefinition of <span class="math inline">\(\theta^Y\)</span>. That is we see a change in the endogenous nodes but these in turn imply a change in the interpretation of the exogenous nodes pointing into existing endogenous nodes (such as <span class="math inline">\(Y\)</span> in this example). We can think of part of <span class="math inline">\(\theta^Y\)</span> being splintered off and captured by <span class="math inline">\(\theta^K\)</span> or <span class="math inline">\(C\)</span>.</p>
<p>Return to models <span class="math inline">\(M\)</span> and <span class="math inline">\(M&#39;\)</span> in Figure <a href="theory.html#fig:Highlow">6.1</a>(a).
Importantly, in moving from the higher- to the lower-level model, we have effectively <em>split</em> the nodal-type term <span class="math inline">\(\theta^Y\)</span> into two parts: <span class="math inline">\(\theta^{Y_\text{lower}}\)</span> and <span class="math inline">\(\theta^K\)</span>. Intuitively, in the higher-level model, (a), <span class="math inline">\(Y\)</span> is a function of <span class="math inline">\(X\)</span> and <span class="math inline">\(\theta^Y\)</span>, the latter representing all things other than <span class="math inline">\(X\)</span> than can affect <span class="math inline">\(Y\)</span>. Or, in the language of our nodal-type setup, <span class="math inline">\(\theta^Y\)</span> represents all of the (unspecified) sources of variation in <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span>. When we insert <span class="math inline">\(K\)</span> into the model, however, <span class="math inline">\(X\)</span> now does not directly affect <span class="math inline">\(Y\)</span> but only does so via <span class="math inline">\(K\)</span>. Further, we model <span class="math inline">\(X\)</span> as acting on <span class="math inline">\(K\)</span> in a manner conditioned by <span class="math inline">\(\theta^K\)</span>, which represents all of the (unspecified) factors determining <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(K\)</span>. The key thing to notice here is that <span class="math inline">\(\theta^K\)</span> now represents <em>a portion of the variance that <span class="math inline">\(\theta^Y\)</span> represented in the higher-level graph</em>: some of the variation in <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span> now arises from variation in <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(K\)</span>, which is captured by <span class="math inline">\(\theta^K\)</span>. So, for instance, <span class="math inline">\(X\)</span> might have no effect on <span class="math inline">\(Y\)</span> because <span class="math inline">\(\theta^K\)</span> takes on the value <span class="math inline">\(\theta^K_{00}\)</span>, so that <span class="math inline">\(X\)</span> has no effect on <span class="math inline">\(K\)</span>. Put differently, any effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> must arise from an effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(K\)</span>; so <span class="math inline">\(\theta^K\)</span>’s value must be either <span class="math inline">\(\theta^K_{01}\)</span> or <span class="math inline">\(\theta^K_{10}\)</span> for <span class="math inline">\(X\)</span> to affect <span class="math inline">\(Y\)</span>.<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a> What <span class="math inline">\(\theta^K\)</span> represents, then, is that part of the original <span class="math inline">\(\theta^Y\)</span> that arose from some force other than <span class="math inline">\(X\)</span> operating at the <em>first</em> step of the causal chain from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>.
So now, <span class="math inline">\(\theta^Y\)</span> in the lower-level graph is not quite the same entity as it was in the higher-level graph. In the original graph, <span class="math inline">\(\theta^Y\)</span> represented <em>all</em> sources of variation in <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span>. In the lower-level model, with <span class="math inline">\(K\)</span> as mediator, <span class="math inline">\(\theta^Y\)</span> represents only the variation in <span class="math inline">\(K\)</span>’s effect on <span class="math inline">\(Y\)</span>. Put differently, <span class="math inline">\(\theta^Y\)</span> has been expunged of any factors shaping the first stage of the causal process, which now reside in <span class="math inline">\(\theta^K\)</span>. We highlight this change in <span class="math inline">\(\theta^Y\)</span>’s meaning by referring in the second model to <span class="math inline">\(\theta^{Y_\text{lower}}\)</span>.</p>
<!-- Put differently, when we construct the lower-level model in (b), we are taking that part of $Y$ not determined by $X$ and splintering it in two: a non-$X$ input into $K$ and a non-$K$ (and thus also non-$X$) input into $Y$. -->
<p>Consider next model <span class="math inline">\(M^{\prime\prime}\)</span> panel (c) in Figure <a href="theory.html#fig:Highlow">6.1</a>, which also supports (implies) the higher-level model in panel <span class="math inline">\((a)\)</span>. The logical relationship between models <span class="math inline">\((a)\)</span> and <span class="math inline">\((c)\)</span>, however, is somewhat different. Here the lower-level model <em>specifies</em> one of the conditions that comprised <span class="math inline">\(\theta^Y\)</span> in the higher-level model. In specifying a moderator, <span class="math inline">\(C\)</span>, we have extracted <span class="math inline">\(C\)</span> from <span class="math inline">\(\theta^Y\)</span>, leaving <span class="math inline">\(\theta^{Y_\text{lower}}\)</span> to represent all factors <em>other than <span class="math inline">\(C\)</span></em> that condition <span class="math inline">\(Y\)</span>’s response to its parents. More precisely, <span class="math inline">\(\theta^{Y_\text{lower}}\)</span> now represents the set of nodal types defining how <span class="math inline">\(Y\)</span> responds jointly to <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span>. Again, the relabeling as <span class="math inline">\(\theta^{Y_\text{lower}}\)</span> reflects this change in the term’s meaning. Whereas in Model <span class="math inline">\(M^{\prime}\)</span> we have extracted <span class="math inline">\(\theta^K\)</span> from <span class="math inline">\(\theta^Y\)</span>, in Model <span class="math inline">\(M^{\prime\prime}\)</span>, it is <span class="math inline">\(C\)</span> itself that we have extracted from <span class="math inline">\(\theta^Y\)</span>, substantively specifying what had been just a random disturbance.</p>
<!-- We return to the quantitative implications of this redefinition of endogeneous nodes below.  -->
<!-- In model $M$, $\theta^Y$ captures all the features of units that result in them responding in one or another way to $X$. However in model $M'$, $\theta^{Y_{lower}}$  only needs to capture the way that $Y$ responds to $K$ --- that part of the effect of $X$ on $Y$ that operates via $X$'s impact on $K$ can be partitioned off. in model $M''$, $\theta^{Y_{lower}}$  only needs to capture the way that $Y$ responds to $X$  goven $K$, uncertainty about the value of $K$ can be partialed out.  -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Highlow"></span>
<img src="ii_files/figure-html/Highlow-1.png" alt="Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes." width="80%" />
<p class="caption">
Figure 6.1: Here we represent the simple claim that one variable causes another, and two theories — lower-level models — that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes.
</p>
</div>
<p><strong>Type-reducing theorization.</strong> There is a second way in which we might imagine a model being implied by another model that does not involve a change in nodes. Let <span class="math inline">\(\Theta(\mathcal M_1)\)</span> denote the set of causal types in model <span class="math inline">\(\mathcal M_1\)</span>. Then we can say that <span class="math inline">\(\mathcal M_0\)</span> implies <span class="math inline">\(\mathcal M_1\)</span> if <span class="math inline">\(\Theta(\mathcal M_0)\subseteq \Theta(\mathcal M_1)\)</span>. Informally this means that any relation admitted by theory <span class="math inline">\(\mathcal M_0\)</span> is representable in model <span class="math inline">\(\mathcal M_1\)</span>, though the converse may not be true. We can think of a theory of <span class="math inline">\(\mathcal M_1\)</span> as a <em>restriction</em> of ranges of <span class="math inline">\(\Theta(M_1)\)</span>.</p>
<p>We illustrate the idea in Figure <a href="theory.html#fig:Highlowreduce">6.2</a>. In panel (a) of the figure, we have a model, <span class="math inline">\(\mathcal M_1\)</span> in which <span class="math inline">\(Z\)</span> can have both a direct and an indirect effect (via <span class="math inline">\(X\)</span>) on <span class="math inline">\(Y\)</span>. Suppose that we believed that <span class="math inline">\(\mathcal M_1\)</span> was technically true but overly permissive, in the sense that it allowed for causal relations that we do not in fact believe are operating. We might believe, for instance, that <span class="math inline">\(Z\)</span> has no direct effect on <span class="math inline">\(Y\)</span> and that <span class="math inline">\(Z\)</span> has no negative effects on <span class="math inline">\(X\)</span> — the beliefs we would need to hold to treat <span class="math inline">\(Z\)</span> as an instrument for <span class="math inline">\(X\)</span>. We could thus write down a lower-level model, <span class="math inline">\(\mathcal M_0\)</span>,in which we have <em>reduced</em> the type space accordingly. Specifically, in <span class="math inline">\(\mathcal M_0\)</span>, we would restrict the nodal types at <span class="math inline">\(Y\)</span> to only the <span class="math inline">\(\theta^Y_{0000}\)</span>, <span class="math inline">\(\theta^Y_{1100}\)</span>, <span class="math inline">\(\theta^Y_{0011}\)</span>, and <span class="math inline">\(\theta^Y_{1111}\)</span>; and we would reduce the nodal types at <span class="math inline">\(X\)</span> to <span class="math inline">\(\theta^X_{00}\)</span>, <span class="math inline">\(\theta^X_{01}\)</span>, and <span class="math inline">\(\theta^X_{11}\)</span>. In panel (b), we (somewhat loosely) represent <span class="math inline">\(\mathcal M_0\)</span>. We have now eliminated the arrow from <span class="math inline">\(Z\)</span> to <span class="math inline">\(Y\)</span> to represent the dropping of all nodal types involving a direct effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span>; not pictured is the montonicity assumption at <span class="math inline">\(X\)</span>. However, we have relabeled the nodal-type nodes for both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to represent the fact that these are different objects from the nodal type nodes in the higher-level model.<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a></p>
<p>Thus, while we can theorize by adding substantive nodes to a model and thus splitting types, we can also theorize by maintaining existing nodes but constraining relations among them. In both forms of theorization, we start with a model that allows for a broad, and possibly unknown, range of possibilities: for instance, a broad range of paths through which or conditions under which <span class="math inline">\(X\)</span> might affect <span class="math inline">\(Y\)</span> or a broad range of causal effects operating at each node. Theorization of both forms then involves making a <em>stronger</em> claim: for instance, a claim about <em>how</em> or <em>when</em> <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> (via type-splintering) or a claim about the <em>particular</em> causal effects operating at a given node (via type-reduction). In both forms of theory, believing the stronger claim in the lower-level model implies believing the weaker claim in the higher-level model. Further, both modes of theorization also map nicely onto common ways in which we think about theory-development in the social sciences: we theorize mechanisms, sources of heterogeneity, and directions of effects (starting with a belief that <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>, for instance, and moving to a more constrained belief about whether that effect is positive or negative).</p>
<p>Finally, as we speak to below, theorization of both forms can generate gains for causal inference, by allowing us to use data in ways that we are unable to use it in the higher-level model.</p>
<p><strong>Preserving (conditional) independencies</strong></p>
<p>Not all potential mappings from higher- to lower-levels are permitted. In particular, when theorizing, we may <em>add</em> but may not <em>remove</em> independencies implied by the original model. If two variables are independent— or conditionally independent given a third variable—in one model, then this same relation of independence (or conditional independence) must be captured in any theory of that model. For instance, if we start with a model of the form <span class="math inline">\(X \rightarrow Y \leftarrow W\)</span>, where <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span> are independent, we could not theorize this model by adding an arrow from <span class="math inline">\(X\)</span> to <span class="math inline">\(W\)</span>. A theory can have <em>additional</em> conditional independencies not present in the higher-level model, as in the example in Figure <a href="theory.html#fig:Highlowreduce">6.2</a>. But we may not theorize <em>away</em> (conditional) independencies insisted on by our higher-level claim.</p>
<p>This is a key part of what it means for the lower-level model to <em>justify</em> the higher-level model. A model makes claims about what is (conditionally) independent of what. The claims about conditional independence implied by the higher-level model must therefore be warranted by (conditional) independencies operating in the lower-level model. If we introduce new dependencies via theorization, then our higher-level model (which excludes these dependencies) would no longer be justified by the lower-level model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Highlowreduce"></span>
<img src="ii_files/figure-html/Highlowreduce-1.png" alt="Here we represent theorization via type-reduction. Though we show the removal of an arrow to help convey the idea, we would in fact reduce types by imposing restrictions on the nodal types at Y within the same DAG." width="80%" />
<p class="caption">
Figure 6.2: Here we represent theorization via type-reduction. Though we show the removal of an arrow to help convey the idea, we would in fact reduce types by imposing restrictions on the nodal types at Y within the same DAG.
</p>
</div>
</div>
<div id="probabilistic-causal-models" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Probabilistic causal models</h3>
<p>At the structural level, then, there are two types of theory, or two types of relations between levels of model: those defined by type-splintering and those defined by type-reduction. In general, we will want to be working with probabilistic causal models — i.e., those that include distributions over nodal types. We can describe straightforwardly how distributions in a higher-level model relate to — and must change with — distributions at the lower level. Indeed, it is these relations that unlock the opportunity for reaping empirical gains from theory.</p>
<p><strong>Theoretical implications of probabilistic models.</strong> Suppose we start with the mediation model in panel (b) of Figure <a href="theory.html#fig:Highlow">6.1</a>. Wen then add to it a distribution over <span class="math inline">\(\theta^K\)</span> and <span class="math inline">\(\theta^Y_{lower}\)</span>, giving us a probabilistic causal model that we will denote <span class="math inline">\(\mathcal M^p_{lower}\)</span>. <span class="math inline">\(\mathcal M^p_{lower}\)</span>, in turn, implies a higher-level probabilistic model, <span class="math inline">\(\mathcal M^p_{higher}\)</span>, formed from the structure of Model (a) in Figure <a href="theory.html#fig:Highlow">6.1</a>, and a <em>particular</em> distribution over <span class="math inline">\(\theta^Y\)</span>: specifically, <span class="math inline">\(\theta^Y\)</span> will have the distribution that preserves the causal relations implied by the beliefs in <span class="math inline">\(\mathcal M^p_{lower}\)</span>. Thus, for instance, the probability that <span class="math inline">\(X\)</span> has a positive effect on <span class="math inline">\(Y\)</span> in <span class="math inline">\(\mathcal M^p_{higher}\)</span> is <span class="math inline">\(\theta^{Y}_{01}\)</span>; the probability that <span class="math inline">\(X\)</span> has a positive effect on <span class="math inline">\(Y\)</span> in <span class="math inline">\(\mathcal M^p_{lower}\)</span> is <span class="math inline">\(\theta^{K_{lower}}_{01}\theta^{Y_{lower}}_{01} + \theta^{K_{lower}}_{10}\theta^{Y_{lower}}_{10}\)</span>. Consistency then requires that <span class="math inline">\(\theta^{M_{lower}}_{01}\theta^{Y_{lower}}_{01} + \theta^{K_{lower}}_{10}\theta^{Y_{lower}}_{10} = \theta^{Y_{higher}}_{01}\)</span>. So the value of <span class="math inline">\(\theta^{Y_{higher}}_{01}\)</span> is <em>implied</em> by <span class="math inline">\(\theta^{K_{lower}}_{01},\theta^{Y_{lower}}_{01}, \theta^{K_{lower}}_{10},\theta^{Y_{lower}}_{10}\)</span>, but not vice-versa.</p>
<p><strong>Deducing models from theory and data.</strong> Now we can see what happens if we bring data to the lower-level model. A probabilistic causal model coupled with data implies another probabilistic causal model via Bayes rule. For this reason, we can fruitfully think of an initial model as being a <em>theory of</em> an updated model, coupled with data. Thought of in this way we have clarity over what is meant when we turn to theory to support a claim, but also what is meant when we seek to justify a theory. We might imagine a scholar arguing: “<span class="math inline">\(\mathcal M_1\)</span>: <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in country <span class="math inline">\(j\)</span>.” When pushed for a justification for the claim they provide the lower level model: “<span class="math inline">\(\mathcal M_0:\)</span> the average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> in countries with feature <span class="math inline">\(C=1\)</span> is 0.95, making it likely that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in this case.” Here <span class="math inline">\(\mathcal M_1\)</span> is implied by <span class="math inline">\(\mathcal M_0\)</span> coupled with data <span class="math inline">\(C=1\)</span>. If pushed further as to why that theory is itself credible they might point to a lower level model consisting of structural model <span class="math inline">\(X\rightarrow Y \leftarrow C\)</span> plus flat priors coupled with data on <span class="math inline">\(X,Y\)</span> and <span class="math inline">\(C\)</span>. At each stage, as more justification is provided, the researcher formally provides lower-level models.</p>
<p>Moving up, as more data is provided, more “specific” higher level models emerge, justified by lower models plus data. These models are more specific in the sense that they are implied by the higher level models, plus data, but not vice versa. But they are also (generally) more specific in a second sense: that they make stronger statements about how causal processes operate.<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a> They place greater weight on a smaller set of causal claims.<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a></p>
<!-- FLAG:  -- see use of specific here -->
<p>As the simplest illustration, we might imagine beginning with an <span class="math inline">\(X\rightarrow Y\)</span> model, <span class="math inline">\(\mathcal M_1\)</span>, in which, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are binary, and we we believe that <span class="math inline">\(Y\)</span> possibly responds to <span class="math inline">\(X\)</span>. If we have “flat” priors over causal types, in the sense described in Chapter <a href="bayeschapter.html#bayeschapter">5</a>, then our prior uncertainty over the proposition that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, under this model, is large; as is our uncertainty that <span class="math inline">\(Y=1\)</span> is due to <span class="math inline">\(X=1\)</span> in a given case. In other words, given our theory, we are uncertain about the proposition <span class="math inline">\(\mathcal M_3\)</span>: <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span>. However, if we then receive a lot of data, <span class="math inline">\(\mathcal D\)</span>, showing strong relations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then our updated model <span class="math inline">\(\mathcal M_2\)</span>, formed from combining <span class="math inline">\(\mathcal D\)</span> and <span class="math inline">\(\mathcal M_1\)</span> allows us to infer that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in this case with greater certainty.</p>
<p>Thus our new theory <span class="math inline">\(\mathcal M_2\)</span> is (a) formally similar to <span class="math inline">\(\mathcal M_1\)</span>, (b) formed as a product of past theory plus evidence, here justified by <span class="math inline">\(\mathcal M_1\)</span> given data <span class="math inline">\(\mathcal D\)</span>, and (c) capable of providing sharper implications than past theory.<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a></p>
<p>In this way, Bayesian updating provides a simple and coherent way of thinking about the integration of theory and data.</p>
<!-- ### Generalizing a model -->
<!-- A lower-level model, $M^\prime$, can also be a representation of $M$ in which a node has been introduced that permits variation in a feature of context that is fixed and taken-for-granted in $M$. Here, $M^\prime$ theorizes $M$ in the sense of embedding $M$ within a *more general* set of beliefs about how the world works. $M$ then becomes a special case of the theorized relations, one that holds when we condition on some data, specifying some particularity of context. -->
<!-- To illustrate this approach to theorization, consider again graphs (a) and (c) in Figure \ref{fig:Highlow}. We have discussed how the graph in panel (c) can represent a disaggregation of $\theta^Y$ from panel (a) into $\theta^{Y_\text{lower}}$ and $C$. An alternative possibility, however, is to employ a moderation model that represents a more general claim than the higher-level model that it supports. For instance, the graph in panel (a) might represent the causal function $Y=X+\theta^Y$. In this model, $X$ always has an effect on $Y$. The graph in panel (c), in turn, might represent the more general function, $Y=XC+\theta^Y$ (where $C$ is binary). Now, whether $X$ has an effect depends on the value of $C$. In particular, model (c) combined with the observation $C=1$ directly implies model (a). Model (a) is a special case of model (c) that holds in and only in the context $C=1$. In answer to the question, "Why do you believe model (a)?" one could respond with model (c) plus the observation $C=1$.  -->
<!-- Note the difference in how theorization has proceeded for the two moderation models (both graphically represented in panel (c)). When we introduce moderation via the disaggregation of nodes, we pull content out of $\theta^Y$ and specify it substantively. When we generalize, on the other hand, we *add* a node, a source of variation not factored into the original model at all. We can think of that variation as implicitly conditioned-on in the higher-level model. -->
<!-- As a secondary matter, note that the $\theta$ term pointing into $Y$ may or may not be altered by the addition. In particular, we will explore later, if $\theta^Y$ is a vessel for causal types, then the number of possible causal types---and, thus, $\theta^Y$'s range---must expand as we add nodes pointing into $Y$. But in the functions we have used in our illustration, $\theta^{Y_\text{lower}}$ remains unchanged when we add the $C$ node. In both the higher- and lower-level models, $\theta^Y$ represents precisely the same random disturbance.  -->
<!-- $M^\prime$ is a *theory* of $M$ in that it, in a sense, helps explain the dependencies of $Y$ on $X$ more fully than does $M$. -->
<!-- Since lower-level models imply higher level models, we think of theories as implying the models they are theorizing. If we believe structural causal model $M'$, then we also must believe the structure of model $M$. If it is possible that $X$ can affect $K$ and possible that $K$ can affect $Y$, then it is possible that $X$ can affect $Y$. Similarly, if we believe Model $M^{\prime\prime}$, then we must also believe Model $M$: if it is true that $X$ can affect $Y$, possibly in ways that are moderated by $C$, then it is trivially true that $X$ can affect $Y$.  -->
<!-- AJ: I do not understand this line that was in the paragraph above: "The converse is not true, however. It is not possible to still believe that $X$ can effect $Y$ if you do not think that $X$ can affect $K$." Of course, it's possible, if you don't believe M' is the right lower-level model, right? I think I'm missing what we're getting at here. -->
<!-- ##  Consistency  -->
<!-- Given model $M_1$, what kinds of lower level models can serve as theories for $M_1$? -->
<!-- In general the mappings between higher-level claims and theories may not be one-to-one. A single theory can support multiple higher-level models. Moreover, a single higher-level relation can be supported by multiple, possibly incompatible lower-level theories.  -->
<!-- To illustrate, consider two "lower level" theories of democratization: -->
<!-- * ($L_1$): $Inequality \rightarrow Democratization  \leftarrow Mobilization$   -->
<!-- * ($L_2$): $Inequality \rightarrow Mobilization \rightarrow Democratization$ -->
<!-- Note how these theories are incompatible with one another. While  $Inequality$ and $Democratization$ are independent in $L_1$, they are causally related in $L_2$. Moreover, in $L_2$, $Inequality$ and $Democratization$ are related only through $Mobilization$, while in $L_1$, $Democratization$ is directly affected by $Inequality$.^[Put differently, these two theories record different relations of conditional independence: in $L_1$, $Inequality$ and $Mobilization$ are unconditionally independent, but they are not unconditionally independent in $L_2$. Also, in $L_2$, $Inequality$ is independent of $Democratization$ conditional on $Mobilization$; but this is not the case in $L_1$.] -->
<!-- Now, consider the following three higher-level claims: -->
<!-- * ($H_1$): $Inequality \rightarrow Mobilization$ -->
<!-- * ($H_2$): $Inequality \rightarrow Democratization$ -->
<!-- * ($H_3$): $Mobilization \rightarrow Democratization$ -->
<!-- $H_1$ can be supported only by one of these theories: only in $L_2$, and not in $L_1$, does $Inequality$ cause $Mobilization$.^[In addition, a *conditional* higher-level model $((Inequality \rightarrow Democratization)|Mobilization=1)$ can be supported by model $L_1$ but not by model $L_2$, where holding $Mobilization$ constant would sever the dependence of $Democratization$ on $Inequality$.] -->
<!-- However, our other two hypotheses could each rest on *both* of the lower-level theories, even though those two theories are incompatible with one another. $H_1$ could be derived from (explained by) either theory: though the two theories differ on whether mobilization is a mediator or a moderator, they agree that inequality can affect democratization. Similarly, both theories imply $H_2$, in which $Mobilization$ affects $Democratization$, even though the two theories disagree on whether inequality is an antecedent to mobilization or a moderator of its effect. -->
<!-- Thus, multiple theories can usually be proposed to explain any given causal effect, and those theories need not be consistent with *each other*.  When seeking an explanation for, say, $H_1$, the choice between $L_1$ and $L_2$ cannot be dictated by logical mappings between models; it must be drawn from a substantive belief about which set of causal dependencies operates in the world. On the other hand, $L_2$ *is* logically ruled out as an explanation of $H_3$.  -->
<!-- It is also true that any given theory logically implies multiple *higher-level* claims about causal relations. $L_2$ implies both $H_2$ and $H_3$.  -->
<!-- Note, however, that the multiple higher-level claims that follow from a theory *must* be compatible with one another.  -->
<!-- What, more precisely are these consistency requirements? -->
<!-- So far we have been discussing the *structural* components of theories: we have seen how a given causal structure can be justified in terms of a more detailed causal structure. But theories also involve claims about what *kinds of effects* operate between variables. For instance, the belief that inequality *generates* democratization, of course, represents a different kind of theory from the belief that inequality *prevents* democratization, although the two might share a DAG structure. As discussed in Chapter \@ref(models), a *probabilistic* causal model comprises both a causal structure and beliefs about distributions over the exogenous nodes --- i.e., in our setup, over a model's nodal types. In an $I \rightarrow D$ model, for instance, probabilistic beliefs include beliefs about the probability that a given case has the type $\theta^D_{01}$ types and the probability that it has the type $\theta^D_{10}$. Thus, as we have with structure, we can think about how a set of beliefs about the distribution of nodal types at a higher level can be supported by beliefs about nodal-type distributions in a lower-level model. -->
<!-- ### Elaboration -->
<!-- Theorization starts with the proliferation of substantive variables---adding beliefs about intervening steps in a causal process. But, critically, it also involves an accompanying disaggregation of unexplained variation. Addition and splintering thus go hand-in-hand: the *insertion* of a mediator between $X$ and $Y$ also involves the *splintering* of $Y$'s type node ($\theta_Y$).    -->
<!-- Other possible ways of elaborating a modelincluding adding *antecedent conditions*---causes of nodes that were exogenous in the higher-level model----and adding *downstream effects*: outcomes of nodes that were terminal in the higher-level model. -->
<!-- ```{r incompat, echo = FALSE, fig.width = 11, fig.height = 7, fig.align="center", out.width='80%', fig.cap = "A higher-level model and a lower-level model that is impermissible."} -->
<!-- par(mfrow = c(2,1)) -->
<!-- par(mar=c(1.5,1.5,3.5,1.5)) -->
<!-- hj_dag(x = c(1,2,1.5,2), -->
<!--        y = c(1,1,1,2), -->
<!--        names = c( -->
<!--          expression(paste(Inequality)), -->
<!--          expression(paste("Democratization")), -->
<!--          expression(paste("Mobilization")), -->
<!--          expression(paste(theta^D[higher]))), -->
<!--        arcs = cbind( c(1,3,4), -->
<!--                      c(3,2,2)), -->
<!--        title = "(a) Higher-level model", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .1 -->
<!-- ) -->
<!-- hj_dag(x = c(1,2,1.5, 2, 1.5), -->
<!--        y = c(1,1,1, 2, 1.5), -->
<!--        names = c( -->
<!--          expression(paste(Inequality)), -->
<!--          expression(paste("Democratization")), -->
<!--          expression(paste("Mobilization")), -->
<!--          expression(paste(theta^D[lower])), -->
<!--          expression(paste("Ethnic homogeneity")) -->
<!--          ), -->
<!--        arcs = cbind( c(1,3, 4, 5, 5), -->
<!--                      c(3,2, 2, 1, 2)), -->
<!--        title = "(b) An incompatible lower-level model", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .1 -->
<!-- ) -->
<!-- ``` -->
<!-- The central principle governing allowable elaborations is that a lower-level model *must not introduce dependencies between variables that were omitted in the higher-level model.* We provide an example of a violation of this principle in Figure \@ref(fig:incompat).  -->
<!-- We start with a higher-level model, in panel (a), in which inequality affects democratization through mobilization. We then elaborate the model in panel (b) by adding ethnic homogeneity as a moderator of mobilization's effect. However, because ethnic homogeneity is also modeled here as affecting inequality, we have now introduced a source of dependence between inequality and democratization that was omitted from the higher-level model. In panel (a), democratization and inequality were dependent only via mobilization; and so they are conditionally independent given mobilization. In panel (b), democratization and mobilization are additionally dependent via their common cause, ethnic homogeneity. By the rules governing causal graphs (see Chapter \ref{models}), the higher-level model specifically *prohibited* this second source of dependency---since all dependencies between variables must be represented.  -->
<!-- TEXT -->
<!-- We can also read Figure \@ref(fig:runningsubs) as telling us the set of claims for which the lower-level graph in Figure \ref{fig:running} can serve as a theory. As we can see, the range of claims that a moderately complex model can theorize is vast. For each simpler claim, moreover, there may be other possible lower-level graphs---theories besides ---consistent with it. -->
<!-- Nodes with no parents in $\mathcal{U}\cup\mathcal{V}$ cannot be eliminated as this would entail a loss of information. The graph in Figure \ref{fig:K}(d) illustrates the importance of this. Here $K$ is a cause of both $X$ and $Y$, in other words it is a possible confounder. A higher-level graph that does not include $K$ still requires a $U_K$ node pointing into both $K$ and $Y$ to capture the fact that there is a confounder. -->
<!-- ^[The conditioning approach can also handle theoretical propositions in the form of structural causal models that make no immediate empirical claims but still have "empirical content" in the sense of being able to inform *conditional* claims. The claim "if $X$ then $Y$" says nothing about $P(Y)$ by itself. However, it says a lot about $P(Y)$ if $P(X)$ is known.] -->
<!-- One  effect of elimination is to render seemingly deterministic relations effectively probabilistic. For example, in the lower level graph $C$ is a deterministic function of $X$ and $S$. But in higher level graphs it can depend probabilistically on one of these: in submodel 21, $C$  depends probabilistically on  $X$ since $S$ is now a stochastic disturbance; in 34 $C$ depends probabilistically on $S$. This illustrates how unobserved or unidentified  features render a model "as-if" stochastic. Conversely, models that exclude this form of uncertainty implicitly claim model-completeness. -->
<!-- Consider a second manner in which a higher level model  can be deduced from a lower level model, this time in conjunction with data (or more broadly, ancillary claims): -->
<!-- 2. A higher level model may be formed by conditioning on values of nodes in a lower level model. Conversely, a higher-level functional model, $M$, can be theorized via a lower-level $M^\prime$ in which conditions shaping the operation of the causal effect in $M$, unspecified in $M$, are now specified. -->
<!-- To illustrate this approach, consider again the graphs in Figure \ref{fig:K}. Above we described how the graph in  panel (a) can be produced by aggregating $U_Y^{\text{lower}}$ and $U_K$ from panel (c). An alternative possibility is to simplify by conditioning: we derive a higher-level graph from $M^\prime$ by fixing the value of $K$. For instance, if $Y=XK+U_Y^{\text{lower}}$ in $M'$, then at $K=1$, we have the submodel $M_k$ in which $Y=X+U_Y^{\text{lower}}$. Note that, in generating a submodel by conditioning on $K$, we retain the term $U_Y^{\text{lower}}$ as we have not added causal force into $Y$'s unspecified parent. -->
<!-- Perhaps surprisingly, in this treatment, the theoretical support for a causal model is itself just another causal model: a set of beliefs about structural relations between variables. Thus, a theory is an object that is formally similar to an empirical claim.  -->
<!-- Would like to clarify last sentence above. -->
<!-- This next paragraph is hard to follow. The u's come out of nowhere. What's a mapping from R1 to R1? Generally seems like the points could be made more simply. -->
<!-- I THINK THE UNIVERSALITY AND PRECISION MATERIAL IS INTERESTING BUT NOT SOMETHING WE NEED TO DO ON OUR ROAD TO USING CAUSAL MODELS FOR CAUSAL INFERENCE. AND IT'S NOT EASY. SO I SUGGEST WE CUT.

We can, however, use the approach to assessment of two features sometimes considered important to assess empirical content of a theory: the level of *universality* of a theory and the degree of *precision* of a theory [@popper2005logic, @glockner2011empirical]. ***DEFINE THESE HERE.*** For instance, consider a theory over $X_1, X_2, A, B, Y$ that specified $X_1, X_2 \rightarrow Y \leftarrow A, B, g$ with functional equations: -->
<!-- $$Y = \left\{ \begin{array}{ccc}  -->
<!-- A + BX_1 & \text{ if } & X_2 = 1\\    -->
<!--   g(X_1) &\text{ if } & X_2 = 0 \end{array} \right.$$  -->
<!-- where the domain of $g$, $\mathcal{R}(g)$, is the set of all functions that map from $\mathbb{R}^1$ to $\mathbb{R}^1$, and the ranges of $A$ and $B$ are the real number line. Say the distributions over $A, B, X_1, X_2$, and  $g$ are not specified. Then the theory makes a precise claim conditional on $u_1, u_2, X_1, X_2$, and  $g$. But since the distribution over $\mathcal{R}(g)$ is not provided by the theory, the theory only claims knowledge of a functional form for $Y$ for those cases in which $X_2=1$. Thus in this case the *universality* of the theory for the claim "$Y$ is a linear function of $X$," is  $P(X_2=1)$. This is the domain over which the theory has something to say about this proposition. Note that in this case the universality is not  provided by the theory, but is rather an external proposition that depends on additional data. The *precision* of the theory depends both on the claim of interest and the distribution of root variables. For example, the precision of the theory for the causal effect of $X_1$ on $Y$ when $X_2=1$ depends on the distribution of $B$: the theory is more precise about this causal effect the less uncertainty there is about the value of $B$. Moreover, a theory that specified that $B$ has large variance would be making a precise claim about causal *heterogeneity*, even if it was imprecise about the causal effect. Again this feature cannot be read from the theory without access to ancillary information that the theory itself does not provide. -->
<!-- AJ comments: -->
<!-- - Not clear what the "this approach" is that this is illustrating a feature of. Expressing theory as structural equations?  -->
<!-- - There seems to be a more specific point here than just that we can assess universality and precision. In fact, it's actually that we *can't* assess these things purely from structural models. We need probabilistic models or information on variable values, no? -->
<!-- - Universality seems an odd term for a concept that is continuous. Generality? Coverage? -->
<!-- - There was switching between precision and specificity. I've gone with precision, but could see either. -->
<!-- Better to replace type with variables.  -->
<!-- For example rather thatn $X \rightarrow Y \leftarrow Q$ where $Q$ takes on values of the four tyes.  -->
<!-- ROUGH NOTES -->
<!-- Say $Y = AX +B(1- X)$ in which case  uncertainty over functional forms, or  uncertainty about undefuned causal types  can be reconceptualized as uncertainty around positive and negative drivers -->
<!-- Drawing on different theories for subcomponents; eg theory of human decision making.  -->
<!-- More or less specified theories.  -->
<!-- Encapsulated CPDs are one way to wrap subtheories. Two identical DAGS could have two distinct theoretical underpinnings in the sense of having different encapsulated CPDs.    -->
<!-- When is one theory more general than another? When is one more specified than another? -->
<!-- If you specify a more detailed theory, the theory has more testable implications, but it is less general.  -->
<!-- Theory T'' is implied by theory T' if V'' is a subset of V' and the relations in T'' are implied by the relations in the reduced set T'' -->
<!-- For example:  -->
<!-- T'': A = 1 with prob .5, B = 1 with prob .5, if A = 1; 0 otherwise, C = 1 if B = 1 -->
<!-- T'': A = 1 with prob .5, C = 1 with prob .5, if A = 1; 0 otherwise -->
<!-- Different  subparts of theories may be implied by distinct theories provided implication relations satisfied -->
<!-- eg T'' part 1 may be implied by T, and T' implied by T'''', but T'' and T'''' not mutually consistent -->
<!-- Theory T'' explains more than T' if it identifies more causal relations -->
<!-- Theory T'' is more fertile  than T' if it implies more theories (same?) -->
<!-- Theory T'' has more observable implications that Theory T' if.... -->
<!-- Theory T'' is more falsifiable than theory T'.... -->
<!-- Theory T'' is more general (wide scope) than theory T' if its conditioning set is a  subset of theory T''s conditioning set -->
<!-- Theory T'' is more parsimonious  than theory T' if it predicts the same or more causal relations  with fewer nodes -->
<!-- Theory T'' is more complete (fully specified) than theory T' if it has lower prior variance (?) -->
<!-- Key -- there is no distinction between aleatory and epistemic uncertainty. It is all epistemic.  -->
<!-- eg the goalie might well randomize, but if we do not know which way she will go it is because we do not know  -->
<!-- what randomizing device she is using -->
<!-- Key: priors specified over all relations -->
<!-- Possibly need that nodes have classes -- eg utility -- that are used for some lo level theories -->
<!-- Question; ultimately is htere only one net and the universe is a realization of it? -->
<!-- SAME THING HERE. AN INTERESTING POINT, BUT A SEPARATE ENDEAVOR.

Functional (but not probabilistic)  causal models allow for the representation of logically derived relations between nodes without implying any unconditional empirical claims; that is, all claims may be of the  *if-then* variety, as is typical for example of propositions derived from game theoretic models. The process of connecting such models to the empirical claims can be thought of as the embedding of these incomplete models within larger structures.  -->
<!-- Consider for example the claim that in normal form games,  players play Nash equilibrium.  This claim in itself is not a tautology; that is, it is not a result. It can be contrasted for example with the *analytic result* that when rational players play a game and players have common knowledge of the game structure and of player rationality they will only play ''rationalizable'' strategies. Even still, the Nash claim does provide a set of analytically derived functional equations that relate nodes that describe game forms to actions  taken, and from actions to utilities. Representation as a causal graph can make explicit what conditional independencies are assumed in the move from analytic results to empirical claims. For example, are actions independent of the game form conditional on beliefs about the game form; are utilities independent of expectations conditional on actions, and so on. -->
<!-- We give an example of one such model below when we turn to extensive-form games for a lower-level theory that supports our running example.   -->
<!-- #### Relation to technical literature -->
<!-- Formally moving from lower level DAG to a higher level DAG requires *marginalization*: assessing the joint marginal distribution of observed nodes in the higher level graph over the distribution of unobserved nodes in the lower level graph.  -->
<!-- <!-- In Verma and Pearl 1990 (Equivalence and Synthesis of Causal Models) these higher level models are called "embedded causal models." -->
<!-- Unfortunately there is no guarantee that the  margin of a distribution that is consistent with a lower level DAG will be consistent with any higher level DAG (techically, "DAGS are not closed under marginalization").  -->
<!-- In response, various types of richer graphs have been developed, such as "acyclic directed mixed graphs" (ADMGs) Maximal Ancestral Graphs (Spirtes and Richardson, Ancestral graph Markov models, 2002), or mDAGs (Evans 2015 (Graphs for Margins)). See also (Wermuth 2011) -->
<!-- ADMGs for example have directed and bidirected edges but no directed cycles and are closed under marginalization. -->
<!-- We use two approaches in our applications to engage with this problem. First we generally allow for *unobserved confounding* in models.  Second we will allow for the estimation of lower level models with unobserved nodes. -->
<!-- ### Integration of theory and empirics -->
<!-- The understanding of theory as a causal model has important implications for how we think about the relationship between theory and evidence and about theoretical development.  -->
<!-- In a common hypothetico-deductive framework, we formulate a theory *a priori*, derive its empirical predictions, and then test the theory by examining whether those predictions are borne out by the data. If the evidence does not line up with the predictions, the theory is falsified. We might then respond by amending the theory, but the amended theory must then be tested against new data. -->
<!-- As we will see in chapters to come, when we confront a causal model with data, we do not seek to confirm or falsify the model, but to update beliefs over its parameters. In particular, we can update beliefs about the (possibly joint) distribution of nodal types in the population of interest, potentially arriving a new theory.  -->
<!-- To illustrate the difference, consider @saunders2011leaders' argument about transformative foreign military intervention, discussed in Chapter \@ref(illustratemodels). As we noted, Saunders' argument---the particular causal effects operating across the causal linkages---represents one of roughly 4 million possible combinations of nodal types that could operate within the causal structure that the argument (on our reading, at least) assumes. For instance, her argument implies that the nodal type $\theta^B$ takes on a value such that the conditions $F=1, P=1,$ and $T=1$ generate $B=1$, and we get $B=0$ otherwise; that $\theta^I$ takes on a value such that $B=1, F=1$ and $B=0, F=0$ produce $I=1$, for either value of $A$; and $A$ has a positive effect on $I$ whenever $B \neq F$; and so on, for all endogenous nodes. It would make little sense to read Saunders' argument as implying that this combination of nodal types is the only one operating in the world. But we might think of the argument as implying that this combination appears with some frequency---it implies a belief that this combination of nodal types characertizes some substantial proportion of cases in the relevant population.  -->
<!-- If we were then to confront this model with data (observations of some set of nodes for some set of cases), we would be seeking not to *falsify* the theory but to learn about the distribution of nodal types, relative to some prior belief about this distribution. We could, for instance, start with only the causal structure and flat priors over all nodal types. Or we might start with Saunders' theoretical argument itself as a basis for placing greater prior weight on the nodal types implied by that argument. Updating based on new data then has the potential to shift our beliefs within this model's parameter space, toward beliefs in the prevalence of some types at each node and away from others. Our posterior estimates might suggest that the set of effects that Saunders' theorizes are more or less prevalent than we had *ex ante* believed. And, equally importantly, we can expect the very same kind of learning about all *other* nodal types that possible under this causal structure. That is, we can learn about the prevalence of effects across this causal structure that are *not* anticipated by Saunders' argument at all---such as, say, the prevalence of negative effects of $A$ on $I$ or of positive effects of $B \neq F$ on $I$s. -->
<!-- Of course, in updating over nodal types we are evaluating the relevance of Saunders' argument; if the nodal types implied by her theory turn out to have greater weight in our posteriors, her argument has in a sense been more successful in explaining the world than if those nodal types end up with less weight. But what we come away with is more than the assessment of "a" theory, understood as a single set of propositions about the way the world works. We learn about the world of the model as a whole---about all of the causal effects that it allows. On this view, we do not proceed via pure deduction, from *a priori* theory to empirical test. While our models might be informed by deductive reasoning (see Chapter appendix), we also *develop* theory through engagement with the data.  -->
<!-- If a lower level model is invoked to justify a higher level model then the relations implied by the lower level models must obtain in the lower level model. The lower level model must be consistent with the higher level model an vice versa. This consistency requirement relates both to structural and probabilistic relations.  -->
<!-- For instance, suppose we start with an $I \rightarrow D$ model and the belief that $Pr(\theta^D=\theta^D_{01}) = 0.5$ -- i.e., effects are positive $50\%$ of the time. On a structural level, as we have seen, this model could be supported by the model $I \rightarrow M \rightarrow D$. But what beliefs about causal effects in this lower-level model might support the belief that effects of $I$ on $D$ are positive $50\%$ of the time?  -->
<!-- As with structural mappings, we can imagine a whole range of probabilistic beliefs at the lower level that would be consistent with a given higher-level belief. In a $I \rightarrow M \rightarrow D$ model, we can arrive at a positive effect of $I$ on $D$ either through a combination of the nodal types $\theta_{01}^M$ and $\theta_{01}^{D_{\text lower}}$ (linked positive effects) or of the nodal types $\theta_{10}^M$ and $\theta_{10}^{D_{\text lower}}$ (linked negative effects). So, for instance, a lower-level model in which $Pr(\theta^M=\theta_{01}^M)=0.5$, $Pr(\theta^{D_{\text lower}}=\theta_{01}^{D_{\text lower}})=0.5$, $Pr(\theta^M=\theta_{10}^M)=0.5$, and $Pr(\theta^{D_{\text lower}}=\theta_{10}^{D_{\text lower}})=0.5$ would be consistent with the belief that $Pr(\theta^D=\theta^D_{01}) = 0.5$. So too, however, would a lower-level model in which $Pr(\theta^M=\theta_{01}^M)=0.707$, $Pr(\theta^{D_{\text lower}}=\theta_{01}^{D_{\text lower}})=0.707$, $Pr(\theta^M=\theta_{10}^M)=0$, and $Pr(\theta^{D_{\text lower}}=\theta_{10}^{D_{\text lower}})=0$ equally implies $Pr(\theta^D=\theta^D_{01}) = 0.5$ in the higher-level model.  -->
<!-- Meanwhile, there is a whole range of beliefs about effects in the lower-level model that do not justify the higher-level belief. For instance,  any lower-level model in which $(Pr(\theta^D=\theta^D_{00}) + Pr(\theta^D=\theta^D_{11})) > 0.5$ is inconsistent with the belief in a 0.5 probability of positive $I \rightarrow D$ effects and cannot serve as a theory of the higher-level model. -->
<!-- We will generally want to think of lower-level models with quite different distributional beliefs as representing different theories, as they will typically capture quite different substantive beliefs about how the world works. We can think of the first lower-level model, above, as one in which inequality can cause democratization via two mechanisms, either by causing mass-mobilization which causes democratizaiton or by preventing mass-mobilization which prevents democratization --- while the second model is a theory in which only the former mechanism operates. -->
<!-- #### Type consistency in a mediation model -->
<!-- We might then wonder *how* inequality might exert its effect on democratization. One possible answer, drawing on our model in Chapter \@ref(models) is that inequality can affect mass-mobilization, which in turn can affect democratization. This explanatory claim is visually represented in Panel (b) of the figure. Here, we can see that any effect of $I$ on $D$ runs through $M$. There are details of this graph that we will delve into later. But for now, it is sufficient to see that we have partly explained variation left unexplained by model (a). Model (b) allows us to say, for instance, that whether inequality has an effect on democratization has to depend on whether inequality has an effect on mobilization. Model (b) thus theorizes, in one important sense, a part of inequality's effect that is left untheorized in model (a).  -->
<!-- Whereas Model (a) has nodal types defined for $D$,^[All models also have a type defined for node $I$, but $\theta^I$ is unaffected by the movement between these models.] Model (b) has nodal types defined both for node $M$ and for node $D$. We thus allow $I$ to have a positive, negative, or no effect on $M$, with $\theta^M$ taking on four possible values, $\theta_{10}^M,\theta_{01}^M,\theta_{00}^M$,and $\theta_{11}^M$. Further, we allow for $M$ to have a positive, negative, or no effect on $D$, with $\theta^D_{\text{lower}}$'s possible values again being one of $\theta_{10}^{D_{\text lower}}$, $\theta_{01}^{D_{\text lower}}$, $\theta_{00}^{D_{\text lower}}$, $\theta_{11}^{D_{\text lower}}$. -->
<!-- We can now think about _combinations_ of nodal types in the lower-level model as mapping onto nodal types in the higher-level model. Table \@ref(tab:highlowmapping) illustrates. -->
<!-- |                    | $\theta_{10}^{D_{lower}}$  | $\theta_{01}^{D_{lower}}$  | $\theta_{00}^{D_{lower}}$  | $\theta_{11}^{D_{lower}}$  | -->
<!-- |:-------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:---------------------------:| -->
<!-- | $\theta_{10}^{M}$  | $\theta_{01}^{D_{higher}}$ | $\theta_{10}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- | $\theta_{01}^{M}$  | $\theta_{10}^{D_{higher}}$ | $\theta_{01}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- | $\theta_{00}^{M}$  | $\theta_{11}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- | $\theta_{11}^{M}$  | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- Table: (\#tab:highlowmapping) Mapping from lower-level nodal types on $M$ and $D$ to higher-level nodal types on $D$.  -->
<!-- For instance, in a case in which both $\theta^M=\theta^M_{01}$ (there is a positive effect of $I$ on $M$) and $\theta^{D_{\text{lower}}}=\theta_{01}^{D_{lower}}$ (there is a positive effect of $M$ on $D$), we have a positive effect of $I$ on $D$---meaning that, in the _higher-level_ model, $\theta^{D_{higher}}=\theta^{D_{higher}}_{01}$. Two linked *negative* effects at the lower level likewise generate a positive effect of $I$ on $D$---and so map onto the same higher-level nodal type, $\theta^{D_{higher}}=\theta^{D_{higher}}_{01}$.  -->
<!-- Further, it is easy to see that if there is no causal effect at _either_ the $I \rightarrow M$ step _or_ the $M \rightarrow D$ step, we will have one of the null effect types at the higher level. This is because, in Model (b), $I$ cannot affect $D$ unless there are causal effects at both constituent steps. In other words, in Model (b), $I$ can affect $D$ _only_ through $M$ in this model; there are no direct effects or other pathways permitted. -->
<!-- To foreshadow the discussion in later chapters, these mappings of nodal types between levels are critical. They allow us to draw inferences about causal relations at a lower level and then *use* those inferences to answer questions posed at a higher level. For instance, if we can learn about the effect of inequality on mass mobilization (a question posed at the lower level), then we can apply that learning to answering a question about whether inequality affects democratization (a higher-level question). -->
<!-- The lower-level functional equations are formally similar though now each unit's outcome (given $X$) depends on two event probabilities: one that determines type with respect to the effect of $X$ on $K$ ($t_{ij}^{K}$), and one with respect to the effect of $K$ on $Y$ ($u_{ij}^{Y}$): -->
<!-- $$Y(K, u_{ij}^{Y}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } K=0 \\ j & \text{ if } K=1 \end{array}  \right.$$ -->
<!-- $$K(X, u_{ij}^{K}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } X=0 \\ j & \text{ if } X=1 \end{array}  \right.$$ -->
<!-- Thus, in the lower-level model, there are sixteen types that derive from the cross product of two independent random terms. -->
<!-- Critically, one can derive the higher-level types from the lower level types, and beliefs about the higher level types from beliefs about the lower level types. For example, using the nomenclature in @humphreys2015mixing: -->
<!-- \begin{eqnarray*} -->
<!-- \text{adverse: }u_{10}^{high} &=& u_{01}^{K}\&u_{10}^{Y} \text{ or } u_{10}^{K}\&u_{01}^{Y} \\ -->
<!-- \text{beneficial: }u_{01}^{high} &=& u_{01}^{K}\&u_{01}^{Y} \text{ or }  u_{10}^{K}\&u_{10}^{Y} \\ -->
<!-- \text{chronic: } u_{00}^{high} &=& u_{00}^{Y} \text{ or }  u_{00}^{K}\&u_{01}^{Y} \text{ or }  u_{11}^{K}\&u_{10}^{Y}\\ -->
<!-- \text{destined: }u_{11}^{high} &=& u_{11}^{Y} \text{ or }  u_{00}^{K}\&u_{10}^{Y} \text{ or }  u_{11}^{K}\&u_{01}^{Y} -->
<!-- \end{eqnarray*} -->
<!-- In the same way, the higher-level probabilities are implied by the lower level probabilities. -->
<!-- \begin{eqnarray*} -->
<!-- \text{adverse: }\lambda_{10}^{high} &=& \lambda_{01}^{K}\lambda_{10}^{Y} + \lambda_{10}^{K}\lambda_{01}^{Y} \\ -->
<!-- \text{beneficial: }\lambda_{01}^{high} &=& \lambda_{01}^{K}\lambda_{01}^{Y} + \lambda_{10}^{K}\lambda_{10}^{Y} \\ -->
<!-- \text{chronic: } \lambda_{00}^{high} &=& \lambda_{00}^{Y} + \lambda_{00}^{K}\lambda_{01}^{Y} + \lambda_{11}^{K}\lambda_{10}^{Y}\\ -->
<!-- \text{destined: }\lambda_{11}^{high} &=& \lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y} -->
<!-- \end{eqnarray*} -->
<!-- Importantly, even without specifying a distribution over $U_K$ or $U_Y^{\text{lower}}$, a lower-level structural model could be informative by restricting the *ranges* of  $U_K$ or $U_Y^{\text{lower}}$. For instance, a lower level theory that imposed a monotonicity condition (no adverse effects) might exclude $t^K_{10}$ and $t^y_{10}$---that is, increasing $X$ never reduces $K$, and increasing $K$ never reduces $Y$.  -->
<!-- We return  to this example below and show how observation  of $K$ can yield inference on causal estimands when  the theory places this kind of a structure on relationships. -->
<!-- #### Type consistency in a moderation model -->
<!-- Alternatively, we might wonder *why* or under what conditions inequality causes democratization. Our simple claim, in panel (a), allows that $I$ *can* cause $D$, but provides no information about the conditions under which it does so. Those conditions are implicitly embedded within $\theta^D$, where they are left unspecified. We could, however, theorize some of what is left unsaid in in panel (a). We do this in panel (c), where we posit ethnic homogeneity ($E$) as a moderator of  inequality's effect on democratization. Panel (c) represents a theory of panel (a) in that it can help account for variation in causal effects that is unaccounted for in Model (a). -->
<!-- Put differently, Model (c) gives substantive meaning to an aspect of the phenomenon that is merely residual variation in Model (a). Model (a) provides no account of why inequality has the effects it does, relying fully on $\theta^D$ as a placeholder for this uncertainty. In Model (c), $\theta^D$ plays a more modest role, with the substantive variable of ethnic homogeneity now doing some of the work of determining inequality's possible effects.  -->
<!-- We note one final possibility. Imagine that we started with the quite *specific* claim that inequality sometimes has a positive effect on democratization and sometimes has no effect (with democratization happening for other reasons). Suppose we believed this claim to be true for some, possibly not well defined, domain of cases.^[This claim could be graphically represented by panel (a), but would involve a more restricted range for $\theta^D$ and simpler functional equation, involving only two types.] Model (c) could serve as a theory of this more specific claim in that Model (c), paired with some data, could explain the claim. In particular, Model (c) paired with the data $E=1$---we are in an ethnically homogeneous context---produces the more specific claim. Here, it is the theory *plus an observation* of context that accounts for the specific claim.  -->
<!-- Similarly, take the functional equation $f_1: Y=X_1X_2$. Coupled with data $X_2=1$, $f_1$ implies the functional equation $f_2: Y=X_1$.  -->
<!-- ### Causal types in lower level models  -->
<!-- We have discussed theorization largely from a graphical perspective, showing how features of causal graphs change (nodes get split, combined, added, or removed) as we move down or up levels. But there is more that happens beneath the surface of a graphical structure when we theorize a claim: the space of causal types itself changes. We walk through how this works for the mediation and moderation theories described above.  -->
<!-- ### Mediation {#medtheory} -->
<!-- We begin with a simple claim: there are two binary variables, $X$ and $Y$, and $X$ may have an effect on $Y$. This claim is represented in Figure \@ref(fig:Highlow)(a) above. In this graph, $X$ is independent of $\theta^Y$, which means that it is as if $X$ is randomly assigned. -->
<!-- We will let $\theta^Y$ be a variable that ranges across our four different causal types, conditioning how $Y$ responds to $X$. While  $a, b, c$, and $d$ were heuristically useful as a way of introducing the  idea of a causal type, things will soon get more complicated, so it will be useful to have more flexible notation. Going forward, we will usually refer to causal types using $\theta$ notation, with subscripts and superscripts used to denote potential outcomes and outcome variables. In our binary $X \rightarrow Y$ setup, we can indicate the causal type governing $Y$'s response with notation of the form $\theta^Y_{ij}$, where $i$ and $j$ represent $Y$'s potential outcomes. Specifically, $i$ represents the value $Y$ takes on when $X=0$, while $j$ represents the value $Y$ takes on when $X=1$.^[The functional equation for $Y$ is then given by:  -->
<!-- $$Y(x, \theta_{ij}^{Y_\text{higher}}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } x=0 \\ j & \text{ if } x=1 \end{array}  \right.$$] Thus, the translation from $a, b, c$ and $d$ notation is: -->
<!-- * *a*: $\theta_{10}^Y$. A negative effect implies that $Y$ is $1$ when $X=0$ and $0$ when $X=1$. -->
<!-- * *b*: $\theta_{01}^Y$. A positive effect implies that $Y$ is $0$ when $X=0$ and $1$ when $X=1$. -->
<!-- * *c*: $\theta_{00}^Y$. A null "chronic" effect implies that $Y$ is $0$ regardless of $X$'s value. -->
<!-- * *d*: $\theta_{11}^Y$. A null "destined" effect implies that $Y$ is $1$ regardless of $X$'s value. -->
<!-- To be clear, these $\theta_{ij}^Y$ terms are not random variables; they are the four _values_ (types) that the type-variable $\theta^Y$ can take on. -->
<!-- represented with the notation $u_{ij}$: we read the subscripts to mean that a unit of type $u_{ij}$ has outcome $i$ when $X=0$ and $j$ when $X=1$. Then let $u_Y^{higher}$ have a multinomial distribution over the four values of  $u_{ij}$ with event probabilities  $\lambda_{ij}^{higher}$. ; for example, let $u_X\sim \text{Unif}[0,1]$ and $X = \mathbb{1}(u_K<\pi^K)$. -->
<!-- For example if $U_Y^{higher}$ is distributed normally and $Y$ takes on the value 1 if $bX+u_Y^{higher}$ is above some threshold, we have a probit model.  -->
<!-- Now consider a theory that specifies a variable intervening between $X$ and $Y$. This theory is depicted in Figure \@ref(fig:Highlow)(b) above, where $M$ mediates the relationship. We see that there are now two $\theta$ terms, each representing a set of causal types for a different step in the causal chain. While $\theta^Y$ represented $Y$'s response to its parent $X$, $\theta^Y_{\text{lower}}$ represents $Y$'s response to its "new" parent, $M$. We now also need to conceive of a causal type capturing $M$'s response to $X$, and we let $\theta^M$ represent this type.^[This graph assumes no confounding in the mediating relationship either as the two $\theta$ terms and $X$'s assignment are all independent of one another.] -->
<!-- Now consider the alternative lower-level theory in which  $E$ is posited as a second parent of $D$. This graph contains the substantive assumptions that $E$'s value is determined independently of $I$'s, as well as the assumption that $I$ and $E$ are both as-if randomly assigned. -->
<!-- In this graph, we again have a $\theta_D^{\text{lower}}$ term, but it is a different object from $\theta_D^{\text{lower}}$ in the mediation graph. In this moderation model, $\theta_D^{\text{lower}}$ is more complex as it determines the mapping from two binary variables into $D$. $D$'s nodal type in this setup now represents how a case's outcome will respond to four different possible combinations of $I$ and $E$ values. Rather than four nodal types for $D$, we now have 16, as there are 16 possible ways in which one binary variable might respond to two binary parent variables (see Table  \@ref(tab:PO16) in Chapter \@ref(models)). -->
<!-- In Table \@ref(tab:PO16b) we give a mapping from a subset of these lower-level types to the higher-level types corresponding to Model (a).  -->
<!-- ----------------------------------------------------------------------------------------------------------------------------------- -->
<!--    Lower Type                  $I=0,E=0$     $I=0,E=1$     $I=1,E=0$      $I=1, E=1$    Higher Type -->
<!-- -------------------------  ------------  -------------  -------------  -------------  ------------------------------------------------ -->
<!-- $\theta^{D}_{0000}$            0           0             0             0              $\theta^{D}_{00}$ -->
<!-- $\theta^{D}_{0001}$            0           0             0             1              $\theta^{D}_{01}$ if $E=1$, else $\theta^D_{00}$  -->
<!-- $\theta^{D}_{0010}$            0           0             1             0              $\theta^D_{00}$ if $E=1$, else $\theta^D_{01}$ -->
<!-- $\theta^{D}_{0011}$            0           0             1             1              $\theta^{D}_{01}$ -->
<!-- $\vdots$                       $\vdots$    $\vdots$      $\vdots$      $\vdots$       $\vdots$      -->
<!-- $\theta^{D}_{1110}$            1           1             1             0              $\theta^D_{11}$ if $E=0$, else $\theta^D_{10}$ -->
<!-- $\theta^{D}_{1111}$            1           1             1             1              $\theta^D_{11}$ -->
<!-- ----------------------------------------------------------------------------------------------------------------------------- -->
<!-- Table: (\#tab:PO16b) Values for $D$ given $E$ and $I$. With two binary causal variables, there are 16 nodal types: 16 ways in which $D$ depends on $I$ and $E$. These lower level types map into higher level types for a model in which $D$ depends on $I$ only, as shown in the final column.  -->
<!-- To illustrate, $\theta_Y^{\text{lower}}=$: -->
<!-- * $\theta_{00}^{11}$ means that $I$ has no effect under any value of $E$, and $E$ has a positive effect under any value of $I$.  -->
<!-- * $\theta_{10}^{10}$ implies that $I$ always has a negative effect, and $E$ never has an effect.  -->
<!-- * $\theta_{01}^{11}$ represents one kind of conditional effect: $I$ has a positive effect only when $E=0$, and $E$ has a positive effect only when $I=0$. -->
<!-- Importantly we see that the mapping between lower- and higher-level types can depend on the value of the moderator. More generally, since we can think of the value of exogeneous nodes, $E$ and $I$, as being nodal types for those nodes,^[In other words, saying $E=1$ in this model is the same as saying $\theta^{E}=\theta^{E}_{1}$. An exogenous nodes nodal type *is* the value to which it has been exogenously "assigned."] we can think of the lower level nodal type as a concatenation of the higher-level nodal types for $E$ and $D$. Thus, we can think of the the higher-level type as depending uniquely on the fully specified lower level type. -->
<!-- For instance, a case can have type $\theta^D_{01}$ in the higher-level model if it is of type $\theta^D_{0010}$ _and_ c in the lower-level model. This is a case for which $I$ has a positive effect on $D$ when $E=0$ _and_ in which $E$ _is in fact_ 0. On the other hand, the same lower-level $D$ type  in combination with $\theta^{E}_1$ maps onto the type $\theta_{10}$ in the higher-level model---a type in which $D$ responds *negatively* to $I$.  -->
<!-- In later chapters, we represent all lower- to higher-level mappings relevant to a question of interest with the use of "type-reduction" tables that allow one to readily see how inferences drawn at one level inform causal questions posed at another level. -->
<!-- We let $u_Y^{\text{lower}}$ in this graph denote a multinomial distribution over the sixteen values of  $u_{ij}^{gh}$ with event probabilities  $\lambda_{ij}^{gh}$. -->
<!-- I changed abcd scripts above to ghij and made corresponding (I think) changes below. I don't care what it is but abcd obviously could be confusing in this context. -->
<!-- The sixteen types are illustrated in Table \@ref(tab:types2X) in the appendices. -->
<!-- Again, the types in the higher level mapping are functions of the types in the lower-level mapping. For example,  a unit has type $u_{01}$ in the higher level model if $K=1$ and it is of type $u_{00}^{01},u_{10}^{01},u_{01}^{01}$, or $u_{11}^{01}$, or if $K=0$ and it is of type $\lambda_{01}^{00},\lambda_{01}^{10},\lambda_{01}^{01}$, or $\lambda_{01}^{11}$.  -->
<!-- We write this as: -->
<!-- $$u_{01} =  ((K=1) \land (t^{lower} \in \{u_{00}^{01} \cup u_{10}^{01} \cup  u_{01}^{01} \cup u_{11}^{01} \}) \lor  ((K=0) \land (t^{lower} \in \{\lambda_{01}^{00} \cup \lambda_{01}^{10} \cup \lambda_{01}^{01} \cup \lambda_{01}^{11}\})$$ -->
<!-- In the same way, the probability of type $u_{01}$ can be written in terms of the parameters of the lower-level graph.  Importantly, the parameters of the higher-level distribution  $u_Y^{higher}$ depend on both $u_K$ and $u_Y^{\text{lower}}$. Thus, unlike the mediation case above, the probative value depends on the likelihood of an *observable* event occurring. Specifically, the share of a given higher-level type is given by: -->
<!-- $$\lambda_{ij} = P(u_Y^{higher} = u_{ij}) = \pi^K\left(\lambda_{00}^{gh}+\lambda_{10}^{gh}+\lambda_{01}^{gh}+\lambda_{11}^{gh}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{ij}^{00}+\lambda_{ij}^{10}+\lambda_{ij}^{01}+\lambda_{ij}^{11}\right)$$ -->
<!-- For example: -->
<!-- $$\lambda_{00} = P(u_Y^{higher} = u_{00}) = \pi^K\left(\lambda_{00}^{00}+\lambda_{10}^{00}+\lambda_{01}^{00}+\lambda_{11}^{00}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{00}^{00}+\lambda_{00}^{10}+\lambda_{00}^{01}+\lambda_{00}^{11}\right)$$ -->
<!-- Conditional probabilities follow in the usual way. Consider, for instance, the case where it is known that $X=Y=1$ and so the posterior probability of type $u_{01}$ is simply $P(i \in u_{01} | X=Y=1) = \frac{\lambda_{01}}{\lambda_{01}+\lambda_{11}}$. Note that $\pi^x$ does not appear here as this $X$ is orthogonal to $u_Y$. The probability of type $u_{01}$, knowing that $X=Y=1$, can be written in terms of the parameters of the $u$ distributions in the lower-level graph.  -->
<!-- $$P(i \in u_{01} | X=Y=1) = \frac{ -->
<!-- \pi^K\left(\lambda_{00}^{01}+\lambda_{10}^{01}+\lambda_{01}^{01}+\lambda_{11}^{01}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{01}^{00}+\lambda_{01}^{10}+\lambda_{01}^{01}+\lambda_{01}^{11}\right) -->
<!-- }{ -->
<!-- \sum_{i = 0}^1\left(\pi^K\left(\lambda_{00}^{i1}+\lambda_{10}^{i1}+\lambda_{01}^{i1}+\lambda_{11}^{i1}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{i1}^{00}+\lambda_{i1}^{10}+\lambda_{i1}^{01}+\lambda_{i1}^{11}\right) -->
<!-- \right)}$$ -->
<!-- We return below to this example and describe how the lower-level model can be used to generate inferences on relations implied by the higher level model.  -->
<!-- ## Rules for moving between higher- and lower-level models -->
<!-- Thinking about models as conditionally nested within one another can be empirically useful. It provides a way of generating empirical leverage on a causal question by plumbing more deeply our background knowledge about a domain of interest. When we more fully specify higher-level claims via a more elaborate, lower-level model, we are a making explicit unspecified conditions on which the higher-level relationships depend. In doing this, we are identifying potentially observable nodes that might be informative about our research question. -->
<!-- As we develop lower-level models to support our claims, or determine which claims are supported by our theories, what kinds of moves are we permitted to make?  -->
<!-- ## Illustration of unpacking nodal types -->
<!-- We now show more specifically how sets of nodal types in lower-level models map into nodal types in higher-level models.  -->
<!-- For concreteness, let us return to our democratization example and consider first the very basic claim that inequality can have an affect on democratization. We represent this simple claim in Figure \@ref(fig:demtheory5), Panel (a). In this simple model, $I$ may sometimes have an effect on $D$ and sometimes not; and that effect may be positive or negative. $I$'s effect will, of course, depend on the case's nodal type on $D$.  -->
<!-- ```{r demtheory5, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='80%', fig.cap = "DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a)."} -->
<!-- par(mfrow = c(3,1)) -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(1,2,2), -->
<!--        y = c(1,1,2), -->
<!--        names = c( -->
<!--          expression(paste(I)), -->
<!--          expression(paste("D")),   -->
<!--          expression(paste(theta[D]))), -->
<!--        arcs = cbind( c(1, 3), -->
<!--                      c(2, 2)), -->
<!--        title = "(a) A Claim: Inequality Causes Democratization", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .1 -->
<!-- ) -->
<!-- hj_dag(x = c(1,2,2, 1.5, 1.5), -->
<!--        y = c(1,1,2, 1  , 2), -->
<!--        names = c( -->
<!--          expression(paste(I)), -->
<!--          expression(paste("D")),   -->
<!--          expression(paste(theta[D])), -->
<!--          expression(paste(M)), -->
<!--          expression(paste(theta[M]))  -->
<!--          ), -->
<!--        arcs = cbind( c(1, 3, 5, 4), -->
<!--                      c(4, 2, 4, 2)), -->
<!--        title = "(b) A Theory: How Inequality Causes Democratization", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .1 -->
<!-- ) -->
<!-- hj_dag(x = c(1,2, 2, 1.5), -->
<!--        y = c(1,1, 2, 2), -->
<!--        names = c( -->
<!--          expression(paste(I)), -->
<!--          expression(paste("D")),   -->
<!--          expression(paste(theta[D])), -->
<!--          expression(paste(E))  -->
<!--          ), -->
<!--        arcs = cbind( c(1, 3, 5, 4), -->
<!--                      c(2, 2, 4, 2)), -->
<!--        title = "(c) Another theory: When Inequality Causes Democratization", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .1 -->
<!-- ) -->
<!-- ``` -->
<!-- Next, the figure shows two models that can each *explain* Model (a), though in different ways. Model (b) answers the explanatory question, "*How* does inequality affect democratization?" Model (c) answers the explanatory question, "Why does inequality's effect on democratization vary?" or "Under what conditions does $I$ have a given effect on $D$?" Both theories provide richer, more interpretable accounts of the phenomenon of interest than the simpler model that they are theorizing. -->
<!-- These lower-level models also imply a set of nodal types that are richer than that implied by (a). Recall that in Chapter \@ref(models), we considered the idea that at any node, a nodal type may be conceptualized as a case-specific exogenous disturbance that governs the mapping from input variables to outcome variables. The type node $\theta^D$ can take on the values $\theta^D_{10}, \theta^D_{01}, \theta^D_{00}, \theta^D_{11}$, and this node can take on different values in different cases. However, differences in $\theta^D$'s value are left entirely unaccounted for. We are saying nothing about *why* inequality causes democratization in some places, prevents democratization in other places, and has no effect in still other places.  -->
<!-- Let us consider how Models (b) and (c) provide answers to these questions and how these answers map onto these models' nodal types. -->
<!-- <!-- To fix this idea going forward, we make a shift in notation and use $\theta$ to indicate that a node is a receptacle for causal types. Thus, $\theta_D$ here captures the case's causal type, or $I$'s causal effect on $D$ for a given case.  -->
<!-- In particular, if we  deploy our four-causal-type function from Chapter \@ref(models) we have:  -->
<!--   * $a$: $\theta^D=\theta^D_{10}$, then $D=1-I$ ($I$ has a negative effect on $D$) -->
<!--   * $b$: $\theta^D=\theta^D_{01}$, then $D=I$ ($I$ has a positive effect on $D$) -->
<!--   * $c$: $\theta^D=\theta^D_{00}$, then $D=0$ ($I$ has no causal effect) -->
<!--   * $d$: $\theta^D=\theta^D_{11}$, then $D=1$ ($I$ has no causal effect) -->
<!-- Knowing $\theta$ tells us how $D$ responds to $I$ and it ignores any heterogeneity between units as long they respond in the same way. For any causal type the model is *consistent* with $I$'s causal effect operating for different reasons for different units, but  -->
<!-- ### Implied consistency of priors -->
<!-- We caution that the mappings of distributional beliefs between levels is not always intuitive. Suppose, for instance, that we begin with no information about causal effects in a model and so want to set flat priors, meaning that we accord equal prior weight to all nodal types at each node. In Model (a), flat priors would mean putting an 0.25 weight on each of the following: positive effects, negative effects, zero effects with $D$ always $0$, and negative effects with $D$ always $1$. Now, suppose we engage in the same flat-prior-setting in the lower-level model, Model (b). That is, we put equal weight (i.e., 0.25 across the board) on all nodal types at node $M$ and equal weight (0.25 across the board) on all nodel types at node $D$. Surprisingly perhaps, setting flat priors at this lower level in fact implies a strong weighting *against* either positive or negative causal effects at the *higher* level. Now, we are saying that the probability of a positive effect at the higher level is $(0.25 \times 0.25) + (0.25 \times 0.25) = 0.125$; and likewise for negative effects.  -->
<!-- Put simply, the fact that the lower-level model involves more mediating steps between $I$ and $D$ means that more things have "line up"  for an $I \rightarrow D$ effect to emerge --- and so causal effects will be rarer under a flat distribution of nodal types in this model than they are in the simpler, higher-level model. Another way to think about this is that simply by spelling out the mediating steps in a causal chain, we can, perhaps indadvertently, generate beliefs that causal effects at the higher level are weaker than we might have thought. We can, of course, set priors at the lower level that would map onto flat priors at the higher level.^[Placing roughly 0.35 weight on a positive effect at each mediating step and 0.35 weight on a negative effect at each step implies approximately a 0.25 probability of a positive $I \rightarrow D$ effecta and a 0.25 probability of a negative $I \rightarrow D$ effect.] Our advice to researchers is, simply, to check for consistency of priors across levels: to ask whether the priors that we set at a lower level imply beliefs at the higher level that we are willing to live with.  -->
</div>
</div>
<div id="theorygains" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Gains from theory</h2>
<p>We now turn to consider how to think about whether a theory is <em>useful</em>. We are comfortable with the idea that theories, or models more generally, are wrong. Models are not full and faithful reflections of reality; they are maps designed for a particular purpose. We make use of them because we think that they <em>help</em> in some way.</p>
<!-- In the case of causal models, the purpose is to capture relationships of independence and possible causal dependence. As we have shown, that is a purpose that allows for the stripping away of detail---though it also forbids certain simplifications (such as any simplification that removes a dependency between variables).  -->
<p>But how do they actually help, and can we quantify the gains we get from using them?</p>
<p>We think we can. Using the notion of hierarchies of models, imagine we begin with model <span class="math inline">\(\mathcal M_1\)</span>, which together with data <span class="math inline">\(\mathcal D\)</span>, implies claim <span class="math inline">\(\mathcal M_2\)</span>. We then posit theory <span class="math inline">\(\mathcal M_0\)</span> of <span class="math inline">\(\mathcal M_1\)</span>, so <span class="math inline">\(\mathcal M_0\)</span> implies <span class="math inline">\(\mathcal M_1\)</span>. But when we bring <span class="math inline">\(\mathcal D\)</span> to <span class="math inline">\(\mathcal M_0\)</span> we get a new model, <span class="math inline">\(\mathcal M_2&#39;\)</span>, that is different—and, hopefully, better—than <span class="math inline">\(\mathcal M_2\)</span>. Our gain from theory <span class="math inline">\(\mathcal M_0\)</span> should be some summary of how much better <span class="math inline">\(\mathcal M_2&#39;\)</span> is than <span class="math inline">\(\mathcal M_2\)</span>.</p>
<p>Here is an illustration using a theory that allows use of the “front-door criterion.”</p>
<p>Imagine that we have data on three variables, <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(K\)</span>. We begin, however, with a model <span class="math inline">\(\mathcal M_1\)</span> with confounding: <span class="math inline">\(C \rightarrow X \rightarrow Y \leftarrow C\)</span>. <span class="math inline">\(\mathcal M_1\)</span> includes nodes for two of the three variables we have data on, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, but not <span class="math inline">\(K\)</span>. Assume, further, that we do not have data on node <span class="math inline">\(C\)</span>, the confound.</p>
<p>Suppose that we observe a strong correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and infer <span class="math inline">\(\mathcal M_2\)</span>: that <span class="math inline">\(X\)</span> is a likely cause <span class="math inline">\(Y\)</span>. Our inference under <span class="math inline">\(\mathcal M_2\)</span> is, however, quite uncertain because we are aware that the correlation may be due to the confound <span class="math inline">\(C\)</span>.</p>
<p>Suppose now that we posit the lower-level model <span class="math inline">\(\mathcal M_0\)</span>: <span class="math inline">\(C \rightarrow X \rightarrow K \rightarrow Y \leftarrow C\)</span>. <span class="math inline">\(\mathcal M_0\)</span> now lets us make better use of data <span class="math inline">\(K\)</span>. If we observe, for instance, that <span class="math inline">\(X\)</span> and <span class="math inline">\(K\)</span> are uncorrelated, then we infer with confidence that in fact <span class="math inline">\(X\)</span> did not cause <span class="math inline">\(Y\)</span>, despite the correlation.</p>
<p>Thus, in return for specifying a theory of <span class="math inline">\(\mathcal M_1\)</span>, we have been able to make better use of data and form a more confident conclusion. In this case, stating the theory, <span class="math inline">\(\mathcal M_0\)</span>, does not alter our <em>priors</em> over our query. Our prior over the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> may be identical under <span class="math inline">\(\mathcal M_1\)</span> and <span class="math inline">\(\mathcal M_0\)</span>—but our conclusions, given data, differ because the theory lets us make use of the data on <span class="math inline">\(K\)</span>, which we could not do under <span class="math inline">\(\mathcal M_1\)</span> (which did not include <span class="math inline">\(K\)</span>).</p>
<p>In other situations, we might imagine invoking a theory that does not necessarily involve new data but that allows us to make different, perhaps tighter inferences using the same data. An example might be the invocation of a type-reducing theory that involves a monotonicity restriction or exclusion restriction that allows for identification of a quantity that would not be identifiable without the theory.</p>
<p>Thus, one reason to theorize our models — develop lower-level models that make stronger claims — is to be able to reap greater inferential leverage from the more elaborate theory.</p>
<p>But are we, in fact, better off?</p>
<p>We might imagine answering the question in different ways: from an internal or external position, and from an <em>ex ante</em> or <em>ex post</em> perspective.</p>
<p>In all cases we ask how much better do we do as a result of making use of a theory.</p>
<p>If we are willing to posit an external ground truth, then we can define “better” in objective terms. An <em>ex post</em>, objective way of operationalizing “better” is to assess the size of the error we make relative to the ground truth, from an inference that uses a theory, compared to an inference that does not make use of the theory. An objective <em>ex ante</em> approach might be to ask what the expected error is from conclusions one draws given a theory. For instance: how wrong are we likely to be if we base our best guess on our posterior mean? “How wrong” might be operationalized in terms of mean squared error—the square of the distance between the truth and the posterior mean.<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a></p>
<p>A more subjective approach would be to ask about the reductions in posterior variance. <em>Ex post</em> we can define “better” as the reduction in posterior variance from conclusions that make use of a theory compared to conclusions that do not. We might also think about the <em>expected posterior variance</em>: how certain do you expect you will be after you make use of this new information?</p>
<p>More formally, imagine a situation in which there is an unknown parameter <span class="math inline">\(q\)</span> and we have a data strategy that produces a distribution over data <span class="math inline">\(k\)</span>, given <span class="math inline">\(q\)</span>. Let <span class="math inline">\(p(q,k)\)</span> denote the joint prior distribution over <span class="math inline">\(q\)</span> and <span class="math inline">\(k\)</span> with marginal distributions <span class="math inline">\(p(k)\)</span> and <span class="math inline">\(p(q)\)</span>. For any <span class="math inline">\(k\)</span> there is posterior estimate <span class="math inline">\(q_k\)</span>.</p>
<p>The squared error, given <span class="math inline">\(k\)</span> is just <span class="math inline">\((q - q_k)^2\)</span>.</p>
<p>The <em>expected</em> squared error is:</p>
<p><span class="math display">\[ESE := \int_q\int_k \left({q}_k-q\right)^2p(k, q)dkdq \]</span></p>
<p>This takes the error one might get with respect to any true value of the parameter (<span class="math inline">\(q\)</span>), given the data one might see given <span class="math inline">\(q\)</span> and the inferences one might draw.</p>
<p>For any <span class="math inline">\(k\)</span> we might write the posterior variance as <span class="math inline">\(v_k\)</span>.</p>
<p>The <em>expected</em> posterior variance can be written:</p>
<p><span class="math display">\[EV := \int_k v_k p(k)dk\]</span></p>
<p>This takes the posterior variance, given some data, over all the possible data one might see given marginal distribution <span class="math inline">\(p(k)\)</span>.</p>
<p>Interestingly, if we assess expectations using the same priors as you use for for forming posteriors the expected posterior variance and expected squared error are equivalent <span class="citation">(<a href="#ref-scharf1991statistical" role="doc-biblioref">Scharf 1991</a>)</span>.
To see this, we take advantage of the fact that <span class="math inline">\(p(q,k) = p(k)p(q|k) = p(q)p(k|q)\)</span> and that <span class="math inline">\(p(q|k)\)</span> gives the posterior distribution of <span class="math inline">\(q\)</span> given <span class="math inline">\(k\)</span>. We then have:</p>
<!-- $$ -->
<!-- \begin{eqnarray} -->
<!-- ESE &=& \int_q\int_k \left({q}_k-q\right)^2p(q,k)dkdq \\ -->
<!--     &=& \int_k\int_q \left({q}_k-q\right)^2p(q,k)dq dk \\ -->
<!--     &=& \int_k\int_q \left({q}_k-q\right)^2p(k)p(q|k)dq dk \\ -->
<!--     &=& \int_k\int_q \left({q}_k-q\right)^2p(q|k)dq p(k)dk \\ -->
<!--     &=& \int_k\left[\int_q \left({q}_k-q\right)^2p(q|k)dq\right]p(k)dk \\ -->
<!--     &=& \int_k v_k p(k)dk \\ -->
<!--     & = & EV -->
<!-- \end{eqnarray} -->
<!-- $$ -->
<p><span class="math display">\[\begin{eqnarray}
ESE &amp;=&amp; \int_q\int_k \left({q}_k-q\right)^2p(q,k)dkdq \\
    &amp;=&amp; \int_k\int_q \left({q}_k-q\right)^2p(k)p(q|k)dq dk \\
    &amp;=&amp; \int_k\left[\int_q \left({q}_k-q\right)^2p(q|k)dq\right]p(k)dk \\
    &amp;=&amp; \int_k v_k p(k)dk  = EV
\end{eqnarray}\]</span></p>
<p>Note that the key move is in recognizing that <span class="math inline">\(p(q |k)\)</span> is in fact the posterior distribution on <span class="math inline">\(q\)</span> given <span class="math inline">\(k\)</span>. In using this we assume that the same distribution is used for assessing error and for conducing analysis—that is we take the researcher’s prior to be the relevant one for assessing error.</p>
<p>Moreover, it is easy to see that whenever inferences are sensitive to <span class="math inline">\(K\)</span>, the <em>expected</em> variance of the posterior will be lower than the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of <span class="math inline">\(K\)</span>, given what is already known from observation of <span class="math inline">\(W\)</span>.<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a><br />
<span class="math display">\[Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))\]</span></p>
<p>However, although <em>expected</em> posterior variance goes down, it is still always possible that posterior variance rises. The increase in uncertainty does not, however, mean you haven’t been learning. Rather, you have learned that things aren’t as simple as you thought.</p>
<!-- For illustration say that it is known that $X=1, Y=1$ and that, given this information (playing the role of $W$), the posterior probability that a unit is of type $b$---for whom $Y$ would be 0 were $X=0$ (and not type $d$, for which $Y$ would be 1 regardless) is $p$. Say then that that under some theory we have $\phi_b := \Pr(K=1 | Y(0)=0, Y(1)=1, X=1)$, $\phi_d := \Pr(K=1 | Y(0)=1, Y(1)=1, X=1)$.  -->
<!-- Then what is the value added of this theory? Define $Q$ here as the query regarding whether the unit is a $b$ type. Then the prior variance, $Var(Q|W)$, is simply $p(1-p)^2 +(1-p)p^2 = p(1-p)$.  -->
<!-- <!-- Would be best to  write down the theory as a structural equation that has phi_j as p(K=1|j) -->
<!-- To calculate $E_{K|W}(Var(Q|K,W))$, note that the posterior if $K$ is observed is $\frac{\phi_bp}{\phi_bp+\phi_d(1-p)}$. Let us call this $\hat{q}_K$, and the belief when $K$ is not observed $\hat{q}_{\overline{K}}$. -->
<!-- In that case the  *expected error* is:  -->
<!-- $$\text{Expected Error} = p\phi_b\left(1-\hat{q}_K\right)^2+(1-p)\phi_d\hat{q}_K^2+p(1-\phi_b)\left(1-\hat{q}_{\overline{K}}\right)^2+(1-p)(1-\phi_d)\hat{q}_{\overline{K}}^2$$ -->
<!-- where the four terms are the errors when $K$ is seen for a $b$ type, when $K$ is seen for a $d$ type, when $K$ is not seen for a $b$ type, and when $K$ is not see for a $d$ type. -->
<!-- Defining $\rho_K = (p\phi_b+(1-p)\phi_d)$ as the probability of observing $K$ given the prior, we can write the posterior variance as: -->
<!-- $$\text{Expected Posterior Variance} = \rho_K\hat{q}_K(1-\hat{q}_K)+(1-\rho_K)\hat{q}_{\overline{K}}(1-\hat{q}_{\overline{K}})$$ -->
<!-- <!-- Making use of the fact that $\rho_K\hat{q}_K = ({\phi_bp+\phi_d(1-p)})\frac{\phi_bp}{\phi_bp+\phi_d(1-p)} = \phi_bp$ and similarly  -->
<!-- <!-- $(1-\rho_K)\hat{q}_{\overline{K}} = (1-\phi_b)p$, this can be written in terms of primitives as: -->
<!-- With a little manipulation, both of these expressions simplify to: -->
<!-- $$\text{Expected Posterior Variance} =p(1-p)\left(\frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} + \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\right)$$ -->
<!-- The gains are then: -->
<!-- $$\text{Gains} =1- \frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} - \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}$$ -->
<!-- Note that we still learn even if our posterior variance increases. For example say $p = 1/5$, $\phi_d = 1/3$, $\phi_b = 2/3$ and we observe $K=1$. Then our prior variance is $p(1-p) = 4/15$. Our posterior is $1/3$ and our posterior variance is 2/9, an increase. Even still although we are more uncertain we are wiser since we attribute a squared error to the guesses made by our former selves now of $(1/3)(1-1/5)^2 + (2/3)(0 - 1/5)^2 = 6/25$. -->
<!-- <!-- .2 2/3 / .2 2/3 + .8 1/3 ) = .4/ .4 + .8    -->
<!-- One approach to assessing the contribution of a theory is to calculate the mean reduction in Bayes risk. Suppose we start with a baseline model with variables in the set $\mathcal W$ and want to answer a query, $Q$. We can then define the gains from theory as:  -->
<!-- $$\text{Gains from theory} = 1- \frac{E_{K|\mathcal W}(Var(Q|K,\mathcal W))}{Var(Q|\mathcal W)}$$ -->
<!-- <!-- <!-- AJ: Have tried to explain what Q and W are, and walk through the intuition behind this formulation. Make sure this is right. Also should the E not be in the denominator too? -->
<!-- We can think of the Bayes risk as the inverse of an $R^2$ measure in a regression framework (see also @gelman2006bayesian): it is the variance in our query given what we have observed. Here we are defining the gains from theory as the degree to which we have reduced that variance by observing $K$ and $\mathcal W$, relative to just observing $\mathcal W$.  -->
<p>One way to capture this idea that, although we are more uncertain, we think we are better off now than we were, is to ask: how much better are our guesses having observed <span class="math inline">\(K\)</span> compared to what we would have guessed before, <em>given</em> what we know having observed <span class="math inline">\(K\)</span>? We will call this “Wisdom” to reflect the idea that it values appreciation of justifiable uncertainty:</p>
<!-- <!-- AJ: Needs a sentence here introducing the terminology. Is this our term, expected wisdom? -->
<!-- Expected wisdom. -->
<p><span class="math display">\[Wisdom  = \int(q_0 - q)^2 - (q_k - q)^2 p(q | k)dq\]</span></p>
<p>This metric captures how much better off we are with the guess we have made given current data (<span class="math inline">\(q_k\)</span>) compared to the guess we would have made if we had a theory that did not let us make use of it (<span class="math inline">\(q_0\)</span>), knowing what we know having observe <span class="math inline">\(K\)</span> (<span class="math inline">\(p(q|k)\)</span>.</p>
<p>An advantage of this conceptualization is that we can still record gains in learning even if the learning operates such that the posterior variance is larger than the prior variance. Even so, the implications for strategy are the same since wisdom is maximized by a strategy that reduces expected squared error.</p>
<p>Thus expected wisdom, is:</p>
<p><span class="math display">\[\begin{eqnarray}
\text{Expected Wisdom}  &amp;=&amp; \int_q(q_0 - q)^2dq - \int_k\int_q(q_k - q)^2 p(q, k)dqdk\\
&amp;=&amp; \text{Prior variance} - \text{Expected Posterior Variance}
\end{eqnarray}\]</span></p>
<p>We close with a reminder. Although expected reduction in variance and expected wisdom are both positive, both are are fundamentally subjective ideas, that presuppose the theory is correct. In contrast the expected error measure can be assessed under rival theoretical propositions and so allow for the real possibility that the gains of invoking a theory are negative.</p>
</div>
<div id="formal-theories-and-causal-models" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Formal theories and causal models</h2>
<p>It is relatively easy to see how the ideas above play out for what might be called empirical models. But in social sciences “theory” is a term sometimes reserved for what might be called analytic models. In this last section we work through how to use this framework when seeking to bring analytic models to data.</p>
<p>Let’s start with analytic models. As an example we might consider the existence of “Nash equilibria.” Nash considered a class of settings (“normal form games”) in which each player <span class="math inline">\(i\)</span> can choose an action <span class="math inline">\(\sigma_i\)</span> from set <span class="math inline">\(\Sigma_i\)</span> and receives a payoff <span class="math inline">\(u_i\)</span> that depends on the actions of all players. A particular game, <span class="math inline">\(\Gamma\)</span> is the collection of players, action sets, and payoffs.</p>
<p>Nash’s theorem relates to the existence of a collection of strategies with the property that each strategy would produce the greatest utility for each player given the strategies of the other players. Such a collection of strategies is called a Nash equilibrium.</p>
<p>The claim that such a collection of strategies exists in these settings is an analytic claim. Unless there are errors in the derivation of the result, the claim is true in the sense that the conclusions follow from the assumptions. There is no evidence that we could go looking for in the world to assess the claim. The same can be said of the theoretical claims of many formal models in social sciences; they are theoretical conclusions of the if-then variety <span class="citation">(<a href="#ref-clarke2012model" role="doc-biblioref">Clarke and Primo 2012</a>)</span>.</p>
<p>We will refer to theories of this form as “analytic theories.”</p>
<p>When researchers refer to a theory of populism or a theory of democratization however they generally do not have such pure theories in mind. Rather they have in mind what might be called “applied theories” (or perhaps more simply “scientific theories” or “empirical theories”): general claims about the relations between objects in the world. The distinction here corresponds to the distinction in <span class="citation"><a href="#ref-peressini1999applying" role="doc-biblioref">Peressini</a> (<a href="#ref-peressini1999applying" role="doc-biblioref">1999</a>)</span> between “pure mathematical theories” and “mathematized scientific theories.”<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a></p>
<!-- In the standard ("hypothetico-deductive") model; a  theory originates in the mind and is then retained or rejected if it is inconsistent with data [@popper2014conjectures]. In practice, this  sometimes requires a step in which a pure theory is translated either wholesale or piecemeal into an aplied theory via a  set of empirical statements which are put to the test.   -->
<p>Applied theory, in this sense, is a collection of claims with <em>empirical</em> content: an applied theory refers to a set of propositions of causal relations in the world that might or might not hold, and is susceptible to assessment using data. These theories might look formally a lot <em>like</em> analytic theories but it is better to think of them as translations at most. The relations between nodes of an applied theory are a matter of conjecture not a matter of necessity.<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a></p>
<!-- Using the causal modelling framework a possible approach is to embed direct empirical analogs of pure theoretical propositions into causal models. Endow nodes with physical interpretations and  let functional relations capture the ideal operation of the theory.  The causal model can naturally include uncertainty about any or all assumptions and claims of the theory, as well as a representation of the conditions under which the claims of the theory are expected to hold.  -->
<p>Though it is not standard practice, formal models produced by game theorists can often be translated and then represented using the notation of structural causal models in this way. Moreover, doing so may be fruitful. Using the approach described above we can then assess the utility of the applied theory, if not the pure theory itself.</p>
<p>For two players, for instance, we might imagine a representation of a game as shown in Figure <a href="theory.html#fig:nfg">6.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nfg"></span>
<img src="ii_files/figure-html/nfg-1.png" alt="Formal structure of a normal form game." width="80%" />
<p class="caption">
Figure 6.3: Formal structure of a normal form game.
</p>
</div>
<p>Here the only functional equations are the utility functions. The utilities, given actions, are the implications of the theory, and so this is just a theory of how outcomes depend on social actions. It is not—yet—a behavioral theory.</p>
<p>In contrast to Nash’s theorem regarding the existence of equilibria, a behavioral theory might claim that in problems that can be represented as normal form games, players indeed play Nash equilibrium. This is a theory about how people act in the world. We might call it Nash’s theory.</p>
<p>How might this theory be represented as a causal model? Figure <a href="theory.html#fig:nfg2">6.4</a> provides one representation.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nfg2"></span>
<img src="ii_files/figure-html/nfg2-1.png" alt="Formal structure of a normal form game." width="80%" />
<p class="caption">
Figure 6.4: Formal structure of a normal form game.
</p>
</div>
<p>Here beliefs about the game form (<span class="math inline">\(\Gamma\)</span>) results in strategy choices by actors. If players play according to Nash’s theory, <em>the functional equations for the strategy choices are given by the Nash equilibrium solution itself</em>, with a refinement in case of multiplicity.</p>
<p>This model represents what we expect to happen in a game under Nash’s theory and we can indeed see if the relations between nodes in the world look like what we expect under the theory. But it does not provide much of an <em>explanation</em> for behavior.</p>
<p>A lower level causal model might help. In Figure <a href="theory.html#fig:nfg3">6.5</a>
the game form <span class="math inline">\(\Gamma\)</span> determines the beliefs about what actions the other player would make (thus <span class="math inline">\(\sigma_2^e\)</span> is 1’s belief about 2’s actions). The functional equations for <span class="math inline">\(\sigma_2^e\)</span> and <span class="math inline">\(\sigma_1^e\)</span> might, for instance, be the Nash equilibrium solution itself: that is, players expect other players to play according to the Nash equilibrium (or in the case of multiple, a particular equilibrium selected using some refinement). The beliefs in turn, together with the game form (which contains <span class="math inline">\(u1, u_2\)</span>), are what cause the players to select a particular action. The functional equation for <span class="math inline">\(\sigma_1\)</span> might thus be <span class="math inline">\(\sigma_1 = \arg \max_\sigma u_1(\sigma, \sigma_2^e)\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nfg3"></span>
<img src="ii_files/figure-html/nfg3-1.png" alt="Formal structure of a normal form game." width="80%" />
<p class="caption">
Figure 6.5: Formal structure of a normal form game.
</p>
</div>
<p>This representation implies a set of relations that can be compared against empirical patterns. Do players indeed hold these beliefs when playing a given game? are actions indeed consistent with beliefs in ways specified by the theory. It provides a theory of beliefs and a theory of individual behavior as well as an explanation for social outcomes.</p>
<p>The model in Figure <a href="theory.html#fig:nfg3">6.5</a> provides a foundation of sorts for Nash’s theory. It suggests that players play Nash equilibria <em>because</em> they expect others to and they are utility maximizers. But this is not the only explanation that can be provided; alternatively behavior might line up with the theory without passing through beliefs at all as suggested in some accounts from evolutionary game theory that show how processes might select for behavior that corresponds to Nash even if agents are unaware of the game they are playing.</p>
<p>One might step still further back and ask <em>why</em> would actors form these beliefs, or take these actions, and answer in terms of assumptions about actor rationality. Figure <a href="theory.html#fig:nfg4">6.6</a> for instance is a model in which actor rationality might vary and might influence beliefs about the actions of others as well as reactions to those beliefs. Fully specified functional equations might specify not only how actors act when rational but also how they react when they are not. In this sense the model in Figure <a href="theory.html#fig:nfg4">6.6</a> both nests Nash’s theory and provides an explanation for why actors conform to the predictions of the theory.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nfg4"></span>
<img src="ii_files/figure-html/nfg4-1.png" alt="Formal structure of a normal form game." width="80%" />
<p class="caption">
Figure 6.6: Formal structure of a normal form game.
</p>
</div>
<p>In a final elaboration we can represent a kind of underspecification of Nash’s theory that make it difficult to take the theory to data. In the above we assumption that players chose actions based on expectations that the other player would play the Nash equilibrium—or that the theory would specify which equilibrium in the case of multiplicity. But it is well known that Nash’s theory often does not provide a unique solution. This indeterminacy can be captured in the Causal model as shown in Figure <a href="theory.html#fig:nfg5">6.7</a> where a common shock—labelled <span class="math inline">\(\nu\)</span>, and interpreted as norms—interacts with the game form to determine the expectations of other players.</p>
<p>The functional equation for expectations can then allow for the possibility that (i) there is a unique equilibrium invariably chosen and played by both (ii) or a guarantee that players are playing one or other equilibrium together but uncertainty over which one is played, or (iii) the possibility that players are in fact out of sync, with each playing optimal strategies given beliefs but nevertheless not playing the same equilbria.</p>
<p>Nash’s theory likely corresponds to position (ii). It can be captured by functional equations on beliefs given <span class="math inline">\(\nu\)</span> but the theory does not specify <span class="math inline">\(\nu\)</span>, in the same way that it does not specify <span class="math inline">\(\Gamma\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nfg5"></span>
<img src="ii_files/figure-html/nfg5-1.png" alt="A normal form game with a representation of equilibrium selection norms." width="80%" />
<p class="caption">
Figure 6.7: A normal form game with a representation of equilibrium selection norms.
</p>
</div>
<p>We highlight three points from this discussion.</p>
<p>First the discussion highlights that thinking of theory as causal models does not force a sharp move away from abstract analytic theories; close analogues of these can often be incorporated in the same framework. This is true even for equilibrium analysis that seems to involve a kind of simultaneity on first blush.</p>
<p>Second, the discussion highlights how the causal modelling framework can make demands for specificity from formal theories. For instance specifying a functional relations from game form to actions requires a specification of a selection criterion in the event of multiple equilibria. Including agent rationality as a justification for the theory invites a specification for what would happen absent rationality.</p>
<p>Third the example shows a way of building a bridge from pure theory to empirical claims. One can think of Nash’s theory as an entirely data free set of claims. When translate into an applied theory—a set of proposition about the ways actual players <em>might</em> behave—and represented as a causal model, we are on a path to being able to use data to refine the theory. Thus we might begin with a formal specification like that in Figure <a href="theory.html#fig:nfg5">6.7</a> but with initial uncertainty about player rationality, optimizing behavior, and equilibrium selection. This theory nests Nash but does not presume the theory to be a valid description of processes in the world. Combined with data, however, we shift to a more refined theory that selects Nash from the lower level model.</p>
<p>Finally, we can then apply the ideas of section <a href="theory.html#theorygains">6.2</a> to applied formal theories and ask: is the theory useful? For instance, does data on player rationality help us better understand the relationship between game structure and welfare?</p>
<!-- Other possible measures of gains from theory might include the simple correlation between $K$ and $Q$, or entropy-based measures (see @zhang2003properties for many more possibilities).  -->
<!-- For this problem the correlation is given by (see appendix): -->
<!-- $$\rho_{KQ} = \frac{(\phi_b+\phi_d)(1-2p)(p(1-p))^{.5}}{ -->
<!-- (p\phi_b+(1-p)\phi_d)(1-(p\phi_b+(1-p)\phi_d)))^{.5}}$$ -->
<!-- One might also use a measure of "mutual information" from information theory: -->
<!-- $$I(Q,K) = \sum_q \sum_k P(q,k)\log\left(\frac{P(q,k)}{P(q)P(k)}\right)$$ -->
<!-- <!-- here: -->
<!-- <!-- \begin{equation} -->
<!-- <!-- \begin{aligned} -->
<!-- <!-- I(Q,K) ={} & p\phi_b\log\left(\frac{\phi_b}{p\phi_b+(1-p)\phi_d}\right)+ (1-p)\phi_d\log\left(\frac{\phi_d}{p\phi_b+(1-p)\phi_d}\right) \\ -->
<!-- <!--       & +p(1-\phi_b)\log\left(\frac{1-\phi_b}{1-p\phi_b-(1-p)\phi_d}\right)+ -->
<!-- <!-- (1-p)(1-\phi_d)\log\left(\frac{1-\phi_d}{1-p\phi_b-(1-p)\phi_d}\right) -->
<!-- <!-- \end{aligned} -->
<!-- <!-- \end{equation} -->
<!-- To express this mutual information as a share of variation explained, we could divide $I(Q,K)$ by the entropy of $Q$, $H(Q)$ where $H(Q) = -\sum_qP(q)\log(P(q))$. The resulting ratio can  be interpreted as 1 minus the ratio of the entropy of $Q$ conditional (on $K$) to the unconditional entropy of $Q$. -->
<!-- For this example, Figure \ref{fig:probative_value} shows gains as a function of $\phi_b$ given a fixed value of $\phi_d$. The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely.  -->
<!-- ```{r probativevalue, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='.7\\textwidth', fig.cap = "The solid line shows gains in precision (reduced posterior variance) for different values of $\\phi_b$ given $\\phi_d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\\phi_b - \\phi_d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$."} -->
<!-- gains = function(p, phi_b, phi_d){ -->
<!--   1- (phi_b*phi_d)/(phi_b*p +phi_d*(1-p)) - (1-phi_b)*(1-phi_d)/((1-phi_b)*p+(1-phi_d)*(1-p)) -->
<!-- } -->
<!-- corr_qk <- function(p, phi_b, phi_d){ -->
<!--   ((phi_b-phi_d)*(p*(1-p))^{.5})/ -->
<!--   (((p*phi_b+(1-p)*phi_d)*(1-(p*phi_b+(1-p)*phi_d)))^{.5}) -->
<!--   } -->
<!-- # Mutual Information -->
<!-- mi_qk <- function(p, phi_b, phi_d, base = 2){ -->
<!--  p*phi_b*        log({phi_b}   / {p*phi_b+(1-p)*phi_d}, base = base)+ -->
<!-- (1-p)*phi_d*     log({phi_d}   / {p*phi_b+(1-p)*phi_d}, base = base)+ -->
<!-- p*(1-phi_b)*     log({1-phi_b} / {1-p*phi_b-(1-p)*phi_d}, base = base)+ -->
<!-- (1-p)*(1-phi_d)* log({1-phi_d}/ {1-p*phi_b-(1-p)*phi_d}, base = base) -->
<!--   } -->
<!-- norm_mi_qk <-  function(p, phi_b, phi_d, base = 2){ -->
<!--     -mi_qk(p, phi_b, phi_d, base = base)/(p*log(p, base = base)+(1-p)*log(1-p, base = base))} -->
<!-- phi_b = seq(0,1,.01) -->
<!-- plot(phi_b, gains(.75, phi_b, .25), type = "l", xlab = expression(paste(phi[b])), ylab = "Probative Value") -->
<!--   points(phi_b, abs(corr_qk(.75, phi_b, .25)), type = "l", lty=2) -->
<!--   points(phi_b, (norm_mi_qk(.75, phi_b, .25)), type = "l", lty = 3) -->
<!--   points(phi_b, abs(phi_b - .25), type = "l", lty = 4) -->
<!--   title("Reduced posterior variance, correlation, mutual information") -->
<!-- text(.8, c(.15, .25, .3, .62, .41), c("I(K,Q)/H(Q)","(Reduced posterior variance)", "Gains",  expression(paste(abs(phi[b]-phi[d]))), "Cor(K,Q)")) -->
<!-- #plot(abs(corr_qk(.75, phi_b, .25)), gains(.75, phi_b, .25), type = "l", xlab = "Probative value") -->
<!-- #lines(abs(phi_b - .25), gains(.75, phi_b, .25), type = "l", lty=2) -->
<!-- #lines(norm_mi_qk(.75, phi_b, .25), gains(.75, phi_b, .25), type = "l", lty=2) -->
<!-- ``` -->
<!-- Clarke and Primo see models as useful to the extent that they are similar to features of the real world in ways related to the model's purpose. Along these lines, a causal model will be useful to the extent that it posits relations of independence that are similar to those prevailing in the domain under investigation. -->
<!-- ### Connections to other writing on theory -->
<!-- We close this chapter by considering how the understanding of theory that we work with in this book compares to other prominent understandings of theory.  -->
<!-- **Theory as model.** Although @clarke2012model argue for a separation of the ideas of model and theory, it is common for social scientists to use the terms interchangeably to denote an abstract representation of some part of the world that is of interest. For instance, a model may stipulate that outcome $X$ can have a positive effect on $Y$ because $X$ can cause $M$ and $M$ can cause $Y$.  One can read from a model how things work in the context of the model: for instance, if $M$ does not obtain, then under this model, $X$ does not cause $Y$.   One can use a model to make claims about the world only by assuming a mapping from elements in the model to elements in the worlds.  In this sense a  model is best thought of as an object that may or may not be useful [@clarke2012model]; whether the model itself is true or false is, in this usage, not a coherent question.  -->
<!-- **Theory as empirical claim.** In common usage, "a theory of" a phenomenon is a direct claim about the phenomenon, in the world. The claim that natural resources cause conflict is a theoretical claim of this form. The claim is certainly not true by definition, and empirical evidence can be used to assess it. In this claim, the *theory*, as usually understood, is certainly thin; the claim is no more than an empirical proposition, and it possesses no internal logic. Yet, more elaborate collections of empirical propositions are easily constructed. For instance: natural resources cause conflict because they can finance secessionist claims in resource rich areas.^[This latter claim does seem to possess something like a logic; though it does not take much to see that the logic is just a slightly more elaborate set of empirical claims. The outcomes do not follow  *logically* from the causes----there is no logical reason why secessionist claims would cause conflict, but the theory---as a collection of claims---has implications similar to those in the model in the paragraph above: if there are no secessionist claims, then under this theory, natural resources are not causing conflict.] Theory in this sense can certainly be right or wrong. -->
<!-- In this book we take a somewhat idealist position and assume that we are permanently inhabiting a world of models.  -->
<!-- The distinction between the last two accounts is sometimes confusing, and  @clarke2012model make a case for cleaning up the language on this front. In their account, drawing on @giere2010explaining, a theory might be best thought of as a set of models accompanied by hypotheses linking the model to the question of interest in the  world.  -->
<!-- We see our approach to theory as models as following in the spirit of  @clarke2012model and  @giere2010explaining yet also as being consistent with the treatment of models in the literature on probabilistic causal models with which this book is centrally engaged. A nice feature is that it preserves a close associated between theory and explanation and it incorporates naturally the notion of deduction without requiring that models themselves are statements of the  *if-then* variety. -->
<!-- **Theory as generalization** In another of the many uses of "theory," political scientists often think of theorization as generalization. For @Van-Evera:1997 and @przeworski1970logic, for instance, theories are by their nature general statements that we can use to explain specific events. In this view, "Diamond resources caused Sierra Leone's civil war" is a case-specific explanation; "Natural resource endowments cause civil war" is a theoretical formulation.  -->
<!-- In our treatment of theory as a lower-level causal model, however, there is no generic sense in which a theory is more or less general than the higher-level claim that it explains. In this book's framework, we _can_ theorize by generalizing: when we elaborate a model by building in variation in a factor that was held constant in the higher-level claim, we are making the model more general in scope. If our natural resources claim implicitly applies only to weak states, we can theorize this claim by allowing state strength to vary and articulating how the natural-resource effect hinges on that claim.  -->
<!-- However, when we theorize by disaggregating nodes---say, by adding intervening causal steps---we have in fact made a more _specific_ claim. Natural resources may cause civil war under a broad set of circumstances. Natural resources will cause civil war *through looting by rebel groups* under an almost certainly narrower set of circumstances. Here, the more elaborate argument---the theorization of *why* $X$ causes $Y$---is actually a stronger claim, with narrower scope, than the simpler one that it supports.  -->
<!-- **The value of parsimony** @Van-Evera:1997 and @przeworski1970logic also express a common view in characterizing _parsimony_ as a quality of good theory. While they recognize that parsimony must often be traded off against other goods, such as accuracy and generality, _ceteris paribus_ a more parsimonious theory---one that uses fewer causal variables to explain variation in a given outcome---is commonly understood to be a better theory.  -->
<!-- We do not take issue with the idea that simpler models and explanations are, all else equal, better. But the succeeding chapters also demonstrate a distinctive and important way in which all else will often not be equal when we seek to use theory to guide research design and support causal inference. To foreshadow the argument to come, the elaboration of more detailed, lower-level models can direct us to new opportunities for learning. As we unpack a higher-level claim, we will often be identifying additional features of a phenomenon the observation of which can shed light on causal questions of interest. Moreover, our background beliefs---the prior knowledge on which causal inference must usually rest---are often more informative at lower levels than at higher levels: it will, for instance, often be easier for us express beliefs about causal effects for smaller steps along a causal chain than about an overarching $X \rightarrow Y$ effect.  -->
<!-- Making things more complicated, of course, still makes things more complicated. And we should avoid doing so when the payoff is small, as it will sometimes be. But in the pages to come, we will also see a distinct set of benefits that arise from drilling more deeply into our basis of prior knowledge when formulating inferential strategies. -->
<!-- , with these claims,  perhaps derived or inspired from some model via a statement that the model represents the world faithfully for some purpose.  -->
<!-- The key difference as we see it is between representations of a system, a model, and claims that the model itself represents another system---the world---in some ways. The difference betw  -->
<!-- ### Illustration of a Mapping from a Game to a DAG -->
<!-- Our running example supports a set of higher level models, but it can also  be *implied* by a lower level models. Here we illustrate with an example in which the lower level model is a game theoretic model, together with a solution.^[Such representations have been discussed as multi agent influence diagrams, for example in @koller2003multi or @white2009settable on "settable systems"--- an extension of the "influence diagrams" described by @dawid2002influence.]  -->
<!-- In Figure \@ref(fig:tree) we show a game in which nature first decides on the type of the media and the politician -- is it a media that values reporting on corruption or not? Is the politician one who has a dominant strategy to engage in corruption or one who is sensitive to the risks of media exposure? In the example the payoffs to all players are fully specified, though for illustration we include parameter $b$ in the voter's payoffs which captures utility gains from sacking a politician that has had a negative story written about them *whether or not they actually engaged in corruption*. A somewhat less specific, though more easily defended, theory would not specify particular numbers as in the figure, but rather assume ranges on payoffs that have the same strategic implications.   -->
<!-- The theory is then the  game plus a solution to the game. Here for a solution the theory specifies subgame perfect equilibrium. -->
<!-- In the subgame perfect  equilibrium of the game; marked out on the game tree (for the case  $b=0$) the sensitive politicians do not engage in corruption when there is a free press -- otherwise they do; a free press writes up any acts of corruption, voters throw out the politician if indeed she is corrupt and this corruption is reported by the press.   -->
<!-- As with any structural model, the theory says what will happen but also what *would* happen if things that should not happen indeed happened.  -->
<!-- ```{r tree, echo=FALSE, fig.width = 15, fig.height = 12, fig.cap = "\\label{fig:tree} A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which  $b=0$."} -->
<!-- H <-  matrix(c(rep("O", 32),  -->
<!-- rep("X=1, S=1", 8), rep("X=1, S=0", 8), rep("X=0, S=1", 8), rep("X=0, S=0",8 ), -->
<!-- rep(rep(c("C","NC"), each = 4 ), 4), -->
<!-- rep(rep(c("R","NR"), each = 2 ), 8), -->
<!-- rep(c("Y","NY"), 16)),  -->
<!--  32)[32:1,] -->
<!-- in.history = function(action) rowSums(H==action)>0 -->
<!-- P <- cbind(rep(1, 32),  -->
<!--            rep(2, 32),  -->
<!--            rep(3, 32),  -->
<!--            rep(4, 32))[32:1,] -->
<!-- U <- matrix(NA, 32, 4) -->
<!-- U[,2] <- in.history("C") -    -->
<!--           2*in.history("Y") +  -->
<!--           2*(in.history("X=0, S=0")+ in.history("X=1, S=0"))*in.history("C")  -->
<!-- # Media gains only when it does reliable story  -->
<!-- U[,3] <- in.history("NR") +    -->
<!--           2*in.history("R")*in.history("C")*(in.history("X=1, S=0")+ in.history("X=1, S=1"))  -->
<!-- # Voters prefer firing if reports on corrupt politician -->
<!-- U[,4] <- in.history("NY") +    -->
<!--           2*in.history("Y")*in.history("C")*in.history("R")  -->
<!-- gt_tree(H,U,P, player.names = c("Nature", "Gov", "Media", "Voters"),          -->
<!--   mark.branches=((ncol(H)-1):2), -->
<!--   print.utilities = c(FALSE, TRUE, TRUE, TRUE), -->
<!--   force_solution = TRUE, warnings = FALSE) -->
<!-- text(6.6, (1:32)[in.history("Y") & in.history("R")]- .02, expression(italic(+b)) , cex = 1.2)  -->
<!-- ``` -->
<!-- To draw this  equilibrium as a DAG we include nodes for every action taken, nodes for features that determine the game being played, and the utilities at the end of the game.  -->
<!-- If equilibrium claims are justified by claims about the beliefs of actors then these could also appear as nodes. To be clear however these are not required to represent the game  or the equilibrium, though they can capture assumed logics underlying the equilibrium choice. For instance a theorist might claim that humans are wired so that whenever they are playing a "Stag Hunt" game they play "defect." The game and this solution can be represented on a DAG without reference to the  beliefs of actors about the action of other players. However, if the *justification* for the equilibrium involves optimization given the beliefs of other players, a lower level DAG could represent this by having a node for the  game description that points to beliefs about the actions of others, that then points to choices. In a game with dominant strategies, in contrast, there would be no arrows from these beliefs to actions. -->
<!-- For our running example, nodes could usefully include the politician's expectations, since the government's actions depend on expectations of the actions of others. However, given the game there is no gain from  including the media's expectations of the voter's actions since in this case the media's actions do not depend on expectations of the voters actions then these expectations should be included.   -->
<!-- In Figure \@ref(fig:gamedag) we provide two examples of DAGs that illustrate lower level models that support our running example.  -->
<!-- The upper panel gives a DAG reflecting equilibrium play in the game described in Figure \@ref(fig:tree). Note that in this game there is an arrow between $C$ and $Y$ even though $Y$ does not depend on $C$ for some values of $b$---this is because conditional independence requires that two variables are independent for *all* values of the conditioning set. For simplicity also we mark $S$ and $X$, along with $b$ as features that affect which subgame is being played---taking the subgames starting after Nature's move. Note that the government's expectations of responses by others matters, but the expectations of other players do not matter given this game and solution. Note that the utilities appear twice in a sense. They appear in the subgame node, as they are part of the definition of the game--though here they are the utilities that players expect at each terminal node; when they appear at the end of the DAG they are the utilities that actually arise (in theory at least).  -->
<!-- The lower level DAG  is very low and much more general, representing the theory that in three player games of complete information, players engage in backwards induction and choose the actions that they expect to maximize utility given their beliefs about the actions of others. The DAG assumes that players know what game is being played ("Game"), though this could also be included for more fundamental justification of behavioral predictions. Each action is taken as a function of the beliefs about the game, the expectations about the actions of others, and knowledge of play to date. The functional equations---not shown---are given by optimization and belief formation assuming optimization by others.   -->
<!-- ```{r gamedag, echo = FALSE, fig.width = 12, fig.height = 10, out.width='\\textwidth', fig.cap = "The upper panel shows a causal graph that describes  relations between nodes suggested by analysis of  the  game  in Figure \\ref{fig:tree} and which can imply the causal graph of  Figure \\ref{fig:running}. The game itself  (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors.   The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''", fig.align="center", warning = FALSE} -->
<!-- par(mfrow = c(2,1)) -->
<!-- par(mar=c(1,1,3.5,1)) -->
<!-- x = c(0, 1, 2, 2,  3,  3, 4, 5) -->
<!-- y = c(0, 0, 2, -2, 2, -2.5, -2, 0) -->
<!-- names = c("S, X, b",                                        #1  -->
<!--           "Subgame",                                           #2  -->
<!--           "E: Gov's Beliefs\nabout responses by\n Media and Voters",    #3 -->
<!--           "Corruption",                                       #4 -->
<!--           "",            #5 -->
<!--           "Report",                                       #6 -->
<!--           "Remove\nGovernment",                                       #7 -->
<!--           "Utilities"                          #8 -->
<!-- ) -->
<!-- hj_dag(x =  x, -->
<!--        y = y, -->
<!--        names = c(names, " ", " "), -->
<!--        arcs = cbind( c(1,rep(2,5)  ,3, c(4,6,7),  4, 4, 6), -->
<!--                      c(2,3:4, 6:8,      4,  rep(8, 3), 6, 7, 7)), -->
<!--        title = "Lower DAG: Backwards induction in a game with 3 players  with one  move  each", -->
<!--        contraction = .22, -->
<!--        padding = .5) -->
<!-- x = c(0, 1, 2, 2,  3,  3, 4, 5) -->
<!-- y = c(0, 0, 2, -2, 2, -2.5, -2, 0) -->
<!-- names = c("Context",                                        #1  -->
<!--           "Game",                                           #2  -->
<!--           "1's Beliefs\nabout actions \n 2|1 and 3|2,1",    #3 -->
<!--           "Action 1",                                       #4 -->
<!--           "2's Beliefs\nabout actions \n 3|2,1",            #5 -->
<!--           "Action 2",                                       #6 -->
<!--           "Action 3",                                       #7 -->
<!--           "Utilities"                          #8 -->
<!-- ) -->
<!-- hj_dag(x =  x, -->
<!--        y = y, -->
<!--        names = c(names, " ", " "), -->
<!--        arcs = cbind( c(1,rep(2,6)  ,3, 5, c(4,6,7),  4, 4, 6), -->
<!--                      c(2,3:8,      4, 6, rep(8, 3), 6, 7, 7)), -->
<!--        title = "Still lower: Backwards induction, 3 player game with one  move for each player", -->
<!--        contraction = .2, -->
<!--        padding = .5) -->
<!-- ``` -->
<!-- These lower level graphs can themselves provide clues for assessing relations in the higher level graphs. For instance, the lower level model might specify that the value of $b$ in the game affects the actions of the government only through their beliefs about the behavior of voters, $E$. These beliefs may themselves have a stochastic component, $U_E$. Thus  $b$ high  might be thought to reduce the effect of media on corruption. For instance if $b \in \mathbb{R}_+$, we have $C= 1-FG(1-\mathbb{1}(b>1))$. If $X$ is unobserved and one is interested in whether $S=0$ caused corruption, knowledge of $b$ is informative. It is a root node in the causal estimand. If $b>1$ then $S=0$ did not cause corruption. However if $b$ matters only because of its effect on $E$ then the query depends on $U_E$.  In this case, while knowing $b$ is informative about whether $S=0$ caused $C=1$, knowing $E$ from the lower level graph is more informative. -->
<!-- Note that the  model we have examined here involves no terms for $U_C$, $U_R$ and $U_Y$---that is, shocks to outcomes given action. Yet clearly any of these could exist. One could imagine a version of this game with "trembling hands," such that errors are always made with some small probability, giving rise to a much richer set of predictions.  These can be  represented in the game tree as moves by nature between actions chosen and outcomes realized. Importantly in a strategic environment such noise could give rise to different types of conditional independence. For instance say that a Free Press only published its report on corruption with  probability $\pi^R$, then with $\pi^R$ high enough the sensitive government might decide it is worth engaging in corruption even if there is a free press; in this case the arrow from $X$ to $C$ would be removed. Interestingly in this case as the error rate rises, $R$ becomes less likely, meaning that the effect of a $S$ on $Y$ becomes gradually weaker (since governments that are not sensitive become  more likely to survive) and then drops to 0 as sensitive governments start acting just like nonsensitive governments.  -->

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-clarke2012model" class="csl-entry">
Clarke, Kevin A, and David M Primo. 2012. <em>A Model Discipline: Political Science and the Logic of Representations</em>. New York: Oxford University Press.
</div>
<div id="ref-geweke2014analysis" class="csl-entry">
Geweke, John, and Gianni Amisano. 2014. <span>“Analysis of Variance for Bayesian Inference.”</span> <em>Econometric Reviews</em> 33 (1-4): 270–88.
</div>
<div id="ref-heckerman1991toward" class="csl-entry">
Heckerman, David E, Eric J Horvitz, and Bharat N Nathwani. 1991. <span>“Toward Normative Expert Systems: The Pathfinder Project.”</span> <em>Methods of Information in Medicine</em> 31: 90I105.
</div>
<div id="ref-pearl2009causality" class="csl-entry">
———. 2009. <em>Causality</em>. Cambridge university press.
</div>
<div id="ref-peressini1999applying" class="csl-entry">
Peressini, Anthony. 1999. <span>“Applying Pure Mathematics.”</span> <em>Philosophy of Science</em> 66: S1–13.
</div>
<div id="ref-raiffa1961applied" class="csl-entry">
Raiffa, Howard, and Robert Schlaifer. 1961. <span>“Applied Statistical Decision Theory.”</span>
</div>
<div id="ref-scharf1991statistical" class="csl-entry">
Scharf, Louis L. 1991. <em>Statistical Signal Processing</em>. Vol. 98. Addison-Wesley Reading, MA.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="42">
<li id="fn42"><p>We note that our definition of theory differs somewhat from that given in <span class="citation"><a href="#ref-pearl2009causality" role="doc-biblioref">Pearl</a> (<a href="#ref-pearl2009causality" role="doc-biblioref">2009</a>)</span> (p207): there a theory is a (functional) causal model and a restriction over <span class="math inline">\(\times_j \mathcal{R}(U_j)\)</span>, that is, over the collection of contexts envisionable. Our definition also considers probabilistic models as theories, allowing statements such as “the average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is 0.5.”<a href="theory.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p>As we emphasize further below, it is in fact only the random, unknown component of the <span class="math inline">\(X\rightarrow K\)</span> link that makes the addition of <span class="math inline">\(K\)</span> potentially informative as a matter of research design: if <span class="math inline">\(K\)</span> were a deterministic function of <span class="math inline">\(X\)</span> only, then knowledge of <span class="math inline">\(X\)</span> would provide full knowledge of <span class="math inline">\(K\)</span>, and nothing could be learned from observing <span class="math inline">\(K\)</span>.<a href="theory.html#fnref43" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p>We drop the arrow in Figure <a href="theory.html#fig:Highlowreduce">6.2</a>, however, in order to help visually convey the difference between the two models. In fact, we would construct <span class="math inline">\(\mathcal M_0\)</span> by placing restrictions at nodes in <span class="math inline">\(\mathcal M_1\)</span>, rather than by changing the model’s structure, so that the allowed types in <span class="math inline">\(\mathcal M_0\)</span> form a subset of those in <span class="math inline">\(\mathcal M_1\)</span>.<a href="theory.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p>This is not universally true, a point we return to below.<a href="theory.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>In frequentist frameworks we often think of analysis as implementing up-or-down empirical tests against data to parse between theories that should be maintained and theories that should be rejected. In a Bayesian framework we think more continuously of shifting our beliefs across causal possibilities within a multi-dimensional theoretical space.<a href="theory.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>As a general matter an updated theory may not provider sharper claims for all queries. That is, in practice, posterior variance over queries can increase with more data. As a simple illustration: say, we start out thinking that the probability that an outcome is due to conditions A, B, or C is .9, .05, and .05, respectively. If I find evidence that convinces me that A is not the cause, then I shift (a) to greater certainty about whether A was the cause but (b) greater uncertainty about whether B was the cause.<a href="theory.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>Other loss functions could be used, including functions that take account of the costs of collecting additional data or to the risks associated with false diagnoses. For instance, in <span class="citation"><a href="#ref-heckerman1991toward" role="doc-biblioref">Heckerman, Horvitz, and Nathwani</a> (<a href="#ref-heckerman1991toward" role="doc-biblioref">1991</a>)</span>, an objective function is generated using expected utility gains from diagnoses generated based on new information over diagnoses based on what is believed already. In their treatment <span class="citation">(<a href="#ref-heckerman1991toward" role="doc-biblioref">Heckerman, Horvitz, and Nathwani 1991</a>, Equation 6)</span>, the expected value of new information <span class="math inline">\(K\)</span>, given existing information <span class="math inline">\(W\)</span> is: <span class="math inline">\(\sum{K}P(K|W)( EU(d(Q,W,K)|W, K) - EU(d(Q, W)|W, K))\)</span> where <span class="math inline">\(EU\)</span> is expected utility and <span class="math inline">\(d\)</span> is the optimal inference (diagnosis) given available data. Note that the diagnosis can take account of <span class="math inline">\(K\)</span> when it is observed, but the expected utility depends on <span class="math inline">\(K\)</span> whether or not it is observed, as <span class="math inline">\(K\)</span> carries information about the state of interest.<a href="theory.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p>See <span class="citation"><a href="#ref-raiffa1961applied" role="doc-biblioref">Raiffa and Schlaifer</a> (<a href="#ref-raiffa1961applied" role="doc-biblioref">1961</a>)</span>. A similar expression can be given for the expected posterior variance from learning <span class="math inline">\(K\)</span> in addition to <span class="math inline">\(W\)</span> when <span class="math inline">\(W\)</span> is not yet known. See, for example, Proposition 3 in <span class="citation"><a href="#ref-geweke2014analysis" role="doc-biblioref">Geweke and Amisano</a> (<a href="#ref-geweke2014analysis" role="doc-biblioref">2014</a>)</span>.<a href="theory.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>Or see the distinction, for instance in in Keynes, between pure theory and applied theory.<a href="theory.html#fnref50" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p><span class="citation"><a href="#ref-peressini1999applying" role="doc-biblioref">Peressini</a> (<a href="#ref-peressini1999applying" role="doc-biblioref">1999</a>)</span> distinguishes between “applied mathematical theories” and “mathematized scientific theories” on the grounds that not all mathematized theories are an application of a pure theory.<a href="theory.html#fnref51" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayeschapter.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pt.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false,
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ii.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
