<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Case selection as a Decision Problem | Integrated Inferences</title>
  <meta name="description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Case selection as a Decision Problem | Integrated Inferences" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="dnieperriver.png" />
  <meta property="og:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="github-repo" content="rstudio/ii" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Case selection as a Decision Problem | Integrated Inferences" />
  
  <meta name="twitter:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="twitter:image" content="dnieperriver.png" />

<meta name="author" content="Macartan Humphreys and Alan Jacobs" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="wide.html">
<link rel="next" href="justifying-models.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="headers\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Model based causal inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-case-for-causal-models"><i class="fa fa-check"></i><b>1.1</b> The Case for Causal Models</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#the-limits-to-design-based-inference"><i class="fa fa-check"></i><b>1.1.1</b> The limits to design-based inference</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#qualitative-and-mixed-method-inference"><i class="fa fa-check"></i><b>1.1.2</b> Qualitative and mixed-method inference</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#connecting-theory-and-empirics"><i class="fa fa-check"></i><b>1.1.3</b> Connecting theory and empirics</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-contributions"><i class="fa fa-check"></i><b>1.2</b> Key contributions</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-road-ahead"><i class="fa fa-check"></i><b>1.3</b> The Road Ahead</a></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Causal Models</a><ul>
<li class="chapter" data-level="2.1" data-path="models.html"><a href="models.html#the-counterfactual-model"><i class="fa fa-check"></i><b>2.1</b> The counterfactual model</a></li>
<li class="chapter" data-level="2.2" data-path="models.html"><a href="models.html#causal-models-and-directed-acyclic-graphs"><i class="fa fa-check"></i><b>2.2</b> Causal Models and Directed Acyclic Graphs</a><ul>
<li class="chapter" data-level="2.2.1" data-path="models.html"><a href="models.html#components-of-a-causal-model"><i class="fa fa-check"></i><b>2.2.1</b> Components of a Causal Model</a></li>
<li class="chapter" data-level="2.2.2" data-path="models.html"><a href="models.html#interpretation-of-functional-equations"><i class="fa fa-check"></i><b>2.2.2</b> Interpretation of functional equations</a></li>
<li class="chapter" data-level="2.2.3" data-path="models.html"><a href="models.html#rules-for-graphing-causal-models"><i class="fa fa-check"></i><b>2.2.3</b> Rules for graphing causal models</a></li>
<li class="chapter" data-level="2.2.4" data-path="models.html"><a href="models.html#conditional-independence-from-dags"><i class="fa fa-check"></i><b>2.2.4</b> Conditional independence from DAGs</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="models.html"><a href="models.html#illustrations"><i class="fa fa-check"></i><b>2.3</b> Illustrations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="models.html"><a href="models.html#welfare-state-reform-pierson-1994"><i class="fa fa-check"></i><b>2.3.1</b> Welfare state reform: Pierson (1994)</a></li>
<li class="chapter" data-level="2.3.2" data-path="models.html"><a href="models.html#military-interventions-saunders-2011"><i class="fa fa-check"></i><b>2.3.2</b> Military Interventions: Saunders (2011)</a></li>
<li class="chapter" data-level="2.3.3" data-path="models.html"><a href="models.html#development-and-democratization-przeworski-and-limongi-1997"><i class="fa fa-check"></i><b>2.3.3</b> Development and Democratization: Przeworski and Limongi (1997)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="models.html"><a href="models.html#chapter-appendix"><i class="fa fa-check"></i><b>2.4</b> Chapter Appendix</a><ul>
<li class="chapter" data-level="2.4.1" data-path="models.html"><a href="models.html#steps-for-constructing-causal-models"><i class="fa fa-check"></i><b>2.4.1</b> Steps for constructing causal models</a></li>
<li class="chapter" data-level="2.4.2" data-path="models.html"><a href="models.html#model-construction-in-code"><i class="fa fa-check"></i><b>2.4.2</b> Model construction in code</a></li>
<li class="chapter" data-level="2.4.3" data-path="models.html"><a href="models.html#test-yourself-can-you-read-conditional-independence-from-a-graph"><i class="fa fa-check"></i><b>2.4.3</b> Test yourself! Can you read conditional independence from a graph?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>3</b> Theories as causal models</a><ul>
<li class="chapter" data-level="3.1" data-path="theory.html"><a href="theory.html#theory-as-a-lower-level-model"><i class="fa fa-check"></i><b>3.1</b> Theory as a “lower-level” model</a></li>
<li class="chapter" data-level="3.2" data-path="theory.html"><a href="theory.html#illustration-of-unpacking-causal-types"><i class="fa fa-check"></i><b>3.2</b> Illustration of unpacking causal types</a><ul>
<li class="chapter" data-level="3.2.1" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-mediation-model"><i class="fa fa-check"></i><b>3.2.1</b> Type disaggregation in a mediation model</a></li>
<li class="chapter" data-level="3.2.2" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-moderation-model"><i class="fa fa-check"></i><b>3.2.2</b> Type disaggregation in a moderation model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="theory.html"><a href="theory.html#rules-for-moving-between-higher--and-lower-level-models"><i class="fa fa-check"></i><b>3.3</b> Rules for moving between higher- and lower-level models</a><ul>
<li class="chapter" data-level="3.3.1" data-path="theory.html"><a href="theory.html#moving-down-levels"><i class="fa fa-check"></i><b>3.3.1</b> Moving down levels</a></li>
<li class="chapter" data-level="3.3.2" data-path="theory.html"><a href="theory.html#moving-up-levels"><i class="fa fa-check"></i><b>3.3.2</b> Moving up levels</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="theory.html"><a href="theory.html#conclusion"><i class="fa fa-check"></i><b>3.4</b> Conclusion</a></li>
<li class="chapter" data-level="3.5" data-path="theory.html"><a href="theory.html#chapter-appendices"><i class="fa fa-check"></i><b>3.5</b> Chapter Appendices</a><ul>
<li class="chapter" data-level="3.5.1" data-path="theory.html"><a href="theory.html#summary-boxes"><i class="fa fa-check"></i><b>3.5.1</b> Summary Boxes</a></li>
<li class="chapter" data-level="3.5.2" data-path="theory.html"><a href="theory.html#illustration-of-a-mapping-from-a-game-to-a-dag"><i class="fa fa-check"></i><b>3.5.2</b> Illustration of a Mapping from a Game to a DAG</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i><b>4</b> Causal Questions</a><ul>
<li class="chapter" data-level="4.1" data-path="questions.html"><a href="questions.html#case-level-causal-effects"><i class="fa fa-check"></i><b>4.1</b> Case-level causal effects</a></li>
<li class="chapter" data-level="4.2" data-path="questions.html"><a href="questions.html#case-level-causal-attribution"><i class="fa fa-check"></i><b>4.2</b> Case-level causal attribution</a></li>
<li class="chapter" data-level="4.3" data-path="questions.html"><a href="questions.html#case-level-explanation"><i class="fa fa-check"></i><b>4.3</b> Case-level explanation</a></li>
<li class="chapter" data-level="4.4" data-path="questions.html"><a href="questions.html#average-causal-effects"><i class="fa fa-check"></i><b>4.4</b> Average causal effects</a></li>
<li class="chapter" data-level="4.5" data-path="questions.html"><a href="questions.html#causal-paths"><i class="fa fa-check"></i><b>4.5</b> Causal Paths</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayeschapter.html"><a href="bayeschapter.html"><i class="fa fa-check"></i><b>5</b> Bayesian Answers</a><ul>
<li class="chapter" data-level="5.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-basics"><i class="fa fa-check"></i><b>5.1</b> Bayes Basics</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-instances"><i class="fa fa-check"></i><b>5.1.1</b> Simple instances</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-rule-for-discrete-hypotheses"><i class="fa fa-check"></i><b>5.1.2</b> Bayes’ Rule for Discrete Hypotheses</a></li>
<li class="chapter" data-level="5.1.3" data-path="bayeschapter.html"><a href="bayeschapter.html#the-dirichlet-family-and-bayes-rule-for-continuous-parameters"><i class="fa fa-check"></i><b>5.1.3</b> The Dirichlet family and Bayes’ Rule for Continuous Parameters</a></li>
<li class="chapter" data-level="5.1.4" data-path="bayeschapter.html"><a href="bayeschapter.html#moments"><i class="fa fa-check"></i><b>5.1.4</b> Moments</a></li>
<li class="chapter" data-level="5.1.5" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-estimation-in-practice"><i class="fa fa-check"></i><b>5.1.5</b> Bayes estimation in practice</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-applied"><i class="fa fa-check"></i><b>5.2</b> Bayes applied</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayesian-inference-on-queries"><i class="fa fa-check"></i><b>5.2.1</b> Bayesian Inference on Queries</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-bayesian-process-tracing"><i class="fa fa-check"></i><b>5.2.2</b> Simple Bayesian Process Tracing</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayeschapter.html"><a href="bayeschapter.html#three-principles-of-bayesian-updating"><i class="fa fa-check"></i><b>5.3</b> Three principles of Bayesian updating</a><ul>
<li class="chapter" data-level="5.3.1" data-path="bayeschapter.html"><a href="bayeschapter.html#AppPriors"><i class="fa fa-check"></i><b>5.3.1</b> Priors matter</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simultaneous-joint-updating"><i class="fa fa-check"></i><b>5.3.2</b> Simultaneous, joint updating</a></li>
<li class="chapter" data-level="5.3.3" data-path="bayeschapter.html"><a href="bayeschapter.html#posteriors-are-independent-of-the-ordering-of-data"><i class="fa fa-check"></i><b>5.3.3</b> Posteriors are independent of the ordering of data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Model-Based Causal Inference</b></span></li>
<li class="chapter" data-level="6" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>6</b> Process Tracing with Causal Models</a><ul>
<li class="chapter" data-level="6.1" data-path="pt.html"><a href="pt.html#process-tracing-and-causal-models"><i class="fa fa-check"></i><b>6.1</b> Process tracing and causal models</a><ul>
<li class="chapter" data-level="6.1.1" data-path="pt.html"><a href="pt.html#the-intuition"><i class="fa fa-check"></i><b>6.1.1</b> The intuition</a></li>
<li class="chapter" data-level="6.1.2" data-path="pt.html"><a href="pt.html#a-formalization-of-the-general-approach"><i class="fa fa-check"></i><b>6.1.2</b> A formalization of the general approach</a></li>
<li class="chapter" data-level="6.1.3" data-path="pt.html"><a href="pt.html#illustration-with-code"><i class="fa fa-check"></i><b>6.1.3</b> Illustration with code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pt.html"><a href="pt.html#five-principles"><i class="fa fa-check"></i><b>6.2</b> Five principles</a><ul>
<li class="chapter" data-level="6.2.1" data-path="pt.html"><a href="pt.html#classic-qualitative-tests-are-special-cases-of-updating-on-a-model"><i class="fa fa-check"></i><b>6.2.1</b> Classic qualitative tests are special cases of updating on a model</a></li>
<li class="chapter" data-level="6.2.2" data-path="pt.html"><a href="pt.html#conditional-independence-alone-does-not-provide-probative-value"><i class="fa fa-check"></i><b>6.2.2</b> Conditional independence alone does not provide probative value</a></li>
<li class="chapter" data-level="6.2.3" data-path="pt.html"><a href="pt.html#uncertainty-does-not-alter-inference-for-single-case-causal-inference"><i class="fa fa-check"></i><b>6.2.3</b> Uncertainty does not alter inference for single case causal inference</a></li>
<li class="chapter" data-level="6.2.4" data-path="pt.html"><a href="pt.html#probative-value-requires-d-connection"><i class="fa fa-check"></i><b>6.2.4</b> Probative value requires <span class="math inline">\(d-\)</span>connection</a></li>
<li class="chapter" data-level="6.2.5" data-path="pt.html"><a href="pt.html#probative-value"><i class="fa fa-check"></i><b>6.2.5</b> Probative value</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ptapp.html"><a href="ptapp.html"><i class="fa fa-check"></i><b>7</b> Application: Process Tracing with a Causal Model</a><ul>
<li class="chapter" data-level="7.1" data-path="ptapp.html"><a href="ptapp.html#inequality-and-democratization-the-debate"><i class="fa fa-check"></i><b>7.1</b> Inequality and Democratization: The Debate</a></li>
<li class="chapter" data-level="7.2" data-path="ptapp.html"><a href="ptapp.html#a-structural-causal-model"><i class="fa fa-check"></i><b>7.2</b> A Structural Causal Model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ptapp.html"><a href="ptapp.html#forming-priors"><i class="fa fa-check"></i><b>7.2.1</b> Forming Priors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ptapp.html"><a href="ptapp.html#results"><i class="fa fa-check"></i><b>7.3</b> Results</a></li>
<li class="chapter" data-level="7.4" data-path="ptapp.html"><a href="ptapp.html#pathways"><i class="fa fa-check"></i><b>7.4</b> Pathways</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ptapp.html"><a href="ptapp.html#cases-with-incomplete-data"><i class="fa fa-check"></i><b>7.4.1</b> Cases with incomplete data</a></li>
<li class="chapter" data-level="7.4.2" data-path="ptapp.html"><a href="ptapp.html#inferences-for-cases-with-observed-democratization"><i class="fa fa-check"></i><b>7.4.2</b> Inferences for cases with observed democratization</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ptapp.html"><a href="ptapp.html#model-definition-and-inference-in-code"><i class="fa fa-check"></i><b>7.5</b> Model definition and inference in code</a></li>
<li class="chapter" data-level="7.6" data-path="ptapp.html"><a href="ptapp.html#concluding-thoughts"><i class="fa fa-check"></i><b>7.6</b> Concluding thoughts</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixing.html"><a href="mixing.html"><i class="fa fa-check"></i><b>8</b> Integrated inferences</a><ul>
<li class="chapter" data-level="8.1" data-path="mixing.html"><a href="mixing.html#theres-only-ever-one-case"><i class="fa fa-check"></i><b>8.1</b> There’s only ever one case</a></li>
<li class="chapter" data-level="8.2" data-path="mixing.html"><a href="mixing.html#general-procedure"><i class="fa fa-check"></i><b>8.2</b> General procedure</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mixing.html"><a href="mixing.html#estimation"><i class="fa fa-check"></i><b>8.2.1</b> Estimation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mixing.html"><a href="mixing.html#illustration"><i class="fa fa-check"></i><b>8.3</b> Illustration</a></li>
<li class="chapter" data-level="8.4" data-path="mixing.html"><a href="mixing.html#illustrated-inferences"><i class="fa fa-check"></i><b>8.4</b> Illustrated inferences</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mixing.html"><a href="mixing.html#xy-model"><i class="fa fa-check"></i><b>8.4.1</b> XY model</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mixing.html"><a href="mixing.html#considerations"><i class="fa fa-check"></i><b>8.5</b> Considerations</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mixing.html"><a href="mixing.html#the-identification-problem"><i class="fa fa-check"></i><b>8.5.1</b> The identification problem</a></li>
<li class="chapter" data-level="8.5.2" data-path="mixing.html"><a href="mixing.html#continuous-data"><i class="fa fa-check"></i><b>8.5.2</b> Continuous data</a></li>
<li class="chapter" data-level="8.5.3" data-path="mixing.html"><a href="mixing.html#measurement-error"><i class="fa fa-check"></i><b>8.5.3</b> Measurement error</a></li>
<li class="chapter" data-level="8.5.4" data-path="mixing.html"><a href="mixing.html#spillovers"><i class="fa fa-check"></i><b>8.5.4</b> Spillovers</a></li>
<li class="chapter" data-level="8.5.5" data-path="mixing.html"><a href="mixing.html#clustering-and-other-violations-of-independence"><i class="fa fa-check"></i><b>8.5.5</b> Clustering and other violations of independence</a></li>
<li class="chapter" data-level="8.5.6" data-path="mixing.html"><a href="mixing.html#parameteric-models"><i class="fa fa-check"></i><b>8.5.6</b> Parameteric models</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mixing.html"><a href="mixing.html#conclusion-1"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixingapp.html"><a href="mixingapp.html"><i class="fa fa-check"></i><b>9</b> Mixed-Method Application: Inequality and Democracy Revisited</a><ul>
<li class="chapter" data-level="9.1" data-path="mixingapp.html"><a href="mixingapp.html#a-trained-model"><i class="fa fa-check"></i><b>9.1</b> A trained model</a></li>
<li class="chapter" data-level="9.2" data-path="mixingapp.html"><a href="mixingapp.html#data"><i class="fa fa-check"></i><b>9.2</b> Data</a></li>
<li class="chapter" data-level="9.3" data-path="mixingapp.html"><a href="mixingapp.html#inference"><i class="fa fa-check"></i><b>9.3</b> Inference</a><ul>
<li class="chapter" data-level="9.3.1" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-cause-democracy"><i class="fa fa-check"></i><b>9.3.1</b> Did inequality <em>cause</em> democracy?</a></li>
<li class="chapter" data-level="9.3.2" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-prevent-democracy"><i class="fa fa-check"></i><b>9.3.2</b> Did inequality <em>prevent</em> democracy?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="mixingapp.html"><a href="mixingapp.html#prior-posterior-comparison-for-multiple-estimands"><i class="fa fa-check"></i><b>9.4</b> Prior / posterior comparison for multiple estimands</a></li>
<li class="chapter" data-level="9.5" data-path="mixingapp.html"><a href="mixingapp.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
</ul></li>
<li class="part"><span><b>III Design Choices</b></span></li>
<li class="chapter" data-level="10" data-path="elements-of-design.html"><a href="elements-of-design.html"><i class="fa fa-check"></i><b>10</b> Elements of Design</a><ul>
<li class="chapter" data-level="10.1" data-path="elements-of-design.html"><a href="elements-of-design.html#model-inquiry-data-strategy-answer-strategy"><i class="fa fa-check"></i><b>10.1</b> Model, inquiry, data strategy, answer strategy</a><ul>
<li class="chapter" data-level="10.1.1" data-path="elements-of-design.html"><a href="elements-of-design.html#defining-a-model"><i class="fa fa-check"></i><b>10.1.1</b> Defining a model</a></li>
<li class="chapter" data-level="10.1.2" data-path="elements-of-design.html"><a href="elements-of-design.html#choosing-estimands"><i class="fa fa-check"></i><b>10.1.2</b> Choosing estimands</a></li>
<li class="chapter" data-level="10.1.3" data-path="elements-of-design.html"><a href="elements-of-design.html#selecting-a-data-strategy"><i class="fa fa-check"></i><b>10.1.3</b> Selecting a data strategy</a></li>
<li class="chapter" data-level="10.1.4" data-path="elements-of-design.html"><a href="elements-of-design.html#answer-strategy"><i class="fa fa-check"></i><b>10.1.4</b> Answer strategy</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="elements-of-design.html"><a href="elements-of-design.html#evaluating-a-mixed-method-design"><i class="fa fa-check"></i><b>10.2</b> Evaluating a mixed method design</a><ul>
<li class="chapter" data-level="10.2.1" data-path="elements-of-design.html"><a href="elements-of-design.html#other-loss-functions"><i class="fa fa-check"></i><b>10.2.1</b> Other loss functions</a></li>
<li class="chapter" data-level="10.2.2" data-path="elements-of-design.html"><a href="elements-of-design.html#other-measures-of-a-gain-from-a-theory"><i class="fa fa-check"></i><b>10.2.2</b> Other measures of a gain from a theory</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-of-in-code"><i class="fa fa-check"></i><b>10.3</b> Illustration of in code</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="clue-selection-as-a-decision-problem.html"><a href="clue-selection-as-a-decision-problem.html"><i class="fa fa-check"></i><b>11</b> Clue Selection as a Decision Problem</a><ul>
<li class="chapter" data-level="11.1" data-path="clue-selection-as-a-decision-problem.html"><a href="clue-selection-as-a-decision-problem.html#a-strategic-approach"><i class="fa fa-check"></i><b>11.1</b> A strategic approach</a></li>
<li class="chapter" data-level="11.2" data-path="clue-selection-as-a-decision-problem.html"><a href="clue-selection-as-a-decision-problem.html#clue-selection-for-the-running-example"><i class="fa fa-check"></i><b>11.2</b> Clue selection for the running example</a><ul>
<li class="chapter" data-level="11.2.1" data-path="clue-selection-as-a-decision-problem.html"><a href="clue-selection-as-a-decision-problem.html#dynamic-strategies"><i class="fa fa-check"></i><b>11.2.1</b> Dynamic Strategies</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="clue-selection-as-a-decision-problem.html"><a href="clue-selection-as-a-decision-problem.html#clue-selection-for-the-democracy-model"><i class="fa fa-check"></i><b>11.3</b> Clue selection for the Democracy model</a></li>
<li class="chapter" data-level="11.4" data-path="clue-selection-as-a-decision-problem.html"><a href="clue-selection-as-a-decision-problem.html#conclusion-2"><i class="fa fa-check"></i><b>11.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="wide.html"><a href="wide.html"><i class="fa fa-check"></i><b>12</b> Going wide and going deep</a><ul>
<li class="chapter" data-level="12.1" data-path="wide.html"><a href="wide.html#intuitions-does-a-sufficiently-large-n-always-trump-k"><i class="fa fa-check"></i><b>12.1</b> Intuitions: Does a sufficiently large <span class="math inline">\(N\)</span> always trump <span class="math inline">\(K\)</span>?</a></li>
<li class="chapter" data-level="12.2" data-path="wide.html"><a href="wide.html#evaluating-strategies"><i class="fa fa-check"></i><b>12.2</b> Evaluating strategies</a></li>
<li class="chapter" data-level="12.3" data-path="wide.html"><a href="wide.html#varieties"><i class="fa fa-check"></i><b>12.3</b> Varieties of mixing</a></li>
<li class="chapter" data-level="12.4" data-path="wide.html"><a href="wide.html#probative-value-of-clues"><i class="fa fa-check"></i><b>12.4</b> Probative value of clues</a></li>
<li class="chapter" data-level="12.5" data-path="wide.html"><a href="wide.html#effect-heterogeneity"><i class="fa fa-check"></i><b>12.5</b> Effect Heterogeneity</a></li>
<li class="chapter" data-level="12.6" data-path="wide.html"><a href="wide.html#uncertainty-regarding-assignment-processes"><i class="fa fa-check"></i><b>12.6</b> Uncertainty Regarding Assignment Processes</a></li>
<li class="chapter" data-level="12.7" data-path="wide.html"><a href="wide.html#uncertainty-regarding-the-probative-value-of-clues"><i class="fa fa-check"></i><b>12.7</b> Uncertainty regarding the probative value of clues</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="caseselection.html"><a href="caseselection.html"><i class="fa fa-check"></i><b>13</b> Case selection as a Decision Problem</a><ul>
<li class="chapter" data-level="13.1" data-path="caseselection.html"><a href="caseselection.html#explorations"><i class="fa fa-check"></i><b>13.1</b> Explorations</a></li>
<li class="chapter" data-level="13.2" data-path="caseselection.html"><a href="caseselection.html#diagnosing-case-selection-strategies-procedure"><i class="fa fa-check"></i><b>13.2</b> Diagnosing case-selection strategies: procedure</a></li>
<li class="chapter" data-level="13.3" data-path="caseselection.html"><a href="caseselection.html#evaluating-types-of-strategies"><i class="fa fa-check"></i><b>13.3</b> Evaluating types of strategies</a></li>
<li class="chapter" data-level="13.4" data-path="caseselection.html"><a href="caseselection.html#compare-multiple-data-strategies"><i class="fa fa-check"></i><b>13.4</b> Compare multiple data strategies</a></li>
<li class="chapter" data-level="13.5" data-path="caseselection.html"><a href="caseselection.html#experiments"><i class="fa fa-check"></i><b>13.5</b> Experiments</a></li>
<li class="chapter" data-level="13.6" data-path="caseselection.html"><a href="caseselection.html#chapter-appendix-accounting-for-case-selection"><i class="fa fa-check"></i><b>13.6</b> Chapter Appendix: Accounting for case selection</a><ul>
<li class="chapter" data-level="13.6.1" data-path="caseselection.html"><a href="caseselection.html#independent-case-selection-strategy"><i class="fa fa-check"></i><b>13.6.1</b> Independent case selection strategy</a></li>
<li class="chapter" data-level="13.6.2" data-path="caseselection.html"><a href="caseselection.html#conditional-random-case-selection"><i class="fa fa-check"></i><b>13.6.2</b> Conditional random case selection</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Models in Question</b></span></li>
<li class="chapter" data-level="14" data-path="justifying-models.html"><a href="justifying-models.html"><i class="fa fa-check"></i><b>14</b> Justifying models</a><ul>
<li class="chapter" data-level="14.1" data-path="justifying-models.html"><a href="justifying-models.html#bounds-on-probative-value"><i class="fa fa-check"></i><b>14.1</b> Bounds on probative value</a></li>
<li class="chapter" data-level="14.2" data-path="justifying-models.html"><a href="justifying-models.html#the-possibility-of-identification-of-probative-value-from-experimental-data"><i class="fa fa-check"></i><b>14.2</b> The possibility of identification of probative value from experimental data</a><ul>
<li class="chapter" data-level="14.2.1" data-path="justifying-models.html"><a href="justifying-models.html#mediator"><i class="fa fa-check"></i><b>14.2.1</b> Mediator</a></li>
<li class="chapter" data-level="14.2.2" data-path="justifying-models.html"><a href="justifying-models.html#moderator"><i class="fa fa-check"></i><b>14.2.2</b> Moderator</a></li>
<li class="chapter" data-level="14.2.3" data-path="justifying-models.html"><a href="justifying-models.html#case-level-bounds-from-mixed-data"><i class="fa fa-check"></i><b>14.2.3</b> Case level bounds from mixed data</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="justifying-models.html"><a href="justifying-models.html#learning-across-populations"><i class="fa fa-check"></i><b>14.3</b> Learning across populations</a></li>
<li class="chapter" data-level="14.4" data-path="justifying-models.html"><a href="justifying-models.html#different-models-for-different-sites"><i class="fa fa-check"></i><b>14.4</b> Different models for different sites</a><ul>
<li class="chapter" data-level="14.4.1" data-path="justifying-models.html"><a href="justifying-models.html#observational-and-experimental"><i class="fa fa-check"></i><b>14.4.1</b> Observational and experimental</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="justifying-models.html"><a href="justifying-models.html#causal-discovery"><i class="fa fa-check"></i><b>14.5</b> Causal discovery</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="evaluation.html"><a href="evaluation.html"><i class="fa fa-check"></i><b>15</b> Evaluating models</a><ul>
<li class="chapter" data-level="15.1" data-path="evaluation.html"><a href="evaluation.html#inferences-when-you-dont-buy-your-priors"><i class="fa fa-check"></i><b>15.1</b> Inferences when you don;t buy your priors</a></li>
<li class="chapter" data-level="15.2" data-path="evaluation.html"><a href="evaluation.html#tools-for-evaluating-models"><i class="fa fa-check"></i><b>15.2</b> Tools for evaluating models</a><ul>
<li class="chapter" data-level="15.2.1" data-path="evaluation.html"><a href="evaluation.html#check-conditional-independencies"><i class="fa fa-check"></i><b>15.2.1</b> Check conditional independencies</a></li>
<li class="chapter" data-level="15.2.2" data-path="evaluation.html"><a href="evaluation.html#check-confounding-assumptions"><i class="fa fa-check"></i><b>15.2.2</b> Check confounding assumptions</a></li>
<li class="chapter" data-level="15.2.3" data-path="evaluation.html"><a href="evaluation.html#check-prior-dependence"><i class="fa fa-check"></i><b>15.2.3</b> Check prior dependence</a></li>
<li class="chapter" data-level="15.2.4" data-path="evaluation.html"><a href="evaluation.html#check-fit"><i class="fa fa-check"></i><b>15.2.4</b> Check fit</a></li>
<li class="chapter" data-level="15.2.5" data-path="evaluation.html"><a href="evaluation.html#assess-likelihoods-under-different-models"><i class="fa fa-check"></i><b>15.2.5</b> Assess likelihoods under different models</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="evaluation.html"><a href="evaluation.html#evaluating-the-democracy-inequality-model"><i class="fa fa-check"></i><b>15.3</b> Evaluating the Democracy-Inequality model</a><ul>
<li class="chapter" data-level="15.3.1" data-path="evaluation.html"><a href="evaluation.html#prior-check"><i class="fa fa-check"></i><b>15.3.1</b> Prior check</a></li>
<li class="chapter" data-level="15.3.2" data-path="evaluation.html"><a href="evaluation.html#monotonic-restrictions"><i class="fa fa-check"></i><b>15.3.2</b> Monotonic restrictions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>16</b> Final Words</a><ul>
<li class="chapter" data-level="16.1" data-path="final-words.html"><a href="final-words.html#general-lessons"><i class="fa fa-check"></i><b>16.1</b> General lessons</a></li>
<li class="chapter" data-level="16.2" data-path="final-words.html"><a href="final-words.html#worries-about-what-you-have-to-put-in"><i class="fa fa-check"></i><b>16.2</b> Worries about what you have to put in</a></li>
<li class="chapter" data-level="16.3" data-path="final-words.html"><a href="final-words.html#limits-on-what-you-can-get-out"><i class="fa fa-check"></i><b>16.3</b> Limits on what you can get out</a></li>
<li class="chapter" data-level="16.4" data-path="final-words.html"><a href="final-words.html#a-world-of-models-practical-steps-forward-for-collective-cumulation"><i class="fa fa-check"></i><b>16.4</b> A world of models: Practical steps forward for collective cumulation</a></li>
</ul></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li class="chapter" data-level="17" data-path="examplesappendix.html"><a href="examplesappendix.html"><i class="fa fa-check"></i><b>17</b> Analysis of canonical models with <code>gbiqq</code></a><ul>
<li class="chapter" data-level="17.1" data-path="examplesappendix.html"><a href="examplesappendix.html#x-causes-y-no-confounding"><i class="fa fa-check"></i><b>17.1</b> <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, no confounding</a></li>
<li class="chapter" data-level="17.2" data-path="examplesappendix.html"><a href="examplesappendix.html#x-causes-y-with-unmodelled-confounding"><i class="fa fa-check"></i><b>17.2</b> <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, with unmodelled confounding</a></li>
<li class="chapter" data-level="17.3" data-path="examplesappendix.html"><a href="examplesappendix.html#x-causes-y-with-confounding-modelled"><i class="fa fa-check"></i><b>17.3</b> <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, with confounding modelled</a></li>
<li class="chapter" data-level="17.4" data-path="examplesappendix.html"><a href="examplesappendix.html#simple-mediation-model"><i class="fa fa-check"></i><b>17.4</b> Simple mediation model</a></li>
<li class="chapter" data-level="17.5" data-path="examplesappendix.html"><a href="examplesappendix.html#simple-moderator-model"><i class="fa fa-check"></i><b>17.5</b> Simple moderator model</a></li>
<li class="chapter" data-level="17.6" data-path="examplesappendix.html"><a href="examplesappendix.html#an-iv-model"><i class="fa fa-check"></i><b>17.6</b> An IV model</a></li>
<li class="chapter" data-level="17.7" data-path="examplesappendix.html"><a href="examplesappendix.html#a-model-that-allows-application-of-the-frontdoor-criterion"><i class="fa fa-check"></i><b>17.7</b> A model that allows application of the frontdoor criterion</a></li>
<li class="chapter" data-level="17.8" data-path="examplesappendix.html"><a href="examplesappendix.html#a-model-with-a-violation-of-sequential-ignorability"><i class="fa fa-check"></i><b>17.8</b> A model with a violation of sequential ignorability</a></li>
<li class="chapter" data-level="17.9" data-path="examplesappendix.html"><a href="examplesappendix.html#learning-from-a-collider"><i class="fa fa-check"></i><b>17.9</b> Learning from a collider</a></li>
<li class="chapter" data-level="17.10" data-path="examplesappendix.html"><a href="examplesappendix.html#a-model-mixing-observational-and-experimental-data"><i class="fa fa-check"></i><b>17.10</b> A model mixing observational and experimental data</a></li>
<li class="chapter" data-level="17.11" data-path="examplesappendix.html"><a href="examplesappendix.html#transportation-of-findings-across-contexts"><i class="fa fa-check"></i><b>17.11</b> Transportation of findings across contexts</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/macartan/gbiqq/" target="blank">Uses gbiqq</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Integrated Inferences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="caseselection" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Case selection as a Decision Problem</h1>
<hr />
<p>With a causal model in hand, together with priors over parameters, you can assess in advance what conclusions you will draw from different observations and assess what kinds of observations are most worth seeking. We draw out the implications of this idea for case selection.</p>
<hr />
<p>A critical decision for scholars employing mixed methods is to determine which cases are most valuable for within-case analysis.</p>
<p>A host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> data. Perhaps the most common strategy is to select cases in which <span class="math inline">\(X=1\)</span> and <span class="math inline">\(Y=1\)</span> and look to see whether in fact <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases “on the regression line” or, for some purposes, cases “off the regression line” (e.g., <span class="citation">Lieberman (<a href="#ref-Lieberman2005nested">2005</a>)</span>). Some scholars suggest ensuring variation in <span class="math inline">\(X\)</span> (most prominently, <span class="citation">King, Keohane, and Verba (<a href="#ref-king1994designing">1994</a>)</span>), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> (<span class="citation">Fearon and Laitin (<a href="#ref-FL2008">2008</a>)</span>, <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span>).</p>
<p>Which cases you should choose will likely depend on the purposes to which you want to put them.</p>
<p>A matching strategy for instance—selecting cases that are comparable on many features but that differ on <span class="math inline">\(X\)</span>—replicates at a small scale the kind of inference done by matching estimators with large-<span class="math inline">\(n\)</span> data. The strategy emphasize the inferences to be made from <span class="math inline">\(X,Y\)</span> variation rather than inferences drawn specifically from within case information beyond what is available in the measurement of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. (Citations needed.)</p>
<p>Other treatments seek to use qualitative information to check assumptions made in <span class="math inline">\(X, Y\)</span> analysis: for example, is the measurement of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> reliable in critical cases? (Citations needed) For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the <span class="math inline">\(X,Y\)</span> inferences: for example influential cases that have unusually extreme values on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims.</p>
<p>A third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman’s “model-building” mode of “nested analysis” (<span class="citation">Lieberman (<a href="#ref-Lieberman2005nested">2005</a>)</span>). Here it may be cases off the regression line that are of interest.</p>
<p>Weller and Barnes (CITE article) on case selection focus on (a) X/Y relations and (b) whether the cases are useful for hypothesis generation.</p>
<p>In what follows, we focus on a simpler goal: given existing <span class="math inline">\(X, Y\)</span> data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>?</p>
<p>The basic insight of this chapter is simple enough: <em>the optimal strategy for case selection for a model-based analysis can be determined by the model and the query</em>, just as we saw for the optimal clue-selection strategy in Chapter <a href="#Clues"><strong>??</strong></a>. Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as:</p>
<ul>
<li>go where the probative value is, and</li>
<li>sample from <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> values in proportion to their occurrence in the population,</li>
<li>invest in collections of cases that provide complementary learning.</li>
</ul>
<p>Beyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with <span class="math inline">\(X, Y, K\)</span> data.</p>
<p>For the general intuition, recall that the probative value of a process-tracing test hinges on the difference in clue likelihoods associated with the alternative hypotheses in play for a given case. Recall that for different values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> cell, we want to use process tracing to help us distinguish between two specific types that are consistent with the <span class="math inline">\(X, Y\)</span> pattern. Which types are in question varies across <span class="math inline">\(X,Y\)</span> combinations. Table <a href="caseselection.html#tab:FP">13.1</a> illustrates.</p>
<table>
<caption><span id="tab:FP">Table 13.1: </span>. The ambiguity about types in each <span class="math inline">\(X, Y\)</span> cell.</caption>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(Y=0\)</span></th>
<th align="center"><span class="math inline">\(Y=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(X=0\)</span></td>
<td align="center"><span class="math inline">\(b\)</span> or <span class="math inline">\(c\)</span></td>
<td align="center"><span class="math inline">\(a\)</span> or <span class="math inline">\(d\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(X=1\)</span></td>
<td align="center"><span class="math inline">\(a\)</span> or <span class="math inline">\(c\)</span></td>
<td align="center"><span class="math inline">\(b\)</span> or <span class="math inline">\(d\)</span></td>
</tr>
</tbody>
</table>
<p>Thus, in the <span class="math inline">\(X=0, Y=0\)</span> cell, what would be most useful is a clue that has high probative value in distinguishing between an untreated (<span class="math inline">\(X=0\)</span>) <span class="math inline">\(b\)</span> type and and an untreated <span class="math inline">\(c\)</span> type. For a case in the <span class="math inline">\(X=1, Y=0\)</span> cell, on the other hand, what matters is how well the clue can discriminate between treated (<span class="math inline">\(X=1\)</span>) <span class="math inline">\(a\)</span> and <span class="math inline">\(c\)</span> types. In our notation, it is the difference in <span class="math inline">\(\phi_{jx}\)</span> values for that indicates these cell-specific degrees of leverage.</p>
<p>To illustrate, consider a situation in which for a given clue we have <span class="math inline">\(\phi_{b1}\)</span>=0.5<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>; <span class="math inline">\(\phi_{d1}\)</span>=0.5;<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> <span class="math inline">\(\phi_{b0}\)</span>=0.5^<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>; and <span class="math inline">\(\phi_{c0}\)</span>=0.1<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. In this situation, searching for the clue in <span class="math inline">\(X=Y=1\)</span> cases will yield no leverage since the clue does not discriminate between the two types (<span class="math inline">\(b\)</span> and <span class="math inline">\(d\)</span>) that need to be distinguished given <span class="math inline">\(X=Y=1\)</span>. Here there is no additional learning about <span class="math inline">\(\lambda_b\)</span> that can be gained from looking for the clue. In contrast, <span class="math inline">\(X=0, Y=0\)</span> cases will be informative since the clue is much better at distinguishing between <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> types—the two types in contention for this kind of case. Thus, although process tracing here does not provide information on the prevalence of positive causal effects (<span class="math inline">\(b\)</span> types) for an <span class="math inline">\(X=Y=1\)</span> case, it does provide information when <span class="math inline">\(X=Y=0\)</span>.</p>
<p>While it is common practice for mixed-method researchers to perform their process tracing “on the regression line,” the BIQQ framework suggests that the gains to process tracing for different <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> values in fact depend on the particular constellations of <span class="math inline">\(\phi\)</span> values for the potentially available clues. More generally, the framework allows one to assess the expected gains from any given case-selection strategy <em>ex ante</em> once priors have been specified.</p>
<div id="explorations" class="section level2">
<h2><span class="header-section-number">13.1</span> Explorations</h2>
<p>Most closely related to our analysis in this chapter is the contribution of <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span>, who build on <span class="citation">Seawright and Gerring (<a href="#ref-SeawrightGerring2008">2008</a>)</span>. While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches. As we do, <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> focus on a situation with binary <span class="math inline">\(X,Y\)</span> data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, <span class="math inline">\(Z_i\)</span> is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to “treatment”, or an <span class="math inline">\(X=1\)</span> value). <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly.</p>
<p>Our setup differs from that in <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> in a few ways. <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> paramaterize differently, though this difference is not important.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> Perhaps the most important difference between our analysis and that in <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> is that we connect the inference strategy to process-tracing approaches. Whereas <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> assume that causal types can be read directly, we assume that these are inferred <em>imperfectly</em> from clues. As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of <span class="math inline">\(X\)</span>. And, as in the baseline model, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases.</p>
<!-- Are Herron and Quinn's priors Jeffrey priors? -->
<p>Here we assume that the case selection decision is made after observing the <span class="math inline">\(XY\)</span> distribution and we explore a range of different possible contingency tables. In <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> the distribution from which the contingency tables are drawn is fixed, though set to exhibit an expected observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large <span class="math inline">\(XY\)</span> data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases.</p>
<p>Another important difference, is that in many of their analyses, <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has <em>beliefs</em> about the true treatment effect that correspond to their priors, and for whom there is therefore no <em>expected</em> bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher’s beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)?</p>
<p>Finally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by <span class="citation">Seawright and Gerring (<a href="#ref-SeawrightGerring2008">2008</a>)</span>) and evaluating them, we define a strategy as the particular distribution over <span class="math inline">\(XY\)</span> cells to be examined and proceed to examine <em>every possible strategy</em> given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies—those strategies that perform similarly—emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies.</p>
<p>Despite these various differences, our results will agree in key ways with those in <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span>.</p>
</div>
<div id="diagnosing-case-selection-strategies-procedure" class="section level2">
<h2><span class="header-section-number">13.2</span> Diagnosing case-selection strategies: procedure</h2>
<p>We describe here the procedure through which we can use <code>gbiqq</code> to diagnose and compare case-selection strategies given (i) a causal model, (ii) any data we have already observed, and (iii) the causal query we seek to answer. The general intuition is that we can use the causal model and any previously observed data to estimate what observations we are more or less likely to make under a given case-selection strategy, and then figure out how far off from the (under the model) true estimand we can expect to be under the strategy, given whatever causal question we seek to answer.</p>
<p>To illustrate the procedure, suppose that we want to estimate the average treatment effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> in a population and have initially observed <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> data on 6 cases. The initial data are summarized in Table <a href="#tab:6xycases"><strong>??</strong></a>.</p>
<table>
<thead>
<tr class="header">
<th align="left">event</th>
<th align="left">strategy</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X0Y0</td>
<td align="left">XY</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">X1Y0</td>
<td align="left">XY</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">X0Y1</td>
<td align="left">XY</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">X1Y1</td>
<td align="left">XY</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>As we can see, there are four cases where <span class="math inline">\(X=Y\)</span>, and two where <span class="math inline">\(X \neq Y\)</span>. Suppose that we are now considering gathering process-tracing evidence for 1 of these cases to inform our estimate of the ATE. There are many different case-selection strategies we might pursue, and each of these can give rise to different possible data and thus to different possible conclusions. What should we do?</p>
<p><strong>DAG</strong>. We start, as always, with a DAG representing our beliefs about which variables we believe to be direct causes of other variables. For the current illustration, suppose that we are operating with a simple mediation model, <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span>.</p>
<p><strong>Given data.</strong> If we have already observed something in a set of cases, we can use this information to condition our strategy for searching for further information. For instance, if we have observed <span class="math inline">\(X\)</span>’s and <span class="math inline">\(Y\)</span>’s value in a set of cases, we might select cases for process tracing based on their values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Further, what we have already observed in the cases may constrain what possible data we could end up with once we have collected the additional (process tracing) data.</p>
<p><strong>Priors</strong>. As when conducting mixed-method inference, we can set qualitative restrictions and/or differential quantitative weights on the (possibly conditional) nodal types in the model. And we can indicate our uncertainty over the latter, by setting the <span class="math inline">\(\alpha\)</span> parameters of the relevant Dirichlet distributions. For the current example, let us define restrictions at both the <span class="math inline">\(M\)</span> and <span class="math inline">\(Y\)</span> nodes, positing beliefs that <span class="math inline">\(X\)</span> never has a negative effect on <span class="math inline">\(M\)</span> and that <span class="math inline">\(M\)</span> never has a negative effect on <span class="math inline">\(Y\)</span>. Let us further assume that we have flat priors over the remaining nodal types and posit similar assignment propensities for all types (no unobserved confounding).</p>
<p><strong>Query</strong>. We define our query. This might, for instance, be the share of cases in the population in which <span class="math inline">\(X\)</span> has a positive effect on <span class="math inline">\(Y\)</span>; or it might be <span class="math inline">\(X\)</span>’s average effect on <span class="math inline">\(Y\)</span>. We can use the general procedure to identify case-selection strategies for any causal query that can be defined on a DAG. And, importantly, the optimal case-selection strategy may depend on the query. For instance, the best case-selection strategy for estimating the average causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> may not be the same as the best strategy for figuring out for what proportion of the population <span class="math inline">\(X\)</span> has a positive effect on <span class="math inline">\(Y\)</span>.</p>
<p><strong>Define one or more strategies</strong>. A strategy is defined, generically, as the search for data on a given set of <em>nodes</em>, in a given <em>number</em> of cases randomly selected <em>conditional</em> on some information we already have about potential cases. Let us assume here that our strategy will involve uncovering <span class="math inline">\(M\)</span>’s value in 1 case—but we are wondering how to choose this case. Consider four possible strategies, conditional on the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> values that we already know. We could do process tracing on a randomly selected <span class="math inline">\(X=1, Y=1\)</span> case, a randomly selected <span class="math inline">\(X=0, Y=0\)</span> case, the <span class="math inline">\(X=1, Y=0\)</span> case, or the <span class="math inline">\(X=0, Y=1\)</span> case. We itemize this set of possible strategies in the first column in Table <a href="#tab:caseselect1"><strong>??</strong></a>.</p>
<p><strong>Possible data</strong>. For each strategy, there are multiple possible sets of data that we could end up observing. In particular, the data we could end up with will be the <span class="math inline">\(X,Y\)</span> patterns we have already observed plus <em>either</em> <span class="math inline">\(M=0\)</span> <em>or</em> <span class="math inline">\(M=1\)</span> in the case that our strategy leads us to select for process tracing. We represent the data possibilities (showing just the possible <span class="math inline">\(M\)</span> values) in the second column in Table <a href="#tab:caseselect1"><strong>??</strong></a>. Thus, for instance, for a strategy in which we choose a random <span class="math inline">\(X=1, Y=1\)</span> case, we could end up observing the initial <span class="math inline">\(X,Y\)</span> pattern plus <span class="math inline">\(M=0\)</span> in one of the <span class="math inline">\(X=1, Y=1\)</span> cases, or the initial <span class="math inline">\(X,Y\)</span> pattern plus <span class="math inline">\(M=1\)</span> in one of the <span class="math inline">\(X=1, Y=1\)</span> cases.</p>
<p><strong>Probability of the data</strong>. We now calculate a probability of each possible data realization, given the model and the data (the <span class="math inline">\(X\)</span>’s and <span class="math inline">\(Y\)</span>’s) that we have already observed. In practice, we do this in <code>gbiqq</code> via simulation. Starting with the model together with our priors, we update our beliefs about <span class="math inline">\(\lambda\)</span> based on the initial <span class="math inline">\(X,Y\)</span> data. This posterior now represents our <em>prior</em> for the purposes of the process tracing; it represents what we believe about causal-type share allocations in the population, having seen the <span class="math inline">\(X,Y\)</span> data only. We then use this posterior to draw a series of <span class="math inline">\(\lambda\)</span> values.</p>
<p>Given that the ambiguity matrix gives us the mapping from causal types to data realizations, we can calculate for each <span class="math inline">\(lambda\)</span> draw the probability of each data possibility given that particular <span class="math inline">\(\lambda\)</span> and the strategy. We then average across repeated <span class="math inline">\(\lambda\)</span> draws. (Since <span class="math inline">\(\lambda\)</span>’s are being drawn from our prior, we are automatically weighting more heavily those <span class="math inline">\(\lambda\)</span>’s that we believe to be most likely.) We show the data probabilities in the third column of Table <a href="#tab:caseselect1"><strong>??</strong></a>: one probability for each data-possibility given each strategy.</p>
<p><strong>Posterior variance on estimate given the data</strong>. For each data possibility, we can then use <code>gibiqq</code> to ask what inference we would get from each data possibility, given whatever query we seek to answer. What we are in fact interested in for the purposes of case selection is the <em>variance</em> of the posterior. We indicate in the fourth column of Table <a href="#tab:caseselect1"><strong>??</strong></a> the posterior variance for each possible data realization.</p>
<p><strong>Expected posterior variance under each strategy</strong>. The quantity of ultimate interest is the posterior variance that we expect to end up with under each <em>strategy</em>. Calculating this expectation is now elementary as we have both the posterior variance arising from each data possibility and the probability of each data possibility (given our prior beliefs and the data already observed). The expected posterior variance is simply an average of the posterior variances under each data possibility, weighted by the probability of each data possibility. The final column provides the expected posterior variances, one for each strategy.</p>
<p>FLAG: Write code and generate results table, caseselect1. Columns are: Strategy, Possible Data [with row split into M=0, M=1], Probability of Data (for each M value), Posterior Variance (for each M value), Expected Posterior Variance (one value per strategy, weighted average over M values). Strategies are select an <span class="math inline">\(X=0, Y=0\)</span> case; select an <span class="math inline">\(X=1, Y=1\)</span> case; select an <span class="math inline">\(X=1, Y=0\)</span> case; select an <span class="math inline">\(X=0, Y=1\)</span> case.</p>
<p>We see that XXXXXXX</p>
<p>We can similarly diagnose strategies for a plan to process-trace any number of the cases for which we have <span class="math inline">\(X,Y\)</span> data. In Table <a href="#tab:caseselect3"><strong>??</strong></a>, we show results for six of the possible strategies we might employ for collecting <span class="math inline">\(M\)</span> for 3 cases. We compare going deeper into two <span class="math inline">\(X=Y=1\)</span> cases and one <span class="math inline">\(X=Y=0\)</span> case; two <span class="math inline">\(X=Y=1\)</span> cases and an <span class="math inline">\(X=0, Y=1\)</span> case; two <span class="math inline">\(X=Y=1\)</span> cases and an <span class="math inline">\(X=1, Y=0\)</span> case; and an <span class="math inline">\(X=0, Y=1\)</span>, an <span class="math inline">\(X=1, Y=0\)</span>, and an <span class="math inline">\(X=1, Y=1\)</span> case; two <span class="math inline">\(X=Y=0\)</span> cases and one <span class="math inline">\(X=Y=1\)</span> case; two <span class="math inline">\(X=Y=0\)</span> cases and an <span class="math inline">\(X=0, Y=1\)</span> case; two <span class="math inline">\(X=Y=0\)</span> cases and an <span class="math inline">\(X=1, Y=0\)</span> case; and an <span class="math inline">\(X=0, Y=1\)</span>, an <span class="math inline">\(X=1, Y=0\)</span>, and an <span class="math inline">\(X=1, Y=0\)</span> case.</p>
<p>FLAG: Write code and generate results table, caseselect3. Same structure as caseselect1.</p>
<p>We can now see that……</p>
</div>
<div id="evaluating-types-of-strategies" class="section level2">
<h2><span class="header-section-number">13.3</span> Evaluating types of strategies</h2>
<p>The qualitative case-selection literature has identified a range of possible strategies for choosing cases for in-depth analysis. These include, for instance, selecting for variation on <span class="math inline">\(X\)</span> (KKV), selecting for variation on <span class="math inline">\(Y\)</span>, selecting cases on the regression line (Seawright and Gerring, Lieberman), and selecting off the regression line (Seawright and Gerring, Seawright 2017, Lieberman).[These authors view selection off the regression line as best for arriving at inductive insight. We address this strategy primarily to show the contrast with the on-the-line strategy.] While we have not seen it advocated elsewhere, we might add to this list the strategy of selecting cases that are representative in their <span class="math inline">\(X,Y\)</span> values of the larger set of cases from which we are selecting. (FLAG: insert proper citations). While it is difficult to clearly distinguish these strategies from each other with a small initial set of <span class="math inline">\(X, Y\)</span> cases, we can do so readily if we start with a larger set of <span class="math inline">\(X,Y\)</span> cases.</p>
<p>In Table <a href="#caseselectlots"><strong>??</strong></a>, we show the results of diagnoses of each of these five classes of strategies, assuming that we will be process-tracing 6 cases. In each diagnosis we start with 500 <span class="math inline">\(X, Y\)</span> cases, with 100 <span class="math inline">\(X=Y=0\)</span> cases, 200 <span class="math inline">\(X=Y=1\)</span> cases, 130 <span class="math inline">\(X=0, Y=1\)</span> cases, and 70 <span class="math inline">\(X=1, Y=0\)</span> cases. The regression line here represents a positive association. We work with the same model and priors as above. Importantly, this means that the results we show here are not <em>general</em> evaluations of these strategies, but contingent on this particular model and set of priors. And that is precisely our point: optimal case-selection will always hinge on our model, a claim that we demonstrate further below.</p>
<!-- FLAG: Create Table (caseselectlots) with these diagnoses. Do not represent data possibilities in the table (too many). Strategies are: -->
<!-- 1. Variation in $X$: Randomly select 3 $X=0$ cases and 3 $X=1$ cases -->
<!-- 2. Variation in $Y$: Randomly select 3 $Y=0$ cases and 3 $Y=1$ cases -->
<!-- 3. On the regression line: Randomly select 3 from the two $X=Y=0$ cell and 3 from the $X=Y=1$ cell. -->
<!-- 4. Off the regression line: Randomly select 3 from the $X=0, Y=1$ cell and 3 from the $X=1, Y=0$ cell. -->
<!-- 5. Representativeness: Randomly select 6 cases -->
<!-- We can see that.... -->
<!-- ## Different models, different strategies -->
<!-- ## Different queries, different strategies -->
<!-- Suppose, now, that she is choosing between two strategies: -->
<!-- A. Select two $X=Y=1$ cases and one $X=Y=0$ case for investigation. She is thus choosing here cases "on the regression line" generated by the $X,Y$ data pattern.  -->
<!-- B. Take one case from each of the diagonal cells and one from the off-diagonal $X=1, Y=0$ cell -->
<!-- From which strategy should she expect to learn more about the average, population-level causal effect of $X$ on $Y$, given her prior beliefs? We answer the question by considering each data pattern that she might see given a strategy and then calculating (a) the probability with which she expects to see such an outcome, given her priors (or more accurately, given her posterior after observing the $X,Y$ pattern only), and (b) the uncertainty (posterior variance) that she would have upon seeing that pattern. We then calculate the expected posterior variance of the strategy by considering all possible data patterns that could emerge from that strategy and their probabilities. We ignore the fact that in principle the subjective probability of observing one pattern is correlated with the subjective probability of observing another pattern. -->
<!-- ^[For example, a researcher may be uncertain regarding $\phi_b$; if it is high then the probability of observing any profile of outcomes with many clues observed is higher than if $\phi_b$ is low; this introduces a correlation between outcomes that have similar clue observations. However only one of these patterns will be observed in fact. NOTE ON RISK-NEUTRALITY HERE?] -->
<!-- Tables 11.1 and 11.2 present the results of our comparison of the researcher's two strategies. Table 11.1 examines strategy A, while Table 11.2 examines strategy B. In each table, the lefthand column lists all possible realizations of clue data that might be found under the strategy in question. In the notation that we use here, each place in the four-digit sequence refers to one cell in the 2-by-2 table implied by a binary $X$ and binary $Y$ variable. The ordering is:  -->
<!-- * $X=Y=0$ -->
<!-- * $X=0, Y=1$ -->
<!-- * $X=1, Y=0$ -->
<!-- * $X=Y=1$  -->
<!-- As an easy mnemonic, the outside digits are "on-the-diagonal" (on the regression line consistent with a positive causal effect); the inside digits are off-the-diagonal. Thus, for instance, the sequence $1001$ means that we have seen the clue in two on-the-diagonal cases: an $X=Y=0$ case and a $X=Y=1$ case. The sequence 0200 means that we have seen the clue in two cases in the $X=0, Y=1$ cell. -->
<!-- In each row, we see information about one possible data pattern that we could potentially observe under the strategy. The second column indicates the probability that this data pattern will arise, given the priors and the $X,Y$ data. The third column then indicates the uncertainty---the posterior variance---that we would be left with given the information provided by each clue pattern. In the final column, we provided the expected posterior variance for each strategy, given the posterior variances for each clue pattern, weighting each clue pattern by its probability of occurring.  -->
<p>We consider a situation in which one has access to <span class="math inline">\(X,Y\)</span> data of the form:</p>
<table>
<thead>
<tr class="header">
<th align="left">event</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X0Y0</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">X1Y0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">X0Y1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">X1Y1</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>and one assesses four strategies. Strategy <span class="math inline">\(A\)</span> choses two cases on the regression line (one in the X=Y=0 cell and onne in the X=Y=1 cell); strategy <span class="math inline">\(B\)</span> chooses off the regression line and straregy C selects on the dependent variable – choosing cases with <span class="math inline">\(Y = 1\)</span>.</p>
<p>Different strategies <span class="math inline">\(A - D\)</span> can each yield up to four different data types. Each one of these is likely to arise with a different probabilities and is associated with a different inference. This allows is to assess the <em>expected</em> posterior variance associated with each strategy. We exclude foor the moment the random strategy which produces up to 30 different data types given the available cases.</p>
<pre><code>## 
## FALSE 
##    17</code></pre>
<p>For each of these possible strategies we can assess the posterior that we would obtain.</p>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 13.2: </span>Each colum shows a possible distribution of data that can be generated from a given strategy. Each strategy generates one of four different data patterns. We calcualte the probability of each, given the data seen so far, and the posterior variance given each data realization.</caption>
<thead>
<tr class="header">
<th align="left">Event</th>
<th align="left">A1</th>
<th align="left">A2</th>
<th align="left">A3</th>
<th align="left">A4</th>
<th align="left">B1</th>
<th align="left">B2</th>
<th align="left">B3</th>
<th align="left">B4</th>
<th align="left">C1</th>
<th align="left">C2</th>
<th align="left">C3</th>
<th align="left">C4</th>
<th align="left">D1</th>
<th align="left">D2</th>
<th align="left">D3</th>
<th align="left">D4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X0M0Y0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">X0M0Y1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">X0M1Y0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">X0M1Y1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">X0Y0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">X0Y1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">X1M0Y0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">X1M0Y1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">X1M1Y0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">X1M1Y1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">X1Y0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">X1Y1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">Probability</td>
<td align="left">0.38</td>
<td align="left">0.04</td>
<td align="left">0.5</td>
<td align="left">0.08</td>
<td align="left">0.48</td>
<td align="left">0.21</td>
<td align="left">0.2</td>
<td align="left">0.12</td>
<td align="left">0.33</td>
<td align="left">0.38</td>
<td align="left">0.1</td>
<td align="left">0.19</td>
<td align="left">0.33</td>
<td align="left">0.38</td>
<td align="left">0.1</td>
<td align="left">0.2</td>
</tr>
<tr class="even">
<td align="left">Posterior variance</td>
<td align="left">0.09</td>
<td align="left">0.05</td>
<td align="left">0.14</td>
<td align="left">0.09</td>
<td align="left">0.13</td>
<td align="left">0.13</td>
<td align="left">0.13</td>
<td align="left">0.13</td>
<td align="left">0.09</td>
<td align="left">0.14</td>
<td align="left">0.08</td>
<td align="left">0.14</td>
<td align="left">0.08</td>
<td align="left">0.14</td>
<td align="left">0.08</td>
<td align="left">0.13</td>
</tr>
</tbody>
</table>
<p>Thus each strategy generates one of four different data patterns. We calcualte the probability of each, given the data seen so far, and the posterior variance given each data realization. The implied <em>expected</em> variance under each strategy is summarized below:</p>
<table>
<thead>
<tr class="header">
<th align="left">Strategy</th>
<th align="right">Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="right">0.1134</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">0.1313</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="right">0.1175</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">0.1136</td>
</tr>
</tbody>
</table>
</div>
<div id="compare-multiple-data-strategies" class="section level2">
<h2><span class="header-section-number">13.4</span> Compare multiple data strategies</h2>
<p>We now apply the procedure systematically to a set of strategies for multiple models and given differnet types of pre-existing data.</p>
<pre><code>## Warning in kable_markdown(x, padding = padding, ...): The table should have
## a header (column names)</code></pre>
<p>||
||
||
||</p>
<ul>
<li><p>@all here is where we need most backup help right now — these examples show the basic analysis — interest is in understanding how posterior variance differs for difference data strategies given difference <code>givens</code> and different background models.</p></li>
<li><p>Steps –</p>
<ul>
<li>Do wrapper to make it a little faster to generate results from a new strategy / mode in one line (that uses saved results when possible)</li>
<li>Extend current example to all four “gather M for 1 case” strategies and all 8 “gather M for 2 cases” (could be two of the same sort or two different)</li>
<li>Then do the same for (a) a model with monotonicity of X to M and M to Y assumed and (b) a X -&gt; Y &lt;- M model with complementarity of X and M assumed – (LM) I don’t understand how model (a) is different from the model above.</li>
<li>Even better if possible to graph some of the results, even with confidnece intervals. You can see the previous graphs we had in K1.pdf - K4.pdf in “6 Book”</li>
</ul></li>
<li><p>@lily — strategies that involve “look at M in each of teh X/Y cells” will involve using the full data strategies of <code>make_possible_data</code> with <code>condition = list("X==0 &amp; Y==0", "X==0 &amp; Y==1")</code> etc</p></li>
</ul>
<p>OLD TEXT: In this example, we see that the researcher would expect to be better off—in the sense of having less posterior uncertainty—by focusing her process-tracing efforts where a greater share of the population of cases lies: on the regression line. Taking one observation in each of three cells has her devoting much of her effort to a case that is relatively unrepresentative of the population she wishes to learn about.</p>
<p>In the experiments that follow, we implement this kind of simulation for all possible clue strategies—for a fixed number of clues sought—and report the expected posterior variance.</p>
</div>
<div id="experiments" class="section level2">
<h2><span class="header-section-number">13.5</span> Experiments</h2>
<p>In all of the graphs, we start with 16 “quantitative” cases: cases for which we have observed an <span class="math inline">\(X\)</span> and a <span class="math inline">\(Y\)</span> value. We are then choosing some subset of these cases for process tracing. Within each group of nine graphs, we are sampling a fixed number of cases for process tracing: 1 case in the first set, 2 cases in the second set, 3 cases in the third set, and 4 cases in the final set. We treat process tracing in a case as the search for one clue in that case, though this “one clue” could be conceived of as a collection of clues that jointly have a given probative value.</p>
<p>Within each set of graphs, we see how different case selection strategies fare as we vary two features of the research situation. Moving down the rows of graphs, we vary the distribution of the 16 cases over an <span class="math inline">\(XY\)</span> table. In the first row, the 16 cases are spread evenly across the 4 <span class="math inline">\(X,Y\)</span> cells; in the second row, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively correlated; in the third row, <span class="math inline">\(Y=1\)</span> is observed only in cases with <span class="math inline">\(X=1\)</span> (i.e., the <span class="math inline">\(X=0, Y=1\)</span> cell is empty).</p>
<p>Moving across the columns of graphs, we vary the probative value of the clue that we are looking for in the process tracing. What is changing is for what kind of a case—in terms of its <span class="math inline">\(X,Y\)</span> values—the clue is most probative (i.e., doubly decisive). Where the clue is doubly decisive, finding the clue present or finding it absent both nail down the type of the case. Wherever the clue is not doubly decisive, it is assumed to be a “hoop test” for an <span class="math inline">\(a\)</span> type (in the <span class="math inline">\(X=1, Y=0\)</span> and <span class="math inline">\(X=0, Y=1\)</span> cells) and for a <span class="math inline">\(b\)</span> type (in the <span class="math inline">\(X=0, Y=1\)</span> cells). This means that <em>not</em> finding the clue rules out the case’s being an <span class="math inline">\(a\)</span> or a <span class="math inline">\(b\)</span> type in the relevant cells; thus, the clue is still informative, but less so than if it were doubly decisive. In the first column, the clue is doubly decisive for all kinds of cases. In the second column, the clue is doubly decisive only for <span class="math inline">\(X=1\)</span> cases, and a hoop test otherwise. In the third column, the clue is doubly decisive only for <span class="math inline">\(Y=1\)</span> cases, and a hoop test otherwise. And in the final column, the clue is doubly decisive only “on the regression line” consistent with a positive effect, and a hoop test otherwise.</p>
<p>Each case selection strategy is indicated on each graph using the same four-digit pattern that we used to indicate data realizations in the example above. Throughout, what we seek to estimate is the average causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> (or <span class="math inline">\(\lambda_b-\lambda_a\)</span>). The strategy’s vertical placement on the graph indicates the “loss”, i.e., expected posterior uncertainty, associated with the strategy. Thus, a lower placement indicates greater learning. Around each dot, we also provide 90 percent simulation error bars, though these are hidden by the dots themselves when the simulation error is very small, as it is in most cases.</p>
<p>In all simulations, we assume that there is no confounding, and we start (before seeing the <span class="math inline">\(X, Y\)</span> pattern) with flat priors over the distribution of causal types in the population.</p>
<p>In the graphs with more than one process tracing case, we also color-code and group together families of strategies as indicated in the figure caption.</p>
<p>We begin with the simplest problem, where only one case is to be chosen for process tracing. In this situation, as seen in our first set of figures, two principle emerge. First, it is better select cases in the <span class="math inline">\(X,Y\)</span> conditions where the probative value lies. More informative clues generate more learning; so if probative value varies across types of cases, this should have direct implications for case selection. Second, it is better to select cases from the largest cells. The second principle is perhaps less obvious, but it derives from a sampling logic: learning about a case drawn randomly from a cell gives you information about that cell and so the larger the cell the more cases there is learning about.</p>
<p>Perhaps just as important is what does not emerge as a principle: all else equal there is no reason to focus on either the <span class="math inline">\(X=Y=1\)</span> cases or on the diagonal (<span class="math inline">\(X=Y\)</span>): all four cells are symmetric (ceteris paribus) in that they all exhibit an ambiguity between <span class="math inline">\(a\)</span>’s and <span class="math inline">\(b\)</span>’s on the one hand and <span class="math inline">\(c\)</span>’s and <span class="math inline">\(d\)</span>’s on the other. There is thus nothing intrinsically informative about the cases on the diagonal. We do see that choosing on the diagonal is beneficial when <span class="math inline">\(X\)</span> is positively correlated with <span class="math inline">\(Y\)</span>, but this is because the population of cases is concentrated along the diagonal; this is an illustration of the representativeness principle, not of some special feature of the diagonal.</p>
<embed src="./Figures/K1.pdf" width="100%" style="display: block; margin: auto;" type="application/pdf" />
<p>The figure shows gains from different strategies involving process tracing in 1 case. In each simulation, we start with 16 “quantitative” cases. Moving down the rows of graphs, we vary the distribution of these cases over an <span class="math inline">\(XY\)</span> table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing.</p>
<p>More interesting patterns start emerging once we can choose two cases. We see the same two basic principles matter—go for probative value and for representativeness—but we now also see that there are complementarities in learning between different types of cases. In fact, for symmetric problems—as in the upper left panel, where the cases are evenly spread out and probative value is strong everywhere—we see a ranking between four families: first cases that fix <span class="math inline">\(Y\)</span> (at 0 or 1) and spread on <span class="math inline">\(X\)</span>, second cases that fix <span class="math inline">\(X\)</span> and spread on <span class="math inline">\(Y\)</span>, third cases that are on or off the diagonal, and fourth cases that focus on a single cell. Seeking clues on the diagonal does emerge as a good strategy (see for example the left figure in the middle row), but this appears to arise because the diagonal is data dense, not because there are particular complementarities of probative value there. In the bottom left figure for example, we see that in the case where there is data for <span class="math inline">\(X=1, Y=0\)</span> but little data for <span class="math inline">\(X=0, Y=1\)</span> , selecting cases distributed over the <span class="math inline">\(Y=0\)</span> cells is about as informative as selecting on the diagonal. Selecting off the diagonal is one of the worst strategies here and is dominated by selecting all data from a dense cell.</p>
<p>Skip now to our graph of results where 4 cases are selected. As in previous figures we see strong gains for strategies that select cases proportionate to the size off cells. This is mot clear in the top left figure where one case per (equally sized) cell is the best strategy. More subtly it can be seen in the bottom left where the 2-0-1-1 strategy dominates — this is a strategy that spreads roughly proportionately even if that means leaving some cells unrepresented. Strikingly in the base case various hybrid strategies do quite well, likely reflecting the fact that are as close to optimal spreads as possible. On and off diagonal strategies do poorly unless there is a strong diagonal, in which case these can dominate spreading across cells.</p>
<p>Overall relatively simple patterns emerge though these differ in some ways from text book suggestions. First focusing on probative value is key. Second seeking larger cells and balancing cases across cells appears fruitful. Third, and less intuitively, some combinations appear to gain more leverage than others. On and off diagonal strategies for example seem weaker in general than strategies that fix <span class="math inline">\(X\)</span> or that fix <span class="math inline">\(Y\)</span>. Strategies that fix <span class="math inline">\(Y\)</span> and allow variation on <span class="math inline">\(X\)</span> seem strong, again, ceteris paribus.</p>
<p>Perhaps the most striking result from the simulations is that the optimal choice depends on many features. A simple rule, or even these core principles, may not get identify the right strategy. Ye the right strategy can be calculated, at least if one is willing to lay out beliefs on causal structure and probative value.</p>
<div class="figure">
<embed src="Figures/K2.pdf" />
<p class="caption">Gains from different strategies involving process tracing in 2 cases. We treat process tracing in a case as the search for one clue in that case, though this “clue” could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 “quantitative” cases. Moving down the rows of graphs, we vary the distribution of these cases over an <span class="math inline">\(XY\)</span> table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. Families of strategies are grouped and color-coded as follows: red=maximally dispersing across cells; yellow=</p>
</div>
<div class="figure">
<embed src="Figures/K3.pdf" />
<p class="caption">Gains from different strategies involving process tracing in 3 cases. We treat process tracing in a case as the search for one clue in that case, though this “clue” could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 “quantitative” cases. Moving down the rows of graphs, we vary the distribution of these cases over an <span class="math inline">\(XY\)</span> table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing.</p>
</div>
<div class="figure">
<embed src="Figures/K4.pdf" />
<p class="caption">Gains from different strategies involving process tracing in 4 cases. We treat process tracing in a case as the search for one clue in that case, though this “clue” could be conceived of as a collection of clues that jointly have the probative value indicated. Simulations invole 16 units distributed over an <span class="math inline">\(XY\)</span> table in three patterns (rows) and variation over the probative value of different clues (columns).</p>
</div>
</div>
<div id="chapter-appendix-accounting-for-case-selection" class="section level2">
<h2><span class="header-section-number">13.6</span> Chapter Appendix: Accounting for case selection</h2>
<div id="independent-case-selection-strategy" class="section level3">
<h3><span class="header-section-number">13.6.1</span> Independent case selection strategy</h3>
<p>We have focused on cases in which the researcher examines a fixed number of cases for clue information. An alternative strategy that produces a simpler likelihood is one in which each case is selected for within-case data gathering with some independent probability. The likelihood below introduces a case selection probability <span class="math inline">\(\kappa_{xy}\)</span> that covers this case and allows for the possibility that selection probabilities are different for different <span class="math inline">\(X,Y\)</span> combinations.</p>
<p>Thus we assume that <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> data is observed for all cases under study, but that <span class="math inline">\(K\)</span> data may be sought for only a subset of these (we use the wildcard symbol ’‘<span class="math inline">\(*\)</span>’’ to denote that the value of the clue is unknown). We let <span class="math inline">\(n_{xyk}\)</span> denote the number of cases of each type. Then, again assuming data is independently and identically distributed, the likelihood is:</p>
<p><span class="math display">\[\Pr(\mathcal{D}|\theta)= {\text{Multinomial}}(n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*} |
w_{000}, w_{001},w_{00*},w_{010}, w_{010},w_{01*}, w_{100}, w_{101},w_{10*},w_{110},w_{111} ,w_{11*})\]</span></p>
<p>where the event probabilities are now given by:</p>
<p><span class="math display">\[{\left( \begin{array}{c}
w_{000} \\ w_{001} \\  \vdots \\ w_{11*} \end{array} \right)=
\left( \begin{array}{c}
\lambda_b(1-\pi_b)\kappa_{00}(1-\phi_{b0}) + \lambda_c(1-\pi_c)\kappa_{00}(1-\phi_{c0})\\
\lambda_b(1-\pi_b)\kappa_{00}\phi_{b0} + \lambda_c(1-\pi_c)\kappa_{00}\phi_{c0}\\
\vdots \\
\lambda_b\pi_{b}(1-\kappa_{11}) + \lambda_d\pi_{d}(1-\kappa_{11})
\end{array} \right)}\]</span></p>
Note we use a Greek symbol for the case selection probabilities to highlight that these may also be unknown and be an object of inquiry, entering into the vector of parameters, <span class="math inline">\(\theta\)</span>.

<p>While we have assumed in the canonical model that <span class="math inline">\(X,Y\)</span> cases are selected at random, this need not be the case. Say instead that each case of type <span class="math inline">\(j\)</span> is selected into the study with probability <span class="math inline">\(\rho_j\)</span>. In that case, assuming independent selection of cases for qualitative analysis, the likelihood function is now:</p>
<p><span class="math display">\[\Pr(\mathcal{D}|\theta) = {\text{Multinomial}}(n, w)\]</span>
where: <span class="math display">\[n = (n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*})\]</span></p>
<p>and the event probabilities, <span class="math inline">\(w\)</span>, are now, given by:</p>
<p><span class="math display">\[\left( \begin{array}{c}
w_{000} \\ w_{001} \\  \vdots \\ w_{11*}
\end{array} \right)=
\left( \begin{array}{c}
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_b)\kappa_{00}(1-\phi_{b0}) +
\frac{\rho_c \lambda_c}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_c)\kappa_{00}(1-\phi_{c0})\\
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_b)\kappa_{00}\phi_{b0}+
\frac{\rho_c \lambda_c}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_c)\kappa_{00}\phi_{c0}\\
\vdots \\
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}\pi_{b}(1-\kappa_{11})+
\frac{\rho_d \lambda_d}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_{11}}\pi_{d}(1-\kappa_{11})
\end{array} \right)\]</span></p>
<p>Note we have used a Greek symbol for the selection probabilities to highlight that these probabilities may be unknown and could enter into the set of parameters of interest, <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="conditional-random-case-selection" class="section level3">
<h3><span class="header-section-number">13.6.2</span> Conditional random case selection</h3>
<p>Finally consider the likelihood for a design in which a researcher selects to search for clues as a function of the <span class="math inline">\(X,Y\)</span> data. This is a somewhat harder case because the size of each <span class="math inline">\(X,Y\)</span> group is stochastic. Let <span class="math inline">\(n_{xy} = n_{xy0}+n_{xy1}+n_{xy*}\)</span> denote the number of cases with particular values on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and let <span class="math inline">\(n_{XY}=(n_{00},n_{01},n_{10},n_{11})\)</span> denote the collection of <span class="math inline">\(n_{xy}\)</span> values.</p>
<p>Say now that conditional on the <span class="math inline">\(X,Y\)</span> observations, a researcher sets a target of <span class="math inline">\(k_{xy}(n_{XY})\)</span> cases for clue examination (note here that the number of clues sought for a particular <span class="math inline">\(X,Y\)</span> combination can be allowed to depend on what is observed across all <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> combinations). Then the likelihood is:
<span class="math display">\[\text{Multinomial}(n_{XY}|w_{XY})\prod_{x\in\{0,1\},y \in\{0,1\}}\text{Binom}(n_{xy1}|k_{xy}(n_{xy}), \psi_{xy1})\]</span></p>
<p>The multinomial part of this expression gives the probability of observing the particular <span class="math inline">\(X,Y\)</span> combinations; the event probabilities for these depend on <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\pi\)</span> only | for example <span class="math inline">\(w_{11} = \lambda_b \pi_b+\lambda_d \pi_d\)</span>. The subsequent binomials give the probability of observing the clue patterns conditional on searching for a given number of clues (<span class="math inline">\(k_{xy}(n_{xy})\)</span>) and given an event probability <span class="math inline">\(\psi_{xy1}\)</span> for seeing a clue given that the clue is sought for an <span class="math inline">\(x,y\)</span> combination; thus for example:
<span class="math display">\[ \psi_{111} = \frac{\lambda_b \pi_b}{\lambda_b \pi_b+\lambda_d \pi_d} \phi_{b1} + \frac{\lambda_d \pi_d}{\lambda_b \pi_b+\lambda_d \pi_d} \phi_{d1}\]</span></p>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-FL2008">
<p>Fearon, James, and David Laitin. 2008. “Integrating Qualitative and Quantitative Methods.” In <em>Oxford Handbook of Political Methodology</em>, edited by Janet M. Box-Steffenmeier, David Collier, and Henry E Brady, 756–76. Cambridge, UK: Oxford University Press.</p>
</div>
<div id="ref-HerronQuinn">
<p>Herron, Michael C, and Kevin M Quinn. 2016. “A Careful Look at Modern Case Selection Methods.” <em>Sociological Methods &amp; Research</em> 45 (3): 458–92.</p>
</div>
<div id="ref-king1994designing">
<p>King, G., R.O. Keohane, and S. Verba. 1994. <em>Designing Social Inquiry: Scientific Inference in Qualitative Research</em>. Princeton University Press. <a href="http://books.google.de/books?id=A7VFF-JR3b8C">http://books.google.de/books?id=A7VFF-JR3b8C</a>.</p>
</div>
<div id="ref-Lieberman2005nested">
<p>Lieberman, Evan S. 2005. “Nested Analysis as a Mixed-Method Strategy for Comparative Research.” <em>American Political Science Review</em> 99 (03): 435–52. <a href="https://doi.org/10.1017/S0003055405051762">https://doi.org/10.1017/S0003055405051762</a>.</p>
</div>
<div id="ref-SeawrightGerring2008">
<p>Seawright, Jason, and John Gerring. 2008. “Case Selection Techniques in Case Study Research: A Menu of Qualitative and Quantitative Options.” <em>Political Research Quarterly</em> 61 (2): 294–308. <a href="https://doi.org/10.1177/1065912907313077">https://doi.org/10.1177/1065912907313077</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.<a href="caseselection.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>The probability of observing the clue for a <span class="math inline">\(b\)</span> type (positive causal effect) case with <span class="math inline">\(X=1\)</span>.<a href="caseselection.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>The probability of observing the clue for a <span class="math inline">\(d\)</span> type (zero causal effect, <span class="math inline">\(Y\)</span> fixed at 1) case with <span class="math inline">\(X=1\)</span>.<a href="caseselection.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>The probability of observing the clue for a <span class="math inline">\(b\)</span> type (positive causal effect) case with <span class="math inline">\(X=0\)</span>.<a href="caseselection.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>The probability of observing the clue for a <span class="math inline">\(c\)</span> type (zero causal effect, <span class="math inline">\(Y\)</span> fixed at 0) case with <span class="math inline">\(X=0\)</span>.<a href="caseselection.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p><span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> have a parameter <span class="math inline">\(\theta\)</span> that governs the distribution of data over <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and then, conditional on <span class="math inline">\(X,Y\)</span> values, a set of parameters <span class="math inline">\(\psi_{xy}\)</span> that describe the probability of a case’s being of a given causal type. We take both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\psi_{xy}\)</span> to derive from the fundamental distribution of causal types and assignment probabilities. Thus, for example, <span class="math inline">\(\psi_{00}\)</span> from <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> corresponds to <span class="math inline">\(\frac{(1-\pi_b)\lambda_b}{(1-\pi_b)\lambda_b + (1-\pi_c)\lambda_c}\)</span> in our notation. The difference in paramaterization does have implications for interpretations of the priors. For example flat priors over <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\psi\)</span> implies a tighter distribution that a uniform prior over the causal types. In fact <span class="citation">Herron and Quinn (<a href="#ref-HerronQuinn">2016</a>)</span> use priors with greater variance than uniform in any event.<a href="caseselection.html#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="wide.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="justifying-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
