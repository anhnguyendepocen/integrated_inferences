<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Final Words | Integrated Inferences</title>
  <meta name="description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Final Words | Integrated Inferences" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="dnieperriver.png" />
  <meta property="og:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="github-repo" content="rstudio/ii" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Final Words | Integrated Inferences" />
  
  <meta name="twitter:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="twitter:image" content="dnieperriver.png" />

<meta name="author" content="Macartan Humphreys and Alan Jacobs" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="evaluation.html"/>
<link rel="next" href="examplesappendix.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="headers/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Model based causal inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-case-for-causal-models"><i class="fa fa-check"></i><b>1.1</b> The Case for Causal Models</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#the-limits-to-design-based-inference"><i class="fa fa-check"></i><b>1.1.1</b> The limits to design-based inference</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#qualitative-and-mixed-method-inference"><i class="fa fa-check"></i><b>1.1.2</b> Qualitative and mixed-method inference</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#connecting-theory-and-empirics"><i class="fa fa-check"></i><b>1.1.3</b> Connecting theory and empirics</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-contributions"><i class="fa fa-check"></i><b>1.2</b> Key contributions</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-road-ahead"><i class="fa fa-check"></i><b>1.3</b> The Road Ahead</a></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Causal Models</a><ul>
<li class="chapter" data-level="2.1" data-path="models.html"><a href="models.html#the-counterfactual-model"><i class="fa fa-check"></i><b>2.1</b> The counterfactual model</a><ul>
<li class="chapter" data-level="2.1.1" data-path="models.html"><a href="models.html#generalizing-to-outcomes-with-many-causes"><i class="fa fa-check"></i><b>2.1.1</b> Generalizing to outcomes with many causes</a></li>
<li class="chapter" data-level="2.1.2" data-path="models.html"><a href="models.html#deterministic-relations"><i class="fa fa-check"></i><b>2.1.2</b> Deterministic relations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="models.html"><a href="models.html#causal-models-and-directed-acyclic-graphs"><i class="fa fa-check"></i><b>2.2</b> Causal Models and Directed Acyclic Graphs</a><ul>
<li class="chapter" data-level="2.2.1" data-path="models.html"><a href="models.html#components-of-a-causal-model"><i class="fa fa-check"></i><b>2.2.1</b> Components of a Causal Model</a></li>
<li class="chapter" data-level="2.2.2" data-path="models.html"><a href="models.html#rules-for-graphing-causal-models"><i class="fa fa-check"></i><b>2.2.2</b> Rules for graphing causal models</a></li>
<li class="chapter" data-level="2.2.3" data-path="models.html"><a href="models.html#conditional-independence-from-dags"><i class="fa fa-check"></i><b>2.2.3</b> Conditional independence from DAGs</a></li>
<li class="chapter" data-level="2.2.4" data-path="models.html"><a href="models.html#a-simple-running-example"><i class="fa fa-check"></i><b>2.2.4</b> A simple running example</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="models.html"><a href="models.html#chapter-appendix"><i class="fa fa-check"></i><b>2.3</b> Chapter Appendix</a><ul>
<li class="chapter" data-level="2.3.1" data-path="models.html"><a href="models.html#steps-for-constructing-causal-models"><i class="fa fa-check"></i><b>2.3.1</b> Steps for constructing causal models</a></li>
<li class="chapter" data-level="2.3.2" data-path="models.html"><a href="models.html#model-construction-in-code"><i class="fa fa-check"></i><b>2.3.2</b> Model construction in code</a></li>
<li class="chapter" data-level="2.3.3" data-path="models.html"><a href="models.html#test-yourself-can-you-read-conditional-independence-from-a-graph"><i class="fa fa-check"></i><b>2.3.3</b> Test yourself! Can you read conditional independence from a graph?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="illustratemodels.html"><a href="illustratemodels.html"><i class="fa fa-check"></i><b>3</b> Illustrating Causal Models</a><ul>
<li class="chapter" data-level="3.1" data-path="illustratemodels.html"><a href="illustratemodels.html#welfare-state-reform-pierson-1994"><i class="fa fa-check"></i><b>3.1</b> Welfare state reform: Pierson (1994)</a></li>
<li class="chapter" data-level="3.2" data-path="illustratemodels.html"><a href="illustratemodels.html#military-interventions-saunders-2011"><i class="fa fa-check"></i><b>3.2</b> Military Interventions: Saunders (2011)</a></li>
<li class="chapter" data-level="3.3" data-path="illustratemodels.html"><a href="illustratemodels.html#development-and-democratization-przeworski-and-limongi-1997"><i class="fa fa-check"></i><b>3.3</b> Development and Democratization: Przeworski and Limongi (1997)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>4</b> Theories as causal models</a><ul>
<li class="chapter" data-level="4.1" data-path="theory.html"><a href="theory.html#theory-as-a-lower-level-model"><i class="fa fa-check"></i><b>4.1</b> Theory as a “lower-level” model</a></li>
<li class="chapter" data-level="4.2" data-path="theory.html"><a href="theory.html#illustration-of-unpacking-causal-types"><i class="fa fa-check"></i><b>4.2</b> Illustration of unpacking causal types</a><ul>
<li class="chapter" data-level="4.2.1" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-mediation-model"><i class="fa fa-check"></i><b>4.2.1</b> Type disaggregation in a mediation model</a></li>
<li class="chapter" data-level="4.2.2" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-moderation-model"><i class="fa fa-check"></i><b>4.2.2</b> Type disaggregation in a moderation model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="theory.html"><a href="theory.html#rules-for-moving-between-higher--and-lower-level-models"><i class="fa fa-check"></i><b>4.3</b> Rules for moving between higher- and lower-level models</a><ul>
<li class="chapter" data-level="4.3.1" data-path="theory.html"><a href="theory.html#moving-down-levels"><i class="fa fa-check"></i><b>4.3.1</b> Moving down levels</a></li>
<li class="chapter" data-level="4.3.2" data-path="theory.html"><a href="theory.html#moving-up-levels"><i class="fa fa-check"></i><b>4.3.2</b> Moving up levels</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="theory.html"><a href="theory.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a><ul>
<li class="chapter" data-level="4.4.1" data-path="theory.html"><a href="theory.html#quantifying-the-gains-of-a-theory"><i class="fa fa-check"></i><b>4.4.1</b> Quantifying the gains of a theory</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="theory.html"><a href="theory.html#chapter-appendices"><i class="fa fa-check"></i><b>4.5</b> Chapter Appendices</a><ul>
<li class="chapter" data-level="4.5.1" data-path="theory.html"><a href="theory.html#summary-boxes"><i class="fa fa-check"></i><b>4.5.1</b> Summary Boxes</a></li>
<li class="chapter" data-level="4.5.2" data-path="theory.html"><a href="theory.html#illustration-of-a-mapping-from-a-game-to-a-dag"><i class="fa fa-check"></i><b>4.5.2</b> Illustration of a Mapping from a Game to a DAG</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i><b>5</b> Causal Queries</a><ul>
<li class="chapter" data-level="5.1" data-path="questions.html"><a href="questions.html#case-level-causal-effects"><i class="fa fa-check"></i><b>5.1</b> Case-level causal effects</a></li>
<li class="chapter" data-level="5.2" data-path="questions.html"><a href="questions.html#case-level-causal-attribution"><i class="fa fa-check"></i><b>5.2</b> Case-level causal attribution</a></li>
<li class="chapter" data-level="5.3" data-path="questions.html"><a href="questions.html#case-level-explanation"><i class="fa fa-check"></i><b>5.3</b> Case-level explanation</a></li>
<li class="chapter" data-level="5.4" data-path="questions.html"><a href="questions.html#average-causal-effects"><i class="fa fa-check"></i><b>5.4</b> Average causal effects</a></li>
<li class="chapter" data-level="5.5" data-path="questions.html"><a href="questions.html#causal-paths"><i class="fa fa-check"></i><b>5.5</b> Causal Paths</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayeschapter.html"><a href="bayeschapter.html"><i class="fa fa-check"></i><b>6</b> Bayesian Answers</a><ul>
<li class="chapter" data-level="6.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-basics"><i class="fa fa-check"></i><b>6.1</b> Bayes Basics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-instances"><i class="fa fa-check"></i><b>6.1.1</b> Simple instances</a></li>
<li class="chapter" data-level="6.1.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-rule-for-discrete-hypotheses"><i class="fa fa-check"></i><b>6.1.2</b> Bayes’ Rule for Discrete Hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="bayeschapter.html"><a href="bayeschapter.html#the-dirichlet-family-and-bayes-rule-for-continuous-parameters"><i class="fa fa-check"></i><b>6.1.3</b> The Dirichlet family and Bayes’ Rule for Continuous Parameters</a></li>
<li class="chapter" data-level="6.1.4" data-path="bayeschapter.html"><a href="bayeschapter.html#moments"><i class="fa fa-check"></i><b>6.1.4</b> Moments</a></li>
<li class="chapter" data-level="6.1.5" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-estimation-in-practice"><i class="fa fa-check"></i><b>6.1.5</b> Bayes estimation in practice</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-applied"><i class="fa fa-check"></i><b>6.2</b> Bayes applied</a><ul>
<li class="chapter" data-level="6.2.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-bayesian-process-tracing"><i class="fa fa-check"></i><b>6.2.1</b> Simple Bayesian Process Tracing</a></li>
<li class="chapter" data-level="6.2.2" data-path="bayeschapter.html"><a href="bayeschapter.html#a-generalization-bayesian-inference-on-queries"><i class="fa fa-check"></i><b>6.2.2</b> A Generalization: Bayesian Inference on Queries</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayeschapter.html"><a href="bayeschapter.html#three-principles-of-bayesian-updating"><i class="fa fa-check"></i><b>6.3</b> Three principles of Bayesian updating</a><ul>
<li class="chapter" data-level="6.3.1" data-path="bayeschapter.html"><a href="bayeschapter.html#AppPriors"><i class="fa fa-check"></i><b>6.3.1</b> Priors matter</a></li>
<li class="chapter" data-level="6.3.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simultaneous-joint-updating"><i class="fa fa-check"></i><b>6.3.2</b> Simultaneous, joint updating</a></li>
<li class="chapter" data-level="6.3.3" data-path="bayeschapter.html"><a href="bayeschapter.html#posteriors-are-independent-of-the-ordering-of-data"><i class="fa fa-check"></i><b>6.3.3</b> Posteriors are independent of the ordering of data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Model-Based Causal Inference</b></span></li>
<li class="chapter" data-level="7" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>7</b> Process Tracing with Causal Models</a><ul>
<li class="chapter" data-level="7.1" data-path="pt.html"><a href="pt.html#process-tracing-and-causal-models"><i class="fa fa-check"></i><b>7.1</b> Process tracing and causal models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="pt.html"><a href="pt.html#the-intuition"><i class="fa fa-check"></i><b>7.1.1</b> The intuition</a></li>
<li class="chapter" data-level="7.1.2" data-path="pt.html"><a href="pt.html#a-formalization-of-the-general-approach"><i class="fa fa-check"></i><b>7.1.2</b> A formalization of the general approach</a></li>
<li class="chapter" data-level="7.1.3" data-path="pt.html"><a href="pt.html#illustration-with-code"><i class="fa fa-check"></i><b>7.1.3</b> Illustration with code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="pt.html"><a href="pt.html#five-principles"><i class="fa fa-check"></i><b>7.2</b> Five principles</a><ul>
<li class="chapter" data-level="7.2.1" data-path="pt.html"><a href="pt.html#classic-qualitative-tests-are-special-cases-of-updating-on-a-model"><i class="fa fa-check"></i><b>7.2.1</b> Classic qualitative tests are special cases of updating on a model</a></li>
<li class="chapter" data-level="7.2.2" data-path="pt.html"><a href="pt.html#a-dag-alone-does-not-get-you-probative-value"><i class="fa fa-check"></i><b>7.2.2</b> A DAG alone does not get you probative value</a></li>
<li class="chapter" data-level="7.2.3" data-path="pt.html"><a href="pt.html#uncertainty-does-not-alter-inference-for-single-case-causal-inference"><i class="fa fa-check"></i><b>7.2.3</b> Uncertainty does not alter inference for single case causal inference</a></li>
<li class="chapter" data-level="7.2.4" data-path="pt.html"><a href="pt.html#probative-value-requires-d-connection"><i class="fa fa-check"></i><b>7.2.4</b> Probative value requires <span class="math inline">\(d-\)</span>connection</a></li>
<li class="chapter" data-level="7.2.5" data-path="pt.html"><a href="pt.html#probative-value"><i class="fa fa-check"></i><b>7.2.5</b> Probative value</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptapp.html"><a href="ptapp.html"><i class="fa fa-check"></i><b>8</b> Application: Process Tracing with a Causal Model</a><ul>
<li class="chapter" data-level="8.1" data-path="ptapp.html"><a href="ptapp.html#inequality-and-democratization-the-debate"><i class="fa fa-check"></i><b>8.1</b> Inequality and Democratization: The Debate</a></li>
<li class="chapter" data-level="8.2" data-path="ptapp.html"><a href="ptapp.html#a-structural-causal-model"><i class="fa fa-check"></i><b>8.2</b> A Structural Causal Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptapp.html"><a href="ptapp.html#forming-priors"><i class="fa fa-check"></i><b>8.2.1</b> Forming Priors</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptapp.html"><a href="ptapp.html#results"><i class="fa fa-check"></i><b>8.3</b> Results</a></li>
<li class="chapter" data-level="8.4" data-path="ptapp.html"><a href="ptapp.html#pathways"><i class="fa fa-check"></i><b>8.4</b> Pathways</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ptapp.html"><a href="ptapp.html#cases-with-incomplete-data"><i class="fa fa-check"></i><b>8.4.1</b> Cases with incomplete data</a></li>
<li class="chapter" data-level="8.4.2" data-path="ptapp.html"><a href="ptapp.html#inferences-for-cases-with-observed-democratization"><i class="fa fa-check"></i><b>8.4.2</b> Inferences for cases with observed democratization</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ptapp.html"><a href="ptapp.html#model-definition-and-inference-in-code"><i class="fa fa-check"></i><b>8.5</b> Model definition and inference in code</a></li>
<li class="chapter" data-level="8.6" data-path="ptapp.html"><a href="ptapp.html#concluding-thoughts"><i class="fa fa-check"></i><b>8.6</b> Concluding thoughts</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing.html"><a href="mixing.html"><i class="fa fa-check"></i><b>9</b> Integrated inferences</a><ul>
<li class="chapter" data-level="9.1" data-path="mixing.html"><a href="mixing.html#theres-only-ever-one-case"><i class="fa fa-check"></i><b>9.1</b> There’s only ever one case</a></li>
<li class="chapter" data-level="9.2" data-path="mixing.html"><a href="mixing.html#general-procedure"><i class="fa fa-check"></i><b>9.2</b> General procedure</a><ul>
<li class="chapter" data-level="9.2.1" data-path="mixing.html"><a href="mixing.html#estimation"><i class="fa fa-check"></i><b>9.2.1</b> Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="mixing.html"><a href="mixing.html#illustration"><i class="fa fa-check"></i><b>9.3</b> Illustration</a></li>
<li class="chapter" data-level="9.4" data-path="mixing.html"><a href="mixing.html#illustrated-inferences"><i class="fa fa-check"></i><b>9.4</b> Illustrated inferences</a><ul>
<li class="chapter" data-level="9.4.1" data-path="mixing.html"><a href="mixing.html#xy-model"><i class="fa fa-check"></i><b>9.4.1</b> XY model</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="mixing.html"><a href="mixing.html#considerations"><i class="fa fa-check"></i><b>9.5</b> Considerations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="mixing.html"><a href="mixing.html#the-identification-problem"><i class="fa fa-check"></i><b>9.5.1</b> The identification problem</a></li>
<li class="chapter" data-level="9.5.2" data-path="mixing.html"><a href="mixing.html#continuous-data"><i class="fa fa-check"></i><b>9.5.2</b> Continuous data</a></li>
<li class="chapter" data-level="9.5.3" data-path="mixing.html"><a href="mixing.html#measurement-error"><i class="fa fa-check"></i><b>9.5.3</b> Measurement error</a></li>
<li class="chapter" data-level="9.5.4" data-path="mixing.html"><a href="mixing.html#spillovers"><i class="fa fa-check"></i><b>9.5.4</b> Spillovers</a></li>
<li class="chapter" data-level="9.5.5" data-path="mixing.html"><a href="mixing.html#clustering-and-other-violations-of-independence"><i class="fa fa-check"></i><b>9.5.5</b> Clustering and other violations of independence</a></li>
<li class="chapter" data-level="9.5.6" data-path="mixing.html"><a href="mixing.html#parameteric-models"><i class="fa fa-check"></i><b>9.5.6</b> Parameteric models</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="mixing.html"><a href="mixing.html#conclusion-1"><i class="fa fa-check"></i><b>9.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mixingapp.html"><a href="mixingapp.html"><i class="fa fa-check"></i><b>10</b> Mixed-Method Application: Inequality and Democracy Revisited</a><ul>
<li class="chapter" data-level="10.1" data-path="mixingapp.html"><a href="mixingapp.html#a-trained-model"><i class="fa fa-check"></i><b>10.1</b> A trained model</a></li>
<li class="chapter" data-level="10.2" data-path="mixingapp.html"><a href="mixingapp.html#data"><i class="fa fa-check"></i><b>10.2</b> Data</a></li>
<li class="chapter" data-level="10.3" data-path="mixingapp.html"><a href="mixingapp.html#inference"><i class="fa fa-check"></i><b>10.3</b> Inference</a><ul>
<li class="chapter" data-level="10.3.1" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-cause-democracy"><i class="fa fa-check"></i><b>10.3.1</b> Did inequality <em>cause</em> democracy?</a></li>
<li class="chapter" data-level="10.3.2" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-prevent-democracy"><i class="fa fa-check"></i><b>10.3.2</b> Did inequality <em>prevent</em> democracy?</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="mixingapp.html"><a href="mixingapp.html#exercises"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mm.html"><a href="mm.html"><i class="fa fa-check"></i><b>11</b> Mixing models</a><ul>
<li class="chapter" data-level="11.1" data-path="mm.html"><a href="mm.html#a-jigsaw-puzzle-integrating-across-a-model"><i class="fa fa-check"></i><b>11.1</b> A jigsaw puzzle: Integrating across a model</a></li>
<li class="chapter" data-level="11.2" data-path="mm.html"><a href="mm.html#combining-observational-and-experimental-data"><i class="fa fa-check"></i><b>11.2</b> Combining observational and experimental data</a></li>
<li class="chapter" data-level="11.3" data-path="mm.html"><a href="mm.html#transportation-of-findings-across-contexts"><i class="fa fa-check"></i><b>11.3</b> Transportation of findings across contexts</a></li>
<li class="chapter" data-level="11.4" data-path="mm.html"><a href="mm.html#multilevel-models-meta-analysis"><i class="fa fa-check"></i><b>11.4</b> Multilevel models, meta-analysis</a></li>
</ul></li>
<li class="part"><span><b>III Design Choices</b></span></li>
<li class="chapter" data-level="12" data-path="elements-of-design.html"><a href="elements-of-design.html"><i class="fa fa-check"></i><b>12</b> Elements of Design</a><ul>
<li class="chapter" data-level="12.1" data-path="elements-of-design.html"><a href="elements-of-design.html#model-inquiry-data-strategy-answer-strategy"><i class="fa fa-check"></i><b>12.1</b> Model, inquiry, data strategy, answer strategy</a><ul>
<li class="chapter" data-level="12.1.1" data-path="elements-of-design.html"><a href="elements-of-design.html#defining-a-model"><i class="fa fa-check"></i><b>12.1.1</b> Defining a model</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="elements-of-design.html"><a href="elements-of-design.html#evaluating-a-design"><i class="fa fa-check"></i><b>12.2</b> Evaluating a design</a><ul>
<li class="chapter" data-level="12.2.1" data-path="elements-of-design.html"><a href="elements-of-design.html#expected-error-and-expected-posterior-variance"><i class="fa fa-check"></i><b>12.2.1</b> Expected error and expected posterior variance</a></li>
<li class="chapter" data-level="12.2.2" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-1"><i class="fa fa-check"></i><b>12.2.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-of-design-decaration-in-code"><i class="fa fa-check"></i><b>12.3</b> Illustration of Design Decaration in code</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="clue.html"><a href="clue.html"><i class="fa fa-check"></i><b>13</b> Clue Selection as a Decision Problem</a><ul>
<li class="chapter" data-level="13.1" data-path="clue.html"><a href="clue.html#core-logic"><i class="fa fa-check"></i><b>13.1</b> Core logic</a></li>
<li class="chapter" data-level="13.2" data-path="clue.html"><a href="clue.html#a-strategic-approach"><i class="fa fa-check"></i><b>13.2</b> A strategic approach</a><ul>
<li class="chapter" data-level="13.2.1" data-path="clue.html"><a href="clue.html#clue-selection-with-a-simple-example"><i class="fa fa-check"></i><b>13.2.1</b> Clue selection with a simple example</a></li>
<li class="chapter" data-level="13.2.2" data-path="clue.html"><a href="clue.html#dependence-on-prior-beliefs"><i class="fa fa-check"></i><b>13.2.2</b> Dependence on prior beliefs</a></li>
<li class="chapter" data-level="13.2.3" data-path="clue.html"><a href="clue.html#clue-selection-for-the-democratization-model"><i class="fa fa-check"></i><b>13.2.3</b> Clue selection for the democratization model</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="clue.html"><a href="clue.html#dynamic-strategies"><i class="fa fa-check"></i><b>13.3</b> Dynamic Strategies</a></li>
<li class="chapter" data-level="13.4" data-path="clue.html"><a href="clue.html#conclusion-2"><i class="fa fa-check"></i><b>13.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="caseselection.html"><a href="caseselection.html"><i class="fa fa-check"></i><b>14</b> Mixed methods data strategies</a><ul>
<li class="chapter" data-level="14.1" data-path="caseselection.html"><a href="caseselection.html#case-selection-strategies"><i class="fa fa-check"></i><b>14.1</b> Case selection strategies</a><ul>
<li class="chapter" data-level="14.1.1" data-path="caseselection.html"><a href="caseselection.html#no-general-rules"><i class="fa fa-check"></i><b>14.1.1</b> No general rules</a></li>
<li class="chapter" data-level="14.1.2" data-path="caseselection.html"><a href="caseselection.html#specific-case-walk-through"><i class="fa fa-check"></i><b>14.1.2</b> Specific case walk through</a></li>
<li class="chapter" data-level="14.1.3" data-path="caseselection.html"><a href="caseselection.html#case-selection-from-causal-models-a-simulation-based-approach"><i class="fa fa-check"></i><b>14.1.3</b> Case selection from causal models: a simulation-based approach</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="caseselection.html"><a href="caseselection.html#wide-or-deep"><i class="fa fa-check"></i><b>14.2</b> Wide or Deep</a><ul>
<li class="chapter" data-level="14.2.1" data-path="caseselection.html"><a href="caseselection.html#walk-through-of-a-simple-comparison"><i class="fa fa-check"></i><b>14.2.1</b> Walk-through of a simple comparison</a></li>
<li class="chapter" data-level="14.2.2" data-path="caseselection.html"><a href="caseselection.html#results-from-simulations"><i class="fa fa-check"></i><b>14.2.2</b> Results from simulations</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="caseselection.html"><a href="caseselection.html#principles"><i class="fa fa-check"></i><b>14.3</b> Principles</a></li>
</ul></li>
<li class="part"><span><b>IV Models in Question</b></span></li>
<li class="chapter" data-level="15" data-path="justifying-models.html"><a href="justifying-models.html"><i class="fa fa-check"></i><b>15</b> Justifying models</a><ul>
<li class="chapter" data-level="15.1" data-path="justifying-models.html"><a href="justifying-models.html#nothing-from-nothing"><i class="fa fa-check"></i><b>15.1</b> Nothing from nothing</a></li>
<li class="chapter" data-level="15.2" data-path="justifying-models.html"><a href="justifying-models.html#justifying-the-classic-process-tracing-tests"><i class="fa fa-check"></i><b>15.2</b> Justifying the classic process tracing tests</a></li>
<li class="chapter" data-level="15.3" data-path="justifying-models.html"><a href="justifying-models.html#justification-from-experimental-designs"><i class="fa fa-check"></i><b>15.3</b> Justification from experimental designs</a><ul>
<li class="chapter" data-level="15.3.1" data-path="justifying-models.html"><a href="justifying-models.html#mediator"><i class="fa fa-check"></i><b>15.3.1</b> Mediator</a></li>
<li class="chapter" data-level="15.3.2" data-path="justifying-models.html"><a href="justifying-models.html#moderator"><i class="fa fa-check"></i><b>15.3.2</b> Moderator</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="justifying-models.html"><a href="justifying-models.html#causal-discovery"><i class="fa fa-check"></i><b>15.4</b> Causal discovery</a></li>
<li class="chapter" data-level="15.5" data-path="justifying-models.html"><a href="justifying-models.html#exercise"><i class="fa fa-check"></i><b>15.5</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="evaluation.html"><a href="evaluation.html"><i class="fa fa-check"></i><b>16</b> Evaluating models</a><ul>
<li class="chapter" data-level="16.1" data-path="evaluation.html"><a href="evaluation.html#five-strategies"><i class="fa fa-check"></i><b>16.1</b> Five Strategies</a><ul>
<li class="chapter" data-level="16.1.1" data-path="evaluation.html"><a href="evaluation.html#check-conditional-independence"><i class="fa fa-check"></i><b>16.1.1</b> Check conditional independence</a></li>
<li class="chapter" data-level="16.1.2" data-path="evaluation.html"><a href="evaluation.html#computational-clues"><i class="fa fa-check"></i><b>16.1.2</b> Computational clues</a></li>
<li class="chapter" data-level="16.1.3" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value-are-the-data-unexpected-given-your-model"><i class="fa fa-check"></i><b>16.1.3</b> Bayesian <span class="math inline">\(p\)</span> value: Are the data unexpected given your model?</a></li>
<li class="chapter" data-level="16.1.4" data-path="evaluation.html"><a href="evaluation.html#leave-one-out-loo-cross-validation"><i class="fa fa-check"></i><b>16.1.4</b> Leave-one-out (LOO) cross-validation</a></li>
<li class="chapter" data-level="16.1.5" data-path="evaluation.html"><a href="evaluation.html#sensitivity"><i class="fa fa-check"></i><b>16.1.5</b> Sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="evaluation.html"><a href="evaluation.html#evaluating-the-democracy-inequality-model"><i class="fa fa-check"></i><b>16.2</b> Evaluating the Democracy-Inequality model</a><ul>
<li class="chapter" data-level="16.2.1" data-path="evaluation.html"><a href="evaluation.html#check-assumptions-of-conditional-independence"><i class="fa fa-check"></i><b>16.2.1</b> Check assumptions of conditional independence</a></li>
<li class="chapter" data-level="16.2.2" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value"><i class="fa fa-check"></i><b>16.2.2</b> Bayesian <span class="math inline">\(p\)</span>-value</a></li>
<li class="chapter" data-level="16.2.3" data-path="evaluation.html"><a href="evaluation.html#loo-validation"><i class="fa fa-check"></i><b>16.2.3</b> LOO validation</a></li>
<li class="chapter" data-level="16.2.4" data-path="evaluation.html"><a href="evaluation.html#sensitivity-to-priors"><i class="fa fa-check"></i><b>16.2.4</b> Sensitivity to priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>17</b> Final Words</a><ul>
<li class="chapter" data-level="17.1" data-path="final-words.html"><a href="final-words.html#rethinking-model-based-inquiry"><i class="fa fa-check"></i><b>17.1</b> Rethinking model-based inquiry</a></li>
<li class="chapter" data-level="17.2" data-path="final-words.html"><a href="final-words.html#lessons-learned-along-the-way"><i class="fa fa-check"></i><b>17.2</b> Lessons learned along the way</a></li>
<li class="chapter" data-level="17.3" data-path="final-words.html"><a href="final-words.html#rules-of-thumb-for-reasoning-about-learning"><i class="fa fa-check"></i><b>17.3</b> Rules of thumb for reasoning about learning</a></li>
<li class="chapter" data-level="17.4" data-path="final-words.html"><a href="final-words.html#worries-about-what-you-have-to-put-in"><i class="fa fa-check"></i><b>17.4</b> Worries about what you have to put in</a></li>
<li class="chapter" data-level="17.5" data-path="final-words.html"><a href="final-words.html#limits-on-what-you-can-get-out"><i class="fa fa-check"></i><b>17.5</b> Limits on what you can get out</a></li>
<li class="chapter" data-level="17.6" data-path="final-words.html"><a href="final-words.html#know-what-you-impose-when-you-impose-structure"><i class="fa fa-check"></i><b>17.6</b> Know what you impose when you impose structure</a></li>
<li class="chapter" data-level="17.7" data-path="final-words.html"><a href="final-words.html#looking-ahead"><i class="fa fa-check"></i><b>17.7</b> Looking ahead</a></li>
</ul></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li class="chapter" data-level="18" data-path="examplesappendix.html"><a href="examplesappendix.html"><i class="fa fa-check"></i><b>18</b> <code>CausalQueries</code></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/macartan/gbiqq/" target="blank">Uses gbiqq</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Integrated Inferences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="final-words" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> Final Words</h1>
<p>The central idea of this book is that we can learn about the world by combining new evidence with causal models that represent prior knowledge about causal relations in the domain of interest.</p>
<!-- many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account for how and under what conditions causal relations play out.  -->
<p>The growth of randomized experiments and other design-based approaches over the last two decades has made it possible to dispense with or diminish the role of background assumptions for some research questions and contexts. This is a remarkable achievement that has put the testing of some hypotheses and estimation of some causal quantities on a firmer footing. At the same time, there are limits to model-free social science. In particular, design-based inference relies on (as-good-as) random assignment by the researcher or by nature, placing bounds on the kinds of causes and contexts we can investigate. The approach is also generally limited to estimating a single causal quantity: the average causal effect.</p>
<p>Building on pioneering work by scholars in computer science, statistics, and philosophy, we have outlined a principled approach to, and provided software tools for, mobilizing prior knowledge to learn from new data in situations where randomization is unavailable and to answer questions for which randomization is unhelpful. In this approach, causal models are <em>guides</em> to research design, <em>machines</em> for inference, and <em>objects</em> of inquiry. As guides, the models yield expectations about the learning that can be derived from a given case or set of cases and from a given type of evidence, conditional on the question being asked. As inferential machines, models allow updating on that query once the data are in hand. Finally, when we confront a model with data, we learn about the parameters of the model itself, which can be used to answer a range of other causal questions and allowing cumulation of knowledge across studies.</p>
<div id="rethinking-model-based-inquiry" class="section level2">
<h2><span class="header-section-number">17.1</span> Rethinking model-based inquiry</h2>
<p>Intellectual currents in empirical social science have been rushing against model-based inquiry for the last decade or more. Profound uneasiness with the assumptions required to draw causal inferences from observational regression estimates has prompted a flight to the epistemic safety of random assignment. With a minimal set of assumptions, a randomized experiment allows for an unbiased estimate of an important quantity, the average treatment effect. Randomized experiments offer answers that are easy to defend. Model-based analysis offers answers that will always be model-dependent. To the extent that what we our after is confidence in our conclusions, the move from models to strong designs represents unquestionable progress.</p>
<p>What we hope that readers will take from this book, however, is that it is a fallacy to think of explicitly model-based inquiry as a second-best alternative to model-free experimentation. It is a different way of learning about the world altogether. Whether our data are generated from an experiment or arise observationally, organizing inquiry around a causal model has a number of distinct advantages:</p>
<p><strong>Many questions.</strong> When we update a causal model, we do not estimate a single causal quantity of interest: we learn about <em>the model</em>. Most concretely, when we encounter new data, we update our beliefs about <em>all</em> parameters in the model at the same time. We can then use the updated parameters to answer very broad classes of causal questions, well beyond the population-level average effect. These include case-level questions (<em>Does <span class="math inline">\(X\)</span> explain <span class="math inline">\(Y\)</span> in this case?</em>), process questions (<em>Through which channel does <span class="math inline">\(X\)</span> affect <span class="math inline">\(Y\)</span>?</em>), and transportability questions (<em>What are the implications of results derived in one context for processes and effects in other contexts?</em>).</p>
<p><strong>Common answer strategy.</strong> Strikingly, these diverse types of questions are all asked and answered in this approach using the same procedure: forming, updating, and querying a causal model. Likewise, once we update a model given a set of data, we can then pose the full range of causal queries to the updated model. In this respect, the causal models approach differs markedly from common statistical frameworks in which distinct estimators are constructed to estimate particular estimands.</p>
<p><strong>Answers without identification.</strong> The approach can be used to generate answers even when queries are not <em>identified.</em> The ability to “identify” causal effects has been a central pursuit of much social science research in recent years. But identification is in some ways a curious goal. A causal quantity is identified, if, with infinite data, the correct value can be ascertained with certainty—informally, the distribution that will emerge is consistent with only one parameter value. Oddly, however knowing that a model, or quantity, is identified in this way does not tell you that estimation with finite data is any good <span class="citation">(Maclaren and Nicholson <a href="#ref-maclaren2019can">2019</a>)</span>. Nor is estimation of a non-identified model with finite data necessarily bad. While there is a tendency to discount models for which quantities of interest are not identified, in fact it is relatively easy to see that conisderable learning is possible even without identification, using the same procedure of updating and querying models. Updating non-identified models can lead to a tightening of posteriors, even if some quantities can never be distinguished from each other.</p>
<!-- Using causal models also provides a clear *procedure* for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw.  -->
<p><strong>Integration</strong> Embedding inference within an explicit causal model brings about an integration across forms of data and beliefs that may otherwise develop in isolation from one another. For one thing, the approach allows us to combine arbitrary mixes of forms of evidence, including data on causes and outcomes and evidence on causal processes (whether from the same or different sets of cases). Further, the causal-model approach ensures that our findings about <em>cases</em> (given evidence about those cases) are informed by what we know about the <em>population</em> to which those cases belong, and vice versa. And, as we discuss further below, approach generates integration between inputs and outputs into the inferential process: it ensures that the way in which we update from the data is logically consistent with our prior beliefs about the world.</p>
<p><strong>A framework for knowledge cumulation.</strong> Closely related to integration is cumulation: a causal-model framework provides a ready-made apparatus for combining information across studies. Thinking in meta-analytic terms, the framework provides a mechanism for combining the evidence from multiple, independent studies. Thinking sequentially, the model updated from one set of data can become the starting point for the next study of the same causal domain.</p>
<p>Yet organizing inquiry around a causal model allows for cumulation in a deeper sense as well. Compared with most prevailing approaches to observational inference—where the background model is typically left implicit or conveyed informally or incompletely—the approach ensures <em>transparency</em> about the beliefs on which inferences rest. Explicitness about starting assumptions allows us to assess the degree of sensitivity of conclusions to our prior beliefs. Sensitivity analyses cannot, of course, tell us which beliefs are right. But they can tell us which assumptions are most in need of defending, pinpointing <em>where more learning would be of greatest value.</em> Those features of our model about which we are most uncertain and that matter most to our conclusions — be it the absence of an arrow, a restriction, a prior over nodal types, the absence of confounding — represent the questions most in need of answers down the road.</p>
<!-- While we have outlined a set of strategies for validating and selecting our models, the model-contingency of conclusions is an important and inescapable limitation of the framework.  -->
<p>As we have developed the approach, our thinking about qualitative, quantitative, and mixed-method inference has shifted. In using causal models and seeing their benefits, we have have also developed a keener sense of the risks they entail. We outline these lessons and these risks next.</p>
</div>
<div id="lessons-learned-along-the-way" class="section level2">
<h2><span class="header-section-number">17.2</span> Lessons learned along the way</h2>
<!-- AJ: This section is hard to figure out a good location for. And it feels a little self-indulgent/autobiographical. Does anyone care at this point in th book how our thinking evolved? -->
<p>We note three ways in which our thinking about inference evolved in the course of this project, from our original article on mixing methods (<span class="citation">Humphreys and Jacobs (<a href="#ref-humphreys2015mixing">2015</a>)</span>) to the completion of this book.</p>
<p><strong>“Within” vs. “Between” Case Evidence.</strong> We embarked on this project motivated by an interest in how qualitative and quantitative data could be formally combined to draw case- and population-level causal inferences. In <span class="citation">Humphreys and Jacobs (<a href="#ref-humphreys2015mixing">2015</a>)</span>, we drew on a common operationalization of “quantitative” and “qualitative” data as akin to “dataset” and “causal process” observations, respectively, as defined by <span class="citation">Collier, Brady, and Seawright (<a href="#ref-collier2010sources">2010</a>)</span>; this is a distinction that roughly maps onto somewhat older notions of “cross-case” and “within-case” forms of analysis (<span class="citation">Mahoney (<a href="#ref-mahoney2000strategies">2000</a>)</span>). In a typical mixed-method setup, we might think of combining a “quantitative” dataset as containing <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (and covariate) observations for many cases with “qualitative” observations on causal processes, such as a mediator <span class="math inline">\(M\)</span>, for a subset of these cases.</p>
<p>In fact, as we came to realize, the apparent distinction here between forms of data has no meaning in the formal setup and analysis of models. There is no need to think of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> observations as being tied to a large-<span class="math inline">\(N\)</span> analysis or to observations of mediating or other processes as being tied to small-<span class="math inline">\(N\)</span> analysis. One could, for instance, have data on <span class="math inline">\(M\)</span> for a large set of cases but data on <span class="math inline">\(Y\)</span> or <span class="math inline">\(X\)</span> for only a small number. Updating the model to learn about the causal query of interest will proceed in the same basic manner. The cross-case/within-case dichotomy plays no role in the way inferences are drawn: given any pattern of data we observe in the cases at hand, we are always assessing the likelihood of that data pattern under different values of the model’s parameters. In this framework, what we have conventionally thought of as qualitative and quantitative inference strategies are not just integrated; the distinction between them breaks down completely.</p>
<p><strong>A single system.</strong> Our initial thinking about integrating inferences was not particularly well integrated. In <span class="citation">Humphreys and Jacobs (<a href="#ref-humphreys2015mixing">2015</a>)</span>, we posited a set of prior beliefs with which the researcher would begin, including beliefs about the values of estimands (<span class="math inline">\(\lambda\)</span> values) and beliefs about the informativeness of within-case (clue) information (<span class="math inline">\(\phi\)</span> values). These sets of beliefs were essentially independent of one another, as they are in many accounts of process tracing, including Bayesian approaches (<span class="citation">Fairfield and Charman (<a href="#ref-FairfieldBayes2015">2017</a>)</span>, <span class="citation">Bennett (<a href="#ref-BennettAppendix">2015</a>)</span>). In the standard Bayesian approach, one articulates a degree of confidence in some hypothesis about the world and one articulates a belief about the value of the evidence one goes looking for (e.g, that the search for the evidence constitutes a “hoop test” for the hypothesis). And these two sets of beliefs can be arbitrarily combined.</p>
<p>What we came to realize, however, was that <em>both</em> sets of beliefs — about the hypothesis being examined and about the probative value of the data — represent substantive probabilistic claims about the world, and in particular about <em>causal relationships</em> in the domain under investigation. They, thus, cannot not be treated as generally independent of one another: our beliefs about causal relations <em>imply</em> our beliefs about the probative value of the evidence. These implications flow naturally in a causal-model framework. When both sets of beliefs are themselves derived from an underlying model representing prior knowledge about the domain of interest, then the same conjectures that inform our beliefs about the hypotheses also inform our beliefs about the informativeness of additional data.</p>
<p><strong>Pretending to have priors.</strong> As we explored the world of causal models, we started out thinking that, in providing priors over causal relations, one is directly stating beliefs about how the world works. For instance, one might believe that either <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> or that it did not; and one might believe either that <span class="math inline">\(M=1\)</span> should be observed in the event that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> or it should not be. These statements are, in fact, clearly model-dependent. Beyond the model required to describe events in such crisp terms, the statements involve counterfactuals on counterfactuals—models of causal processes. Once a model involves assertions of conditional independence, we are clearly in the business of dealing in simplifications. Our priors become less statements of how we believe the world works and become statements about what set of models are least bad within a class of tractable abstractions.</p>
<!-- AJ: There's a clear tension between the above point and the ways in which we talk about models as reflecting "prior beliefs" or "prior knowledge" about the world. Need to resolve this. -->
</div>
<div id="rules-of-thumb-for-reasoning-about-learning" class="section level2">
<h2><span class="header-section-number">17.3</span> Rules of thumb for reasoning about learning</h2>
<!-- AJ: We'd discussed this going here, but I think it would be better placed in a substantive chapter. This is where people will be more likely to look for it.  -->
<ol style="list-style-type: decimal">
<li><p>Learning requires uncertainty. And expected learning goes up as you become more uncertain about what you’ll find. If your causal model puts a very high probability on X having a positive effect on M, and you already know X’s value, you should expect to learn very little from observing M since you’re very likely to see exactly the M value you expect given X. (Currently in Chap. 12)((And we want to make research design choices based on expected learning, not based on the mere possibility of learning: yes, our beliefs will shift if we look for M and find the unexpected value. But because that data-realization is highly unlikely, we expect the learning from observing M to be minimal.</p></li>
<li><p>Pure within-case (or n=1) learning requires informative priors about the nodes to be observed. For instance, in a chain model, where we want to go and observe M, it’s not enough to have an informative prior about the X-&gt;Y relationship. We need an informative prior about the X-&gt;M or M-&gt;Y link in order to learn from M. For instance, are positive X-&gt;M effects more common than negative ones?</p></li>
<li><p>Learning is possible in the absence of informative priors if we have n&gt;1 because it is sometimes possible to draw causal information from correlations. For instance, even if we have no idea what the distribution of X-&gt;M effects are, observing some correlation (or no correlation) between M and X across multiple cases provides information about the likelihood of an X-&gt;Y effect (even if it doesn’t tell us about the direction of the effect).</p></li>
<li><p>If there are different ways a query can be satisfied, evidence against one of those ways is evidence against the query as a whole. Say we have a two-path model — with one direct and one indirect path — and we want to know if X affects Y. We observe a mediator, M, along the indirect path in a set of cases. If the M data pattern is inconsistent with an indirect effect, then this is also evidence against an overall effect. In general, finding evidence against one way the effect can happen reduces our confidence in the effect happening at all.</p></li>
<li><p>Prior beliefs structure the learning from new data. When confronted with new data, a prior will condition updating as though the prior were “trying” to preserve itself. (There are elements of this point in Chap 13 — you learn more about heterogeneity if ATE is known.)((Say, we’re working with a chain model, suppose we have a lot of prior X,Y data consistent with a positive ATE of X on Y. We then process trace an X=1, Y=1 case and an X=0, Y=0 case, observing M=1 in both, so M is uncorrelated with X. We’ve just found evidence against an effect of X on Y, so this will reduce our posterior on the ATE. But the ATE prior also has a way to “preserve” itself: it can force a downward updating of our beliefs about the prevalence negative effects. If we haven’t observed any cases in which negative effects might operate, there is little to constrain that downward updating. The overall result will be updating on positive effects (downwardly) together a some mix of updating on the ATE (downwardly) and updating on negative effects (downwardly).((A corollary is that learning about a kind of case that we directly observe can get “transmitted” to a kind of case that we don’t directly observe via a constraint on our beliefs imposed by the model or priors. This is just a special instance of priors generating probative value: our priors on the ATE makes evidence about positive effects probative about negative effects. If we had flat priors on the ATE, learning about positive effects would have no impact on our beliefs about negative effects.</p></li>
</ol>
<p>A parallel example arises with a two-path model and observation of a mediator, M, along the indirect path in a set of cases. Suppose we find an M data pattern inconsistent with any kind of indirect effect: what happens to our beliefs about the ATE? In general, finding evidence against one way the effect can happen should reduce our confidence in the effect happening at all. However, if we have started out with a strong prior on the ATE but flat priors over whether the effect is a direct or indirect one, then our updating will tend to conserve our ATE beliefs by updating our beliefs about direct effects. So evidence against the indirect effect will function as evidence for direct effects as well as evidence against a total effect.</p>
<p>A further implication for process tracing is that there will generally be sharp limits to what we can learn about overall effects if we study mediators along only <em>some</em> of the theorized pathways, especially if we already have some prior information about effects. The difficulty is that whatever we learn from the mediators we do observe, about the pathways they lie along, will get offset by shifts in our beliefs about other pathways, generated by the constraint in our prior knowledge about the overall effect. Suppose you already have some belief that economic development makes democracy more likely, and you think there may be two mechanisms: one running through a rising middle class and one operating through a more robust and organized working class. Suppose then that I show you that the organization of the working class does not vary with per capita GDP. Rationally, you should then reduce your confidence in the working-class pathway. However, you should also <em>increase</em> your confidence in the operation of the middle-class pathway — because (a) you have prior reason to believe that the overall development <span class="math inline">\(\rightarrow\)</span> democracy effect exists and (b) we have not observed a mediator along the middle-class pathway. On balance, then, learning about just one pathway will not have a large impact on beliefs about the overall effect of GDP on democratization. The larger lesson here is that, if our process tracing strategy involves the examination of mediators to learn about total <span class="math inline">\(X \rightarrow Y\)</span> effects, then how much we stand to learn depends on whether we are collecting diagnostic evidence along <em>all</em> plausible pathways connecting <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>.</p>
<p>To be clear, we do not need to collect mediator clues on all <em>possible</em> pathways. If we have strong priors that one or more possible pathways are very unlikley, then we might safely be able to avoid collecting observations along those pathways without substantially reducing the prospects for learning. Also, the point that we are making here applies to using mediator data to answer queries about the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. If instead we want to know whether a <em>particular pathway</em> is operating, then collecting evidence just on that pathway might be highly informative.</p>
<ol start="6" style="list-style-type: decimal">
<li><p>Stronger priors impose a stronger constraint on learning. If we’ve observed only a small amount of X,Y data in the above chain-model example, then observing the M data in the on-the-regression-line cases will have a weaker impact on our beliefs about negative effects, and a bigger impact on our beliefs about ATE. Ditto for the 2-path model example. However, where prior data/beliefs on the ATE are strong, we’ll learn less about the ATE, and more about negative effects (or the direct path).</p></li>
<li><p>If there are different ways in which a query can be satisfied, evidence against the likelier way is stronger evidence against the query than is evidence against an unlikelier way. In the 2-path model, if we started out thinking that the indirect effect was more likely than the direct effect, then evidence against the direct effect will have a bigger impact on our beliefs about the overall model.</p></li>
<li><p>It is difficult to get empirical leverage on very unlikely queries. And queries may be unlikelier than they appear. Suppose we start with the 2-path model, and want to know if X has a positive effect on Y that rests on a chain of positive effects via M. And suppose, importantly, that we begin with flat priors over all nodal types. Our intuitions likely tell us that this is exactly the kind of question for which an observation of M is the perfect empirical strategy. And that intuition is, in a sense, correct: we can indeed learn about the query by observing M. Seeing M=1 in an X=Y=1 case, for instance. would be evidence consistent with the query while M=0 would be inconsistent. Fine. ((But we will only learn a little from this observation. The reason is that the query itself has a very low prior probability. It may actually not be obvious at first glance just how unlikely our query is to be true. (After all, the model has two causal paths, and we’re asking if positive effects run through one of them, right? Not quite.) Seeing this requires us to think about the joint probabilities implied by the query. First, the query requires X to have a positive effect on M, which we think there’s only a 25% chance of. In addition, the query puts a very narrow constraint on Y’s possible nodal types: Y has to have a nodal type in which M has a positive effect on Y when X does not change, and in which X does not have a positive effect on Y unless M changes from 0 to 1. This pair of conditions is met by only 2 of Y’s 16 nodal types, implying a 12.5% chance. The prior on the query is thus 0.25 x 0.125 = 0.03125. Thus, while observing M=0 takes the probability of the query down to 0%, we started out very close to 0%! And observing M=1 results in only a small uptick, to about 6% because there remain many type combinations consistent with M=1 but that do not fit through the needle-eye of this query.((((</p></li>
</ol>
</div>
<div id="worries-about-what-you-have-to-put-in" class="section level2">
<h2><span class="header-section-number">17.4</span> Worries about what you have to put in</h2>
<p>While we have found the syntax of Directed Acyclic Graphs to provide a flexible framework for setting up causal models, we have also become more keenly aware of some of the limitations of DAGs in representing causal processes. We discuss a few of these here.</p>
<p><strong>Well defined nodes?</strong> A DAG presupposes a set of well-defined nodes that come with location and time stamps. [Point to be elaborated.] This involves a discretization of the world that may not align with how qualitative researchers study the world. For instance, qualitative researchers do not just observe that (a) domino 1 fell and (b) domino 2 fell. They observe that domino 2 fell <em>just</em> as domino 1 hit it.</p>
<!-- AJ: I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->
<p><strong>Acyclic, really?</strong> DAGs are by definition acyclic. And it is not hard to argue that, since cause precedes effect, causal relations <em>should</em> be acyclic for any well-defined nodes. In practice, however, our variables often come with coarse periodizations: there was or was not mobilization in the 1990s; there was or was not democratization in the 1990s. We cannot extract the direction of arrows from the definition of nodes this coarse.</p>
<!-- AJ: Here too, I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->
<p><strong>Coherent underlying causal accounts.</strong> The approach we describe is one in which researchers are asked to provide a coherent model—albeit with uncertainty—regarding the ways in which nodes are causally related to each other. For instance, a researcher interested in using information on <span class="math inline">\(K\)</span> to ascertain whether <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> is expected to have a theory of whether <span class="math inline">\(K\)</span> acts as a moderator or a mediator for <span class="math inline">\(X\)</span>, and whether it is realized before or after <span class="math inline">\(Y\)</span>. Yet it is possible that a researcher has well formed beliefs about the informativeness of <span class="math inline">\(K\)</span> <em>without</em> an underlying model of how <span class="math inline">\(K\)</span> is causally related to <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. Granted, one might wonder where these beliefs come from or how they can be defended. We nonetheless note that one limitation of the approach we have described is that one cannot make use of an observation without a coherent account of that observation’s causal position relative to other variables and relationships of interest.</p>
</div>
<div id="limits-on-what-you-can-get-out" class="section level2">
<h2><span class="header-section-number">17.5</span> Limits on what you can get out</h2>
<p><strong>Complexity.</strong> To maintain simplicity, we have largely focused in this book on models with binary nodes. At first blush, this class of causal models indeed appears very simple. Yet even with binary nodes, complexity rises rapidly as the number of nodes and connections among them increases. As a node goes from having 1 parent to 2 parents to 3 parents, for instance, the number of nodal types — at that node alone — goes from 4 to 16 to 64, with knock-on effects for the number of possible causal types (combinations of nodal types across the model). A move in the direction of continuous variables — say, from binary nodes to nodes with 3 ordinal values — would also involve a dramatic increase in the complexity to the type-space. A potential solution is to move away from a fully non-parametric setting and impose structure on permissible function forms. [To expand on this point]</p>
<!-- AJ: These next two points seem more like advantages to me. They are fundamental features of causal inference we can see more clearly if we use models, not limitations of the approach itself. No? Maybe we should have a section on that sort of thing: limits to causal inference uncovered by the approach. (I've added the model-dependence point.)-->
<p><strong>Limits of qualitative data under ignorable assignments.</strong> A key payoff deploying causal models is the prospect of combining in-depth observations of a small number of cases with less-intensive investigation of a larger number of cases. Yet one of the lessons of the foregoing analysis is that the gains to such mixing may be limited. For instance, where the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> can be identified (assignment of <span class="math inline">\(X\)</span> is as-good-as random), one will learn little about this effect from process observations in a small number of cases. Put differently, case studies simply cannot add much to experimental estimates of the ATE, though there may be other queries of interest on which small-<span class="math inline">\(N\)</span> process evidence can be informative. [To be expanded]</p>
<p><strong>Model-dependence of conclusions</strong> One striking finding of some of the analyses presented here is see how sensitive conclusions can be to what would seem to be quite modest changes to models. We see two ways of thinking about the implications of this fact for a causal-models framework.</p>
<p>One lesson to draw would be that there are tight limits to building inference upon causal models. If results in this approach depend heavily on prior beliefs, which could be wrong, then we might doubt the utility of the framework. Perhaps all that we have done in this book is to make the case for pure design-based inference.</p>
<p>An alternative lesson also offers itself, however. To the extent that our inferences depend on our background causal beliefs, a transparent and systematic engagement with models becomes all the more important. If inferences are not built explicitly on models, we have no way of knowing how fragile they are, how they would change under an alternative set of premises, or what kind of learning we need to undertake if we want to generate more secure conclusions.</p>
<p>We do not see causal models as the only way forward or as a panacea, and we are conscious of the limitations and complexities of the approach we have outlined, as well as the need for extension and elaboration along numerous fronts. Yet we think there is value in further development of forms of empirical social science that can operate with analytic transparency outside the safe inferential confines of random assignment.</p>
</div>
<div id="know-what-you-impose-when-you-impose-structure" class="section level2">
<h2><span class="header-section-number">17.6</span> Know what you impose when you impose structure</h2>
<!-- AJ: Not quite sure what you have in mind for these points -->
<p>e.g. changige expectations of trematne effect learning in a confound model with M unobserved</p>
<p>beliefs about share a in a X-&gt;Y model fom reg data are different to learning from the saem data in a X-&gt;M-&gt;Y model wiht M unobserved.</p>
<p>If you specify X-&gt;M-&gt;Y &lt;-W and W is a smoking gun then it must be smokoning gund for M and for Y</p>
</div>
<div id="looking-ahead" class="section level2">
<h2><span class="header-section-number">17.7</span> Looking ahead</h2>
<p>While this book develops the intellectual underpinnings of the framework, there is still much to be done to expand the approach’s practical utility.</p>
<p><strong>Levels of measurement.</strong> Perhaps the most obvious extension to the setup described in this book is to move beyond binary nodes. This is in principle a simple elaboration of the same nodal-type setup. If, for instance, we moved to nodes with 3 ordered categories, then each of <span class="math inline">\(Y\)</span>’s nodal types in an <span class="math inline">\(X \rightarrow Y\)</span> model would have to register 3 potential outcomes, corresponding to the 3 values that <span class="math inline">\(X\)</span> takes on. And <span class="math inline">\(Y\)</span> would have <span class="math inline">\(3 \times 3 \times 3 = 27\)</span> nodal types (as <span class="math inline">\(Y\)</span> can take on 3 possible values for each possible value of <span class="math inline">\(X\)</span>).</p>
<p>SOMETHING ABOUT MOVING TO CONTINUOUS VARIABLES.</p>
<p>The main cost of this move is further complexity. We suspect that researchers employing higher levels of measurement will likely want to impose more structure on their models via restrictions, such as monotonicity or allowing for concavity but no convexity in causal relations, in order to make the models more interpretable and computationally tractable.</p>
<p><strong>Computational issues.</strong> At the time of writing, the current version of <code>CausalQueries</code> places high computational demands when working with models of even moderate complexity….. MORE TO GO HERE</p>
<p><strong>Generating models.</strong> In this book, we have largely taken it as given that the researcher has found a way to generate the model. But how should researchers go about that important task? We see this as another area in which the approach would benefit greatly from further development. One set of approaches, in the early stages of development, involves eliciting causal beliefs from a set of experts in a given domain……HOPEFULLY FROM ELLIE MURRARY. Madigan, ….on eliciting priors.</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BennettAppendix">
<p>Bennett, Andrew. 2015. “Appendix.” In <em>Process Tracing: From Metaphor to Analytic Tool</em>, edited by Andrew Bennett and Jeffrey Checkel. New York: Cambridge University Press.</p>
</div>
<div id="ref-collier2010sources">
<p>Collier, David, Henry E Brady, and Jason Seawright. 2010. “Sources of Leverage in Causal Inference: Toward an Alternative View of Methodology.” In <em>Rethinking Social Inquiry: Diverse Tools, Shared Standards</em>, edited by David Collier and Henry E Brady, 161–99. Lanham MD: Rowman; Littlefield.</p>
</div>
<div id="ref-FairfieldBayes2015">
<p>Fairfield, Tasha, and Andrew Charman. 2017. “Explicit Bayesian Analysis for Process Tracing: Guidelines, Opportunities, and Caveats.” <em>Political Analysis</em> 25 (3). Cambridge University Press on behalf of the Society for Political Methodology: 363–80.</p>
</div>
<div id="ref-humphreys2015mixing">
<p>Humphreys, Macartan, and Alan M Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” <em>American Political Science Review</em> 109 (04). Cambridge Univ Press: 653–73.</p>
</div>
<div id="ref-maclaren2019can">
<p>Maclaren, Oliver J, and Ruanui Nicholson. 2019. “What Can Be Estimated? Identifiability, Estimability, Causal Inference and Ill-Posed Inverse Problems.” <em>arXiv Preprint arXiv:1904.02826</em>.</p>
</div>
<div id="ref-mahoney2000strategies">
<p>Mahoney, James. 2000. “Strategies of Causal Inference in Small-N Analysis.” <em>Sociological Methods &amp; Research</em> 28 (4): 387–424. doi:<a href="https://doi.org/10.1177/0049124100028004001">10.1177/0049124100028004001</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="evaluation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="examplesappendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["ii.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
