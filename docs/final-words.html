<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Final Words | Integrated Inferences</title>
  <meta name="description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Final Words | Integrated Inferences" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="dnieperriver.png" />
  <meta property="og:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="github-repo" content="rstudio/ii" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Final Words | Integrated Inferences" />
  
  <meta name="twitter:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="twitter:image" content="dnieperriver.png" />

<meta name="author" content="Macartan Humphreys and Alan Jacobs" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="evaluation.html"/>
<link rel="next" href="examplesappendix.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="headers/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Model based causal inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-case-for-causal-models"><i class="fa fa-check"></i><b>1.1</b> The Case for Causal Models</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#the-limits-to-design-based-inference"><i class="fa fa-check"></i><b>1.1.1</b> The limits to design-based inference</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#qualitative-and-mixed-method-inference"><i class="fa fa-check"></i><b>1.1.2</b> Qualitative and mixed-method inference</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#connecting-theory-and-empirics"><i class="fa fa-check"></i><b>1.1.3</b> Connecting theory and empirics</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-contributions"><i class="fa fa-check"></i><b>1.2</b> Key contributions</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-road-ahead"><i class="fa fa-check"></i><b>1.3</b> The Road Ahead</a></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Causal Models</a><ul>
<li class="chapter" data-level="2.1" data-path="models.html"><a href="models.html#the-counterfactual-model"><i class="fa fa-check"></i><b>2.1</b> The counterfactual model</a><ul>
<li class="chapter" data-level="2.1.1" data-path="models.html"><a href="models.html#generalizing-to-outcomes-with-many-causes"><i class="fa fa-check"></i><b>2.1.1</b> Generalizing to outcomes with many causes</a></li>
<li class="chapter" data-level="2.1.2" data-path="models.html"><a href="models.html#deterministic-relations"><i class="fa fa-check"></i><b>2.1.2</b> Deterministic relations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="models.html"><a href="models.html#causal-models-and-directed-acyclic-graphs"><i class="fa fa-check"></i><b>2.2</b> Causal Models and Directed Acyclic Graphs</a><ul>
<li class="chapter" data-level="2.2.1" data-path="models.html"><a href="models.html#components-of-a-causal-model"><i class="fa fa-check"></i><b>2.2.1</b> Components of a Causal Model</a></li>
<li class="chapter" data-level="2.2.2" data-path="models.html"><a href="models.html#rules-for-graphing-causal-models"><i class="fa fa-check"></i><b>2.2.2</b> Rules for graphing causal models</a></li>
<li class="chapter" data-level="2.2.3" data-path="models.html"><a href="models.html#conditional-independence-from-dags"><i class="fa fa-check"></i><b>2.2.3</b> Conditional independence from DAGs</a></li>
<li class="chapter" data-level="2.2.4" data-path="models.html"><a href="models.html#a-simple-running-example"><i class="fa fa-check"></i><b>2.2.4</b> A simple running example</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="models.html"><a href="models.html#chapter-appendix"><i class="fa fa-check"></i><b>2.3</b> Chapter Appendix</a><ul>
<li class="chapter" data-level="2.3.1" data-path="models.html"><a href="models.html#steps-for-constructing-causal-models"><i class="fa fa-check"></i><b>2.3.1</b> Steps for constructing causal models</a></li>
<li class="chapter" data-level="2.3.2" data-path="models.html"><a href="models.html#model-construction-in-code"><i class="fa fa-check"></i><b>2.3.2</b> Model construction in code</a></li>
<li class="chapter" data-level="2.3.3" data-path="models.html"><a href="models.html#test-yourself-can-you-read-conditional-independence-from-a-graph"><i class="fa fa-check"></i><b>2.3.3</b> Test yourself! Can you read conditional independence from a graph?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="illustratemodels.html"><a href="illustratemodels.html"><i class="fa fa-check"></i><b>3</b> Illustrating Causal Models</a><ul>
<li class="chapter" data-level="3.1" data-path="illustratemodels.html"><a href="illustratemodels.html#welfare-state-reform-pierson-1994"><i class="fa fa-check"></i><b>3.1</b> Welfare state reform: Pierson (1994)</a></li>
<li class="chapter" data-level="3.2" data-path="illustratemodels.html"><a href="illustratemodels.html#military-interventions-saunders-2011"><i class="fa fa-check"></i><b>3.2</b> Military Interventions: Saunders (2011)</a></li>
<li class="chapter" data-level="3.3" data-path="illustratemodels.html"><a href="illustratemodels.html#development-and-democratization-przeworski-and-limongi-1997"><i class="fa fa-check"></i><b>3.3</b> Development and Democratization: Przeworski and Limongi (1997)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>4</b> Theories as causal models</a><ul>
<li class="chapter" data-level="4.1" data-path="theory.html"><a href="theory.html#theory-as-a-lower-level-model"><i class="fa fa-check"></i><b>4.1</b> Theory as a “lower-level” model</a></li>
<li class="chapter" data-level="4.2" data-path="theory.html"><a href="theory.html#illustration-of-unpacking-causal-types"><i class="fa fa-check"></i><b>4.2</b> Illustration of unpacking causal types</a><ul>
<li class="chapter" data-level="4.2.1" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-mediation-model"><i class="fa fa-check"></i><b>4.2.1</b> Type disaggregation in a mediation model</a></li>
<li class="chapter" data-level="4.2.2" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-moderation-model"><i class="fa fa-check"></i><b>4.2.2</b> Type disaggregation in a moderation model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="theory.html"><a href="theory.html#rules-for-moving-between-higher--and-lower-level-models"><i class="fa fa-check"></i><b>4.3</b> Rules for moving between higher- and lower-level models</a><ul>
<li class="chapter" data-level="4.3.1" data-path="theory.html"><a href="theory.html#moving-down-levels"><i class="fa fa-check"></i><b>4.3.1</b> Moving down levels</a></li>
<li class="chapter" data-level="4.3.2" data-path="theory.html"><a href="theory.html#moving-up-levels"><i class="fa fa-check"></i><b>4.3.2</b> Moving up levels</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="theory.html"><a href="theory.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a><ul>
<li class="chapter" data-level="4.4.1" data-path="theory.html"><a href="theory.html#quantifying-the-gains-of-a-theory"><i class="fa fa-check"></i><b>4.4.1</b> Quantifying the gains of a theory</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="theory.html"><a href="theory.html#chapter-appendices"><i class="fa fa-check"></i><b>4.5</b> Chapter Appendices</a><ul>
<li class="chapter" data-level="4.5.1" data-path="theory.html"><a href="theory.html#summary-boxes"><i class="fa fa-check"></i><b>4.5.1</b> Summary Boxes</a></li>
<li class="chapter" data-level="4.5.2" data-path="theory.html"><a href="theory.html#illustration-of-a-mapping-from-a-game-to-a-dag"><i class="fa fa-check"></i><b>4.5.2</b> Illustration of a Mapping from a Game to a DAG</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i><b>5</b> Causal Queries</a><ul>
<li class="chapter" data-level="5.1" data-path="questions.html"><a href="questions.html#case-level-causal-effects"><i class="fa fa-check"></i><b>5.1</b> Case-level causal effects</a></li>
<li class="chapter" data-level="5.2" data-path="questions.html"><a href="questions.html#case-level-causal-attribution"><i class="fa fa-check"></i><b>5.2</b> Case-level causal attribution</a></li>
<li class="chapter" data-level="5.3" data-path="questions.html"><a href="questions.html#case-level-explanation"><i class="fa fa-check"></i><b>5.3</b> Case-level explanation</a></li>
<li class="chapter" data-level="5.4" data-path="questions.html"><a href="questions.html#average-causal-effects"><i class="fa fa-check"></i><b>5.4</b> Average causal effects</a></li>
<li class="chapter" data-level="5.5" data-path="questions.html"><a href="questions.html#causal-paths"><i class="fa fa-check"></i><b>5.5</b> Causal Paths</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayeschapter.html"><a href="bayeschapter.html"><i class="fa fa-check"></i><b>6</b> Bayesian Answers</a><ul>
<li class="chapter" data-level="6.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-basics"><i class="fa fa-check"></i><b>6.1</b> Bayes Basics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-instances"><i class="fa fa-check"></i><b>6.1.1</b> Simple instances</a></li>
<li class="chapter" data-level="6.1.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-rule-for-discrete-hypotheses"><i class="fa fa-check"></i><b>6.1.2</b> Bayes’ Rule for Discrete Hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="bayeschapter.html"><a href="bayeschapter.html#the-dirichlet-family-and-bayes-rule-for-continuous-parameters"><i class="fa fa-check"></i><b>6.1.3</b> The Dirichlet family and Bayes’ Rule for Continuous Parameters</a></li>
<li class="chapter" data-level="6.1.4" data-path="bayeschapter.html"><a href="bayeschapter.html#moments"><i class="fa fa-check"></i><b>6.1.4</b> Moments</a></li>
<li class="chapter" data-level="6.1.5" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-estimation-in-practice"><i class="fa fa-check"></i><b>6.1.5</b> Bayes estimation in practice</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-applied"><i class="fa fa-check"></i><b>6.2</b> Bayes applied</a><ul>
<li class="chapter" data-level="6.2.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-bayesian-process-tracing"><i class="fa fa-check"></i><b>6.2.1</b> Simple Bayesian Process Tracing</a></li>
<li class="chapter" data-level="6.2.2" data-path="bayeschapter.html"><a href="bayeschapter.html#a-generalization-bayesian-inference-on-queries"><i class="fa fa-check"></i><b>6.2.2</b> A Generalization: Bayesian Inference on Queries</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayeschapter.html"><a href="bayeschapter.html#three-principles-of-bayesian-updating"><i class="fa fa-check"></i><b>6.3</b> Three principles of Bayesian updating</a><ul>
<li class="chapter" data-level="6.3.1" data-path="bayeschapter.html"><a href="bayeschapter.html#AppPriors"><i class="fa fa-check"></i><b>6.3.1</b> Priors matter</a></li>
<li class="chapter" data-level="6.3.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simultaneous-joint-updating"><i class="fa fa-check"></i><b>6.3.2</b> Simultaneous, joint updating</a></li>
<li class="chapter" data-level="6.3.3" data-path="bayeschapter.html"><a href="bayeschapter.html#posteriors-are-independent-of-the-ordering-of-data"><i class="fa fa-check"></i><b>6.3.3</b> Posteriors are independent of the ordering of data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Model-Based Causal Inference</b></span></li>
<li class="chapter" data-level="7" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>7</b> Process Tracing with Causal Models</a><ul>
<li class="chapter" data-level="7.1" data-path="pt.html"><a href="pt.html#process-tracing-and-causal-models"><i class="fa fa-check"></i><b>7.1</b> Process tracing and causal models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="pt.html"><a href="pt.html#the-intuition"><i class="fa fa-check"></i><b>7.1.1</b> The intuition</a></li>
<li class="chapter" data-level="7.1.2" data-path="pt.html"><a href="pt.html#a-formalization-of-the-general-approach"><i class="fa fa-check"></i><b>7.1.2</b> A formalization of the general approach</a></li>
<li class="chapter" data-level="7.1.3" data-path="pt.html"><a href="pt.html#illustration-with-code"><i class="fa fa-check"></i><b>7.1.3</b> Illustration with code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="pt.html"><a href="pt.html#five-principles"><i class="fa fa-check"></i><b>7.2</b> Five principles</a><ul>
<li class="chapter" data-level="7.2.1" data-path="pt.html"><a href="pt.html#classic-qualitative-tests-are-special-cases-of-updating-on-a-model"><i class="fa fa-check"></i><b>7.2.1</b> Classic qualitative tests are special cases of updating on a model</a></li>
<li class="chapter" data-level="7.2.2" data-path="pt.html"><a href="pt.html#a-dag-alone-does-not-get-you-probative-value"><i class="fa fa-check"></i><b>7.2.2</b> A DAG alone does not get you probative value</a></li>
<li class="chapter" data-level="7.2.3" data-path="pt.html"><a href="pt.html#uncertainty-does-not-alter-inference-for-single-case-causal-inference"><i class="fa fa-check"></i><b>7.2.3</b> Uncertainty does not alter inference for single case causal inference</a></li>
<li class="chapter" data-level="7.2.4" data-path="pt.html"><a href="pt.html#probative-value-requires-d-connection"><i class="fa fa-check"></i><b>7.2.4</b> Probative value requires <span class="math inline">\(d-\)</span>connection</a></li>
<li class="chapter" data-level="7.2.5" data-path="pt.html"><a href="pt.html#probative-value"><i class="fa fa-check"></i><b>7.2.5</b> Probative value</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptapp.html"><a href="ptapp.html"><i class="fa fa-check"></i><b>8</b> Application: Process Tracing with a Causal Model</a><ul>
<li class="chapter" data-level="8.1" data-path="ptapp.html"><a href="ptapp.html#inequality-and-democratization-the-debate"><i class="fa fa-check"></i><b>8.1</b> Inequality and Democratization: The Debate</a></li>
<li class="chapter" data-level="8.2" data-path="ptapp.html"><a href="ptapp.html#a-structural-causal-model"><i class="fa fa-check"></i><b>8.2</b> A Structural Causal Model</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptapp.html"><a href="ptapp.html#forming-priors"><i class="fa fa-check"></i><b>8.2.1</b> Forming Priors</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptapp.html"><a href="ptapp.html#results"><i class="fa fa-check"></i><b>8.3</b> Results</a></li>
<li class="chapter" data-level="8.4" data-path="ptapp.html"><a href="ptapp.html#pathways"><i class="fa fa-check"></i><b>8.4</b> Pathways</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ptapp.html"><a href="ptapp.html#cases-with-incomplete-data"><i class="fa fa-check"></i><b>8.4.1</b> Cases with incomplete data</a></li>
<li class="chapter" data-level="8.4.2" data-path="ptapp.html"><a href="ptapp.html#inferences-for-cases-with-observed-democratization"><i class="fa fa-check"></i><b>8.4.2</b> Inferences for cases with observed democratization</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ptapp.html"><a href="ptapp.html#model-definition-and-inference-in-code"><i class="fa fa-check"></i><b>8.5</b> Model definition and inference in code</a></li>
<li class="chapter" data-level="8.6" data-path="ptapp.html"><a href="ptapp.html#concluding-thoughts"><i class="fa fa-check"></i><b>8.6</b> Concluding thoughts</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixing.html"><a href="mixing.html"><i class="fa fa-check"></i><b>9</b> Integrated inferences</a><ul>
<li class="chapter" data-level="9.1" data-path="mixing.html"><a href="mixing.html#theres-only-ever-one-case"><i class="fa fa-check"></i><b>9.1</b> There’s only ever one case</a></li>
<li class="chapter" data-level="9.2" data-path="mixing.html"><a href="mixing.html#general-procedure"><i class="fa fa-check"></i><b>9.2</b> General procedure</a><ul>
<li class="chapter" data-level="9.2.1" data-path="mixing.html"><a href="mixing.html#estimation"><i class="fa fa-check"></i><b>9.2.1</b> Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="mixing.html"><a href="mixing.html#illustration"><i class="fa fa-check"></i><b>9.3</b> Illustration</a></li>
<li class="chapter" data-level="9.4" data-path="mixing.html"><a href="mixing.html#illustrated-inferences"><i class="fa fa-check"></i><b>9.4</b> Illustrated inferences</a><ul>
<li class="chapter" data-level="9.4.1" data-path="mixing.html"><a href="mixing.html#xy-model"><i class="fa fa-check"></i><b>9.4.1</b> XY model</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="mixing.html"><a href="mixing.html#considerations"><i class="fa fa-check"></i><b>9.5</b> Considerations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="mixing.html"><a href="mixing.html#the-identification-problem"><i class="fa fa-check"></i><b>9.5.1</b> The identification problem</a></li>
<li class="chapter" data-level="9.5.2" data-path="mixing.html"><a href="mixing.html#continuous-data"><i class="fa fa-check"></i><b>9.5.2</b> Continuous data</a></li>
<li class="chapter" data-level="9.5.3" data-path="mixing.html"><a href="mixing.html#measurement-error"><i class="fa fa-check"></i><b>9.5.3</b> Measurement error</a></li>
<li class="chapter" data-level="9.5.4" data-path="mixing.html"><a href="mixing.html#spillovers"><i class="fa fa-check"></i><b>9.5.4</b> Spillovers</a></li>
<li class="chapter" data-level="9.5.5" data-path="mixing.html"><a href="mixing.html#clustering-and-other-violations-of-independence"><i class="fa fa-check"></i><b>9.5.5</b> Clustering and other violations of independence</a></li>
<li class="chapter" data-level="9.5.6" data-path="mixing.html"><a href="mixing.html#parameteric-models"><i class="fa fa-check"></i><b>9.5.6</b> Parameteric models</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="mixing.html"><a href="mixing.html#rules-of-thumb-for-reasoning-about-learning"><i class="fa fa-check"></i><b>9.6</b> Rules of thumb for reasoning about learning</a></li>
<li class="chapter" data-level="9.7" data-path="mixing.html"><a href="mixing.html#conclusion-1"><i class="fa fa-check"></i><b>9.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mixingapp.html"><a href="mixingapp.html"><i class="fa fa-check"></i><b>10</b> Mixed-Method Application: Inequality and Democracy Revisited</a><ul>
<li class="chapter" data-level="10.1" data-path="mixingapp.html"><a href="mixingapp.html#a-trained-model"><i class="fa fa-check"></i><b>10.1</b> A trained model</a></li>
<li class="chapter" data-level="10.2" data-path="mixingapp.html"><a href="mixingapp.html#data"><i class="fa fa-check"></i><b>10.2</b> Data</a></li>
<li class="chapter" data-level="10.3" data-path="mixingapp.html"><a href="mixingapp.html#inference"><i class="fa fa-check"></i><b>10.3</b> Inference</a><ul>
<li class="chapter" data-level="10.3.1" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-cause-democracy"><i class="fa fa-check"></i><b>10.3.1</b> Did inequality <em>cause</em> democracy?</a></li>
<li class="chapter" data-level="10.3.2" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-prevent-democracy"><i class="fa fa-check"></i><b>10.3.2</b> Did inequality <em>prevent</em> democracy?</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="mixingapp.html"><a href="mixingapp.html#exercises"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mm.html"><a href="mm.html"><i class="fa fa-check"></i><b>11</b> Mixing models</a><ul>
<li class="chapter" data-level="11.1" data-path="mm.html"><a href="mm.html#a-jigsaw-puzzle-integrating-across-a-model"><i class="fa fa-check"></i><b>11.1</b> A jigsaw puzzle: Integrating across a model</a></li>
<li class="chapter" data-level="11.2" data-path="mm.html"><a href="mm.html#combining-observational-and-experimental-data"><i class="fa fa-check"></i><b>11.2</b> Combining observational and experimental data</a></li>
<li class="chapter" data-level="11.3" data-path="mm.html"><a href="mm.html#transportation-of-findings-across-contexts"><i class="fa fa-check"></i><b>11.3</b> Transportation of findings across contexts</a></li>
<li class="chapter" data-level="11.4" data-path="mm.html"><a href="mm.html#multilevel-models-meta-analysis"><i class="fa fa-check"></i><b>11.4</b> Multilevel models, meta-analysis</a></li>
</ul></li>
<li class="part"><span><b>III Design Choices</b></span></li>
<li class="chapter" data-level="12" data-path="elements-of-design.html"><a href="elements-of-design.html"><i class="fa fa-check"></i><b>12</b> Elements of Design</a><ul>
<li class="chapter" data-level="12.1" data-path="elements-of-design.html"><a href="elements-of-design.html#model-inquiry-data-strategy-answer-strategy"><i class="fa fa-check"></i><b>12.1</b> Model, inquiry, data strategy, answer strategy</a><ul>
<li class="chapter" data-level="12.1.1" data-path="elements-of-design.html"><a href="elements-of-design.html#defining-a-model"><i class="fa fa-check"></i><b>12.1.1</b> Defining a model</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="elements-of-design.html"><a href="elements-of-design.html#evaluating-a-design"><i class="fa fa-check"></i><b>12.2</b> Evaluating a design</a><ul>
<li class="chapter" data-level="12.2.1" data-path="elements-of-design.html"><a href="elements-of-design.html#expected-error-and-expected-posterior-variance"><i class="fa fa-check"></i><b>12.2.1</b> Expected error and expected posterior variance</a></li>
<li class="chapter" data-level="12.2.2" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-1"><i class="fa fa-check"></i><b>12.2.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-of-design-decaration-in-code"><i class="fa fa-check"></i><b>12.3</b> Illustration of Design Decaration in code</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="clue.html"><a href="clue.html"><i class="fa fa-check"></i><b>13</b> Clue Selection as a Decision Problem</a><ul>
<li class="chapter" data-level="13.1" data-path="clue.html"><a href="clue.html#core-logic"><i class="fa fa-check"></i><b>13.1</b> Core logic</a></li>
<li class="chapter" data-level="13.2" data-path="clue.html"><a href="clue.html#a-strategic-approach"><i class="fa fa-check"></i><b>13.2</b> A strategic approach</a><ul>
<li class="chapter" data-level="13.2.1" data-path="clue.html"><a href="clue.html#clue-selection-with-a-simple-example"><i class="fa fa-check"></i><b>13.2.1</b> Clue selection with a simple example</a></li>
<li class="chapter" data-level="13.2.2" data-path="clue.html"><a href="clue.html#dependence-on-prior-beliefs"><i class="fa fa-check"></i><b>13.2.2</b> Dependence on prior beliefs</a></li>
<li class="chapter" data-level="13.2.3" data-path="clue.html"><a href="clue.html#clue-selection-for-the-democratization-model"><i class="fa fa-check"></i><b>13.2.3</b> Clue selection for the democratization model</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="clue.html"><a href="clue.html#dynamic-strategies"><i class="fa fa-check"></i><b>13.3</b> Dynamic Strategies</a></li>
<li class="chapter" data-level="13.4" data-path="clue.html"><a href="clue.html#conclusion-2"><i class="fa fa-check"></i><b>13.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="caseselection.html"><a href="caseselection.html"><i class="fa fa-check"></i><b>14</b> Mixed methods data strategies</a><ul>
<li class="chapter" data-level="14.1" data-path="caseselection.html"><a href="caseselection.html#case-selection-strategies"><i class="fa fa-check"></i><b>14.1</b> Case selection strategies</a><ul>
<li class="chapter" data-level="14.1.1" data-path="caseselection.html"><a href="caseselection.html#no-general-rules"><i class="fa fa-check"></i><b>14.1.1</b> No general rules</a></li>
<li class="chapter" data-level="14.1.2" data-path="caseselection.html"><a href="caseselection.html#specific-case-walk-through"><i class="fa fa-check"></i><b>14.1.2</b> Specific case walk through</a></li>
<li class="chapter" data-level="14.1.3" data-path="caseselection.html"><a href="caseselection.html#case-selection-from-causal-models-a-simulation-based-approach"><i class="fa fa-check"></i><b>14.1.3</b> Case selection from causal models: a simulation-based approach</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="caseselection.html"><a href="caseselection.html#wide-or-deep"><i class="fa fa-check"></i><b>14.2</b> Wide or Deep</a><ul>
<li class="chapter" data-level="14.2.1" data-path="caseselection.html"><a href="caseselection.html#walk-through-of-a-simple-comparison"><i class="fa fa-check"></i><b>14.2.1</b> Walk-through of a simple comparison</a></li>
<li class="chapter" data-level="14.2.2" data-path="caseselection.html"><a href="caseselection.html#results-from-simulations"><i class="fa fa-check"></i><b>14.2.2</b> Results from simulations</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="caseselection.html"><a href="caseselection.html#principles"><i class="fa fa-check"></i><b>14.3</b> Principles</a></li>
</ul></li>
<li class="part"><span><b>IV Models in Question</b></span></li>
<li class="chapter" data-level="15" data-path="justifying.html"><a href="justifying.html"><i class="fa fa-check"></i><b>15</b> Justifying models</a><ul>
<li class="chapter" data-level="15.1" data-path="justifying.html"><a href="justifying.html#nothing-from-nothing"><i class="fa fa-check"></i><b>15.1</b> Nothing from nothing</a></li>
<li class="chapter" data-level="15.2" data-path="justifying.html"><a href="justifying.html#justifying-the-classic-process-tracing-tests"><i class="fa fa-check"></i><b>15.2</b> Justifying the classic process tracing tests</a></li>
<li class="chapter" data-level="15.3" data-path="justifying.html"><a href="justifying.html#justification-from-experimental-designs"><i class="fa fa-check"></i><b>15.3</b> Justification from experimental designs</a><ul>
<li class="chapter" data-level="15.3.1" data-path="justifying.html"><a href="justifying.html#mediator"><i class="fa fa-check"></i><b>15.3.1</b> Mediator</a></li>
<li class="chapter" data-level="15.3.2" data-path="justifying.html"><a href="justifying.html#moderator"><i class="fa fa-check"></i><b>15.3.2</b> Moderator</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="justifying.html"><a href="justifying.html#causal-discovery"><i class="fa fa-check"></i><b>15.4</b> Causal discovery</a></li>
<li class="chapter" data-level="15.5" data-path="justifying.html"><a href="justifying.html#exercise"><i class="fa fa-check"></i><b>15.5</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="evaluation.html"><a href="evaluation.html"><i class="fa fa-check"></i><b>16</b> Evaluating models</a><ul>
<li class="chapter" data-level="16.1" data-path="evaluation.html"><a href="evaluation.html#five-strategies"><i class="fa fa-check"></i><b>16.1</b> Five Strategies</a><ul>
<li class="chapter" data-level="16.1.1" data-path="evaluation.html"><a href="evaluation.html#check-conditional-independence"><i class="fa fa-check"></i><b>16.1.1</b> Check conditional independence</a></li>
<li class="chapter" data-level="16.1.2" data-path="evaluation.html"><a href="evaluation.html#computational-clues"><i class="fa fa-check"></i><b>16.1.2</b> Computational clues</a></li>
<li class="chapter" data-level="16.1.3" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value-are-the-data-unexpected-given-your-model"><i class="fa fa-check"></i><b>16.1.3</b> Bayesian <span class="math inline">\(p\)</span> value: Are the data unexpected given your model?</a></li>
<li class="chapter" data-level="16.1.4" data-path="evaluation.html"><a href="evaluation.html#leave-one-out-loo-cross-validation"><i class="fa fa-check"></i><b>16.1.4</b> Leave-one-out (LOO) cross-validation</a></li>
<li class="chapter" data-level="16.1.5" data-path="evaluation.html"><a href="evaluation.html#sensitivity"><i class="fa fa-check"></i><b>16.1.5</b> Sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="evaluation.html"><a href="evaluation.html#evaluating-the-democracy-inequality-model"><i class="fa fa-check"></i><b>16.2</b> Evaluating the Democracy-Inequality model</a><ul>
<li class="chapter" data-level="16.2.1" data-path="evaluation.html"><a href="evaluation.html#check-assumptions-of-conditional-independence"><i class="fa fa-check"></i><b>16.2.1</b> Check assumptions of conditional independence</a></li>
<li class="chapter" data-level="16.2.2" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value"><i class="fa fa-check"></i><b>16.2.2</b> Bayesian <span class="math inline">\(p\)</span>-value</a></li>
<li class="chapter" data-level="16.2.3" data-path="evaluation.html"><a href="evaluation.html#loo-validation"><i class="fa fa-check"></i><b>16.2.3</b> LOO validation</a></li>
<li class="chapter" data-level="16.2.4" data-path="evaluation.html"><a href="evaluation.html#sensitivity-to-priors"><i class="fa fa-check"></i><b>16.2.4</b> Sensitivity to priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>17</b> Final Words</a><ul>
<li class="chapter" data-level="17.1" data-path="final-words.html"><a href="final-words.html#the-benefits"><i class="fa fa-check"></i><b>17.1</b> The benefits</a></li>
<li class="chapter" data-level="17.2" data-path="final-words.html"><a href="final-words.html#the-worries"><i class="fa fa-check"></i><b>17.2</b> The worries</a></li>
<li class="chapter" data-level="17.3" data-path="final-words.html"><a href="final-words.html#the-future"><i class="fa fa-check"></i><b>17.3</b> The future</a><ul>
<li class="chapter" data-level="17.3.1" data-path="final-words.html"><a href="final-words.html#the-goal-a-compendium-of-models"><i class="fa fa-check"></i><b>17.3.1</b> The goal: A compendium of models</a></li>
<li class="chapter" data-level="17.3.2" data-path="final-words.html"><a href="final-words.html#the-properties-of-models"><i class="fa fa-check"></i><b>17.3.2</b> The properties of models</a></li>
<li class="chapter" data-level="17.3.3" data-path="final-words.html"><a href="final-words.html#procedures-to-get-there"><i class="fa fa-check"></i><b>17.3.3</b> Procedures to get there</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li class="chapter" data-level="18" data-path="examplesappendix.html"><a href="examplesappendix.html"><i class="fa fa-check"></i><b>18</b> <code>CausalQueries</code></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/macartan/gbiqq/" target="blank">Uses gbiqq</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Integrated Inferences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="final-words" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> Final Words</h1>
<p>The central idea of this book is that we can usefully learn about the world by combining new evidence with prior causal models to produce updated models of how the world works. When asking specific questions—such as whether this caused that or whether one or other channel is important—we look up answers in our updated model of causal processes rather than seeking to the question directly from data.</p>
<!-- many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account for how and under what conditions causal relations play out.  -->
<p>This way of thinking about learning is very different to many standard approaches in the social sciences. It promises benefits, but it also comes with risks. We try to describe both in this closing chapter.</p>
<p>The approach stands in particularly stark contrast to the design based approach to causal inference, which has gained prominence in recent years. Design based approaches have shown that it is possible to greatly diminish the role of background assumptions for some research questions and contexts. This is a remarkable achievement that has put the testing of some hypotheses and estimation of some causal quantities on a firmer footing. It allows researchers to maintain agnostic positions and base their inferences more solidly on what they know to be true—such as how units were sampled and how treatments were assigned—and less on speculations about background data generating processes. Nothing here argues against these strengths.</p>
<p>At the same time, there are limits to model-free social science that affect the kinds of questions we can ask and the conditions we need to be in place to able to generate an answer. Most simply we often don;t understand the design very well, we cannot control the processes we want to study, or the quantities we care can not be identified with design based inferential tools.<br />
<!-- , . In particular, design-based inference relies on (as-good-as) random assignment by the researcher or by nature, placing bounds on the kinds of causes and contexts we can investigate. The approach is also generally limited to estimating a single causal quantity: the average causal effect. --></p>
<p>Building on pioneering work by scholars in computer science, statistics, and philosophy, we have outlined a principled approach to mobilizing prior knowledge to learn from new data in situations where randomization is unavailable and to answer questions for which randomization is unhelpful. In this approach, causal models are <em>guides</em> to research design, <em>machines</em> for inference, and <em>objects</em> of inquiry. As guides, the models yield expectations about the learning that can be derived from a given case or set of cases and from a given type of evidence, conditional on the question being asked. As inferential machines, models allow updating on that query once the data are in hand. Finally, when we confront a model with data, we learn about the parameters of the model itself, which can be used to answer a range of other causal questions and allowing cumulation of knowledge across studies. To complement the conceptual infrastructure we have provided software tools that let researchers build, update, and query binary causal models.</p>
<div id="the-benefits" class="section level2">
<h2><span class="header-section-number">17.1</span> The benefits</h2>
<!-- Intellectual currents in empirical social science have been rushing against model-based inquiry for the last decade or more. Uneasiness with the assumptions required to draw causal inferences from observational regression estimates has prompted a flight to the epistemic safety of random assignment. With a minimal set of assumptions, a randomized experiment allows for an unbiased estimate of an important quantity, the average treatment effect. Randomized experiments offer answers that are easy to defend. Model-based analysis offers answers that will always be model-dependent. To the extent that what we our after is confidence in our conclusions, the move from models to strong designs represents unquestionable progress. -->
<!-- What we hope that readers will take from this book, however, is that it is a fallacy to think of explicitly model-based inquiry as a second-best alternative to model-free experimentation. It is  a different way of learning about the world altogether. Whether our data are generated from an experiment or arise observationally, organizing inquiry around a causal model has a number of distinct advantages: -->
<p>Strategies centered on building, updating, and querying causal models come with a set of striking advantages.</p>
<p><strong>Many questions.</strong> When we update a causal model, we do not estimate a single causal quantity of interest: we learn about <em>the model</em>. Most concretely, when we encounter new data, we update our beliefs about <em>all</em> parameters in the model at the same time. We can then use the updated parameters to answer very broad classes of causal questions, well beyond the population-level average effect. These include case-level questions (<em>Does <span class="math inline">\(X\)</span> explain <span class="math inline">\(Y\)</span> in this case?</em>), process questions (<em>Through which channel does <span class="math inline">\(X\)</span> affect <span class="math inline">\(Y\)</span>?</em>), and transportability questions (<em>What are the implications of results derived in one context for processes and effects in other contexts?</em>).</p>
<p><strong>Common answer strategy.</strong> Strikingly, these diverse types of questions are all asked and answered in this approach using the same procedure: forming, updating, and querying a causal model. Likewise, once we update a model given a set of data, we can then pose the full range of causal queries to the updated model. In this respect, the causal models approach differs markedly from common statistical frameworks in which distinct estimators are constructed to estimate particular estimands.</p>
<p><strong>Answers without identification.</strong> The approach can be used to generate answers even when queries are not <em>identified.</em> The ability to “identify” causal effects has been a central pursuit of much social science research in recent years. But identification is in some ways a curious goal. A causal quantity is identified, if, with infinite data, the correct value can be ascertained with certainty—informally, the distribution that will emerge is consistent with only one parameter value. Oddly, however knowing that a model, or quantity, is identified in this way does not tell you that estimation with finite data is any good <span class="citation">(Maclaren and Nicholson <a href="#ref-maclaren2019can">2019</a>)</span>. Nor is estimation of a non-identified model with finite data necessarily bad. While there is a tendency to discount models for which quantities of interest are not identified, in fact it is relatively easy to see that conisderable learning is possible even without identification, using the same procedure of updating and querying models. Updating non-identified models can lead to a tightening of posteriors, even if some quantities can never be distinguished from each other.</p>
<!-- Using causal models also provides a clear *procedure* for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw.  -->
<p><strong>Integration</strong> Embedding inference within an explicit causal model brings about an integration across forms of data and beliefs that may otherwise develop in isolation from one another. For one thing, the approach allows us to combine arbitrary mixes of forms of evidence, including data on causes and outcomes and evidence on causal processes (whether from the same or different sets of cases). Further, the causal-model approach ensures that our findings about <em>cases</em> (given evidence about those cases) are informed by what we know about the <em>population</em> to which those cases belong, and vice versa. And, as we discuss further below, approach generates integration between inputs and outputs into the inferential process: it ensures that the way in which we update from the data is logically consistent with our prior beliefs about the world.</p>
<p><strong>A framework for knowledge cumulation.</strong> Closely related to integration is cumulation: a causal-model framework provides a ready-made apparatus for combining information across studies. Thinking in meta-analytic terms, the framework provides a mechanism for combining the evidence from multiple, independent studies. Thinking sequentially, the model updated from one set of data can become the starting point for the next study of the same causal domain.</p>
<p>Yet organizing inquiry around a causal model allows for cumulation in a deeper sense as well. Compared with most prevailing approaches to observational inference—where the background model is typically left implicit or conveyed informally or incompletely—the approach ensures <em>transparency</em> about the beliefs on which inferences rest. Explicitness about starting assumptions allows us to assess the degree of sensitivity of conclusions to our prior beliefs. Sensitivity analyses cannot, of course, tell us which beliefs are right. But they can tell us which assumptions are most in need of defending, pinpointing <em>where more learning would be of greatest value.</em> Those features of our model about which we are most uncertain and that matter most to our conclusions — be it the absence of an arrow, a restriction, a prior over nodal types, the absence of confounding — represent the questions most in need of answers down the road.</p>
<p><strong>A framework for learning about strategies.</strong> As we showed in chapters <a href="clue.html#clue">13</a> and <a href="caseselection.html#caseselection">14</a> access to a model provides an explicit formulation of how and what inferences will be drawn from future data patterns provides a formal framework for justifying design decisions. Of course this feature is not unique to model based inference—one can certainly have a model that describes expectations over future data patterns and imagine what inferences you will make using design based inference or any otehr procedure.</p>
<!-- **Limits of qualitative data under ignorable assignments.** A key payoff deploying causal models is the prospect of combining in-depth observations of a small number of cases with less-intensive investigation of a larger number of cases. Yet one of the lessons of the foregoing analysis is that the gains to such mixing may be limited. For instance, where the causal effect of $X$ on $Y$ can be identified (assignment of $X$ is as-good-as random), one will learn little about this effect from process observations in a small number of cases. Put differently, case studies simply cannot add much to experimental estimates of the ATE, though there may be other queries of interest on which small-$N$ process evidence can be informative. [To be expanded] -->
<!-- While we have outlined a set of strategies for validating and selecting our models, the model-contingency of conclusions is an important and inescapable limitation of the framework.  -->
<p><strong>Conceptual clarifications.</strong> Finally, we have found, this framework has been useful for providing conceptual clarification for how to think about qualitative, quantitative, and mixed-method inference. Consider two.</p>
<p>The first is with respect to the difference between “within-case” and “between-case” inference. In <span class="citation">Humphreys and Jacobs (<a href="#ref-humphreys2015mixing">2015</a>)</span>, for instance, we drew on a common operationalization of “quantitative” and “qualitative” data as akin to “dataset” and “causal process” observations, respectively, as defined by <span class="citation">Collier, Brady, and Seawright (<a href="#ref-collier2010sources">2010</a>)</span> ( see also <span class="citation">Mahoney (<a href="#ref-mahoney2000strategies">2000</a>)</span>). In a typical mixed-method setup, we might think of combining a “quantitative” dataset as containing <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (and covariate) observations for many cases with “qualitative” observations on causal processes, such as a mediator <span class="math inline">\(M\)</span>, for a subset of these cases. But this apparent distinction has no meaning in the formal setup and analysis of models. There is no need to think of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> observations as being tied to a large-<span class="math inline">\(N\)</span> analysis or to observations of mediating or other processes as being tied to small-<span class="math inline">\(N\)</span> analysis. One could, for instance, have data on <span class="math inline">\(M\)</span> for a large set of cases but data on <span class="math inline">\(Y\)</span> or <span class="math inline">\(X\)</span> for only a small number. Updating the model to learn about the causal query of interest will proceed in the same basic manner. The cross-case/within-case dichotomy plays no role in the way inferences are drawn: given any pattern of data we observe in the cases at hand, we are always assessing the likelihood of that data pattern under different values of the model’s parameters. In this framework, what we have conventionally thought of as qualitative and quantitative inference strategies are not just integrated; the distinction between them breaks down completely.</p>
<p>A second is with regards to the relationship between beliefs about queries and beliefs about the informativeness of evidence. In many accouns of process tracing, researchers posit a set of prior beliefs about the values of estimands and other—independent—beliefs about the informativeness of within-case information (see also <span class="citation">Fairfield and Charman (<a href="#ref-FairfieldBayes2015">2017</a>)</span>, <span class="citation">Bennett (<a href="#ref-BennettAppendix">2015</a>)</span>). Viewd theough a cusal models lens however <em>both</em> sets of beliefs — about the hypothesis being examined and about the probative value of the data — represent substantive probabilistic claims about the world, and in particular about <em>causal relationships</em> in the domain under investigation. They, thus, cannot not be treated as generally independent of one another: our beliefs about causal relations <em>imply</em> our beliefs about the probative value of the evidence. These implications flow naturally in a causal-model framework. When both sets of beliefs are themselves derived from an underlying model representing prior knowledge about the domain of interest, then the same conjectures that inform our beliefs about the hypotheses also inform our beliefs about the informativeness of additional data. Seen in this way the researcher is under pressure to provide reasons to support beliefs about probative value, but more constructively, they have availabel to them a strategy to do so.</p>
<!-- **Pretending to have priors.** As we explored the world of causal models, we started out thinking that, in providing priors over causal relations, one is directly stating beliefs about how the world works. For instance, one might believe that either $X$ caused $Y$ or that it did not; and one might believe either that $M=1$ should be observed in the event that $X$ caused $Y$ or it should not be. These statements are, in fact, clearly model-dependent. Beyond the model required to describe events in such crisp terms, the statements involve counterfactuals on counterfactuals---models of causal processes. Once a model involves assertions of conditional independence, we are clearly in the business of dealing in simplifications. Our priors become less statements of how we believe the world works and become statements about what set of models are least bad within a class of tractable abstractions. -->
<!-- AJ: There's a clear tension between the above point and the ways in which we talk about models as reflecting "prior beliefs" or "prior knowledge" about the world. Need to resolve this. -->
</div>
<div id="the-worries" class="section level2">
<h2><span class="header-section-number">17.2</span> The worries</h2>
<p>While we have found the syntax of Directed Acyclic Graphs to provide a flexible framework for setting up causal models, we have also become more keenly aware of some of the limitations of DAGs in representing causal processes. We discuss a few of these here.</p>
<p><strong>Well defined nodes?</strong> A DAG presupposes a set of well-defined nodes that come with location and time stamps. Income in time <span class="math inline">\(t\)</span> affects democracy in time <span class="math inline">\(t+1\)</span> which affects income in time <span class="math inline">\(t\)</span>. Yet it is not always easy to figure out how to partition the world into such near event bundles. Income in 1985 is not an “event” exactly but a state, and the temporal ordering with respect to “Democracy 1985” is none too clear. Moreover, even if events are coded into well ordered nodes values on these nodes may poorly capture actual processes, even in simple systems. Consider the simplest set up with a line of dominos. You are interested in whether the fall of the first domino cases the fall of the last one. But the observations of the states of the dominos do not fully capture the causal process even in this simplest of systems. The data might report that (a) domino 1 fell and (b) domino 2 fell. But the observer will notice that domino 2 fell <em>just as</em> domino 1 hit it.</p>
<!-- AJ: I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->
<p><strong>Acyclic, really?</strong> DAGs are by definition acyclic. And it is not hard to argue that, since cause precedes effect, causal relations <em>should</em> be acyclic for any well-defined nodes. In practice, however, our variables often come with coarse periodizations: there was or was not mobilization in the 1990s; there was or was not democratization in the 1990s. We cannot extract the direction of arrows from the definition of nodes this coarse.</p>
<!-- AJ: Here too, I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->
<p><strong>Coherent underlying causal accounts.</strong> The approach we describe is one in which researchers are asked to provide a coherent model—albeit with uncertainty—regarding the ways in which nodes are causally related to each other. For instance, a researcher interested in using information on <span class="math inline">\(K\)</span> to ascertain whether <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> is expected to have a theory of whether <span class="math inline">\(K\)</span> acts as a moderator or a mediator for <span class="math inline">\(X\)</span>, and whether it is realized before or after <span class="math inline">\(Y\)</span>. Yet it is possible that a researcher has well formed beliefs about the informativeness of <span class="math inline">\(K\)</span> <em>without</em> an underlying model of how <span class="math inline">\(K\)</span> is causally related to <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. Granted, one might wonder where these beliefs come from or how they can be defended. We nonetheless note that one limitation of the approach we have described is that one cannot make use of an observation without a coherent account of that observation’s causal position relative to other variables and relationships of interest.</p>
<p><strong>Complexity.</strong> To maintain simplicity, we have largely focused in this book on models with binary nodes. At first blush, this class of causal models indeed appears very simple. Yet even with binary nodes, complexity rises rapidly as the number of nodes and connections among them increases. As a node goes from having 1 parent to 2 parents to 3 parents to 4 parents, for instance, the number of nodal types — at that node alone — goes from 4 to 16 to 256 to 65,536, with knock-on effects for the number of possible causal types (combinations of nodal types across the model). A move in the direction of continuous variables — say, from binary nodes to nodes with 3 ordinal values — would also involve a dramatic increase in the complexity to the type-space.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> There are practical and substantive implications of this. A practical implications is that one can hit computational constraints very quickly for even moderately sized models. Substantively, models can quickly involve more complexity than humans can comfortably understand.</p>
<p>One solution is to move away from a fully non-parametric setting and impose structure on permissible function forms—for example by imposiing motonicity or no high level interactions.</p>
<p>A second approach might be to give up on the commitment to a complete specification of causal relations and seek lower dimensional representations of models that are sufficient for questions we care about. For instance, for an <span class="math inline">\(X \rightarrow Y\)</span> model, if we are interested in understanding the probability that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in a <span class="math inline">\(X=Y=1\)</span> case we need to learn about <span class="math inline">\(\theta^Y_{01}\)</span> and <span class="math inline">\(\theta^Y_{11}\)</span>. But if we care only about the average effect of <span class="math inline">\(X\)</span> then we need to learn only about <span class="math inline">\(\theta^Y_{10}-\theta^Y_{01}\)</span>. In the latter case rather we might work with just two parameters: define <span class="math inline">\(\tau := \theta^Y_{10}-\theta^Y_{01}\)</span> and <span class="math inline">\(\rho := \theta^Y_{11}-\theta^Y_{00}\)</span> then in a case with <span class="math inline">\(X=1\)</span>, the probability that <span class="math inline">\(Y=1\)</span> is <span class="math inline">\(\frac{1+\tau+\rho}{2}\)</span>. Observations of <span class="math inline">\(Y\)</span> then let us update on <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\rho\)</span> without having to specify all the nodal types.</p>
<p><strong>Unintended structure.</strong> The complexity of causal models means that it is easy to generate a fully specified causal model with features that you do not fully udnerstand. In the same way it is possible to make choices between models unaware of differences in assuptions that they have built in.</p>
<p>Consider three examples:</p>
<ul>
<li>You specify a model <span class="math inline">\(X \rightarrow Y\)</span> and assume flat priors over nodal types. The implied prior that <span class="math inline">\(X\)</span> has a positive effect on <span class="math inline">\(Y\)</span> is then 0.25. You add detail by specifying <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> but continue to hold flat priors. In your more detailed model however the probability of a positive effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is just 0.125. Adding the detail requires either moving from flat priors on nodal types of changing priors on aggregate causal relations.</li>
</ul>
<!-- * You specify a model with $X\rightarrow Y$ and update given some data on $X$ and $Y$. You then add detail and specify $X \rightarrow M \rightarrow Y$ with teh same prior beliefs over how $X$ affects $Y$. You update with teh same data but come to different results. -->
<ul>
<li>You specify a model <span class="math inline">\(X \rightarrow Y \leftarrow W\)</span> and build in that <span class="math inline">\(W\)</span> is a smoking gun for the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. You add detail by specifying <span class="math inline">\(X \rightarrow M \rightarrow Y \leftarrow W\)</span>. This means however that <span class="math inline">\(W\)</span> cannot be a smokung gun for <span class="math inline">\(Y\)</span> unless teh <span class="math inline">\(X \rightarrow M\)</span> relation is certain. Why? To be a smoking gun it must be that if <span class="math inline">\(W=1\)</span> you are sure that <span class="math inline">\(X\)</span> causes <span class="math inline">\(M\)</span> and that <span class="math inline">\(M\)</span> causes <span class="math inline">\(Y\)</span> which requires an arrow from <span class="math inline">\(W\)</span> to <span class="math inline">\(M\)</span> and not just from <span class="math inline">\(W\)</span> to <span class="math inline">\(X\)</span>.</li>
</ul>
<p><strong>Model-dependence of conclusions</strong> One striking finding of some of the analyses presented here is see how sensitive conclusions can be to what would seem to be quite modest changes to models. We see two ways of thinking about the implications of this fact for a causal-models framework.</p>
<p>One lesson to draw would be that there are tight limits to building inference upon causal models. If results in this approach depend heavily on prior beliefs, which could be wrong, then we might doubt the utility of the framework. Perhaps all that we have done in this book is to make the case for pure design-based inference.</p>
<p>An alternative lesson also offers itself, however. To the extent that our inferences depend on our background causal beliefs, a transparent and systematic engagement with models becomes all the more important. If inferences are not built explicitly on models, we have no way of knowing how fragile they are, how they would change under an alternative set of premises, or what kind of learning we need to undertake if we want to generate more secure conclusions.</p>
<p>We do not see causal models as the only way forward or as a panacea, and we are conscious of the limitations and complexities of the approach we have outlined, as well as the need for extension and elaboration along numerous fronts. Yet we think there is value in further development of forms of empirical social science that can operate with analytic transparency outside the safe inferential confines of random assignment.</p>
</div>
<div id="the-future" class="section level2">
<h2><span class="header-section-number">17.3</span> The future</h2>
<ul>
<li>What do we want: to be able to know how to update on a set of general questions as new evidence comes in. New info comes in, new event occurs, we want to be able to add that to the store of knowledge in a way that is integrated with what we previously new and updates over general principles. And go the other way: have our inferences on an individual case be informed by what we know generally. (Make clear this is not a hegemonic project.)</li>
<li>Framework for knowledge integration and cumulation</li>
<li>Coordination required</li>
<li>Some challenges of coordinating
<ul>
<li>Enough overlapping nodes?</li>
<li>Same ways of measuring?</li>
</ul></li>
<li>Coordination could take a variety of forms
<ul>
<li>Could just nail down the nodes and how they’re defined. Nothing on priors or arrows. Anyone can make their own model from those nodes — and update using all the available data. Would need to take into account obs vs. exp.</li>
<li>Common queries — everyone comes up with their own model and they compete for predictive performance.</li>
</ul></li>
<li>Qualities we’d want models to have
<ul>
<li>Well-defined</li>
<li>Scope conditions: what’s implicit?</li>
<li>Can talk to other models — interoperable, similar nodes</li>
<li>Would want models to be justified in some way
<ul>
<li>Grounded in theory</li>
<li>Consistent with past data</li>
<li>Reflecting beliefs of community of expert</li>
</ul></li>
</ul></li>
<li>I will give you: Integrated inferences</li>
</ul>
<p>The cumulation of knowledge requires integration. As we acquire new evidence – perhaps from observation of additional cases or new events – we want to be able to update our general beliefs about how the world works by integrating new information with the existing store of knowledge. At the same time, we want our inferences about individual cases to be informed by our beliefs about how the world works in general.</p>
<p>Causal models provide a natural framework for cumulating knowledge on these terms. We have spelled out in this book how the individual researcher can use models to join up new data with prior beliefs and ground case-level inferences in beliefs about population-level parameters. In a scientific discipline, however, cumulation must operate <em>across</em> researchers. For a field to make progress, I need to update <em>my</em> beliefs in light of <em>your</em> new evidence and vice versa.</p>
<p>The challenge is one of fragmentation. Not only do I need access to your evidence, but we have to be operating to some degree with a common causal model of the domain of interest; otherwise, the data that you generate might fail to map onto my model. Moreover, if we are all updating our own separate models, the prospects are dim for agreement about what has been learned. There is also little reason to be optimistic that individual researchers will naturally tend to generate models that align with one another. Not only might our substantive causal beliefs diverge, but we are likely to make differing choices about matters of model-construction — from which nodes to include and the appropriate level of detail to the manner of operationalizing variables.</p>
<p>Turning causal models into vehicles for knowledge cumulation in a field will thus require coordination around models. We are under no illusion that such coordination would be easy. But productive coordination would not require prior agreement about how the world works. It is unlikely that experts in any social scientific domain would converge on a single model representing shared beliefs. Rather, the most likely way forward would involve coordination around a <em>set</em> of models.</p>
<p>One possibility would be to fix (provisionally, at least) the set of nodes relevant in a given domain — including outcomes of interest, potential causes, and mediators and moderators implicated in prevailing theories — and how those nodes are defined. Individual researchers would then be free to develop their own models by draw arrows and setting priors and restrictions in line with their beliefs. Coordination around model nodes would then guide data-collection, as teams running new studies would seek to collect data on at least some subset of the common nodes — allowing, in turn, for all models to be updated as the new data come in.</p>
<p>Another possibility would be to allow for modularity, with researchers modeling different <em>parts</em> of a causal system. For instance, in the field of democratization, one set of researchers might model and collect data on links between inequality on democratization; others might focus on the role of external pressures; while still others might focus on inter-elite bargaining. Coordination would operate at the level of inter-operability. Modules would have to have at least some overlapping nodes for updating from new data to operate across them. Ideally, each module would also take into account any confounding among its nodes that is implied by other modules.</p>
<p>A further, more minimalist mode of coordination would be to allow researchers to develop models that agree only on the inclusion of one or more outcome nodes. As new data comes in, models would then be set in competition over predictive performance.</p>
<p>Coordination would also require agreement on some minimal set of qualities that included models must have. For instance, we would surely want all models to be well defined, following the basic rules of DAG-construction and with clear rules for operationalizing all nodes.</p>
<p>Integration could also be greatly facilitated to the extent that researchers explicitly specify any scope conditions that apply to their model. Researchers are likely to create models with particular contexts or domains of applicability in mind. Specifying those conditions can allow for the assembling of multiple context-specific models into an overarching model in which scope conditions operate as moderators.</p>
<hr />
<p>We began this intellectual journey developing an approach to Bayesian mixed-method analysis in which the researcher stipulates priors beliefs about estimands and the probative value of new evidence, and selection into assignment. Over time we became increasingly dissatisfied with the fact that this approach lacked foundations. These translate into the inputs you need to apply Bayes rule and you can apply Bayes rule without any justification for any of these inputs. Where do prior beliefs about estimands, probative value, and assignment come from? On what basis should readers accept your outputs if you cannot justify your inputs?</p>
<p>This book represents our answer to that problem: a causal model allows the researcher to write down a set of more foundational causal beliefs, from which priors, probative value, and selection effects can be derived. Doing so forces a coherence that is otherwise not required when you turn to Bayes: from the same model you can derive both the prior probability that <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> and that <span class="math inline">\(M\)</span> is informative for <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> and for the probability that <span class="math inline">\(X=1\)</span> depending on whether or not <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>. This model may itself be justified by reference to a more fundamental model that has been updated with prior data or to a model that reflects the beliefs of a community of research consumers.</p>
<p>Yet this answer raises its own question about foundations: where do our foundational causal models come from? While we have provided some guidance on how to construct models and evaluate them, we have largely bracketed the question of how researchers might form, extract, or elicit the substantive causal beliefs that inform models. We see the development of transparent, systematic approaches to model construction as an important next step in the elaboration of the framework.</p>
<div id="the-goal-a-compendium-of-models" class="section level3">
<h3><span class="header-section-number">17.3.1</span> The goal: A compendium of models</h3>
<ul>
<li>Shared models</li>
<li>Models we can defend</li>
<li>Models we can reject</li>
<li>We know what we learn when we learn something new</li>
</ul>
</div>
<div id="the-properties-of-models" class="section level3">
<h3><span class="header-section-number">17.3.2</span> The properties of models</h3>
<p>One could imagine multiple approaches to model-generation, and we seek here to outline a set of desiderata toward which any model-generating technique might plausibly be oriented.</p>
<p><strong>1. Theory based causal models.</strong></p>
<p>A good causal model will usually be interpretable by reference to a plausible theoretical logic. For instance, if our model allows for high inequality to cause mass mobilization, we might place a value on that feature of the model being underwritten by a set of defensible claims about the mechanism through which that effect might unfold.<br />
<!-- Conversely an incoherent model may be suspect. Consider the model: --> <!-- $$\text{Inequality} \rightarrow \text{Mobilization} \rightarrow \text{Democratization} \leftarrow \text{Price Inflation}$$ --> <!-- Is it consistent to allow inequality to have an effect on mobilization but exclude inflation from operating through mobilization? If economic grievances can provoke mass mobilization, as in inequality's effect, should we not also allow for rising food prices to have a similar effect?  --> A theoretical logic is not likely to be <em>sufficient</em> to justify a particular causal model, as there will generally be too many potential theoretical logics around a given domain to narrow down the possibilities sufficiently (especially taking the complexity desideratum, below, into account). Nor will a theoretical logic be strictly necessary for a useful causal model. One could imagine allowing for an effect on <span class="math inline">\(X\)</span> on <span class="math inline">\(M\)</span> in a model based on empirical – perhaps experimental – evidence of such an effect, absent a theorization of the mechanism. Nevertheless underlying theory can clarify what one has to be believe in order to buy the structure of a model.</p>
<p><strong>Would be nice to have an example where theory suggests that X woud be random or soemthing like that</strong></p>
<p><strong>2. History subsuming causal models.</strong></p>
<p>A second justification for causal models would be a maximally complete, accurate incorporation of the information available prior to analysis that speaks to causal relationships within the domain of interest. In general, this desideratum is likely to act as a conservative constraint on the strength of beliefs embedded within a model. For instance, one set of prior information might point toward an <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> chain of effects, yet this is a relatively restrictive model. Incorporating <em>more</em> prior information is unlikely to definitively discredit the possibility of this causal chain, but it might well yield evidence of pathways from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> that do not include <span class="math inline">\(M\)</span>. And if it does, then consistency with prior information would require us to add a direct link from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>, generating a less restrictive model. IS THIS POINT FRAMED RIGHT? OR SHOULD IT INSTEAD BE ABOUT NOT MAKING STRONGER CLAIMS THAN THE PRIOR INFO JUSTIFIES (WHEREAS THE NEXT POINT IS ABOUT NOT MAKING WEAKER CLAIMS THAN YOU CAN)?</p>
<p><strong>3. Leverage.</strong></p>
<p>As we have shown throughout this book, some models will provide more empirical leverage than others, in the sense that observations of their nodes will have greater probative value. Leverage typically derives from the strength of the prior beliefs embedded in a model. For instance, the causal model <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> will tend to provide strong opportunities for updating than the model <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> <em>if</em> we also have non-uniform beliefs over <span class="math inline">\(X\)</span>’s effects on <span class="math inline">\(M\)</span> and <span class="math inline">\(M\)</span>’s effects on <span class="math inline">\(Y\)</span>. One way to put this is that we don’t want to leave precision on the table. While we do not want our models to build in stronger beliefs than prior information can justify, we also would not want a model-generating procedure to yield models that are weaker than they could be.</p>
<p><strong>4. Tractable models.</strong></p>
<p>More complex models are harder to work with. As the number of a node’s parents increases, for instance, the number of nodal types increases steeply. It becomes accordingly harder for researchers to conceptualize and form meaningful priors over these nodal types. Moreover, the computational demands of operating even moderately complex models can be very high.</p>
<p><strong>5. Transparent.</strong></p>
<p>Ideally, we would want any model-generating procedure to make use of prior information via an explicit, transparent set of procedures. If one researcher derives a model, a second researcher should be able to see clearly how the model was generated and, in turn, to assess how the model might have changed if additional or different information had been incorporated.</p>
<p><strong>6. Reliability</strong> The procedure should ideally yield the same model, with the same restrictions and priors, when repeated and regardless of which researcher is implementing the procedure. COMBINE WITH TRANSPARENCY??</p>
<p><strong>7. Dynamic adaptability.</strong> Closely related to transparency is dynamic adaptibility. The base of “prior” information on any topic is constantly changing, and we would value a procedure that allows our models to adapt accordingly. It would be helpful if a procedure that generated a model at time <span class="math inline">\(t\)</span> could be incrementally updated in light of new information for use at time <span class="math inline">\(t+k\)</span>.</p>
<p>There will tend to be tradeoffs across these desiderata. For instance, tractability will sometimes be in tension with a full, accurate use of prior information. As we broaden the base of prior information, we are likely to add nodes as well as arrows (representing possible direct causal effects) between existing nodes. Likewise, taking more prior information into account may mean giving up leverage if, for instance, additional information makes us less certain about the prior weights over nodal types. Some desiderata will be complementary, however: models with more leverage will often include restrictions on nodal types, which will also reduce complexity. In any case, we expect that no single procedure will be able to maximize all desiderata at the same time.</p>
</div>
<div id="procedures-to-get-there" class="section level3">
<h3><span class="header-section-number">17.3.3</span> Procedures to get there</h3>
<p>Consider a few operational possibilities:</p>
<p><strong>Crowdsourcing</strong></p>
<p>One could imagine a platform that allowed researchers to gather and aggregate prior causal beliefs from a set of experts within a field of inquiry. Experts could be asked to weigh in on relationships among a fixed set of variables: which variables have direct effects on which? Experts could also be invited to add nodes, for instance, as mediators between the original set of nodes, as confounders, or as colliders. To generate restrictions and priors, experts could also be asked what kinds of effects (positive, negative, null) are relatively more and less likely. Instruments for eliciting such beliefs might vary widely, but one approach could be a conjoint design: for each node, respondents are presented, successively, with paired sets of assigned values for the node’s parents and then asked for which pair of parental values they expect the node itself to take on a higher value.</p>
<p>Crowdsourcing would likely perform well in achieving comprehensiveness of the prior information base insofar as experts will be, at least implicitly, drawing on a wide range of observations, findings in the literature, and theoretical insights. The procedure is transparent insofar as the elicitation methods, expert sampling frame, and expert responses can be shared; on the other hand, we would have little insight into what information experts themselves have drawn on in arriving at their beliefs. The reliability of a crowdsourced procedure would depend in part on how large the “crowd” is: how many experts can be polled on a given causal domain. A key challenge would be implementing a crowdsourcing in a way that yielded a clear grounding in internally consistent theoretical logics. It is also our hunch that crowdsourced models will tend to be more complex, given that they will likely represent a broader range of causal possibilities and a more diffuse set of beliefs over causal effects. Crowdsourcing could also be implemented adaptively, with some additional effort. One could imagine a platform that would allow new experts to register their views at any time or for “old” experts to update their responses as their beliefs change.</p>
<ul>
<li><p>Formal models: As we show in the appendix to Chapter <a href="theory.html#theory">4</a>, we can readily translate the implications of a game-theoretic model into a causal model. Building a causal model atop a formal model ensures clear theoretical grounding, internal consistency, transparency, and reliability (insofar as a single formal model implies a single causal model). Formal models that yield clear predictions are also likely to generate relatively strong beliefs about causal effects and, thus, a good deal of leverage. Complexity could be more or less arbitrarily determined by the modeler. A key major weakness of a formal-model-based approach would be the flip side of leverage: rather than incorporating a wide array of beliefs, a formal model would instantiate the implications of a fixed set of assumptions set out by the modeler. Moreover, there is no obvious mechanism for adapting a formal model to new information.</p></li>
<li><p>Data-driven approach: Perhaps the most transparent, reliable, adaptive, and conservative approach to building causal models is to rest them directly and maximally on prior data. We discuss in Chapter <a href="justifying.html#justifying">15</a> how one can use experimental data to justify models. At its most conservative, the data-driven approach might involve <em>only</em> building into a model beliefs that can be grounded in an explicit meta-analysis of experimental findings – yet we might also incorporate observational findings. Of course, the most natural way to implement a data-driven approach would be to start with a causal model with minimal structure and then use all available prior data to update it. And, as we show in Chapter @ref{mm}, with the right initial model we can readily update on a mixture of experimental and observational data. It would be extremely straightforward, moreover, to update a data-driven model as new findings come in.</p></li>
</ul>
<p>The limitations of a data-driven approach are, effectively, the flip side of its virtues. Conservatively grounded in what the data can tell us, a data-driven procedure will tend to yield models that have less structure, with higher complexity and less leverage. Involving minimal assumptions, data-derived models will also be unconstrained by theoretical logic or consistency.</p>
<p>Even a maximally data-driven approach cannot be assumption-free. One inescapable assumption is portability: we need to believe that the model that describes the context in which the prior data were generated also applies to the context in which new data are to be generated.</p>
<!-- Crodsourced models; -->
<!-- discpline wide agreement on models -->
<!-- building structures so research can contribute to models -->
<!-- connecting more thoroughly to formal theorey -->
<!-- logical coherence -->
<!-- modularization -->

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BennettAppendix">
<p>Bennett, Andrew. 2015. “Appendix.” In <em>Process Tracing: From Metaphor to Analytic Tool</em>, edited by Andrew Bennett and Jeffrey Checkel. New York: Cambridge University Press.</p>
</div>
<div id="ref-collier2010sources">
<p>Collier, David, Henry E Brady, and Jason Seawright. 2010. “Sources of Leverage in Causal Inference: Toward an Alternative View of Methodology.” In <em>Rethinking Social Inquiry: Diverse Tools, Shared Standards</em>, edited by David Collier and Henry E Brady, 161–99. Lanham MD: Rowman; Littlefield.</p>
</div>
<div id="ref-FairfieldBayes2015">
<p>Fairfield, Tasha, and Andrew Charman. 2017. “Explicit Bayesian Analysis for Process Tracing: Guidelines, Opportunities, and Caveats.” <em>Political Analysis</em> 25 (3). Cambridge University Press on behalf of the Society for Political Methodology: 363–80.</p>
</div>
<div id="ref-humphreys2015mixing">
<p>Humphreys, Macartan, and Alan M Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” <em>American Political Science Review</em> 109 (04). Cambridge Univ Press: 653–73.</p>
</div>
<div id="ref-maclaren2019can">
<p>Maclaren, Oliver J, and Ruanui Nicholson. 2019. “What Can Be Estimated? Identifiability, Estimability, Causal Inference and Ill-Posed Inverse Problems.” <em>arXiv Preprint arXiv:1904.02826</em>.</p>
</div>
<div id="ref-mahoney2000strategies">
<p>Mahoney, James. 2000. “Strategies of Causal Inference in Small-N Analysis.” <em>Sociological Methods &amp; Research</em> 28 (4): 387–424. doi:<a href="https://doi.org/10.1177/0049124100028004001">10.1177/0049124100028004001</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>If, for instance, we moved to nodes with 3 ordered categories, then each of <span class="math inline">\(Y\)</span>’s nodal types in an <span class="math inline">\(X \rightarrow Y\)</span> model would have to register 3 potential outcomes, corresponding to the 3 values that <span class="math inline">\(X\)</span> takes on. And <span class="math inline">\(Y\)</span> would have <span class="math inline">\(3 \times 3 \times 3 = 27\)</span> nodal types (as <span class="math inline">\(Y\)</span> can take on 3 possible values for each possible value of <span class="math inline">\(X\)</span>).<a href="final-words.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="evaluation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="examplesappendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["ii.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
