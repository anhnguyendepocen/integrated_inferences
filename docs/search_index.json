[
["index.html", "Integrated Inferences Preface", " Integrated Inferences Macartan Humphreys and Alan Jacobs Draft!: 2019-09-09 Preface Quick Guide This book has four main parts: Part I introduces causal models and a Bayesian approach to learning about them and drawing inferences from them. Part II applies these tools to strategies of learning from process tracing and mixed methods research, with applications. Part III turn to design decisions, exploring strategies for assessing what kind of data is most useful for addressing different kinds of research questions given knowledge to date about a population or a case. Everything up to Part IV assumes that we have access to models we are happy with. In Part IV we turn to the difficult question of model justification and outline a range of strategies on can use to justify causal models. There is also an accompanying R package currently hosted on github. This can be downloaded and installed as follows: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;macartan/gbiqq&quot;) An appendix shows how to use the package for defining and learning from a set of canonical causal models. "],
["intro.html", "Chapter 1 Introduction 1.1 The Case for Causal Models 1.2 Key contributions 1.3 The Road Ahead", " Chapter 1 Introduction We describe the book’s general approach, and explain how it differs from current approaches in the social sciences. We preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences from evidence. The engineer pressed the button, but the light didn’t turn on. “Maybe the bulb is blown,” she thought. She replaced the bulb, pressed the button and, sure enough, the light turned on. “What just happened?” asked her philosopher friend. “The light wouldn’t turn on because the bulb was busted, but I replaced the bulb and fixed the problem.” “Such hubris!” remarked her friend. “If I understand you, you are saying that pressing the button would have caused a change in the light if the bulb had not been busted.” “That’s right.” “But hold on a second. That’s a causal claim about counterfactual events in counterfactual conditions that you couldn’t have observed. I don’t know where to begin. For one thing, you seem to be inferring from the fact that the light did not go on when you pressed the button the first time that pressing the button the first time had no effect at all. What a remarkable conclusion. Did it never occur to you that the light might have about to turn on anyway—and that your pressing that button at just that moment is what stopped the light going on?” “What’s more,” the philosopher went on, “you seem also to be saying that pressing the button the second time did have an effect because you saw the light go on that second time. That’s rather incredible. That light could be controlled by a different circuit that was timed to turn it on at just the moment that you pressed the button the second time. Did you think about that possibility?” “On top of those two unsubstantiated causal claims,” the philosopher continued, “you are also saying, I think, that the difference between what you believe to be a non-cause on the first pressing and a cause on the second pressing is itself due to the bulb. But, of course, countless other things could have changed! Maybe there was a power outage for a few minutes.” “That hardly ever happens.” “Well, maybe the light only comes on the second time the button is pressed.” “It’s not that kind of button.” “So you say. But even if that’s true, there are still so many other possible factors that could have mattered here—including things that neither of us can even imagine!” The philosopher paused to ponder her friend’s chutzpah. “Come to think of it,” the philosopher went on, “how do you even know the bulb was busted?” “Because the light worked when I replaced the bulb.” “But that means,” the philosopher responded, “that your measurement of the state of the bulb depends on your causal inference about the effects of the button. And we know where that leads. Really, my friend, you are lost.” “So do you want me to put the old bulb back in?” 1.1 The Case for Causal Models In the conversation between the philosopher and the engineer, the philosopher disputes what seems a simple inference. Some of her arguments suggest a skepticism bordering on paranoia and seem easily dismissed. Others seem closer to hitting a mark: perhaps there was nothing wrong with the bulb and the button was just the kind that has to be pressed twice. For an objection like this, we have to rely on the engineer’s knowledge of how the button works. While the philosopher’s skepticism guards against false inferences, it is also potentially paralyzing. Social scientists have been shifting between the poles staked out by the philosopher and the engineer for many years. The engineers bring background knowledge to bear on a question and deploy models of broad processes to make inferences about particular cases. The philosophers bring a skeptical lense and ask for justifications that depend as little as possible on imported knowledge. This book is written for would-be engineers. It is a book about how we can mobilize our background knowledge about how the world works to learn more about the world. It is, more specifically, a study in how we can use causal models of the world to design and implement empirical strategies of causal inferences. But we will try to imagine throughout that the engineers have philosophers looking over their shoulders and will try to figure out ways to equip the engineers with plausible answers to the the philosophers’ worries. By causal models, we mean systems of statements reflecting background knowledge of and uncertainty about possible causal linkages in a domain of interest. There are three closely related motivations for our move to examine the important role that models can play in empirical social inquiry. One is an interest in integrating qualitative knowledge with quantitative approaches, and a view—possibly a controversial one—that process tracing is a model-dependent endeavor. A second is concern over the limits of design-based inference. A third and related motive is an interest in better connecting empirical strategies to theory. 1.1.1 The limits to design-based inference The engineer in our story tackles the problem of causal inference using models: theories of how the world works, generated from past experiences and applied to the situation at hand. The philosopher maintains a critical position, resisting models and the importation of beliefs not supported by evidence in the case at hand. The engineer’s approach recalls the dominant orientation among social scientists until rather recently. At the turn of the current century, multivariate regression had become a nearly indispensable tool of quantitative social science, with a large family of statistical models serving as political scientists’ and economists’ analytic workhorses for the estimation of causal effects. Over the last two decades, however, the philosophers have raised a set of compelling concerns about the assumption-laden nature of standard regression analysis, while also clarifying how valid inferences can be made with limited resort to models in certain research situations. The result has been a growth in the use of design-based inference techniques that, in principle, allow for model-free estimation of causal effects (see Dunning (2012), Gerber, Green, and Kaplan (2004), Druckman et al. (2011), Palfrey (2009) among others). These include lab, survey, and field experiments and natural-experimental methods exploiting either true or “as-if” randomization by nature. With the turn to experimental and natural-experimental methods has come a broader conceptual shift, with a growing reliance on the “potential outcomes” framework as a model for thinking about causation (see Rubin (1974), Splawa-Neyman et al. (1990) among others) and a reduced reliance on models of data-generating processes. The ability to estimate average effects and to calculate \\(p\\)-values and standard errors without resort to models is an extraordinary development. In Fisher’s terms, with these tools, randomization processes provide a “reasoned basis for inference,” placing empirical claims on a powerful footing. While acknowledging the strengths of these approaches, we also take seriously two points of concern. The first concern—raised by many in recent years (e.g., Thelen and Mahoney (2015))—is about design-based inference’s scope of application. While experimentation and natural experiments represent powerful tools, the range of research situations in which model-free inference is possible is inevitably limited. For a wide range of causal conditions of interest to social scientists and to society, controlled experimentation is impossible, and true or “as-if” randomization is absent. Moreover, limiting our focus to those questions for, or situations in which, exogeneity can be established “by design” would represent a dramatic narrowing of social science’s ken. It would be a recipe for, at best, learning more and more about less and less. To be clear, this is not an argument against experimentation or design based inference; yet it is an argument for why social science needs a broader set of tools. The second concern is more subtle. The great advantage of design-based inference is that it liberates researchers from the need to rely on models to make claims about causal effects. The risk is that, in operating model-free, researchers end up learning about effect sizes but not about models. But models are what we want to learn about. Our goal as social scientists is to have a useful model for how the world works, not simply a collection of claims about the effects different causes have had in different times and places. It is through models that we derive an understanding of how things might work in contexts and for processes and variables that we have not yet studied. Thus, our interest in models is intrinsic, not instrumental. By taking models, as it were, out of the equation, we dramatically limit the potential for learning about the world. 1.1.2 Qualitative and mixed-method inference Recent years have seen the elucidation of the inferential logic behind “process tracing” procedures used in qualitative political science and other disciplines. In our read, the logic provided in these accounts depends on a particular form of model-based inference.1 While process tracing as a method has been around for more than three decades (e.g., George and McKeown (1985)), its logic has been most fully laid out by qualitative methodologists over the last 15 years (e.g., A. Bennett and Checkel (2015), George and Bennett (2005), Brady and Collier (2010), Hall (2003), Mahoney (2010)). Whereas King, Keohane, and Verba (1994) sought to derive qualitative principles of causal inference within a correlational framework, qualitative methodologists writing in the wake of “KKV” have emphasized and clarified process-tracing’s “within-case” inferential logic: in process tracing, explanatory hypotheses are tested based on observations of what happened within a case, rather than on covariation between causes and effects across cases. The process tracing literature has also advanced increasingly elaborate conceptualizations of the different kinds of probative value that within-case evidence can yield. For instance, qualitative methodologists have explicated the logic of different test types (“hoop”, “smoking gun”, etc.) involving varying degree of specificity and sensitivity (Collier (2011), Mahoney (2012)). A smoking-gun test is a test that seeks information that is only plausibly present if a hypothesis is true (thus, generating strong evidence for the hypothesis if passed), a hoop test seeks data that should certainly be present if a proposition is true (thus generating strong evidence against the hypothesis if failed), and a doubly decisive test is both smoking-gun and hoop (for an expanded typology, see also Rohlfing (2013)). Other scholars have expressed the leverage provided by process-tracing evidence in Bayesian terms, moving from a set of discrete test types to a more continuous notion of probative value (Fairfield and Charman (n.d.), A. Bennett (2015), Humphreys and Jacobs (2015)).2 Yet, conceptualizing the different ways in which probative value might operate leaves a fundamental question unanswered: what gives within-case evidence its probative value with respect to causal relations? We believe that, fundamentally, the answer lies in researcher beliefs that lies outside of the analysis in question. We enter a research situation with a model of how the world works, and we use this model to make inferences given observed patterns in the data—while at the same time updating those models based on the data. A key aim of this book is to demonstrate how models can — and, in our view, must — play in drawing case-level causal inferences. As we will also argue, along with clarifying the logic of qualitative inference, causal models can also enable the systematic integration of qualitative and quantitative forms of evidence. Social scientists are increasingly pursuing mixed-method research designs. It is becoming increasingly common for scholars to pursue research strategies that combine quantitative with qualitative forms of evidence. A typical mixed-methods study includes the estimation of causal effects using data from many cases as well as a detailed examination of the processes taking place in a few. Prominent examples include Lieberman’s study of racial and regional dynamics in tax policy (Lieberman (2003)); Swank’s analysis of globalization and the welfare state (Swank (2002)); and Stokes’ study of neoliberal reform in Latin America (Stokes (2001)). Major recent methodological texts provide intellectual justification of this trend toward mixing, characterizing small-\\(n\\) and large-\\(n\\) analysis as drawing on a single logic of inference and/or as serving complementary functions (King, Keohane, and Verba, 1994; Brady and Collier, 2004). The American Political Science Association now has an organized section devoted in part to the promotion of multi-method investigations, and the emphasis on multiple strategies of inference research is now embedded in guidelines from many research funding agencies (Creswell and Garrett, 2008). However, while scholars frequently point to the benefits of mixing correlational and process-based inquiry (e.g., Collier, Brady, and Seawright (2010), p.~181), and have sometimes mapped out broad strategies of multi-method research design (Lieberman (2005), Seawright and Gerring (2008)), they have rarely provided specific guidance on how the integration of inferential leverage should unfold. In particular, the literature does has not supplied specific principles for aggregating findings—whether mutually reinforcing or contradictory—across different modes of analysis.A small number of exceptions stand out. In the approach suggested by Gordon and Smith (2004), for instance, available expert (possibly imperfect) knowledge regarding the operative causal mechanisms for a small number of cases can be used to anchor the statistical estimation procedure in a large-N study. Western and Jackman (1994) propose a Bayesian approach in which qualitative information shapes subjective priors which in turn affect inferences from quantitative data. Relatedly, in Glynn and Quinn (2011), researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. Seawright (2016) presents an informal framework in which case studies are used to test the assumptions underlying statistical inferences, such as the assumption of no-confounding or the stable-unit treatment value assumption (SUTVA). Yet we still lack a comprehensive framework that allows us to enter qualitative and quantitative form of information into an integrated analysis for the purposes of answering the wide range of causal questions that are of interests to social scientists, including questions about case-level explanations and causal effects, average causal effects, and causal pathways. As we aim to demonstrate in this book, grounding inference in causal models provides a very natural way of combining information of the \\(X,Y\\) variety with information about the causal processes connecting \\(X\\) and \\(Y\\). The approach can be readily addressed to both the case-oriented questions that tend to be of interest to qualitative scholars and the population-oriented questions that tend to motivate quantitative inquiry. As will become clear, in fact, when we structure our inquiry in terms of causal models, the conceptual distinction between qualitative and quantitative inference becomes hard to sustain. Notably, this is not for the reason that “KKV”’s framework suggests, i.e., that all causal inference is fundamentally about correlating causes and effects. To the contrary, it is that in a causal-model-based inference, what matters for the informativeness of a piece of evidence is how that evidence is connected to our query, given how we think the world works. While the apparatus that we present is formal, the approach—in asking how pieces of evidence drawn from different parts of a process map on to a base of theoretical knowledge—is arguably most closely connected to process tracing in its core logic. 1.1.3 Connecting theory and empirics Theory and empirics have had a surprisingly uncomfortable relationship in political science. In a major recent intervention, for instance, Clarke and Primo (2012) draw attention to and critique political scientists’ extremely widespread reliance on the “hypothetico-deductive” (H-D) framework, in which a theory or model is elaborated, empirical predictions derived, and data sought to test these predictions and the model from which they derive.Clarke and Primo draw on decades of scholarship in the philosophy of science pointing to deep problems with the HD framework, including with the idea that the truth of a model logically derived from first principles can be tested against evidence. This book is also motivated by a concern with the relationship between theory and evidence in social inquiry. In particular, we are struck by the frequent lack of a clear link between theory, on the one hand, and empirical strategy and inference, on the other. We see this ambiguity as relatively common in both qualitative and quantitative work. We can perhaps illustrate it best, however, by reference to qualitative work, where the centrality of theory to inference has been most emphasized. In process tracing, theory is what justifies inferences. In their classic text on case study approaches, George and Bennett (2005) describe process tracing as the search for evidence of “the causal process that a theory hypothesizes or implies” (6). Similarly, Hall (2003) conceptualizes the approach as testing for the causal-process-related observable implications of a theory, Mahoney (2010) indicates that the events for which process tracers go looking are those posited by theory (128), and Gerring (2006) describes theory as a source of predictions that the case-study analyst tests (116). Theory, in these accounts, is supposed to help us figure out where to look for discriminating evidence. What we do not yet have, however, is a systematic account of how researchers can derive within-case empirical predictions from theory and how exactly doing so provides leverage on a causal question. From what elements of a theory can scholars derive informative within-case observations? Given a set of possible things to be observed in a case, how can theory help us distinguish more from less informative observations? Of the many possible observations suggested by a theory, how can we determine which would add probative value to the evidence already at hand? How do the evidentiary requisites for drawing a causal inference, given a theory, depend on the particular causal question of interest—on whether, for instance, we are interested in identifying the cause of an outcome, estimating an average causal effect, or identifying the pathway through which an effect is generated? In short, how exactly can we ground causal inferences from within-case evidence in background knowledge about how the world works? Most quantitative work in political science features a similarly weak integration between theory and research design. The modal inferential approach in quantitative work, both observational and experimental, involves looking for correlations between causes and outcomes, with minimal regard for intervening or surrounding causal relationships.3 In this book, we seek to show how scholars can make much fuller and more explicit use of theoretical knowledge in designing their research projects and analyzing their observations. Like Clarke and Primo, we treat models not as maps of sort: maps, based on prior theoretical knowledge, about causal relations in a domain of interest. Also as in Clarke and Primo’s approach, we do not write down a model in order to test its veracity. Rather, we show how we can systematically use causal models with particular characteristics to guide our empirical strategies and inform our inferences. Grounding our empirical strategy in a model allows us, in turn, to learn about the model itself as we encounter the data. 1.2 Key contributions This book draws on methods developed in the study of Bayesian networks, a field pioneered by scholars in computer science, statistics, and philosophy. Bayesian networks, a form of causal model, have had limited traction to date in political science. Yet the literature on Bayesian networks and their graphical counterparts, directed acyclic graphs (DAGs), is a body of work that addresses very directly the kinds of problems that qualitative and quantitative scholars routinely grapple with.4 Drawing on this work, we show in the chapters that follow how a theory can be formalized as a causal model represented by a causal graph and a set of structural equations. Engaging in this modest degree of formalization, we seek to demonstrate, yields enormous benefits. It allows us, for a wide range of causal questions, to identify a.) a set of variables (or nodes) in the model, including unobservable factors, that represent the causal query and b.) a set of observable variables that are potentially informative about the nodes in the query. For students engaging in process tracing, the payoffs of this approach are that it provides: A grounding for probative value for data of any arbitrary kind, from different parts of any causal network. Including giving guidance on where there can be probative value A way of aggregating inferences from observations drawn from all different parts of the causal network. An approach for assessing a wide variety of estimands: e.g., how does inference differ for causal effects compared to mechanisms? Consistency of probative value, priors, and therefore inferences with how you think the world works. Transparency, allowing for evaluation Design: diagnosis of different evidentiary and case-selection strategies, conditional on how you think the world works and the question you want to answer. For mixed method inference: Systematic integration — using both qual and quant to both help answer any given query. in fact, no fundamental difference between quant and qual data — which may discomfit some readers, who see qual research as fundamentally distinct, but offers big advantages, including: Transparency: how exactly the qual and the quant enter into the analysis. A way to justify the background assumptions you’ve used Learning in both directions: from cases to populations, from populations to cases Which provides a model for cumulation. Models get updated and become priors for new analyses. Design: diagnosis of wide vs deep, as well as evidentiary and case-selection strategies As we will show, using causal models has substantial implications for common methodological advice and practice. To touch on just a few of these: Our elaboration and application of model-based process tracing shows that, given plausible causal models, process tracing’s common focus on intervening causal chains may be much less productive than other empirical strategies, such as examining moderating conditions. Our examination of model-based case-selection indicates that for many common purposes there is nothing particularly especially informative about “on the regression line” cases or those in which the outcome occurred, and that case selection should often be driven by factors that have to date received little attention, such as the population distribution of cases and the probative value of the available evidence. And an analysis of clue-selection as a decision problem shows that the probative value of a piece evidence cannot be assessed in isolation, but hinges critically on what we have already observed. MORE TO COME HERE ELABORATING IMPLICATIONS, COMPARING TO ADVICE AND PRACTICE IN LITERATURE We emphasize that the basic analytical apparatus that we employ in this book is not new, and the book’s aim is not to generate fundamentally novel approaches to study causality in the world. Rather, we see the book’s goals as being of three kinds. First, the book aims to import insight: to introduce political scientists to an approach that has received little attention in the discipline but that can be useful for addressing the sorts of causal causal questions with which political scientists are commonly preoccupied. As a model-based approach, it is a framework especially well suited to a field of inquiry in which exogeneity frequently cannot be assumed by design—that is, in which we often have no choice but to be engineers. Second, the book draws connections between the Bayesian networks approach and key concerns and challenges with which methodologists in our discipline routinely grapple. Working with causal models and DAGs most naturally connects to concerns about confounding and identification that have been central to much quantitative methodological development. Yet we also show how causal models can address issues central to process tracing, such as how to select cases for examination, how to think about the probative value of causal process observations, and how to structure our search for evidence, given finite resources. Third, the book provides a set of usable tools for implementing the approach. We provide intuition and software that researchers can use to make research design choices and draw inferences from the data. 1.3 The Road Ahead The book is divided into four main parts. The first part is about the basics. We start off by describing the kinds of causal estimands of interest. The main goal here is to introduce the key ideas in the study of Bayesian nets and to argue for a focus of interest away from average treatment effects as go-to estimands of interest and towards a focus on causal nets, or causal structures, as the key quantity of interest. The next chapter introduces key Bayesian ideas; what Bayes’ rule is and how to use it. The third chapter connects the study of Bayesian networks to theoretical claims. The key argument here is that nets should be thought of as theories which are themselves supportable by lower level networks (theories). Lower level theories are useful insofar as they provide leverage to learn about processes on higher level networks. The second part applies these ideas to process tracing. Rather than conceptualizing process tracing as has been done in recent work as seeking process level data that is known to be informative about a causal claim, the approach suggested here is one in which the probative value of a clue is derived from its position in a causal network connecting variables of interest. Chapter 5 lays out the key logic of inference from clues and provides general criteria for assessing when it is and is not possible. Chapter 6 provides specific tools for assessing which collections of clues are most informative for a given estimand of interest and outlines a strategy for assessing which clues to gather when in a research process. Chapter 7 applies these tools to the problem of assessing the effects of economic inequality on democratization. The third part moves to mixed data problems — situations in which a researcher contains “quantitative” (\\(X,Y\\)) data on a set of cases and is considering gathering within case (“qualitative”) data on some of these. In chapter 8 we argue that this situation is formally no different to the single case process tracing problem since a collection of cases can always be conceptualized as a single case with vector valued variables. The computational complexity is however greater in these cases and so in this chapter we describe a set of models that may be useful for addressing these problems. In this framework the problem of case selection is equivalent to the kind of problem of clue selection discussed in Chapter 6. For a canonical multicase model however we use simulation approaches to provide guidance for how cases should be selected. The broad conclusion here is that researchers should go where the probative value lies, and all else equal, should select cases approximately proportional to the size of \\(XY\\) strata—whether or not these are “on the regression line.” We conclude this part by revisiting the problem of inequality and democracy introduced in Chapter 7. The fourth part steps back and puts the model-based approach into question. We have been advocating an embrace of models to aid inference. But the dangers of doing this are demonstrably large. The key problem is that with model-based inference, the inferences are only as good as the model. In the end, while we are supporting the efforts of engineers, we know that the philosopher is right. This final part provides four responses to this (serious) concern. The first is that the dependence on models can sound more extreme than it is. Seemingly fixed parameters of models can themselves become quantities of interest in lower-level models, and there can be learning about these when higher-level models are studied. Thus models are both put to use and objects of interest. The second is that different types of conditional statements are possible; in particular as shown in work qualitative graphs. The third response points to the sort of arguments that can be made to support models, most importantly the importation of knowledge from one study to another. The last argument, presented in the last substantive chapter, highlights the tools to evaluate models, using tools that are increasingly standard in Bayesian analysis. Here we go. References "],
["models.html", "Chapter 2 Causal Models 2.1 The counterfactual model 2.2 Causal Models and Directed Acyclic Graphs 2.3 Illustrations 2.4 Chapter Appendix", " Chapter 2 Causal Models We provide a lay language primer on the logic of causal models. While social scientific methods can be addressed to many sorts of questions, matters of causation have long been central to theoretical and empirical work in political science, economics, sociology, and psychology. Causality is also the chief focus of this book. Causal knowledge, however, is not just the end goal of much empirical social science; it is also a key input into causal inference. Rarely do we arrive at causal inquiry fully agnostic about causal relations in the domain of interest. Moreover, our beliefs about how the world works—as we show later in this book—have profound implications for how the research process and inference should unfold. What we need is a language for expressing our prior causal knowledge such that we can full exploit it, drawing inferences and making research design decisions in ways that are logically consistent with our beliefs, and such that others can readily see and assess those underlying premises. Causal models provide such a language. In this chapter we provide a basic introduction to causal models. Subsequent chapters in Part I layer on other foundational components of the book’s framework, including a causal-model-based understanding of theory, the definition of common causal estimands within causal models, and the basics of Bayesian inference. While here we focus on the formal definition of causal models, in Chapter 10 we discuss strategies for generating them. 2.1 The counterfactual model We begin with what we might think of as a meta-model, the counterfactual model of causation. The counterfactual model is the dominant approach to causal relations in the social sciences. At its core, a counterfactual understanding of causation captures a simple notion of causation as “difference-making.”5 In the counterfactual view, to say that \\(X\\) caused \\(Y\\) is to say: had \\(X\\) been different, \\(Y\\) would have been different. Critically, the antecedent, “had \\(X\\) been different,” imagines a controlled change in \\(X\\)—an intervention that altered \\(X\\)’s value—rather than a naturally arising difference in \\(X\\). The counterfactual claim, then, is not that \\(Y\\) is different in those cases in which \\(X\\) is different; it is, rather, that if one could have made \\(X\\) different, \\(Y\\) would have been different. Turning to a substantive example, consider, for instance, the claim that India democratized (\\(Y\\)) because it had a relatively high level of economic equality (\\(X\\)) (drawing on the logic of Boix (2003)). In the counterfactual view, this is equivalent to saying that, had India not had a high level of equality—where we imagine that we made equality in India lower—the country would not have democratized. High economic equality made a difference. Along with this notion of causation as difference-making, we also want to allow for variability in how \\(X\\) acts on the world. \\(X\\) might sometimes make a difference, for some units of interest, yet sometimes not. High levels of equality might generate democratization in some countries or historical settings but not in others. Moroever, while equality might make democratization happen in some times in places, it might prevent that same outcome in others. In political science, we commonly employ the “potential outcomes” framework to describe the different kinds of counterfactual causal relations that might prevail between variables . In this framework we characterize how a given unit responds to a causal variable by positing the outcomes that it would take on at different values of the causal variable. It is quite natural to think about potential outcomes in the context of medical treatment. Consider a situation in which some individuals in a diseased population are observed to have received a drug (\\(X=0\\)) while others have not (\\(X=0\\)). Assume that, subsequently, a researcher observes which individuals become healthy (\\(Y=1\\)) and which do not (\\(Y=0\\)). Let us further stipulate that each individual belongs to one of four unobserved response ‘’types,’’ defined by the potential effect of treatment on the individual:6 adverse: Those who would get better if and only if they do not receive the treatment beneficial: Those who would get better if and only if they do receive the treatment chronic: Those who will remain sick whether or not they receive treatment destined: Those who will get better whether or not they receive treatment We can express this same idea by specifying the set of “potential outcomes” associated with each type of patient, as illustrated in Table 2.1. Table 2.1: . Potential outcomes: What would happen to each of four possible types of case if they were or were not treated. Type a Type b Type c Type d adverse effects beneficial Effects chronic cases destined cases Not treated Healthy Sick Sick Healthy Treated Sick Healthy Sick Healthy In each column, we have simply written down the outcome that a patient of a given type would experience if they are not treated, and the outcome they would experience if they are treated. Throughout the book, we generalize from this toy example. Whenever we have one causal variable and one outcome, and both variables are binary (i.e., each can take on two possible values, 0 or 1), then there are only four sets of possible potential outcomes, or “causal types.” More generally, for any pair of causal and outcome variables, we will use \\(\\theta^Y\\) to denote the causal type at node \\(Y\\). The four types are: a: A negative causal effect of \\(X\\) on \\(Y\\). We write this as: \\(\\theta^Y = \\theta^Y_{10}\\). b: A positive causal effect of \\(X\\) on \\(Y\\). We write this as: \\(\\theta^Y = \\theta^Y_{01}\\). c: No causal effect, with \\(Y\\) “stuck” at 0. We write this as: \\(\\theta^Y = \\theta^Y_{00}\\). d: No causal effect, with \\(Y\\) “stuck” at 1. We write this as: \\(\\theta^Y = \\theta^Y_{11}\\). Table 2.2 summarizes these types in terms of potential outcomes: Table 2.2: . Generalizing from Table 2.1, the table gives for each causal type the values that \\(Y\\) would take on if \\(X\\) is set at \\(0\\) and if \\(X\\) is set at 1. Type a Type b Type c Type d \\(\\theta^Y=\\theta^Y_{10}\\) \\(\\theta^Y=\\theta^Y_{01}\\) \\(\\theta^Y=\\theta^Y_{00}\\) \\(\\theta^Y=\\theta^Y_{11}\\) Set \\(X=0\\) \\(Y(0)=1\\) \\(Y(0)=0\\) \\(Y(0)=0\\) \\(Y(0)=1\\) Set \\(X=1\\) \\(Y(1)=0\\) \\(Y(1)=1\\) \\(Y(1)=0\\) \\(Y(1)=1\\) Returning to our democratization example, let \\(X=1\\) represent a high level of economic equality and \\(X=0\\) its absence, with \\(Y=1\\) representing democratization and \\(Y=0\\) its absence. An \\(a\\) type, then, is any case in which a high level of equality, if it occurs, prevents democratization in a country that would otherwise have democratized. The causal effect of high equality in an \\(a\\) type is \\(= -1\\). A \\(b\\) type is a case in which high equality, if it occurs, generates democratization in a country that would otherwise have remained non-democratic (effect \\(= 1\\)). A \\(c\\) type is a case that will not democratize regardless of the level of equality (effect \\(= 0\\)); and a \\(d\\) type is one that will democratize regardless of the level of equality (again, effect \\(= 0\\)). In this setting, a causal explanation of a given case outcome amounts to a statement about its type. The claim that India democratized because of a high level of equality is equivalent to saying that India democratized and is \\(b\\) type. To claim that Sierra Leone democratized because of low inequality is equivalent to saying that Sierra Leone democratized and is an \\(a\\) type. To claim, on the other hand, that Malawi democratized for reasons having nothing to do with its level of economic equality is to characterize Malawi as a \\(d\\) type (which alreqdy specifies its outcome). We can also use potential-outcomes reasoning for more complex causal relations. For example, supposing there are two binary causal variables \\(X_1\\) and \\(X_2\\), we can specify any given case’s potential outcomes for each of the different possible combinations of causal conditions—there now being four such conditions (as each causal variable may take on \\(0\\) or \\(1\\) when the other is at \\(0\\) or \\(1\\)). We now have 16 causal types: 16 different response patterns that \\(Y\\) might display to changes in \\(X_1\\) and \\(X_2\\). The full set is represented in Table 2.3. (The type numberings are, of course, arbitrary here and included for ease of reference.) We can read off this table that a type 4 is a case in which \\(X_1\\) has a positive causal effect on \\(Y\\) but \\(X_2\\) has no effect, whereas a type 6 is case in which \\(X_2\\) has a positive effect but \\(X_2\\) has none. We also capture interactions, or ways in which the effect of one variable might be conditioned by the value of the other. For instance, a type 5 is case in which \\(X_2\\) has a postive causal effect if and only if \\(X_1\\) is 0, and where \\(X_1\\) has a negative causal effect only when \\(X_2=1\\). As one might imagine, the number of causal types increases rapidly as the number of considered causal variables increases, as it also would if we allowed \\(X\\) or \\(Y\\) to take on more than 2 possible values. However, the basic principle of representing possible causal relations as patterns of potential outcomes remains unchanged. Table 2.3: With two binary causal variables, there are 16 causal types: 16 ways in which \\(Y\\) might respond to changes in the two variables. \\(\\theta^Y\\) if \\(X_1=0, X_2=0\\) if \\(X_1=0,X_2=1\\) if \\(X_1=1,X_2=0\\) if \\(X_1=1, X_2=1\\) \\(\\theta^Y_{0000}\\) 0 0 0 0 \\(\\theta^Y_{0001}\\) 0 0 0 1 \\(\\theta^Y_{0010}\\) 0 0 1 0 \\(\\theta^Y_{0011}\\) 0 0 1 1 \\(\\theta^Y_{0100}\\) 0 1 0 0 \\(\\theta^Y_{0101}\\) 0 1 0 1 \\(\\theta^Y_{0110}\\) 0 1 1 0 \\(\\theta^Y_{0111}\\) 0 1 1 1 \\(\\theta^Y_{1000}\\) 1 0 0 0 \\(\\theta^Y_{1001}\\) 1 0 0 1 \\(\\theta^Y_{1010}\\) 1 0 1 0 \\(\\theta^Y_{1011}\\) 1 0 1 1 \\(\\theta^Y_{1100}\\) 1 1 0 0 \\(\\theta^Y_{1101}\\) 1 1 0 1 \\(\\theta^Y_{1110}\\) 1 1 1 0 \\(\\theta^Y_{1111}\\) 1 1 1 1 Readers will note that, in the counterfactual framework, causal relations are conceptualized as deterministic. A given case has a set of potential outcomes. Any randomness enters the analysis as incomplete knowledge of the factors influencing an outcome. But, in principle, if we knew all of the relevant causal conditions and the complete set of potential outcomes for a case, we could perfectly predict the actual outcome in that case. This understanding of causality—as ontologically deterministic, but empirically imperfectly understood—is compatible with views of causation commonly employed by qualitative researchers (see, e.g., Mahoney (2008)), and with understandings of causal determinism going back at least to Laplace (1901). As we will see, we can readily express this kind of incompleteness of knowledge within a causal model framework; indeed, the way in which causal models manage uncertainty is central to how they allow us to pose questions of interest and to learn from evidence. A further important, if somewhat counter-intuitive implication, of the counterfactual framework lies in how it forces us to think about multiple causes. When seeking to explain the outcome in a case, researchers sometimes proceed as though competing explanations amount to rival causes, where \\(X_1\\) being a cause of \\(Y\\) implies that \\(X_2\\) was not. Did Malawi democratize because it was a relatively economically equal society or because of international pressure to do so? In the counterfactual model, however, causal relations are non-rival. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if either supporter’s vote were different. Put differently, when we say that \\(X\\) caused \\(Y\\) in a given case, we will generally mean that \\(X\\) was a cause, \\(X\\) will rarely be the cause in the sense of being the only thing a change in which would have changed the outcome. Malawi might not have democratized if either a relatively high level of economic equality or international pressure had been absent. For most social phenomena that we study, there will be multiple, and sometimes a great many, difference-makers for any given case outcome. 2.2 Causal Models and Directed Acyclic Graphs Potential outcomes tables can capture quite a lot. We could, for instance, summarize our beliefs about the relationship between economic equality and democratization by saying that we think that the world is comprised of a mixture of \\(a\\), \\(b\\), \\(c\\), and \\(d\\) types, as defined above. We could get more specific and express a belief about what proportions of cases in the world are of each of the four types. For instance, we might believe that \\(a\\) types and \\(d\\) types are quite rare while \\(b\\) and \\(c\\) types are more common. Moreover, our belief about the proportions of \\(b\\) (positive effect) and \\(a\\) (negative effect) cases imply a belief about equality’s average effect on democratization as, in a binary setup, this quantity is imply the proportion of \\(b\\) types minus the proportion of \\(a\\) types. As we have seen, beliefs about even more complex causal relations can be fully expressed in potential-outcomes notation. However, as causal structures become more complex—especially, as the number of variables in a domain increases—a causal model can be a powerful organizing tool. In this section, we show how causal models and their visual counterparts, directed acyclic graphs (DAGs), can represent substantive beliefs about counterfactual causal relationships in the world. The key ideas in this section can be found in many texts (see, e.g., Halpern and Pearl (2005) and Galles and Pearl (1998)), and we introduce here a set of basic principles that readers will need to follow the argumentation in this book. To slightly shift the frame of our running example, suppose that we believe the level of economic inequality can have an effect on whether a country democratizes. We might believe inequality affects the likelihood of democratization by generating demands for redistribution, which in turn can cause the mobilization of lower-income citizens, which in turn can cause democratization. We might also believe that mobilization itself is not just a function of redistributive preferences but also of the degree of ethnic homogeneity, which shapes capacities of lower-income citizens for collective action. We can visualize this model in Figure 2.1. Figure 2.1: A simple causal model in which high inequality (\\(I\\)) affects the democratization (\\(D\\)) via redistributive demands and mass mobilization (\\(M\\)), which is also a function of ethnic homogeneity (\\(E\\)). The arrows show relations of causal dependence between variables. The graph does not capture the ranges of the variables and the functional relations between them. 2.2.1 Components of a Causal Model In the context of this example, let us now consider the three components of a causal model: variables, functions, and distributions. 2.2.1.1 The variables. The first component of a causal model is the set of variables across which the model characterizes causal relations. On the graph in Figure 2.1, the 6 included variables are represented by the 6 nodes. Notice that some of these variables have arrows pointing into them: \\(R, M\\), and \\(D\\) are endogenous variables, meaning that their values are determined entirely by other variables in the model. Other variables have arrows pointing out of them but no arrows pointing into them: \\(I, E\\) and \\(U_D\\) are exogenous variables. Exogenous variables are those that influence other variables in the model but themselves have no causes specified in the model. While \\(I\\) and \\(E\\) have natural interpretations, we might wonder what \\(U_D\\) represents as it does not feature in our substantive claims about how democratization arises. In the world of causal models, \\(U\\) terms are typically used to capture unspecified exogenous influences. Far from being nuisance terms, \\(U\\) variables constitute a key way in which we express uncertainty about the world and, in turn, are often the locus of learning about the questions we are asking. In the present example, we believe democratization to be potentially affected by mobilization, but we also know that democratization is affected by other things, even if we do not know what they are. We can thus think of \\(U_D\\) as a set of unknown factors—factors other than mobilization—that affect democratization.7 In a causal-model framework, we sometimes use familial terms to describe relations among variables. For instance, two nodes directly connected by an arrow are known as “parent” and “child,” while two nodes with a child in common (both directly affect the same variable) are “spouses.” We can also say that \\(I\\) is an “ancestor” of \\(D\\) (a node upstream from \\(D\\)’s parent) and conversely that \\(D\\) is a descendant of \\(I\\) (a node downstream from \\(I\\)’s child). In identifying the variables, we also need to specify the across which they can potentially vary. We might specify, for instance, that all variables in the model are binary, taking on the values 0 or 1. We could, alternatively, define a set of categories across which a variable ranges or allow a variable to take on any real number value or any value between a set of bounds.8 2.2.1.2 The functions. Next, we need to specify our beliefs about the causal relations among the variables in our model. How is the value of one variable affected by, and how does it affect, the values of others? For each endogenous variable—each variable influenced by others in the model—we need to express beliefs about how its value is affected by its parents, its immediate causes. The graph already represents some aspects of these beliefs: the arrows, or directed edges, tell us which variables we believe to be direct causal inputs into other variables. So, for instance, we believe that democratization (\\(D\\)) is determined jointly by mobilization (\\(M\\)) and some exogenous, unspecified factor (or set of factors), \\(U_D\\). We can think of \\(U_D\\) as all of the other influences on democratization, besides mobilization, that we either do not know of or have decided not to explicitly include in the model. We believe, likewise, that \\(M\\) is determined by \\(I\\) and an unspecified exogenous factor (or set of factors), \\(U_M\\). And we are conceptualizing inequality (\\(I\\)) as shaped solely by a factors exogenous to the model, captured by \\(U_I\\). (For all intents and purposes, \\(I\\) behaves as an exogenous variable here since its value is determined solely by an exogenous variable.) We can also, however, express more specific beliefs about causal relations in the form of a causal function.9 Specifying a function means writing down whatever general or theoretical knowledge we have about the direct causal relations between variables. A function specifies how the value that one variable takes on is determined by the values that other variables—its parents—take on. We can specify this relationship in a vast variety of ways. It is useful however to distinguish broadly between parametric and non parameteric approaches. A parametric approch specifies a functional form that relates parents to children. For instance we might model one variable as a linear function of another. For instance, we can write \\(R=\\beta I\\), where \\(\\beta\\) is a parameter that we do not know the value of at the outset of a study but which we wish to learn about. If we believe \\(D\\) to be linearly affected by \\(M\\) but also subject to forces that we do not yet understand and have not yet specified in our theory, then we can write: \\(D=\\beta*M+U_D\\), where \\(U_D\\) represents a random disturbance. We be still more agnostic by, for example including parameters that govern how other parameters operate. Consider, for instance the function, \\(D=\\beta M^{U_D}\\). Here, \\(D\\) and \\(M\\) are linearly related if \\(U_D=1\\), but exponentially if \\(U_D\\) is anything other than \\(1\\). The larger point is that functions can be written to be quite specific or extremely general, depending on the state of prior knowledge about the phenomenon under investigation. The use of a structural model does not require precise knowledge of specific causal relations, even of the functional forms through which two variables are related. With discrete data causal functions can also take fully non-parametric form, allowing for any possible relation between parents and children. Let us, for instance, allow \\(U_D\\) to range across the values \\(a, b, c\\), and \\(d\\). We can then write down the following causal function for \\(D\\): if \\(U_D=a\\), then \\(D=1-M\\) if \\(U_D=b\\), then \\(D=M\\) if \\(U_D=c\\), then \\(D=0\\) if \\(U_D=d\\), then \\(D=1\\) You may recognize here our original four causal types from earlier in this chapter. Here, \\(U_D\\) is an unknown factor that is conditioning the effect of mobilization on democratization, determining whether \\(M\\) has a negative effect, a positive effect, no effect with democratization never occurring, or no effect with democratization bound to occur regardless of mobilization. Using this approach we can similarly express set-theoretic relations of any kind among any number of factors. We may, for instance, believe that an outcome occurs when and only when two conditions are present. Redistributive demands and ethnic homogeneity may be individually necessary and jointly sufficient conditions for mobilization. We can express this belief with a slightly more complex function: \\(M=E R\\). According to this function, \\(M=1\\) (mobilization occurs) if both \\(E=1\\) (ethnic homogeneity is present) and \\(R=1\\) (redistributive preferences are present), but \\(M=0\\) (mobilization does not occur) otherwise. Note that this formulation also builds in causal heterogeneity: here, we are saying that redistributive preferences have an effect on mobilization when and only when ethnic homogeneity is present, and vice versa. We will make considerable use of this non parmetric approach in the pages and chapters to come. In doing so we will switch notation and rather than using \\(U\\) to denote a random disturbance, we will use \\(\\theta\\) to indicate the unknown mapping from inputs to outputs. We use \\(\\theta^X\\) to denote the mapping at node \\(X\\) and refer to \\(\\theta^X\\) as a unit’s nodal causal type.10 We use \\(\\theta\\) to denote the collection of nodal types across all nodes and refer to this simply as the unit’s causal types. If nodal types include values of root nodes, then the causal type fully specifies all outcomes as well as all counterfactual outcomes for a unit. Box: DEFINITION OF NODAL CAUSAL TYPES DEFINITION OF UNIT CAUSAL TYPES It is thus worth dwelling for a moment on what this kind of function is doing. We have started with a graph in which mobilization can have an effect on democratization and the understanding that this effect, both its existence and its sign, may vary across cases. Cases, in other words, may be of different causal types. Further, we do not know what it is that shapes \\(D\\)’s response to \\(M\\)—what makes a case one type versus another. We thus use \\(\\theta_D\\) as a stand-in for the unknown and unspecified moderators of \\(M\\)’s effect. We might, at this stage, wonder what the point is of including \\(\\theta_D\\) in the model; are we not essentially just placing a question mark on the graph? We are, and that is precisely the point. As we will see in later chapters, non-substantive, causal-type nodes can play a key role in specifying (a) what we are uncertain about in a causal network and (b) what we would like to find out. Embedding our questions about the world directly into a model of the world, in turn, allows us to answer those questions in ways systematically and transparently guided by prior knowledge. 2.2.2 Interpretation of functional equations A few important aspects of causal functions stand out. First, unlike regression equations and other equations describing data patterns, these functions express causal beliefs. When we write \\(D=\\beta M\\) as a function, we do not just mean that we believe the values of \\(M\\) and \\(D\\) in the world to be linearly related. We mean that we believe that the value of \\(M\\) determines the value of \\(D\\) through this linear function. Functions are, in this sense, meant as directional statements, with causes on the righthand side and an outcome on the left. Second, to specify functions is to unpack a potentially complex web of causal relations into its constituent causal links. For each variable, we do not need to think through entire sequences of causation that might precede it. We need only specify how we believe it to be affected by its parents—that is to say, those variables pointing directly into it. Our outcome of interest, \\(D\\), may be a shaped by multiple, long chains of causality. To theorize how \\(D\\) is generated, however, we write down how we believe \\(D\\) is shaped by its immediate causes, \\(M\\) and \\(U_D\\). We then, separately, express a belief about how \\(M\\) is shaped by its direct causes, \\(R\\) and \\(E\\). A variable’s function must include as inputs all, and only, those variables that point directly into that variable.11 Third, as in the general potential-outcomes framework, all relations in a causal model are conceptualized as in principle deterministic. There is not as much at stake here though as you might think at first; by this we simply mean that a variable’s value is determined by the values of its parents along with any stochastic or unknown components. We express uncertainty about causal relations, however, either as unknown paramaters (e.g., \\(\\beta\\), above) or as random disturbances, the \\(U\\) terms, or the causal types \\(\\theta\\). Fourth, in a properly specified causal model the values of the exogenous variables—those with no arrows pointing in to them—are sufficient to determine the values of all other variables in the model. Consistent with more informal usage, we refer to a given set of values for all exogenous terms in a model as a context. In causal model, context determines all other values. For instance, in Figure 2.1, knowing the values of \\(I\\), \\(E\\), and \\(U_D\\) as well as the causal functions (including the values of any parameters they contain) would tell us the values of \\(R\\), \\(M\\), and \\(D\\). 2.2.2.1 The distributions Putting these components together gives what is termed a structural causal model. In a structural causal model, all endogenous variables are, either directly or by implication, functions of a case’s context (the values of the set of exogenous variables).12 What we have not yet inscribed into the model, however, is any beliefs about how likely or common different kinds of contexts might be. Thus, for instance, a structural causal model consistent with Figure 2.1 stipulates that \\(I\\), \\(E\\), and \\(U_D\\) may have effects on \\(D\\), but it says nothing in itself about the distribution of \\(I\\), \\(E\\), and \\(U_D\\) themselves, beyond limitations on their ranges.13 We have not said anything, for instance, about how common high inequality is across the relevant domain of cases, how common ethnic homogeneity is, or how unspecified inputs are distributed. In many research situations, we will have beliefs not just about how the world works under different conditions, but also about what kinds of conditions are more likely than others. We can express these beliefs about context as probability distributions over the models exogenous terms.14 For instance, a structural causal model might support a claim of the form: “\\(R\\) has a positive effect on \\(M\\) if and only if \\(E=1\\) holds.” We might, then, add to this a belief that \\(E=1\\) in 25% of cases in the population of interest. Including this belief about context implies, in turn, that \\(R\\) has a positive effect on \\(M\\) a quarter of the time. As with the functions, we can also (and typically would) build uncertainty into this belief by specifying a distribution over possible shares of cases with ethnic homogeneity, with our degree of uncertainty captured by the distribution’s variance. With our non parametric representation of functional forms, we let \\(\\lambda_j^X\\) denote the probability that \\(\\theta^X = \\theta^X_j\\). For instance in a simple \\(X \\rightarrow Y\\) model, \\(\\lambda^Y_{01}\\) denotes the probability that \\(\\theta^Y = \\theta^Y_{01}\\). FLAG: FLESH OUT INCLUDING HOW CONFOUNDING IS TREATED Technical Note on the Markov Property The assumptions that no variable is its own descendant and that the \\(U\\) terms are generated independently make the model Markovian, and the parents of a given variable are Markovian parents. Knowing the set of Markovian parents allows one to write relatively simple factorizations of a joint probability distribution, exploiting the fact (“the Markov condition”) that all nodes are conditionally independent of their nondescendants, conditional on their parents. Variables \\(A\\) and \\(B\\) are “conditionally independent” given \\(C\\) if \\(P(a|b,c) = P(a|c)\\) for all values of \\(a, b\\) and \\(c\\). To see how this Markovian property allows for simple factorization of \\(P\\) for Figure 2.1, note that \\(P(X, R, Y)\\) can always be written as: \\[P(X, R, Y) = P(X)P(R|X)P(Y|R, X)\\] If we believe, as in the figure, that \\(X\\) causes \\(Y\\) only through \\(R\\) then we have the slightly simpler factorization: \\[P(X, R, Y) = P(X)P(R|X)P(Y|R)\\] Or, more generally: \\[\\begin{equation} P(v_1,v_2,\\dots v_n) = \\prod P(v_i|pa_i) \\tag{2.1} \\end{equation}\\] The distribution \\(P\\) on \\(\\mathcal{U}\\) induces a joint probability distribution on \\(\\mathcal{V}\\) that captures not just information about how likely different states are to arise but also the relations of conditional independence between variables that are implied by the underlying causal process. For example, if we thought that \\(X\\) caused \\(Y\\) via \\(R\\) (and only via \\(R\\)), we would then hold that \\(P(Y | R) = P(Y | X, R)\\): in other words if \\(X\\) matters for \\(Y\\) only via \\(R\\) then, conditional on \\(R\\), \\(X\\) should not be informative about \\(Y\\). In this way, a probability distribution \\(P\\) over a set of variables can be consistent with some causal models but not others. This does not, however, mean that a specific causal model can be extracted from \\(P\\). To demonstrate with a simple example for two variables, any probability distribution on \\((X,Y)\\) with \\(P(x)\\neq P(x|y)\\) is consistent both with a model in which \\(X\\) is a parent of \\(Y\\) and with a model in which \\(Y\\) is a parent of \\(X\\). Once we introduce beliefs about the distribution of values of the exogenous terms in a model, we have specified a probabilistic causal model. We need not say much more, for the moment, about the probabilistic components of causal models. But to foreshadow the argument to come, our prior beliefs about the likelihoods of different contexts play a central role in the framework that we present in this book. We will see how the encoding contextual knowledge—beliefs that some kinds of conditions are more common than others—forms a key foundation for causal inference. At the same time, our expressions of uncertainty about context represent scope for learning: it is the very things that we are, at a study’s outset, uncertain about that we can update our beliefs about as we encounter evidence. 2.2.3 Rules for graphing causal models The diagram in Figure 2.1 is a causal DAG (Hernán and Robins 2006). We endow it with the interpretation that an arrow from a parent to a child that a change in the parent can, under some circumstances, induce a change in the child. Though we have already been making use of this causal graph to help us visualize elements of a causal model, we now explicitly point out a number of general features of causal graphs as we will be using them throughout this book. Causal graphs have their own distinctive “grammar,” a set of rules that give them important analytical features. Directed, acyclic. A causal graph represents elements of a causal model as a set of nodes (or vertices), representing variables, connected by a collection of single-headed arrows (or directed edges). We draw an arrow from node \\(A\\) to node \\(B\\) if and only if we believe that \\(A\\) can have a direct effect on \\(B\\). The resulting diagram is a directed acyclic graph (DAG) if there are no paths along directed edges that lead from any node back to itself—i.e., if the graph contains no causal cycles. The absence of cycles (or “feedback loops”) is less constraining than it might appear at first. In particular if one thinks that \\(A\\) today causes \\(B\\) tomorrow which in turn causes \\(A\\) today, we can represent this as \\(A_1 \\rightarrow B \\rightarrow A_2\\) rather than \\(A \\leftrightarrow B\\). That is, we timestamp the nodes, turning what might informally sppear as feedback into a non cyclical chain. Meaning of missing arrows. The absence of an arrow between \\(A\\) and \\(B\\) means that \\(A\\) is not a direct cause of \\(B\\).15 Here lies an important asymmetry: drawing such an arrow does not mean that we know that \\(A\\) does directly cause \\(B\\); but omitting such an arrow implies that we know that \\(A\\) does not directly cause \\(B\\). We say more, in other words, with the arrows we omit than with the arrows that we include. Returning to Figure 2.1, we have here expressed the belief that redistributive preferences exert no direct effect on democratization; we have done so by not drawing an arrow directly from \\(R\\) to \\(D\\). In the context of this model, saying that redistributive preferences have no direct effect on democratization is to say that any effect of redistributive preferences on democratization must run through mobilization; there is no other pathway through which such an effect can operate. This might be a way of encoding the knowledge that mass preferences for redistribution cannot induce autocratic elites to liberalize the regime absent collective action in pursuit of those preferences. The same goes for the effects of \\(I\\) on \\(M\\), \\(I\\) on \\(D\\), and \\(E\\) on \\(D\\): the graph in Figure 2.1 implies that we believe that these effects also do not operate directly, but only along the indicated, mediated paths. Sometimes causes. The existence of an arrow from \\(A\\) to \\(B\\) does not imply that \\(A\\) always has a direct effect on \\(B\\). Consider, for instance, the arrow running from \\(R\\) to \\(M\\). The existence of this arrow requires that \\(M\\) appear somewhere in the \\(M\\)’s functional equation as a variable’s functional equation must include all variables pointing directly into it. Imagine, though, that \\(M\\)’s causal function is specified as: \\(M = RE\\). This function allows for the possibility that \\(R\\) affects \\(M\\), as it will whenever \\(E=1\\). However, it also allows that \\(R\\) will have no effect, as it will when \\(E=0\\). This example also, incidentally, demonstrates another important consequence of context, the values of the exogenous variables: a case’s context determines not just the settings on the endogenous variables, but also the causal effects that prevail among the variables. Under the functional equation \\(M=RE\\), a case’s ethnic-compositional context determines whether or not redistributive preferences will have an effect on mobilization. Representing \\(U\\) on the graph. As a matter of convention, explicitly including \\(U\\) terms is optional. In practice, \\(U\\)’s are often excluded from the visual representation of a model on the understanding that every variable on the graph is subject to some unaccounted-for influence and thus, implicitly, has a \\(U\\) term pointing into it. In this book, we will generally draw the \\(U\\) terms where they are of particular theoretical or analytical interest but will otherwise omit them. Whether we include or omit \\(U\\) terms, we will generally treat those nodes in a graph that have no arrows pointing into them as the exogenous variables that define the context. No excluded common causes, no unobserved confounding please. Any cause common to multiple variables on the graph must itself be represented on the graph. If \\(A\\) and \\(B\\) on a graph are both affected by some third variable, \\(C\\), then we must represent this common cause. Put differently, any two variables without common causes on the graph are taken to be indepedent of one another. Thus, the graph in Figure 2.1 implies that the values of \\(I\\), \\(E\\), and \\(U_D\\) are all determined independently of one another. If in fact we believed that a country’s level of inequality and its ethnic composition were both shaped by, say, its colonial heritage, then this DAG would not be an accurate representation of our beliefs about the world. To make it accurate, we would need to add to the graph a variable capturing that colonial heritage and include arrows running from colonial heritage to both \\(I\\) and \\(E\\). This rule ensures that the graph captures all potential correlations among variables that are implied by our beliefs. If \\(I\\) and \\(E\\) are in fact driven by some common cause, then this means not just that these two variables will be correlated but also that each will be correlated with any consequences of the other. For instance, a common cause of \\(I\\) and \\(E\\) would also imply a correlation between \\(R\\) and \\(E\\). \\(R\\) and \\(E\\) are implied to be independent in the current graph but would be implied to be correlated if a common node pointed into both \\(I\\) and \\(E\\). Of particular interest in Figure 2.1 is the implied independence of \\(U_D\\) from every other node. Imagine, for instance, an additional node pointing into both \\(I\\) and \\(U_D\\). This would represent a classic form of confounding: the assignment of cases to values on the explanatory variable would be correlated with case’s potential outcomes on \\(D\\). The omission of any such pathway is precisely equivalent to expressing the belief that \\(I\\) is exogenous, or (as if) randomly assigned. Representing excluded common causes, unobserved confounding if you have it. It may be however that there are common causes for nodes that we simply do not understand. We are open to the idea that some unknown feature determines both \\(I\\) and \\(D\\). In this case it is as if \\(U_I\\) and \\(U_D\\) are not independently distributed. This is often represented by adding a dotted line, or a two headed arrow, connecting nodes whose shocks are not independent. Figure 2.2 illustrates. In general we will allow for this kind of unobserved confounding in the models in this book and seek to learn about the joint distribution of errors in such cases. Figure 2.2: A DAG with unobserved confounding Licence to exclude variables. The flip side of this rule is that a causal graph, to do the work it must do, does not need to include everything we know about a substantive domain of interest. We may know quite a lot about the causes of economic inequality, for example. But we can safely omit these other factors from the graph as long as they do not affect multiple other variables in the model. Indeed we can choose to capture any number of unspecified factors in a \\(U\\) term. We may be aware of a vast range of forces shaping whether countries democratize, but choose to bracket them for the purposes of an examination of the role of economic inequality. This bracketing is permissible as long as none of these unspecified factors also act on other variables included in the model. You can’t read functional equations from a graph. As should be clear, a DAG does not represent all features of a causal model. What it does record is which variables enter into the structural equation for every other variable: what can directly cause what. But the DAG contains no other information about the form of those causal relations. Thus, for instance, the DAG in Figure 2.1 tells us that \\(M\\) is function of both \\(R\\) and \\(E\\), but it does not tell us whether that joint effect is additive (\\(R\\) and \\(E\\) separately increase mobilization), interactive (the effect of each depends on the value of the other), or whether either effect if linear, curvilinear or something else. This lack of information about functional forms often puzzles those encountering causal graphs for the first time; surely it would be convenient to visually differentiate, say, additive from conditioning effects. As one thinks about the variety of possible causal functions, it quickly becomes clear that there would be no simple visual way of capturing all possible functional relations. Moreover, as we shall now see, causal graphs are a tool designed with a particular analytic purpose in mind—a purpose to which we now turn. 2.2.4 Conditional independence from DAGs If we encode our prior knowledge using the grammar of a causal graph, we can put that knowledge to work for us in powerful ways. In particular, the rules of DAG-construction allow for an easy reading of the conditional independencies implied by our beliefs. To begin thinking about conditional independence, it can be helpful to conceptualize dependencies between variables as generating flows of information. Let us first consider a simple relationship of dependence. Returning to Figure 2.1, the arrow running from \\(I\\) to \\(R\\), implying a direct causal dependency, means that if we expect \\(I\\) and \\(R\\) to be correlated. Put differently, observing the value of one of these variables also gives us information about the value of the other. If we measured redistributive preferences, the graph implies that we would also be in a better position to infer the level of inequality, and vice versa. Likewise, \\(I\\) and \\(M\\) are also linked in a relationship of dependence: since inequality can affect mobilization (through \\(R\\)), knowing the the level of inequality would allow us to improve our estimate of the level of mobilization and vice versa. In contrast, consider \\(I\\) and \\(E\\), which are in this graph indicated as being independent of one another. Learning the level of inequality, according to this graph, would give us no information whatsoever about the degree of ethnic homogeneity, and vice-versa. Moreover, sometimes what you learn depends on what you already know. Suppose that we already knew the level of redistributive preferences. Would we then be in a position to learn about the level of inequality by observing the level of mobilization? According to this graph we would not: since the causal link—and, hence, flow of information between \\(I\\) and \\(M\\)—runs through \\(R\\), and we already know \\(R\\), there is nothing left to be learned about \\(I\\) by also observing \\(M\\). Anything we could have learned about inequality by observing mobilization is already captured by the level of redistributive preferences, which we have already seen. While \\(I\\) and \\(M\\) are dependent—one is informative about the other—if we have not seen \\(R\\), they are independent of one another (uninformative about each other) if we have seen \\(R\\). We can express this idea by saying that \\(I\\) and \\(M\\) are conditionally independent given \\(R\\). We say that two variables, \\(A\\) and \\(C\\), are “conditionally independent” given a set of variables \\(\\mathcal B\\) if, once we have knowledge of the values in \\(\\mathcal B\\), knowledge of \\(A\\) provides no information about \\(C\\) and vice-versa. Taking \\(\\mathcal B\\) into account thus “breaks” any relationship that might exist unconditionally between \\(A\\) and \\(C\\). To take up another example, suppose that war is a cause of both military casualties and price inflation, as depicted in Figure 2.3. Casualties and inflation will then be (unconditionally) correlated with one another because of their shared cause. If I learn that there have been military casualties, this information will lead me to think it more likely that there is also war and, in turn, price inflation (and vice versa). However, assuming that war is their only common cause, we would say that military casualties and price inflation are conditionally independent given war. If we already know that there is war, then we can learn nothing further about the level of casualties (price inflation) by learning about price inflation (casualties). We can think of war, when observed, as blocking the flow of information between its two consequences; everything we would learn about inflation from casualties is already contained in the observation that there is war. Put differently, if we were just to look at cases where war is present (i.e., if we hold war constant), we should find no correlation between military casualties and price inflation; likewise, for cases in which war is absent. Figure 2.3: This graph represents a simple causal model in which war (\\(W\\)) affects both military casualties (\\(C\\)) and price inflation (\\(P\\)). Relations of conditional independence are central to the strategy of statistical control, or covariate adjustment, in correlation-based forms of causal inference, such as regression. In a regression framework, identifying the causal effect of an explanatory variable, \\(X\\), on a dependent variable, \\(Y\\), requires the assumption that \\(X\\)’s value is conditionally independent of \\(Y\\)’s potential outcomes (over values of \\(X\\)) given the model’s covariates. To draw a causal inference from a regression coefficient, in other words, we have to believe that including the covariates in the model “breaks” any biasing correlation between the value of the causal variable and its unit-level effect. As we will explore, however, relations of conditional independence are of more general interest in that they tell us, given a model, when information about one feature of the world may be informative about another feature of the world, given what we already know. By identifying the possibilities for learning, relations of conditional independence can thus guide research design. To see more systematically ow a DAG can reveal conditional independencies, it is useful spell out three pairs of features of the flow of information in causal graphs: Figure 2.4: Three elementary relations of conditional independence. (1a) Information can flow unconditionally along a path of arrows pointing in the same direction. In Panel 1 of Figure , information flows across all three nodes. Learning about any one will tell us something about the other two. (1b) Learning the value of a variable along a path of arrows pointing in the same direction blocks flows of information across that variable. Knowing the value of \\(B\\) in Panel 1 renders \\(A\\) no longer informative about \\(C\\), and vice versa: anything that \\(A\\) might tell us about \\(C\\) is already captured by the information about \\(B\\). (2a) Information can flow unconditionally across the branches of any forked path. In Panel 2 learning only \\(A\\) can provide information about \\(C\\) and vice-versa. (2b) Learning the value of the variable at the forking point blocks flows of information across the branches of a forked path. In Panel 2, learning \\(A\\) provides no information about \\(C\\) if we already know the value of \\(B\\).16 (3a) When two or more arrowheads collide, generating an inverted fork, there is no unconditional flow of information between the incoming sequences of arrows. In Panel 3, learning only \\(A\\) provides no information about \\(C\\), and vice-versa. (3b) Collisions can be sites of conditional flows of information. In the jargon of causal graphs, \\(B\\) in Panel 2 is a “collider” for \\(A\\) and \\(C\\).17 Although information does not flow unconditionally across colliding sequences, it does flow across them conditional on knowing the value of the collider variable or any of its downstream consequences. In Panel 2, learning \\(A\\) does provide new information about \\(C\\), and vice-versa, if we also know the value of \\(B\\) (or, in principle, the value of anything that \\(B\\) causes). The last point is somewhat counter-intuitive and warrants further discussion. It is easy enough to see that, for two variables that are correlated unconditionally, that correlation can be “broken” by controlling for a third variable. In the case of collision, two variables that are not correlated when taken by themselves become correlated when we condition on (i.e., learn the value of) a third variable, the collider. The reason is in fact quite straightforward once one sees it: if an outcome is a joint function of two inputs, then if we know the outcome, information about one of the inputs can provide information about the other input. For example, if I know that you have brown eyes, then learning that your mother has blue eyes makes me more confident that your father has brown eyes. Looking back at our democratization DAG in Figure 2.1, \\(M\\) is a collider for \\(R\\) and \\(E\\), its two inputs. Suppose that we again have the functional equation \\(M=RE\\). Knowing about redistributive preferences alone provides no information whatsoever about ethnic homogeneity since the two are determined independently of one another. On the other hand, imagine that you already know that there was no mobilization. Now, if you observe that there were redistributive preferences, you can figure out the level of ethnic homogeneity: it must be 0. (And likewise in going from homogeneity to preferences.) Using these basic principles, conditional independencies can be read off any DAG. We do so by checking every path connecting two variables of interest and ask whether, along those paths, the flow of information is open or blocked, given any other variables whose values are already observed. Conditional independence is established when all paths are blocked given what we already know; otherwise, conditional independence is absent. 2.3 Illustrations We can provide more of a sense of how one might encode prior knowledge in a causal model by asking how we might construct models in light of extant scholarly works. We undertake this exercise here for three well-known works in comparative politics and international relations: Pierson’s seminal book on welfare-state retrenchment (Pierson (1994)); Elizabeth Saunders’ research on leaders’ choice of military intervention strategies (Saunders (2011)); and Przeworski and Limongi’s work on democratic survival (Przeworski and Limongi (1997)), an instructive counterpoint to Boix’s (Boix (2003)) argument about a related dependent variable. For each, we represent in the form of a causal model the causal knowledge that we might plausibly think we take away from the work in question. Readers might represent these knowledge bases differently; our present aim is merely to illustrate how causal models are constructed, rather than to defend a particular representation (much less the works in question) as accurate. 2.3.1 Welfare state reform: Pierson (1994) The argument in Pierson’s 1994 book Dismantling the Welfare State? challenged prior notions of post-1980 welfare-state retrenchment in OECD countries as a process driven primarily by socioeconomic pressures (slowed growth, rising unemployment, rising deficits, aging populations) and the rise of market-conservative ideologies (embodied, e.g., the ascendance of Thatcher and Reagan). Pierson argues that socioeconomic and ideological forces put retrenchment on the policy agenda, but do not ensure its enactment because retrenchment is a politically perilous process of imposing losses on large segments of the electorate. Governments will only impose such losses if they can do so in ways that allow them avoid blame for doing so—by, for instance, making the losses hard to perceive or responsibility for them difficult to trace. These blame-avoidance opportunities are themselves conditioned by the particular social-program structures that governments inherit. Figure 2.5: A graphical representation of Pierson (1994). While the argument has many more specific features (e.g., different program-structural factors that matter, various potential strategies of blame-avoidance), its essential components can be captured with a relatively simple causal model. We propose such a model in graphical form in Figure . Here, the outcome of retrenchment (\\(R\\)) hinges on whether retrenhcment makes it onto the agenda (\\(A\\)) and on whether blame-avoidance strategies are available to governments (\\(B\\)), and on some unspecified random input (\\(U_R\\)). Retrenchment emerges on the policy agenda as a consequence of both socioeconomic developments (\\(S\\)) and the ascendance of ideologically conservative political actors (\\(C\\)). Inherited program structures (\\(P\\)), meanwhile, determine the availability of blame-avoidance strategies. A few features of this graph warrant attention. As we have discussed, it is the omitted arrows in any causal graph that imply the strongest statements. The graph in Panel (a) implies that \\(C\\), \\(S\\), \\(P\\), and \\(U_R\\)—which are neither connected along a directed path nor downstream from a common cause—are independent of one another. This implies, for instance, that whether conservatives govern is independent of whether program structures will allow for blame-free retrenchment. Thus, as Pierson argues, a Reagan or Thatcher can come to power but nonetheless run up against an opportunity structure that would makes retrenchment politically perilous. Further, in this graph any effect of program structures on retrenchment must run through their effects on blame-avoidance opportunities. One could imagine relaxing this restriction by, for instance, drawing an arrow from \\(P\\) to \\(A\\): program structures might additionally affect retrenchment by conditioning the fiscal costliness of the welfare state, thus helping to determine whether reform makes it onto the agenda. Where two variables are connected by an arrow, moreover, this does not imply that a causal effect will always operate. Consider, for instance, the arrow pointing from \\(A\\) to \\(R\\). The fact that \\(A\\) sometimes affects \\(R\\) and sometimes does not is, in fact, central to Pierson’s argument: conservatives and socioeconomic pressures forcing retrenchment on the agenda will not generate retrenchment if blame-avoidance opportunities are absent. The graph also reflects a choice about where to begin. We could, of course, construct a causal account of how conservatives come to power, how socioeconomic pressures arose, or why programs were originally designed as they were. Yet it is perfectly permissible for us to bracket these antecedents and start the model with \\(C\\), \\(S\\), and \\(P\\), as long as we do not believe that these variables have any antecedents in common. If they do have common causes, then this correlation should be captured in the DAG.18 The DAG itself tells us about the possible direct causal dependencies but is silent on the ranges of and functional relations among the variables. How might we express these? With three endogenous variables, we need three functions indicating how their values are determined. Moreover, every variable pointing directly into another variable must be part of that second variable’s function. Let us assume all variables are binary, with each condition either absent or present. We can capture quite a lot of Pierson’s theoretical logic with the following quite simple functional equations: \\(A=CS\\), implying that retrenchment makes it on the agenda if and only if both conservatives are in power and socioeconomic pressures are high. \\(B=P\\), implying that blame-avoidance opporunities arise when and only when program structures take a particular form \\(R=ABU_R\\). This last functional equation requires a little bit of explanation. Here we are saying that retrenchment will only occur if retrenchment is on the agenda and blame-avoidance opportunities are present (as the expression zeroes out if either of these are 0). Yet even if both are present, the effect on retrenchment also hinges on the value of \\(U_R\\). \\(U_R\\) thus behaves as a causal-type variable with respect to the effect of an \\(AB\\) combination on \\(R\\) and allows for two possible types. When \\(U_R=1\\), the \\(AB\\) combination has a positive causal effect on retrenchment. When \\(U_R=0\\), \\(AB\\) has no causal effect: retrenchment will not occur regardless of the presence of \\(AB\\). A helpful way to conceptualize what \\(U_R\\) is doing is that is capturing a collection of features of a case’s context that might render the case susceptible or not susceptible to an \\(AB\\) causal effect. For instance, Pierson’s analysis suggests that a polity’s institutional structure might widely diffuse veto power such that stakeholders can block reform even when retrenchment is on the agenda and could be pursued without electoral losses. We could think of such a case as having a \\(U_R\\) value of 0, implying that \\(AB\\) has no causal effect. A \\(U_R=1\\) case, with a positive effect, would be one in which the government has the institutional capacity to enact reforms that it has the political will to pursue. 2.3.2 Military Interventions: Saunders (2011) Saunders (2011) asks why, when intervening militarily abroad, do leaders sometimes seek to transform the domestic political institutions of the states they target but sometimes seek only to shape the states’ external behaviors. Saunders’ central explanatory variable is the nature of leaders’ causal beliefs about security threats. When leaders are “internally focused,” they believe that threats in the international arena derive from the internal characteristics of other states. Leaders who are “externally focused,” by contrast, understand threats as emerging strictly from other states’ foreign and security policies. These basic worldviews, in turn, affect the cost-benefit calculations they make about intervention strategies, via two mechanisms. Most simply, these beliefs affect perceptions of the likely security gains from a transformative intervention strategy. In addition, these beliefs affect the kinds of strategic capabilities in which leaders invest, which in turn effects the costliness and likelihood of success of alternative intervention strategies. Calculations about the relative costs and benefits of different strategies then shape the choice between a transformative and non-transformative approach to intervention. Yet leaders can, of course, only choose one of these options if they decide to intervene at all. The decision about whether to intervene depends, in turn, on at least two kinds of considerations. A leader is more likely to intervene against a given target when the nature of the dispute makes the leader’s preferred strategy—given their causal beliefs—appear feasible in this situation; yet leaders may also be pushed to intervene by international or domestic audiences. Figure depicts the causal dependencies in Saunders’ argument in DAG form. Working from left to right, we see that causal beliefs (\\(C\\)) affect the expected net relative benefits of the two strategies (\\(B\\)) both via a direct pathway and via an indirect pathway running through preparedness investments (\\(P\\)). Characteristics of a given target state or dispute (\\(T\\)) likewise influence \\(B\\). The decision about whether to intervene (\\(I\\)) is then a function of three factors: causal beliefs (\\(C\\)), the expected relative net benefits of the strategies (\\(B\\)), and audience pressures (\\(A\\)). Finally, the choice of strategy (\\(S\\)) is a function of whether or not intervention occurs at all (\\(I\\)), cost-benefit comparisons between the two strategies (\\(B\\)), and other, idiosyncratic factors that may operate in various cases (\\(U_S\\)). Figure 2.6: A graphical representation of Saunders’ (2011) argument. This relatively complex DAG illustrates how readily DAGs can depict the multiple pathways through which a given variable might affect another variable, as with the multiple pathways linking \\(C\\) to \\(I\\) and \\(B\\) (and, thus, all of its causes) to \\(S\\). In fact, this graphical representation of the dependencies in some ways throws the multiplicity of pathways into even sharper relief than does a narrative exposition of the argument. For instance, Saunders draws explicit attention to how causal beliefs operate on expected net benefits via both a direct and indirect pathway, both of which are parts of an indirect pathway from \\(C\\) to the outcomes of interest, \\(I\\) and \\(S\\). What is a bit easier to miss without formalization is that \\(C\\) also acts directly on the choice to intervene as part of the feasibility logic: when leaders assess whether their generally preferred strategy would be feasible if deployed against a particular target, the generally preferred strategy is itself a product of their causal beliefs. The DAG also makes helpfully explicit that the two main outcomes of interest—the choice about whether to intervene and the choice about how—are not just shaped by some of the same causes but are themselves causally linked, with the latter depending on the former. Omitted links are also notable. For instance, the lack of an arrow between \\(T\\) and \\(A\\) suggests that features of the target that affect feasibility have no effect on audience pressures. If instead we believed, for instance, that audiences take feasibility into account in demanding intervention, we would want to include a \\(T \\rightarrow A\\) arrow. Turning to variable ranges and functional equations, it is not hard to see how one might readily capture Saunders’ logic in a fairly straightforward set-theoretic manner. All variables except \\(S\\) could be treated as binary with, for instance, \\(C=1\\) representing internally focused causal beliefs, \\(P=1\\) representing preparedness investments in transformation, \\(B=1\\) representing expectations that transformation will be more net beneficial than non-transformation, \\(T=1\\) meaning that a target has characteristics that make transformation a feasible strategy, and so on. Although there are two strategies, we in fact need three values for \\(S\\) because it must be defined for all values of the other variables—i.e., it must take on a distinct categorical value if there is no intervention at all. We could then define functions, such as: \\(B=CPT\\), implying that transformation will only be perceived to be net beneficial in a case if and only if the leader has internally focused causal beliefs, the government is prepared for a transformative strategy, and the target has characteristics that make transformation feasible \\(I=(1-|B-C|)+(1-(1-|B-C|))A\\), implying that intervention can occur under (and only under) either of two alternative sets of conditions: if the generally preferred strategy and the more net-beneficial strategy in a given case are the same (i.e., such that \\(B-C=0\\)) or, when this alignment is absent (i.e., such that \\(|B-C|=0\\)), where audiences pressure a leader to intervene. 2.3.3 Development and Democratization: Przeworski and Limongi (1997) Przeworski and Limongi (1997) argue that democratization occurs for reasons that are, with respect to socioeconomic or macro-structural conditions, largely idiosyncratic; but once a country has democratized, a higher level of economic development makes democracy more likely to survive. Economic development thus affects whether or not a country is a democracy, but only after a democratic transition has occurred, not before. Thus, unlike in Boix (2003), democratization in Przeworski and Limongi’s argument is exogenous, rather than being determined by other variables in the model. Moreover, the dynamic component of Przeworski and Limongi’s argument—the fact that both the presence of democracy and the causal effect of development on democracy depend on whether a democratic transition occurred at a previous point in time—forces us to think about how to capture over-time processes in a causal model. We represent Przeworski and Limongi’s argument in the DAG in Figure 2.7. The first thing to note is that we can capture dynamics by considering democracy at different points in time as separate nodes. According to the graph, whether a country is a democracy in a given period (\\(D_t\\)) is a function, jointly, of whether it was a democracy in the previous period (\\(D_{t-1}\\)) and of the level of per capita GDP in the current period, as well as of other unspecified forces (\\(U_{D_t}\\)) that lie outside the model. Figure 2.7: A graphical representation of Przeworski and Limongi’s argument, where \\(D_{t-1}\\)=democracy in the previous period; \\(GDP_t\\)=per capita GDP in the current period; \\(D_t\\)=democracy in the current period. Second, the arrow running from \\(GDP_{t-1}\\) to \\(D_t\\) means that \\(GDP\\) may affect democracy, not that it always does. Indeed, Przeworski and Limongi’s argument is that development’s effect depends on a regime’s prior state: GDP matters for whether democracies continue to be democracies, but not for whether autocracies go on to become democracies. The lack of an arrow between \\(D_{t-1}\\) and \\(GDP_{t-1}\\), however, implies a (possibly incorrect) belief that democracy and \\(GDP\\) in the last period are independent of one another. Finally, we might consider the kind of causal function that could capture Przeworski and Limongi’s causal logic. In this function, \\(GDP\\) should reduce the likelihood of a transition away from democracy but not affect the probability of a transition to democracy, which should be exogenously determnined. One possible translation of the argument into functional terms is: \\[d_t = 1 (p(1-d_{t-1}) + d_{t-1}(1-q(1-gdp)) &gt; u_{D_t})\\] where \\(d_t\\) and \\(d_{t-1}\\) are binary, representing current and last-period democracy, respectively \\(p\\) is a parameter, varying from 0 to 1, representing the probability that an autocracy democratizes \\(q\\) is a parameter, varying from 0 to 1, representing the probability that a democracy with a GDP of 0 reverts to autocracy \\(gdp\\) represents national per capita GDP, normalized on a 0 to 1 scale for the population of interest. \\(u_{D_t}\\) represents a random, additional input into democracy with a uniform distribution on the 0 to 1 scale the indicator function, \\({1}\\), evaluates the inequality and generates a value of \\(1\\) if and only if it is true Unpacking the equation, the likelihood that a country is a democracy in a given period rises and falls with the expression to the left of the \\(&gt;\\)-operator. This expression itself has two parts, reflecting the difference between the determinants of transitions to democracy (captured by the first part) and the determinants of democratic survival (captured by the second). The first part comes into play—i.e., is non-zero—only for non-democracies. For non-democracies, the expression evaluates simply to \\(p\\), the exogenous probability of democratization. The second part is non-zero only for democracies, where it evaluates to \\(1-q\\)—the inverse of the reversion parameter—times \\(1-gdp\\): thus, the reversion probability falls as national income rises. The inequality is then evaluated by “asking” whether the expression on the left (either \\(p\\) or \\((1-q)gdp\\)) is greater than a number (\\(u_{D_t}\\)) randomly drawn from a uniform distribution between 0 and 1. Thus, higher values for the expression increase the likelihood of democracy while the randomness of the \\(u_{D_t}\\) threshold captures the role of other, idiosyncratic inputs. Note how, while the functional equation nails down certain features of the process, it leaves others up for grabs. In particular, the parameters \\(p\\) and \\(q\\) are assumed to be constant for all autocracies and for all democracies, respectively, but their values are left unspecified. And one could readily write down a function that left even more openness—by, for instance, including an unknown parameter that translates \\(GDP\\) into a change in the probability of reversion or allowing for non-linearities, with unknown parameters, in this effect. 2.4 Chapter Appendix 2.4.1 Steps for constructing causal models Box: Steps for constructing causal models Identify a set of variables in a domain of interest You should specify the range of each variable: is it continuous or discrete? May include \\(U\\) terms representing unspecified, random influences Draw a causal graph (DAG) representing beliefs about causal dependencies among these variables Capture direct effects only Arrows indicate possible, not constant or certain, causal effects The absence of an arrow between two variables indicates a belief of no direct causal relationship between them Ensure that the graph captures all correlations among variables. This means that either (a) any common cause of two or more variables is included on the graph (with implications for Step 1) or (b) correlated variables are connected with a dashed, undirected edge. Write down one causal function for each endogenous variable Each variable’s function must include all variables directly pointing into it on the graph Functions may take any form, as long as each set of possible causal values maps onto a single outcome value Functions may express arbitrary amounts of uncertainty about causal relations State probabilistic beliefs about the distributions of the exogenous variables How common or likely to do we think different values of the exogenous variables are? Are they independently distributed? If in step 2 you drew an undirected edge between nodes then you believe that the connected variables are not independently distributed. 2.4.2 Model construction in code Our gbiqq package provides a set of functions to implement all of these steps concisely for binary models – models in which all variables are dichotomous. # Steps 1 and 2 # We define a model with three binary variables and specified edges between them: model &lt;- make_model(&quot;X -&gt; M -&gt; Y&quot;) # Step 3 # Unrestricted functional forms are allowed by default, though these can # also be reduced. Here we impose monotonicity at each step # by removing one type for M and one for Y model &lt;- set_restrictions(model, labels = list(M = &quot;M10&quot;, Y=&quot;Y10&quot;)) # Step 4 # We set priors over the distribution of (remaining) causal types. # Here we set &quot;jeffreys priors&quot; model &lt;- set_priors(model, prior_distribution = &quot;jeffreys&quot;) # We now have a model defined as an R object. # Later we will ask questions of this model and update it using data. These steps are enough to fully describe a binary causal model. Later in this book we will see how we can ask questions of a model like this but also how to use data to train it. 2.4.3 Test yourself! Can you read conditional independence from a graph? As an exercise, see whether you can identify the relations of conditional independence between \\(A\\) and \\(D\\) in Figure . Figure 2.8: An exercise: \\(A\\) and \\(D\\) are conditionally independent, given which other variable(s)? Are A and D independent: unconditionally? Yes. \\(B\\) is a collider, and information does not flow across a collider if the value of the collider variable or its consequences is not known. Since no information can flow between \\(A\\) and \\(C\\), no information can flow between \\(A\\) and \\(D\\) simply because any such flow would have to run through \\(C\\). if you condition on \\(B\\)? No. Conditioning on a collider opens the flow of information across the incoming paths. Now, information flows between \\(A\\) and \\(C\\). And since information flows between \\(C\\) and \\(D\\), \\(A\\) and \\(D\\) are now also connected by an unbroken path. While \\(A\\) abnd \\(D\\) were independent when we conditioned on nothing, they cease to be independent when we condition on \\(B\\). if you condition on \\(C\\)? Yes. Conditioning on \\(C\\), in fact, has no effect on the situation. Doing so cuts off \\(B\\) from \\(D\\), but this is irrelevant to the \\(A\\)-\\(D\\) relationship since the flow between \\(A\\) and \\(D\\) was already blocked at \\(B\\), an unobserved collider. if you condition on \\(B\\) and \\(C\\)? Yes. Now we are doing two, countervailing things at once. While conditioning on \\(B\\) opens the path connecting \\(A\\) and \\(D\\), conditioning on \\(C\\) closes it again, leaving \\(A\\) and \\(D\\) conditionally independent. Analyzing a causal graph for relations of independence represents one payoff to formally encoding our beliefs about the world in a causal model. We are, in essence, drawing out implications of those beliefs: given what we believe about a set of direct causal relations (the arrows on the graph), what must this logically imply about other dependencies and independencies on the graph, conditional on having observed some particular set of nodes? We show in a later chapter how these implications can be deployed to guide research design, by indicating which parts of a causal system are potentially informative about other parts that may be of interest. References "],
["theory.html", "Chapter 3 Theories as causal models 3.1 Theory as a “lower-level” model 3.2 Illustration of unpacking causal types 3.3 Rules for moving between higher- and lower-level models 3.4 Conclusion 3.5 Chapter Appendices", " Chapter 3 Theories as causal models We introduce the idea of thinking of (applied) theoretical claims as claims within hierarchies of causal models. Lower level models serve as a theory for a higher level model if the higher level model can be deduced from the lower level model. The empirical content of a lower level model is the possible reduction in variance of the higher level model that it can provide. Theory plays an important role in this book’s use of causal models for causal inference. Yet the term “theory” in the empirical social sciences means very different things in different contexts. In this book, we will refer to a theory is an explanation of a phenomeon: a theory provides an account of how or under what conditions a set of causal relationships operate. Moreover, we can express both a theory and the claims being theorized as causal models. A theory, then, is a model that explains and implies another model—possibly with the help of some data. We discuss toward the end of the chapter how this definition of theory relates to common understandings of theory in the social sciences. First, however, we focus on unpacking our working definition. In embedding theorization within the world of causal models, we ultimately have an empirical objective in mind. Theorizing a causal relationship of interest, in our framework, means elaborating our causal beliefs about the world in greater detail. As we show in later chapters, theorizing in the form of a causal model allows us to generate research designs: to identify sources of inferential leverage and to explicitly and systematically link observations of components of a causal system to the causal questions we seek to answer. 3.1 Theory as a “lower-level” model Let us say that a causal model, \\(M^\\prime\\), is a theory of \\(M\\) if \\(M\\) is implied by \\(M^\\prime\\). Theory is, thus, all relative. \\(M^\\prime\\) might itself sit atop a theory, \\(M^{\\prime\\prime}\\), that implies \\(M^\\prime\\). To help fix the idea of theory as “supporting” or “underlying” the model(s) it theorizes, we refer to the theory, \\(M^\\prime\\), as a lower-level model relative to \\(M\\) and refer to \\(M\\) as a higher-level model relative to its theorization, \\(M^\\prime\\).19 We illustrate showing two models, \\(M^\\prime\\), \\(M^{\\prime\\prime}\\) that each imply a model \\(M\\). In each case the lower level models contain additional nodes in a way that allows for a kind of “disaggregation” of exogenous nodes. Figure 3.1: Here we represent the simple claim that one variable causes another, and two theories — lower-level models — that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes. We start with the higher-level model, \\(M\\), represented in Figure 3.1(a). We can then offer the model, \\(M^\\prime\\) in panel (b) as a theory, a lower-level model, of \\(M\\). We have added a node, \\(K\\), in the causal chain between \\(X\\) and \\(Y\\), a familiar mode of theorization. In doing do we have in fact split the error \\(\\theta^Y\\) into two parts: \\(\\theta^{Y_\\text{lower}}\\) and \\(\\theta^K\\). Intuitively, in the higher-level model, (a), \\(Y\\) is a function of \\(X\\) and a disturbance \\(\\theta^Y\\), the latter representing all things other than \\(X\\) than can affect \\(Y\\). In our four-type setup, \\(\\theta^Y\\) represents all of the (unspecified) sources of variation in \\(X\\)’s effect on \\(Y\\). When we add \\(K\\), \\(X\\) now does not directly affect \\(Y\\) but only does so via \\(K\\). Further, we model \\(X\\) as acting on \\(K\\) “with error,” with \\(\\theta^K\\) representing all of the (unspecified) factors determining \\(X\\)’s effect on \\(K\\). The key thing to notice here is that \\(\\theta^K\\) now represents a portion of the variance that \\(\\theta^Y\\) represented in the higher-level graph: some of the variation in \\(X\\)’s effect on \\(Y\\) now arises from \\(X\\)’s effect on \\(K\\), which is captured by \\(\\theta^K\\). So, for instance, \\(X\\) might have no effect on \\(Y\\) because \\(\\theta^K\\) takes on a value such that \\(X\\) has no effect on \\(K\\). Likewise, any effect of \\(X\\) on \\(Y\\) must arise from an effect of \\(X\\) on \\(K\\), captured in \\(\\theta^K\\)’s value.20 What \\(\\theta^K\\) represents, then, is that part of the original \\(\\theta^Y\\) that arose from some force other than \\(X\\) operating at the first step of the causal chain from \\(X\\) to \\(Y\\). So now, \\(\\theta^Y\\) is not quite the same entity in the lower-level graph that it was in the higher-level graph. In the original graph, \\(\\theta^Y\\) represented all sources of variation in \\(X\\)’s effect on \\(Y\\). In the lower-level model, with \\(K\\) as mediator, \\(\\theta^Y\\) represents only random variation in \\(K\\)’s effect on \\(Y\\). \\(\\theta^Y\\) has been expunged of any factors shaping the first stage of the causal process, which now reside in \\(\\theta^K\\). Reflecting a convention that we use throughout the book, we highlight this change in \\(\\theta^Y\\)’s meaning by referring in the second model to \\(\\theta^{Y_\\text{lower}}\\). Theorization here thus starts with the proliferation of substantive variables—adding beliefs about intervening steps in a causal process. But, critically, it also involves an accompanying disaggregation of unexplained variation. Addition and splitting thus go hand-in-hand: the insertion of a mediator between \\(X\\) and \\(Y\\) also involves the splitting of \\(Y\\)’s unspecified parent (\\(\\theta_Y\\)). Consider next model \\(M&#39;&#39;\\) panel (c) in Figure 3.1, which also supports (implies) the higher-level theory in panel \\((a)\\). The logical relationship between models \\((a)\\) and \\((c)\\), however, is somewhat different. Here the lower-level model specifies one of the conditions that comprised \\(\\theta^Y\\) in the higher-level model. In specifying a moderator, \\(C\\), we have extracted \\(C\\) from \\(\\theta^Y\\), leaving \\(\\theta^{Y_\\text{lower}}\\) to represent all factors other than \\(C\\) that condition \\(X\\)’s effect on \\(Y\\). (Again, the relabeling as \\(\\theta^{Y_\\text{lower}}\\) reflects this change in the term’s meaning.) While we might add a \\(\\theta^C\\) term pointing into \\(C\\), this is not necessary. Whereas in Model (b) we have extracted \\(\\theta^K\\) from \\(\\theta^Y\\), in Model (c), it is \\(C\\) itself that we have extracted from \\(\\theta^Y\\), substantively specifying what had been just a random disturbance. Critically, notice that since lower level models imply higher level models we think of theories as implying the models they are theorizing. If you believe Model \\(M&#39;\\), then you also must believe Model \\(M\\). If it is possible that \\(X\\) can affect \\(K\\) and possible that \\(K\\) can affect \\(Y\\) then it is possible that \\(X\\) can affect \\(Y\\). The converse is not true, however. It is not possible to still believe that \\(X\\) can effect \\(Y\\) if you do not think that \\(X\\) can affect \\(K\\). Similarly, if you believe Model (c), then you must also believe Model (a): if it is true that \\(X\\) can affect \\(Y\\), possibly in ways that are moderated by \\(C\\), then it is trivially true, more simply, that \\(X\\) can affect \\(Y\\). 3.2 Illustration of unpacking causal types We now show more specifically how causal types in lower level models map into causal types in higher level models. For concreteness, let us return to our democratization example and consider first the very basic claim that inequality can have an affect on democratization. We represent this simple claim in Figure 3.2, Panel (a). In this simple model, \\(I\\) may sometimes have an effect of \\(D\\), and sometimes not; and that effect may be positive or negative. All of this will depend on the case’s causal type. Figure 3.2: DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a). In addition the figure shows two models that each explain Model (a), though in different ways. Model (b) answers the explanatory question, “How does inequality affect democratization?” Model (c) answers the explanatory question, “Why does inequality’s effect on democratization vary?” Both theories provide richer, more interpretable accounts of the phenomenon of interest than the simpler model that they are theorizing. These lower level models imply a set of causal types that are richer than that implied by (a). Recall that in Chapter 2, we considered the idea that at any node, a causal type may be conceptualized as a case specific disturbance, that governs the mapping from input variables to outcome variables. In particular if we deploy our four-causal-type function from Chapter 2 we have: \\(a\\): \\(\\theta^D=\\theta^D_{10}\\), then \\(D=1-I\\) (\\(I\\) has a negative effect on \\(D\\)) \\(b\\): \\(\\theta^D=\\theta^D_{01}\\), then \\(D=I\\) (\\(I\\) has a positive effect on \\(D\\)) \\(c\\): \\(\\theta^D=\\theta^D_{00}\\), then \\(D=0\\) (\\(I\\) has no causal effect) \\(d\\): \\(\\theta^D=\\theta^D_{11}\\), then \\(D=1\\) (\\(I\\) has no causal effect) Knowing \\(\\theta\\) tells us how \\(D\\) responds to \\(I\\) and it ignores any heterogeneity between units as long they respond in the same way. For any causal type the model is consistent with \\(I\\)’s causal effect operating for different reasons for different units, but these differences are left entirely unaccounted for. 3.2.1 Type disaggregation in a mediation model Model (b) has causal types defined for nodes \\(M\\) and for \\(D\\). As with the overall \\(I,D\\) relationship. We thus allow \\(I\\) to have a positive, negative, or no effect on \\(M\\), with \\(\\theta^M\\) taking on four possible values, again corresponding to \\(a,b,c,d\\) nodal types (now: a: \\(\\theta_{10}^M\\), b: \\(\\theta_{01}^M\\), c: \\(\\theta_{00}^M\\), d: \\(\\theta_{11}^M\\)). And we allow for \\(M\\) to have a positive, negative, or no effect on \\(D\\), with \\(\\theta^D_{\\text{lower}}\\) possible values again being one of four nodal types (\\(\\theta_{10}^D\\), \\(\\theta_{01}^D\\), \\(\\theta_{00}^D\\), \\(\\theta_{11}^D\\)). We can now think about combinations of types in the lower-level model as mapping onto types in the higher-level model. Table 3.1 illustrates. Table 3.1: Mapping from lower level nodal types on \\(M\\) and \\(D\\) to higher level causal types on \\(D\\). \\(\\theta_{10}^{D_{lower}}\\) \\(\\theta_{01}^{D_{lower}}\\) \\(\\theta_{00}^{D_{lower}}\\) \\(\\theta_{11}^{D_{lower}}\\) \\(\\theta_{10}^{M}\\) \\(\\theta_{01}^{D_{higher}}\\) \\(\\theta_{10}^{D_{higher}}\\) \\(\\theta_{00}^{D_{higher}}\\) \\(\\theta_{11}^{D_{higher}}\\) \\(\\theta_{01}^{M}\\) \\(\\theta_{10}^{D_{higher}}\\) \\(\\theta_{01}^{D_{higher}}\\) \\(\\theta_{00}^{D_{higher}}\\) \\(\\theta_{11}^{D_{higher}}\\) \\(\\theta_{00}^{M}\\) \\(\\theta_{11}^{D_{higher}}\\) \\(\\theta_{00}^{D_{higher}}\\) \\(\\theta_{00}^{D_{higher}}\\) \\(\\theta_{11}^{D_{higher}}\\) \\(\\theta_{11}^{M}\\) \\(\\theta_{00}^{D_{higher}}\\) \\(\\theta_{11}^{D_{higher}}\\) \\(\\theta_{00}^{D_{higher}}\\) \\(\\theta_{11}^{D_{higher}}\\) For instance, in a case in which both \\(\\theta^M=\\theta^M_{01}\\) (a positive effect of \\(I\\) on \\(M\\)) and \\(\\theta^{D_{\\text{lower}}}=\\theta_{01}^{D_{lower}}\\) (a positive effect of \\(M\\) on \\(D\\)), we have a positive effect of \\(I\\) on \\(D\\)—meaning that, in the higher-level model, \\(\\theta^{D_{higher}}=\\theta^{D_{higher}}_{01}\\). Two linked negative effects also generate a positive effect of \\(I\\) on \\(D\\) and so map onto the same higher-level type. Further, it is easy to see that if there is no causal effect at either the \\(I \\rightarrow M\\) step or the \\(M \\rightarrow D\\) step, we will have one of the null effect types at the higher level since, in this model, \\(I\\) cannot affect \\(D\\) unless there are causal effects at both constituent steps.21 To foreshadow the discussion in later chapters, these mappings are critical: they allow us to use inferences drawn at a lower level to answer questions posed at a higher level. 3.2.2 Type disaggregation in a moderation model Alternatively, we might wonder when inequality causes democratization. Our simple claim, in panel (a), allows that \\(I\\) can cause \\(D\\), but provides no information about the conditions under which it does so. Those conditions are implicitly embedded within \\(\\theta^D\\), where they are left unspecified. We could, however, theorize some of what is left unsaid in in panel (a). We do this in panel (c), where we posit ethnic homogeneity (\\(E\\)) as a moderator of inequality’s effect on democratization. Panel (c) represents a theory of panel (a) in that it can help account for variation in causal effects that is unaccounted for by the model in (a). Model (c) has thus given substantive meaning to an aspect of the phenomenon that was merely residual variation in Model (a). Model (a) provides no account of why inequality has the effects it does, relying fully on \\(\\theta^D\\) as a placeholder for this uncertainty. In Model (c), \\(\\theta^D\\) plays a more modest role, with ethnic homogeneity doing a good deal of the work of determining inequality’s possible effects. In this graph, we again have a \\(\\theta_D^{\\text{lower}}\\) term, but it is a different object from \\(\\theta_D^{\\text{lower}}\\) in the mediation graph. In this moderation model, \\(\\theta_D^{\\text{lower}}\\) is more complex as it determines the mapping from two binary variables into \\(D\\). A causal type in this setup now represents how a case will respond to four different possible combinations of \\(I\\) and \\(E\\) values. Rather than four causal types, we now have 16, as there are 16 possible ways in which a case might respond to two binary variables (see Table 2.3 in Chapter 2). In Table 3.2 we give a mapping from a subset of these lower level types to the upper level types corresponding to the model in (a). Table 3.2: Values for \\(D\\) given \\(E\\) and \\(I\\). With two binary causal variables, there are 16 nodal types: 16 ways in which \\(Y\\) depends on \\(I\\) and \\(E\\). These lower level types map into higher level types for a model in which \\(Y\\) depends on \\(I\\) only, as shown in the final column. Low Type \\(I=0,E=0\\) \\(I=0,E=1\\) \\(I=1,E=0\\) \\(I=1, E=1\\) High Type \\(\\theta^{D}_{0000}\\) 0 0 0 0 \\(\\theta^{D}_{00}\\) \\(\\theta^{D}_{0001}\\) \\(\\theta^{D}_{0010}\\) 0 0 0 0 0 1 1 0 \\(\\theta^{D}_{01}\\) if \\(E=1\\), else \\(\\theta^D_{00}\\) \\(\\theta^D_{00}\\) if \\(E=1\\), else \\(\\theta^D_{01}\\) \\(\\theta^{D}_{0011}\\) 0 0 1 1 \\(\\theta^{D}_{01}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\theta^{D}_{1110}\\) 1 1 1 0 \\(\\theta^D_{11}\\) if \\(E=0\\), else \\(\\theta^D_{10}\\) \\(\\theta^{D}_{1111}\\) 1 1 1 1 \\(\\theta^D_{11}\\) Importantly we see that the mapping between lower- and higher-level types can depend on the value of the moderator. More generally, since we can think of the value of exogeneous nodes, \\(E\\) and \\(I\\), as being nodal types for those nodes, we can think of the lower level nodal type as a concatenation of the upper level nodal types for \\(E\\) and \\(D\\). Thus we can think of the the higher level type as depending uniquely on the fully specified lower level type. For instance, a case that has type \\(\\theta_{01}\\) in the higher-level model if it has type \\(\\theta_{0010}\\) in the lower-level model and \\(E=0\\). This is a case for which \\(I\\) has a positive effect on \\(D\\) when \\(E=0\\) and in which \\(E\\) is in fact 0. On the other hand, the same lower level type, in combination with \\(E=1\\) maps onto the type \\(\\theta_{10}\\) in the higher-level model—a type in which \\(D\\) responds negatively to \\(I\\). In later chapters, we represent all lower- to higher-level mappings relevant to a question of interest with the use of “type-reduction” tables that allow one to readily see how inferences drawn at one level inform causal questions posed at another level. 3.3 Rules for moving between higher- and lower-level models Thinking about models as conditionally nested within one another can be empirically useful. It provides a way of generating empirical leverage on a causal question by plumbing more deeply our background knowledge about a domain of interest. When we more fully specify higher-level claims via a more elaborate, lower-level model, we are a making explicit unspecified conditions on which the higher-level relationships depend. In doing this, we are identifying potentially observable nodes that might be informative about our research question. As we develop lower-level models to support our claims, or determine which claims are supported by our theories, what kinds of moves are we permitted to make? One important thing to note is that the mappings between higher-level claims and theories may not be one-to-one. A single theory can support multiple higher-level theories. Moreover, a single higher-level relation can be supported by multiple, possibly incompatible lower-level theories. To illustrate, consider two “lower level” theories of democratization: (\\(L_1\\)): \\(Inequality \\rightarrow Democratization \\leftarrow Mobilization\\) (\\(L_2\\)): \\(Inequality \\rightarrow Mobilization \\rightarrow Democratization\\) Note how these theories are incompatible with one another. While \\(Inequality\\) and \\(Democratization\\) are independent in \\(L_1\\), they are causally related in \\(L_2\\). Moreover, in \\(L_2\\), \\(Inequality\\) and \\(Democratization\\) are related only through \\(Mobilization\\), while in \\(L_1\\), \\(Democratization\\) is directly affected by \\(Inequality\\).22 Now, consider the following three higher-level claims: (\\(H_1\\)): \\(Inequality \\rightarrow Democratization\\) (\\(H_2\\)): \\(Mobilization \\rightarrow Democratization\\) (\\(H_3\\)): \\(Inequality \\rightarrow Mobilization\\) \\(H_1\\) could be derived from (explained by) either theory, \\(L_1\\) or \\(L_2\\). Although the two theories are incompatible with one another, in both theories \\(Inequality\\) affects \\(Democratization\\). Both theories likewise imply \\(H_2\\), in which \\(Mobilization\\) affects \\(Democratization\\). \\(H_3\\), however, can be supported only by one of these theories: only in \\(L_2\\), and not in \\(L_1\\), does \\(Inequality\\) cause \\(Mobilization\\).23 Thus multiple (possibly incompatible) theories can usually be proposed to explain any given causal effect. When seeking an explanation for, say, \\(H_1\\), the choice between \\(L_1\\) and \\(L_2\\) is not dictated by logic; it must be drawn from a substantive belief about which set of causal dependencies operates in the world. On the other hand, \\(L_2\\) is logically ruled out as an explanation of \\(H_3\\). Further, any given theory logically implies multiple (necessarily compatible) higher-level claims about causal relations. What, more generally, are the permissible moves across levels? 3.3.1 Moving down levels We have already discussed two possible forms of theorization — moves down a level: (i) disaggregating existing nodes, i.e., by introducing beliefs about mediation or moderation, or (ii) adding nodes representing variation in a feature of context that is implicitly held constant in the higher-level model. There are other possible ways of elaborating a model. For instance, we can add antecedent conditions: causes of nodes that were exogenous in the higher-level model. Likewise, we can add downstream effects: outcomes of nodes that were terminal in the higher-level model. Figure 3.3: A higher-level model and a lower-level model that is impermissible. The central principle governing allowable elaborations is that a lower-level model must not introduce dependencies between variables that were omitted in the higher-level model. We provide an example of a violation of this principle in Figure 3.3. We start with a higher-level model, in panel (a), in which inequality affects democratization through mobilization. We then elaborate the model in panel (b) by adding ethnic homogeneity as a moderator of mobilization’s effect. However, because ethnic homogeneity is also modeled here as affecting inequality, we have now introduced a source of dependence between inequality and democratization that was omitted from the higher-level model. In panel (a), democratization and inequality were dependent only via mobilization; and so they are conditionally independent given mobilization. In panel (b), democratization and mobilization are additionally dependent via their common cause, ethnic homogeneity. By the rules governing causal graphs (see Chapter ), the higher-level specifically prohibited this second source of dependency—since all dependencies between variables must be represented. Put differently, if two variables are independent— or conditionally independent given a third variable—in one model, then this same relation of independence (or conditional independence) must be captured in any theory of that model. A theory can add conditional independencies not present in the higher-level model. For instance, a mediation theory, \\(X \\rightarrow M \\rightarrow Y\\), implies a conditional independence that is not present in the higher-level model that it supports, \\(X \\rightarrow Y\\): in the lower-level model only, \\(X\\) is conditionally independent of \\(Y\\) given \\(M\\). But we may not theorize away (conditional) independencies insisted on by our higher-level claim. 3.3.2 Moving up levels Moving in the other direction, what, in general, are the permissible simplifications of lower-level models? In other words, given a theory, what are the higher-level claims that it can support? When we move up a level — i.e., eliminate one or more nodes — the key rule is that the higher-level graph must take into account: all dependencies among remaining nodes and all variation generated by the eliminated node. We can work out what this means, separately, for eliminating endogenous nodes and for eliminating exogenous nodes. Eliminating endogenous nodes Eliminating an endogenous node means removing a node with parents (direct causes) represented on the graph. If the node also has one or more children, then the node captures a dependency: it links its parents to its children. When we eliminate this node, preserving these dependencies requires that all of the eliminated node’s parents adopt—become parents of—all of the eliminated node’s children. Thus, for instance in panel (b) of Figure 3.1, if we were to eliminate \\(M\\), \\(M\\)’s parents (\\(X\\) and \\(\\theta^M\\)) need to adopt \\(M\\)’s child, \\(Y\\). We see in panel (a) of the figure, the higher-level model, that \\(X\\) is now pointing directly into \\(Y\\). As for \\(\\theta^M\\), it too must now point directly into \\(Y\\)—though we can use a bit of shorthand to make this happen. Recall that \\(\\theta^M\\) represents the part of \\(M\\) that is randomly determined. Rather than drawing two separate disturbance (\\(\\theta\\)) terms pointing into \\(Y\\), however, we more simply represent the combined disturbance term as \\(\\theta^Y_{\\text{higher}}\\), with the ‘’higher’’ signaling the aggregation of roots. (This is, of course, simply reversing the disaggregation that we undertook earlier to move from the higher- to the lower-level model.) More intuitively, when we simplify away a mediator, we need to make sure that we preserve the causal relationships being mediated—both those among substantive variables and any random shocks at the mediating causal steps.24 Eliminating exogenous nodes What about eliminating exogenous nodes—nodes with no parents? For the most part, exogenous nodes cannot be eliminated, but must either be replaced by or incorporated into \\(U\\) (or \\(\\theta\\)) terms. The reason is that we need preserve any dependencies or variation generated by the exogenous node. Figure 3.4 walks through four different situations in which we might want to simplify away the exogenous node, \\(X\\). (Here we use the more generic \\(U\\) notation, though the same principles apply if these are type-receptacles(\\(\\theta\\).) Figure 3.4: Here we represent the basic principles for eliminating exogenous nodes. Multiple children. In (a1), we start with a lower-level model in which \\(X\\) has two children, thus generating a dependency between \\(W\\) and \\(Y\\). If we eliminate \\(X\\), we must preserve this dependency. We can do so, as pictured in (a2), by replacing \\(X\\) with a \\(U\\) term that also points into \\(W\\) and \\(Y\\).25 Though we are no longer specifying what it is that connects \\(W\\) and \\(Y\\), the correlation itself is retained. Substantive spouse. In (b1), \\(X\\) has a spouse that is substantively specified, \\(W\\). If we eliminate \\(X\\), we have to preserve the fact that \\(Y\\) is not fully determined by \\(W\\); something else also generates variation in \\(Y\\). We thus need to replace \\(X\\) with a \\(U\\) term, \\(U_Y\\), to capture the variation in \\(Y\\) that is not accounted for by \\(W\\). \\(U\\)-term spouse. In (c1), \\(X\\) has a spouse that is not substantively specified, \\(U^{Y_\\text{lower}}\\). Eliminating \\(X\\) requires, again, capturing the variance that it generates as a random input. As we already have a \\(U\\) term pointing only into \\(Y\\), we can substitute in \\(U^{Y_\\text{higher}}\\), which represents both \\(U^{Y_\\text{lower}}\\) and the variance generated by \\(X\\).IS THIS FOOTNOTE RIGHT?26 One child, no spouse. In (d1), \\(X\\) has only one child and no spouse. Here we can safely eliminate \\(X\\) with no loss of information. It is always understood that every exogenous node has some cause, and there is no loss of information in simply eliminating a node’s causes if those causes are exogenous and do not affect other endogenous nodes in the model. In (d2) we are simply not specifying \\(Y\\)’s cause, but we have not lost any dependencies or sources of variance that had been expressed in (d1). One interesting effect of eliminating a substantive exogenous node can be to render seemingly deterministic relations effectively probabilistic. In moving from (b1) to (b2), we have taken a component of \\(Y\\) that was determined by \\(X\\) and converting it into a random disturbance. Just as we can explain a more probabilistic claim with a less probabilistic theory, we can derive higher-level claims with greater probabilism from theories with greater determinism. Figure 3.5: A lower-level model from which multiple higher level models can be derived. Figure 3.6: Higher level models derived from the lower level model of Figure X. Nodes that are eliminated are marked in grey; circles denote exogenous nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two other nodes could equivalently be indicated as an undirected edge connecting the two.) Note that \\(M\\), \\(R\\), and \\(D\\) are deterministic functions of \\(I\\) and \\(E\\) in this example. We can apply these principles to a model of any complexity. We illustrate a wider range of simplifications by starting with Figure 3.5, which represents a somewhat amended version of our inequality and democratization model from Chapter 2, with more complex causal relations. Then, in Figure 3.6, we show all permissible reductions of the more elaborate model. We can think of these reductions as the full set of simpler claims (involving at least two nodes) that can be derived from the lower-level theory. In each subgraph, we mark eliminated nodes in grey; those nodes that are circled must be replaced with \\(U\\) terms; and arrows represent the causal dependencies that must be preserved. Note, for instance, that neither \\(E\\) (because it has a spouse) nor \\(I\\) (because it has multiple children) can be simply eliminated; each must be replaced with a \\(U\\) term. Also, the higher-level graph with nodes missing can contain arrows that do not appear at all in the lower-level graph: eliminating \\(M\\), for instance, forces an arrow running from \\(X\\) to \\(R\\) and another running from \\(X\\) to \\(Y\\), as \\(X\\) must adopt \\(M\\)’s children. The simplest elimination is of \\(D\\) itself since it does not encode any dependencies between other variables. We can also read Figure 3.6 as telling us the set of claims for which the lower-level graph in Figure can serve as a theory. As we can see, the range of claims that a moderately complex model can theorize is vast. For each simpler claim, moreover, there may be other possible lower-level graphs—theories besides —consistent with it. 3.3.2.1 Conditioning on nodes A further permissible “upward” move is conditioning on a node. When we condition on a node, we are restricting the higher-level model in scope to situations in which that node’s value is held constant. Doing so allows us to eliminate the node as well as all arrows pointing into it or out of it. Consider three different situations in which we might condition on a node: Exogenous, with multiple children. In simplifying (a1) in Figure 3.4, we need to be sure we retain any dependence that \\(X\\) generates between \\(W\\) and \\(Y\\). However, recalling the rules of conditional independence on a graph (see Chapter ), we know that \\(W\\) and \\(Y\\) are independent conditional on \\(X\\). Put differently, if we restrict the analysis to contexts in which \\(X\\) takes on a constant value, the lower-level model implies that \\(Y\\) and \\(W\\) will be uncorrelated across cases. As fixing \\(X\\)’s value breaks the dependence between \\(Y\\) and \\(W\\), we can drop \\(X\\) (and the arrows pointing out of it) without having to represent that dependence. Exogenous, with spouse. In simplifying (b1) or (c1) in Figure 3.4, we need to account for the variation generated by \\(X\\). If we fix \\(X\\)’s value, however, then we eliminate this variation by assumption and do not need to continue to represent it (or the arrow pointing out of it) on the graph. Endogenous. When we condition on an endogenous node, we can eliminate the node as well the arrows pointing into and out of it. We, again, leverage relations of conditional independence here. If we start with graph (b) in Figure 3.1, and we condition on the mediator, \\(M\\), we sever the link between \\(Y\\) on \\(X\\), rendering them conditionally independent of one another. We can thus remove \\(M\\), the arrow from \\(X\\) to \\(M\\), and the arrow from \\(M\\) to \\(Y\\). In the new model, with \\(M\\) fixed, \\(Y\\) will be entirely determined by the random disturbance \\(\\theta^{Y_\\text{lower}}\\).27 In sum, we can work with models that are simpler than our causal beliefs: we may believe a complex lower-level model to be true, but we can derive from it a sparer set of claims. There may be intervening causal steps or features of context that we believe matter, but that are not of interest for a particular line of inquiry. While these can be removed, we nonetheless have to make sure that their implications for the relations remaining in the model are not lost. Understanding the rules of reduction allow us to undertake an important task: checking which simpler claims are and are not consistent with our full belief set. 3.4 Conclusion We close this chapter by considering how the understanding of theory that we work with in this book compares to other prominent understandings of theory. Theory as tautology. The claim that the number of Nash equilibria is generically odd in finite games is often understood to be a theoretical claim. Unless there are errors in the derivation of the result, the claim is true in the sense that the conclusions follow from the assumptions. There is no evidence that we could go looking for in the world to assess the claim. The same can be said of the theoretical claims of many formal models in social sciences; they are theoretical deductions of the if-then variety (Clarke and Primo 2012). Theory in this sense is true by tautology. By contrast, theory as we define it in this book refers to claims with empirical content: a theory refers to causal relations in the world that might or might not hold, and is susceptible to empirical testing. The deductive logical relations that hold in a causal model are those of conditional independence, as discussed in Chapter 2: for instance, if \\(X\\) causes \\(Y\\) only through \\(M\\) in a theory, then \\(X\\) and \\(Y\\) are conditionally independent given some value of \\(M\\). Theory as a collection of maps. According to Clarke and Primo (2012), building on a semantic view of theory (Giere (2010)), a theory is a collection of models, together with a set of hypotheses linking them to the real world. As in our usage, Clarke and Primo see theories and models as very similar objects: for them, a theory is a system of models; for us, a theory is a supporting model. In both frameworks, there is no real difference in kind between models and theories. Our approach also shares with Clarke and Primo the idea that models are not full and faithful reflections of reality; they are maps designed for a particular purpose. In the case of causal models, the purpose is to capture relationships of independence and possible causal dependence. As we have shown, that is a purpose that allows for the stripping away of detail—though it also forbids certain simplifications (such as any simplification that removes a dependency between variables). Clarke and Primo see models as useful to the extent that they are similar to features of the real world in ways related to the model’s purpose. Along these lines, a causal model will be useful to the extent that it posits relations of independence that are similar to those prevailing in the domain under investigation. Theory as a testable claim In the hypothetico-deductive framework, often traced back to Popper (2002) and highly influential in empirical political science, empirical social science is an activity of theory-testing. Having developed a theory, we then derive from it a set of empirical predictions and then test those predictions against evidence. In Clarke and Primo (2012), we also seek to confirm theories by developing and testing hypotheses about the similarity of a model or theory to particular features of the world. In both cases, a theory is posited—possibly on the basis of logic or background knowledge—and then assessed. The value (truth or usefulness) of the model itself is the object of inquiry. In a causal-model framework, theories are always tentative, and we can subject any model or theory to empirical evaluation, a task to which we turn in Chapter 15. However, in the book’s setup, theories are first and foremost expressions of what we already know and don’t know about a given causal domain when inquiry begins. We encode this background knowledge in order to inform research-design choices and draw inferences from the data. Models and theories are thus, in this sense, the world within which inquiry unfolds. Indeed, as we explore in Chapter 4, the very questions we ask live within—can be represented as parts of—our theories. Theory as generalization In another of the many uses of “theory,” political scientists often think of theorization as generalization. For Van Evera (1997) and Przeworski and Teune (1970), for instance, theories are by their nature general statements that we can use to explain specific events. In this view, “Diamond resources caused Sierra Leone’s civil war” is a case-specific explanation; “Natural resource endowments cause civil war” is a theoretical formulation. In our treatment of theory as a lower-level causal model, however, there is no generic sense in which a theory is more or less general than the higher-level claim that it explains. In this book’s framework, we can theorize by generalizing: when we elaborate a model by building in variation in a factor that was held constant in the higher-level claim, we are making the model more general in scope. If our natural resources claim implicitly applies only to weak states, we can theorize this claim by allowing state strength to vary and articulating how the natural-resource effect hinges on that claim. However, when we theorize by disaggregating nodes—say, by adding intervening causal steps—we have in fact made a more specific claim. Natural resources may cause civil war under a broad set of circumstances. Natural resources will cause civil war through looting by rebel groups under an almost certainly narrower set of circumstances. Here, the more elaborate argument—the theorization of why \\(X\\) causes \\(Y\\)—is actually a stronger claim, with narrower scope, than the simpler one that it supports. The value of parsimony Van Evera (1997) and Przeworski and Teune (1970) also express a common view in characterizing parsimony as a quality of good theory. While they recognize that parsimony must often be traded off against other goods, such as accuracy and generality, ceteris paribus a more parsimonious theory—one that uses fewer causal variables to explain variation in a given outcome—is commonly understood to be a better theory. We do not take issue with the idea that simpler models and explanations are, all else equal, better. But the succeeding chapters also demonstrate a distinctive and important way in which all else will often not be equal when we seek to use theory to guide research design and support causal inference. To foreshadow the argument to come, the elaboration of more detailed, lower-level models can direct us to new opportunities for learning. As we unpack a higher-level claim, we will often be identifying additional features of a phenomenon the observation of which can shed light on causal questions of interest. Moreover, our background beliefs—the prior knowledge on which causal inference must usually rest—are often more informative at lower levels than at higher levels: it will, for instance, often be easier for us express beliefs about causal effects for smaller steps along a causal chain than about an overarching \\(X \\rightarrow Y\\) effect. Making things more complicated, of course, still makes things more complicated. And we should avoid doing so when the payoff is small, as it will sometimes be. But in the pages to come, we will also see a distinct set of benefits that arise from drilling more deeply into our basis of prior knowledge when formulating inferential strategies. 3.5 Chapter Appendices 3.5.1 Summary Boxes BOX 1 Two kinds of theories. Theories are “lower-level” causal models that explain or provide an account of a “higher-level”, simpler model. There are two forms of theorization: The disaggregation of nodes. A single node in a higher-level model can be split into multiple nodes. For instance, for a higher-level model in which \\(X \\rightarrow Y \\leftarrow \\theta^Y\\): Mediation: A mediator, \\(M\\), can be introduced between \\(X\\) and \\(Y\\), thus splitting \\(\\theta^Y\\) into \\(\\theta^M\\) and \\(\\theta^{Y_\\text{lower}}\\). The mediation theory thus explains the \\(X \\rightarrow Y\\) relationship. Moderation: A component of \\(\\theta^Y\\) can be extracted and specified as a substantive variable. This variable is now a substantively conceptualized moderator of the \\(X \\rightarrow Y\\) relationship. The moderation theory thus provides a fuller explanation of why \\(X\\) has different effects on \\(Y\\) in different contexts. Generalization. A feature of context omitted and implicitly held constant in a higher-level model can be explicitly included in the model. The higher-level model is now explained as a special case of a more general set of causal relations. BOX 2 Rules for moving between levels Moving down levels: All (conditional) independencies represented in a higher-level model must be preserved in the lower-level model. When we disaggregate or add nodes to a model, new conditional independencies can be generated. But any variables that are independent or conditionally independent (given a third variable) in the higher-level model must also be independent or conditionally independent in the lower-level model. Moving up levels: We can move up levels by eliminating an exogenous node, eliminating an endogenous node, or conditioning on a node. When we eliminate a node from a model, we must preserve any variation and dependencies that it generates: When eliminating an endogenous node, that node’s parents adopt (become direct causes of) that node’s children. When eliminating an exogenous node, we must usually replace it with a \\(U\\) term. If the node has more than one child, it must be replaced with a \\(U\\) term pointing into both children (or an undirected edge connecting them) to preserve the dependency between its children. If the node has a spouse, the eliminated node’s variation must also be preserved using a \\(U\\) term. Where the spouse is (already) a \\(U\\) term with no other children, \\(U\\) terms can be combined. Since conditioning on a node “blocks” the path through which it connects its children, we can simply eliminate the node and the arrows between it and its children. An exogenous node with no spouse and only one child can be simply eliminated. 3.5.2 Illustration of a Mapping from a Game to a DAG Our running example supports a set of higher level models, but it can also be implied by a lower level models. Here we illustrate with an example in which the lower level model is a game theoretic model, together with a solution.28 In Figure 3.7 we show a game in which nature first decides on the type of the media and the politician – is it a media that values reporting on corruption or not? Is the politician one who has a dominant strategy to engage in corruption or one who is sensitive to the risks of media exposure? In the example the payoffs to all players are fully specified, though for illustration we include parameter \\(b\\) in the voter’s payoffs which captures utility gains from sacking a politician that has had a negative story written about them whether or not they actually engaged in corruption. A somewhat less specific, though more easily defended, theory would not specify particular numbers as in the figure, but rather assume ranges on payoffs that have the same strategic implications. The theory is then the game plus a solution to the game. Here for a solution the theory specifies subgame perfect equilibrium. In the subgame perfect equilibrium of the game; marked out on the game tree (for the case \\(b=0\\)) the sensitive politicians do not engage in corruption when there is a free press – otherwise they do; a free press writes up any acts of corruption, voters throw out the politician if indeed she is corrupt and this corruption is reported by the press. As with any structural model, the theory says what will happen but also what would happen if things that should not happen indeed happened. Figure 3.7: A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature’s move for the case in which \\(b=0\\). To draw this equilibrium as a DAG we include nodes for every action taken, nodes for features that determine the game being played, and the utilities at the end of the game. If equilibrium claims are justified by claims about the beliefs of actors then these could also appear as nodes. To be clear however these are not required to represent the game or the equilibrium, though they can capture assumed logics underlying the equilibrium choice. For instance a theorist might claim that humans are wired so that whenever they are playing a “Stag Hunt” game they play “defect.” The game and this solution can be represented on a DAG without reference to the beliefs of actors about the action of other players. However, if the justification for the equilibrium involves optimization given the beliefs of other players, a lower level DAG could represent this by having a node for the game description that points to beliefs about the actions of others, that then points to choices. In a game with dominant strategies, in contrast, there would be no arrows from these beliefs to actions. For our running example, nodes could usefully include the politician’s expectations, since the government’s actions depend on expectations of the actions of others. However, given the game there is no gain from including the media’s expectations of the voter’s actions since in this case the media’s actions do not depend on expectations of the voters actions then these expectations should be included. In Figure 3.8 we provide two examples of DAGs that illustrate lower level models that support our running example. The upper panel gives a DAG reflecting equilibrium play in the game described in Figure 3.7. Note that in this game there is an arrow between \\(C\\) and \\(Y\\) even though \\(Y\\) does not depend on \\(C\\) for some values of \\(b\\)—this is because conditional independence requires that two variables are independent for all values of the conditioning set. For simplicity also we mark \\(S\\) and \\(X\\), along with \\(b\\) as features that affect which subgame is being played—taking the subgames starting after Nature’s move. Note that the government’s expectations of responses by others matters, but the expectations of other players do not matter given this game and solution. Note that the utilities appear twice in a sense. They appear in the subgame node, as they are part of the definition of the game–though here they are the utilities that players expect at each terminal node; when they appear at the end of the DAG they are the utilities that actually arise (in theory at least). The lower level DAG is very low and much more general, representing the theory that in three player games of complete information, players engage in backwards induction and choose the actions that they expect to maximize utility given their beliefs about the actions of others. The DAG assumes that players know what game is being played (“Game”), though this could also be included for more fundamental justification of behavioral predictions. Each action is taken as a function of the beliefs about the game, the expectations about the actions of others, and knowledge of play to date. The functional equations—not shown—are given by optimization and belief formation assuming optimization by others. Figure 3.8: The upper panel shows a causal graph that describes relations between nodes suggested by analysis of the game in Figure and which can imply the causal graph of Figure . The game itself (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors. The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.’’ These lower level graphs can themselves provide clues for assessing relations in the higher level graphs. For instance, the lower level model might specify that the value of \\(b\\) in the game affects the actions of the government only through their beliefs about the behavior of voters, \\(E\\). These beliefs may themselves have a stochastic component, \\(U_E\\). Thus \\(b\\) high might be thought to reduce the effect of media on corruption. For instance if \\(b \\in \\mathbb{R}_+\\), we have \\(C= 1-FG(1-\\mathbb{1}(b&gt;1))\\). If \\(X\\) is unobserved and one is interested in whether \\(S=0\\) caused corruption, knowledge of \\(b\\) is informative. It is a root node in the causal estimand. If \\(b&gt;1\\) then \\(S=0\\) did not cause corruption. However if \\(b\\) matters only because of its effect on \\(E\\) then the query depends on \\(U_E\\). In this case, while knowing \\(b\\) is informative about whether \\(S=0\\) caused \\(C=1\\), knowing \\(E\\) from the lower level graph is more informative. Note that the model we have examined here involves no terms for \\(U_C\\), \\(U_R\\) and \\(U_Y\\)—that is, shocks to outcomes given action. Yet clearly any of these could exist. One could imagine a version of this game with “trembling hands,” such that errors are always made with some small probability, giving rise to a much richer set of predictions. These can be represented in the game tree as moves by nature between actions chosen and outcomes realized. Importantly in a strategic environment such noise could give rise to different types of conditional independence. For instance say that a Free Press only published its report on corruption with probability \\(\\pi^R\\), then with \\(\\pi^R\\) high enough the sensitive government might decide it is worth engaging in corruption even if there is a free press; in this case the arrow from \\(X\\) to \\(C\\) would be removed. Interestingly in this case as the error rate rises, \\(R\\) becomes less likely, meaning that the effect of a \\(S\\) on \\(Y\\) becomes gradually weaker (since governments that are not sensitive become more likely to survive) and then drops to 0 as sensitive governments start acting just like nonsensitive governments. References "],
["questions.html", "Chapter 4 Causal Questions 4.1 Case-level causal effects 4.2 Case-level causal attribution 4.3 Case-level explanation 4.4 Average causal effects 4.5 Causal Paths", " Chapter 4 Causal Questions Although a lot of empirical work focuses on identifying average causal effects, there is a rich array of other well defined causal questions that can be asked about how variables relate to each other causally. We decribe major families of question and illustrate how these can all be described as questions about the values of nodes in a causal model. The study of causation is central to most empirical social science, whether quantitative analyses of large sets of cases or qualitative, small-\\(N\\) case studies. Yet a general interest in causality masks tremendous heterogeneity in the kinds of causal questions that scholars tend to ask. Returning to our inequality and democratization example, we might seek, for instance, to know inequality’s average impact on democratization across some set of cases. Alternatively, we might be interested in a particular case—say, Mongolia in 1995—and want to know whether this is a context in which inequality has an effect—a question about causal effects at the case level. Relatedly—but distinctly—we might wonder whether the level of democracy in Mongolia in 1995 is causally attributable to the level of inequality in that case. And we may be interested in how causal effects unfold, inquiring about the pathway or mechanism through which inequality affects democratization—a question we can also ask at two levels. We can ask whether inequality affected democratization in Mongolia through mobilization of the masses; and we can ask how commonly inequality affects democratization through mobilization across a broad set of cases. Rather separate methodological literatures have been devoted to the study of average causal effects, the analysis of case-level causal effects and explanations, and the identification of causal pathways. It is typically understood that their analysis requires quite distinct sets of tools. In this chapter, we take a key integrative step in showing that each of these queries can be readily captured in a causal model. More specifically, we demonstrate how causal queries can be represented as question about one or more nodes on a causal graph. When we assimilate our causal questions into a causal model, we are placing what we want to know in formal relation to both what we already know and what we can potentially observe. As we will see in later chapters, this move allows us then to deploy the model to generate strategies of inference: to determine which observations, if we made them, would be likely to yield the greatest leverage on our query, given our prior knowledge about the way the world works. And by the same logic, once we see the evidence, this integration allows us to “update” on our query—figure out in systematic fashion what we have learned—in a manner that takes background knowledge into account. In the remainder of this chapter, we walk through the conceptualization and causal-model interpretation of five key causal queries: Case-level causal effects Case-level causal attribution Case-level explanation Average causal effects Causal pathways These five are not exhaustive of the causal questions that can be captured in causal graphs, but they are among the more common foci of social scientific investigation. –&gt; 4.1 Case-level causal effects The simplest causal question is whether some causal effect operates in an individual case. Does \\(X\\) have an effect on \\(Y\\) in this case? For instance, is Yemen in 1995 a case in which a change in economic inequality would produce a change in whether or not the country democratizes? We could put the question more specifically as a query about a causal effect in a particular direction, for instance: Does inequality have a positive effect on democratization in the case of Yemen in 1995? In counterfactual terms, a query about case-level causation is a question about what would happen if we could manipulate a variable in the case: if we could hypothetically manipulate \\(X\\)’s value in the case, would \\(Y\\)’s value also change? To ask whether a positive (or negative) effect operates for a case is to ask whether a particular counterfactual relation holds in that case. If we assume a binary setup for simplicity, to ask whether inequality has a positive effect on democratization is to ask: if we set \\(I\\) to \\(0\\) would \\(D\\) take on a value of \\(0\\), and if we set \\(I\\) to \\(1\\), would \\(D\\) take on a value of \\(1\\)? (Both of these conditions must hold for \\(I\\) to have a positive effect on \\(D\\).) We can easily represent this kind of query in the context of a causal model. We show the DAG for such a model in Figure . As introduced in Chapter 3, \\(\\theta^Y\\) here represents the causal type characterizing \\(Y\\)’s response to \\(X\\) and, if \\(X\\) and \\(Y\\) are binary, can take on one of four values: \\(\\theta^Y_{10}\\), \\(\\theta^Y_{01}\\), \\(\\theta^Y_{00}\\), and \\(\\theta^Y_{11}\\) (which map onto our original \\(a, b, c\\) and \\(d\\) types). Importantly, given that the value of nodes (or variables) is allowed to vary across cases, this setup allows for \\(\\theta_Y\\)—the causal effect of \\(X\\) on \\(Y\\)—to vary across cases. Thus, \\(X\\) may have a positive effect on \\(Y\\) in one case (with \\(\\theta^Y=\\theta^Y_{01}\\)), \\(X\\) may have a negative (\\(\\theta^Y=\\theta^Y_{10}\\)) or no effect (\\(\\theta^Y=\\theta^Y_{00}\\) or \\(\\theta^Y_{11}\\)) on \\(Y\\) in other cases. Figure 4.1: This DAG is a graphical representation of the simple causal setup in which the effect of \\(X\\) on \\(Y\\) in a given case depends on the case’s causal type, represented by \\(\\theta^Y\\). With a single binary causal variable of interest, we let \\(\\theta_Y\\) take on values \\(\\theta^Y_{ij}\\), with \\(i\\) representing the value \\(Y\\) takes on if \\(X=0\\) and \\(j\\) representing the value \\(Y\\) takes on if \\(X=1\\). With a binary framework outcome, \\(\\theta^Y\\) ranges over the four values: \\(\\theta^Y_{00}\\), \\(\\theta^Y_{10}\\), \\(\\theta^Y_{01}\\) and \\(\\theta^Y_{11}\\). In this model, then, the query, “What is \\(X\\)’s causal effect in this case?” simply becomes a question about the value of \\(\\theta_Y\\). Interpreted as “what is the expected effect of \\(X\\) on \\(Y\\)?” the question becomes one of estimating \\(\\Pr(\\theta^Y = \\theta^Y_{01}) - \\Pr(\\theta^Y = \\theta^Y_{10})\\). Similarly in a mediation model of the form \\(X\\rightarrow M \\rightarrow Y\\), like that discussed in Chapter 2, the question “What is the the expected effect of \\(X\\) on \\(Y\\)?” requires estimating \\[\\Pr((\\theta^M = \\theta^M_{01} &amp; \\theta^Y = \\theta^Y_{01}) | (\\theta^M = \\theta^M_{10} &amp; \\theta^Y = \\theta^Y_{10})) - \\Pr((\\theta^M = \\theta^M_{01} &amp; \\theta^Y = \\theta^Y_{10}) | (\\theta^M = \\theta^M_{10} &amp; \\theta^Y = \\theta^Y_{01}))\\] Of course, these \\(\\theta\\)s are not directly observable: causal types are intrinsically unobserved properties of cases. So, as we will see in later chapters, research design becomes a challenge of determining which observable nodes in the graph are potentially informative about the unobservable nodes that constitute our causal queries. FLAG: SPELL OUT ALL ESTIMANDS AS COLLECTIONS OF CAUSAL TYPES 4.2 Case-level causal attribution A query about causal attribution is related to, but different from, a query about a case-level causal effect. When asking about \\(X\\)’s case-level effect, we are asking, “Would a change in \\(X\\) cause a change in \\(Y\\) in this case?” The question of causal attribution is slightly different: “Did \\(X\\) cause \\(Y\\) to take on the value it did in this case?” More precisely, we are asking, “Given the values that \\(X\\) and \\(Y\\) in fact took on in this case, would \\(Y\\)’s value have been different if \\(X\\)’s value had been different?” For instance, given that we know that inequality in Taiwan was relatively low and that Taiwan democratized in 1996, was low inequality a cause of Taiwan’s democratization in 1996? Put differently, given low economic inequality and democratization in Taiwan in 1996, would the outcome in this case have been different if inequality had been high? This goes beyond simply asking whether Taiwan is a case in which inequality has a causal effect on democratization. Whereas a case-level causal effect is defined in terms of a single \\(\\theta\\) node, we define a causal-attribution query in terms of a larger set of nodes. To attribute \\(Y\\)’s value in a case to \\(X\\), we need to know not only whether this is the kind of case in which \\(X\\) could have an effect on \\(Y\\) but also whether the context is such that \\(X\\)’s value in fact made a difference. Consider, for instance, the general setup in Figure 4.2. Here, \\(Y\\) is a function of two variables, \\(X\\) and \\(W\\). This means that \\(\\theta^Y\\) is somewhat more complicated than in a setup with one causal variable: \\(\\theta^Y\\) must here define \\(Y\\)’s response to different combinations of two other variables, \\(X\\) and \\(W\\), since both of these variables point directly into \\(Y\\). Thus, \\(\\theta^Y\\) must cover the full set of possible causal interactions between two binary causal variables. Figure 4.2: This DAG is a graphical representation of the simple causal setup in which \\(Y\\) depends on two variables \\(X1\\) and \\(X2\\). How \\(Y\\) responds to X1 and X2 depnds on \\(\\theta^Y\\), the DAG itself does not provide information on whether or how X1 and X2 interact with each other. We already saw the set of causal types for a set up like this in Chapter 2 (see Table 2.3). In the table, there are four column headings representing the four possible combinations of \\(X1\\) and \\(X2\\) values. Each row represents one possible pattern of \\(Y\\) values as \\(X1\\) and \\(X2\\) move through their four combinations. Labelling is a little difficult with so many types. One approach used in Chapter 2 is to represent change in \\(X1\\) on the horizontal axis, and change in the second variable, \\(X2\\), on the vertical axis. The value of \\(X1\\) increases from 0 to 1 as we move to the right (from \\(i\\) to \\(j\\) or from \\(g\\) to \\(h\\)). And the value of \\(X2\\) increases from 0 to 1 as we move up (from \\(i\\) to \\(g\\) or from \\(j\\) to \\(h\\)). One way to conceptualize the size of the causal-type “space” is to note that \\(X1\\) can have any of our four causal effects (the four binary types) on \\(Y\\) when \\(X2=0\\); and \\(X1\\) can have any of four causal effects when \\(X2=1\\).29 This yields 16 possible response patterns to combinations of \\(X1\\) and \\(X2\\) values. A query about causal attribution—whether \\(X1 = 1\\) caused \\(Y=1\\)–for the model in in Figure 4.2, would be defined in terms of both \\(X2\\) and \\(\\theta_Y\\). Parallel to our Taiwan example, suppose that we have a case in which \\(Y=1\\) and in which \\(X1\\) was also 1, and we want to know whether \\(X1\\) caused \\(Y\\) to take on the value it did. Answering this question requires knowing whether the case’s type is such that \\(X1\\) would have had a positive causal effect on \\(Y\\), given the value of \\(X2\\) (which we might think of as the context). Thus, given that we start with knowledge of \\(X1\\)’s and \\(Y\\)’s values, our query about causal attribution amounts to a query about two nodes on the graph: (a) the value of \\(X2\\) and (b) whether the value of \\(\\theta^Y\\) is such that \\(X1\\) has a positive causal effect given \\(X2\\)’s value. Suppose, for instance, that we were to observe \\(X2=1\\). We then need to ask whether the causal type, \\(\\theta_Y\\), is such that \\(X1\\) has a positive effect when \\(X2=1\\). Consider type 8, or \\(\\theta_{01}^{11}\\). This is a causal type in which \\(X1\\) has a positive effect when \\(X2=0\\) but no effect when \\(X2=1\\). Put differently, \\(X2=1\\) is a sufficient condition for \\(Y=1\\), meaning that \\(X1\\) makes no difference to the outcome when \\(X2=1\\). In all we have four qualifying types: \\(\\theta_{00}^{01}\\), \\(\\theta_{01}^{01}\\), \\(\\theta_{10}^{01}\\), and \\(\\theta_{11}^{01}\\) (or 2, 4, 10, and 12). In other words, we can attribute a \\(Y=1\\) outcome to \\(X1=1\\) when \\(X2=1\\) and the causal type is one of these four. By parallel reasoning, we can also attribute a \\(Y=1\\) outcome to \\(X1=1\\) when \\(X2=0\\) and the causal type is any of \\(\\theta_{01}^{00}\\), \\(\\theta_{01}^{01}\\), \\(\\theta_{01}^{10}\\), and \\(\\theta_{01}^{11}\\). Thus, a question about causal attribution is a question about the joint value of a set of nodes: about whether the combination of context and causal type is such that changing \\(X\\) would have changed the outcome. 4.3 Case-level explanation So far we have been dealing with causes in the standard counterfactual sense: antecedent conditions a change in which would have produced a different outcome. Sometimes, however, we are interested in identifying antecedent conditions that were not counterfactual difference-makers but that nonetheless generated or produced the outcome. Consider, for instance, a situation in which an outcome was overdetermined: multiple conditions were present, each of which on their own, could have generated the outcome. Then none of these conditions caused the outcome in the counterfactual sense; yet one or more of them may have been distinctively important in producing the outcome. The concept of an actual cause may be useful in putting a finer point on this kind of causal question. Let us first approach the concept at an intuitive level. An antecedent condition, \\(A\\), that played a role in generating an outcome might not be a counterfactual cause because, had it not occurred, some second chain of events set in motion by \\(B\\) would have unfolded, generating the outcome anyway. In the standard counterfactual scenario, \\(A\\) is not a counterfactual cause: take away \\(A\\) and the outcome still happens because of the chain of events emanating from \\(B\\). Yet let us imagine that the fact that \\(A\\) did occur prevented part of \\(B\\)’s chain of consequences from unfolding and itself producing the outcome. Then let us imagine a tweaked counterfactual comparison in which we fix the observed fact that \\(B\\)’s causal sequence did not fully unfold. We can then ask: conditional on \\(B\\)’s sequence not fully unfolding, would \\(A\\) have been a counterfactual cause of the outcome? If so, then we say that \\(A\\) is an “actual cause”\" of the outcome. We have, in a sense, identified \\(A\\) as distinctively important in the production of the outcome, even if it was not a case-level cause in the usual sense. More formally, and using the definition provided by (Halpern 2015), building on (Halpern and Pearl 2005) and others, we say that a condition (\\(X\\) taking on some value \\(x\\)) was an actual cause of an outcome (of \\(Y\\) taking on some value \\(y\\)), where \\(x\\) and \\(y\\) may be collections of events, if: \\(X=x\\) and \\(Y=y\\) both happened there is some set of variables, \\(\\mathcal W\\), such that if they were fixed at the levels that they actually took in the case, and if \\(X\\) were to be changed, then \\(Y\\) would change (where \\(\\mathcal W\\) can also be an empty set) no strict subset of \\(X\\) satisfies 1 and 2 (there is no redundant part of the condition, \\(X=x\\)) The definition thus describes a condition that would have been a counterfactual cause of the outcome if we were to imagine holding constant some set of events that in fact occurred (and that, in reality, might not have been constant if the actual cause had not in fact occurred). A motivating example used in much of the literature on actual causes (e.g. Hall 2004) imagines two characters, Sally and Billy, simultaneously throwing stones at a bottle. Both are great shots and hit whatever they aim at. Sally’s stone hits first, and so the bottle breaks. However, Billy’s stone would have hit had Sally’s not hit, and would have broken the bottle. Did Sally’s throw cause the bottle to break? Did Billy’s? By the usual definition of causal effects, neither Sally’s nor Billy’s action had a causal effect: without either throw, the bottle would still have broken. We commonly encounter similar situations in the social world. We observe, for instance, the onset of an economic crisis and the breakout of war—either of which would be sufficient to cause the government’s downfall—but with the economic crisis occurring first and toppling the government before the war could do so. Yet neither economic crisis nor war made a difference to the outcome. To return to the bottle example, while neither Sally’s nor Billy’s throw is a counterfactual cause, there is an important sense in which Sally’s action obviously broke the bottle, and Billy’s did not. This intuition is confirmed by applying the definition above. Consider first the question: Did Sally’s throw break the bottle? Conditions 1 and 3 are easily satisfied, since Sally throw and the bottle break (Condition 1), and “Sally threw” has no strict subsets (Condition 3). Condition 2 is met if Sally’s throw made a difference, counterfactually speaking; and in determining this, we are permitted to condition on (to fix in the counterfactual comparison) any event or set of events that actually happened (or on on none at all). To see why Condition 2 is satisfied, we have to think of there being three steps in the process: Sally and Billy throw, Sally’s or Billy’s rock hits the bottle, and the bottle breaks. In actuality, Billy’s stone did not hit the bottle. And conditioning on this actually occurring event (Billy’s stone not hitting), the bottle would not have broken had Sally not thrown. From the perspective of counterfactual causation, it may seem odd to condition on Billy’s stone not hitting the bottle when thinking about Sally not throwing the stone since Sally’s throwing the stone was the very thing that prevented Billy from hitting the bottle. Yet Halpern argues that this is an acceptable thought experiment for establishing the importance of Sally’s throw since conditioning is constrained to the actual facts of the case. Moreover, the same logic shows why Billy is not an actual cause. The reason is that Billy’s throw is only a cause in those conditions in which Sally did not hit the bottle. But because Sally actually hit the bottle, we are not permitted to condition on Sally not hitting the bottle in determining actual causation. We thus cannot—even through conditioning on actually occurring events—construct any counterfactual comparison in which Billy’s throw is a counterfactual cause of the bottle’s breaking. The striking result here is that there can be grounds to claim that a condition was the actual cause of an outcome even though, under the counterfactual definition, the effect of that condition on the outcome is 0. (At the same time, all counterfactual causes are automatically actual causes; they meet Condition 2 by conditioning on nothing at all, an empty set \\(\\mathcal W\\).) One immediate methodological implication follows: since actual causes need not be causes, there are risks in research designs that seek to understand causal effects by tracing back actual causes—i.e., the way things actually happened. If we traced back from the breaking of the bottle, we might be tempted to identify Sally’s throw as the cause of the outcome. We would be right only in an actual-causal sense, but wrong in the standard, counterfactual causal sense. Chains of events that appear to “generate” an outcome are not always causes.30 As with other causal queries, the question “Was \\(X=x\\) the actual cause of \\(Y=y\\)?” can be redefined as a question about which values for exogenous nodes produce conditions under which \\(X\\) could have made a difference. To see how, let us run through the Billy and Sally example again, but formally in terms of a model. Consider Figure , where we represent Sally’s throw (\\(S\\)), Billy’s throw (\\(B\\)), Sally’s rock hitting the bottle (\\(H^S\\)), Billy’s rock hitting the bottle (\\(H^B\\)), and the bottle cracking (\\(C\\)). Each endogenous variable has a \\(\\theta\\) term associated with it, capturing its response to its parents. We capture the possible “preemption” effect with the arrow pointing from \\(H^S\\) to \\(H^B\\), allowing that whether Sally’s rock hits to affect whether Billy’s rock hits. Let us again imagine that Sally threw (\\(S=1\\)), Billy threw (\\(B=1\\)), and the bottle cracked (\\(C=1\\)). Let us say that \\(\\theta^{H^B}\\) takes on a value such that (a) \\(H^B=0\\) whenever \\(H^S=1\\) (Sally’s hit preempts Billy’s) and (b) \\(B\\) has a positive effect on \\(H^B\\) when \\(H^S=0\\) (Billy’s throw hits if Sally’s doesn’t). Further, assume that \\(S\\) has a positive effect on \\(H^S\\). Let us finally posit that \\(\\theta^C\\) takes on a value such that \\(C=1\\) if \\(H^B\\) equals \\(1\\).31 This is a set of \\(\\theta\\) values under which the query, “Does \\(S\\) have a causal effect on \\(C\\)?” must be answered in the negative. Similarly, this is a context in which \\(C=1\\) cannot be causally attributed to \\(S=1\\). If Sally had not thrown, then Sally’s rock would not have hit the bottle, which means that Billy’s rock would have hit, and the bottle would still have cracked—still, \\(C=1\\). However, it is still possible that \\(S=1\\) was an actual cause of \\(C=1\\). To complete this query, we need to ask whether there is some node value that we can hold fixed at the value that it actually assumed in the case such that \\(S\\) would have a causal effect on the outcome. Fixing \\(B=1\\) (Billy throws) cannot help (since if Billy throws, Billy hits, and the bottle cracks anyway). However, under \\(S=1\\) and \\(B=1\\), given the \\(\\theta\\) values we have posited, \\(H^B=0\\): Billy’s rock does not hit. If we hold constant that \\(H^B=0\\), then there is an “opportunity” for \\(S\\) to matter in that \\(C\\) is no longer forced to 1 (by Billy’s rock hitting). But for \\(S\\) to matter under his scenario, something else has to be true: \\(\\theta^C\\)’s value must allow for \\(H^S\\) to have a positive effect on \\(C\\) when \\(H^B=0\\). Using our two-cause notation (with \\(H^S\\) on the horizontal axis, and \\(H^B\\) on the vertical), and given that we have already stipulated that \\(C=1\\) when \\(H^B=1\\), the one permissible value for \\(\\theta^C\\) is \\(\\theta^{11}_{01}\\). This is causal type in which neither \\(H^B\\) nor \\(H^S\\) can be causal if both Billy and Sally throw: whenever one variable is 1, the other has no effect. But it is also a type in which each has a causal effect if the other is held at 0. It is also the case, as we have said, that all counterfactual causes are actual causes. They are, quite simply, counterfactual causes when we hold nothing fixed (\\(\\mathcal W\\) is the empty set). Thus, in fact, any \\(\\theta^S\\), \\(\\theta^{H^S}\\) and \\(\\theta^C\\) values in which \\(S\\) has a positive effect when \\(B=1\\) will do. This includes, for instance, a \\(\\theta^C\\) value in which Billy’s hitting has no effect on the bottle (perhaps Billy doesn’t throw hard enough!): e.g., \\(\\theta^{01}_{01}\\). Here, Sally’s throw is both a counterfactual cause and an actual cause of the bottle’s cracking. The larger point is that actual cause queries can, like all other causal queries, be defined as questions about the values of nodes in a causal model. Figure 4.3: This DAG is a graphical representation of the simple causal setup in which the effect of \\(X\\) on \\(Y\\) in a given case depends on the case’s causal type, represented by \\(\\theta^Y\\). With a single binary causal variable of interest, we let \\(\\theta_Y\\) take on values \\(\\theta^Y_{ij}\\), with \\(i\\) representing the value \\(Y\\) takes on if \\(X=0\\) and \\(j\\) representing the value \\(Y\\) takes on if \\(X=1\\). With a binary framework outcome, \\(\\theta^Y\\) ranges over the four values: \\(\\theta^Y_{00}\\), \\(\\theta^Y_{10}\\), \\(\\theta^Y_{01}\\) and \\(\\theta^Y_{11}\\). Actual causes are conceptually useful whenever there are two sufficient causes for an outcome, but one preempts the operation of the other. For instance, we might posit that both the United States’ development of the atomic bomb was a sufficient condition for U.S. victory over Japan in World War II, and that U.S. conventional military superiority was also a sufficient condition and would have operated via a land invasion of Japan. Neither condition was a counterfactual cause of the outcome because both were present. However, holding constant the absence of a land invasion, the atomic bomb was a difference-maker, rendering it an actual cause. The concept of actual cause thus helps capture the sense in which the atomic bomb contributed to the outcome, even if it was not a counterfactual cause. Similarly, the question of how common it is for a condition to be an actual cause can be expressed as values of nodes, possibly including nodes that record parameter values for the relevant exogenous nodes. An extended notion (Halpern 2016, p 81) of actual causes restricts the imagined counterfactual deviations to states that are more likely to arise (more “normal”) than the factual state. We will call this notion a ‘’notable cause.’’ Similarly, one cause, \\(A\\), is “more notable” than another cause, \\(B\\), if a deviation in \\(A\\) from its realized state is (believed to be) more likely than a deviation in \\(B\\) from its realized state. For intuition, we might wonder why a Republican was elected to the presidency in a given election. In looking at some minimal winning coalition of states that voted Republican, we might distinguish between a set of states that always vote Republican and a set of states that usually go Democratic but voted Republican this time. If the coalition is minimal winning, then every state that voted Republican is a cause of the outcome in the standard (difference making) sense. However, only the states that usually vote Democratic are notable causes since it is only for them that the counterfactual scenario (voting Democratic) was more likely to arise than the factual scenario. In a sense, we take the “red” states’ votes for the Republican as given—placing it, as it were, in the causal background—and identify as “notable” those conditions that mattered and easily could have gone differently. By the same token, we can say that, among those states that voted Republican this time, those that more commonly vote Democratic are more notable causes than those that less commonly vote Democratic. Again, whether something is a notable cause, or the likelihood in some population that a condition is a notable cause, can be expressed as a claim about the value of a set of root nodes. Though not a focus of our applied examples we show formally how to estimate these estimands in the Appendix, section XXX. 4.4 Average causal effects A more general query asks about an average causal effect in some population. In counterfactual terms, a question about average causal effects is: if we manipulated the value of \\(X\\) for all cases in the population—first setting \\(X\\) to one value for all cases, then changing it to another value for all cases—by how much would the average value of \\(Y\\) in the population change? Like other causal queries, a query about an average causal effect can be conceptualized as learning about a node in a causal model. We can do this by conceiving of any given case as being a member of a population composed of different causal types. When we seek to estimate an average causal effect, we seek information about the shares of these causal types in the population. More formally and adapted from Humphreys and Jacobs (2015), we can use \\(\\lambda^Y_{ij}\\) to refer to the share of cases in a population that has causal type \\(\\theta^Y_{ij}\\). Thus, given our four causal types above, \\(\\lambda^Y_{10}\\) is the proportion of cases in the population with negative effects; \\(\\lambda_{01}\\) is the proportion of cases with positive effects; and so on. We can, of course, also think of these shares as probabilities; that is, we can think of any given case as being ``drawn’’ from a multinomial distribution with probabilities \\(\\lambda = (\\lambda^Y_{10}, \\lambda^Y_{01}, \\lambda^Y_{00}, \\lambda^Y_{11})\\). One nice feature of this setup, with both \\(X\\) and \\(Y\\) as binary, the average causal effect can be simply characterized as the share of positive-effect cases less the share of negative-effect cases: \\(\\lambda^Y_{01} - \\lambda^Y_{10}\\). Graphically, we can represent this setup by including \\(\\lambda^Y\\) in a more complex causal graph as in Figure . As in our setup for case-level causal effects, \\(X\\)’s effect on \\(Y\\) in a case depends on (and only on) the case’s causal type, \\(\\theta^Y\\). The key difference is that we now model the case’s type not as exogenously given, but as a function of two additional variables: the distribution of causal types in a population and a random process through which the case’s type is “drawn” from that distribution. We represent the type distribution as \\(\\lambda^Y\\) (a vector of values for the proportions \\(\\lambda^Y_{10}, \\lambda^Y_{01}, \\lambda^Y_{00}, \\lambda^Y_{11}\\)) and the random process drawing a \\(\\theta^Y\\) value from that distribution as \\(U_\\theta\\). In this model, our causal query—about \\(X\\)’s average causal effect—is thus defined by the vector \\(\\lambda^Y\\), and specifically by the shares of negative- and positive-causal-effect cases, respectively, in the population. What is \\(X\\)’s average effect on \\(Y\\) amounts to asking: what are the values of \\(\\lambda^Y_{10}\\) and \\(\\lambda^Y_{01}\\)? As with \\(\\theta^Y\\), \\(\\lambda^Y\\) is not directly observable. And so the empirical challenge is to figure out what we can observe that would allow us to learn about \\(\\lambda^Y\\)’s component values?32 Figure 4.4: This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different causal types. As before, \\(X\\)’s effect on \\(Y\\) is a function of a causal-type variable, \\(\\theta^Y\\). Yet here we explicitly model the process through which the case’s type is drawn from a distribution of types in a population. The variable \\(\\lambda\\) is a vector representing the multinomial distribution of causal types in the population while \\(U_\\theta\\) is a random variable representing the draw of each case from the distribution defined by \\(\\lambda\\). A case’s causal type, \\(\\theta^Y\\), is thus a joint function of \\(\\lambda^Y\\) and \\(U^{\\theta_Y}\\). We can, of course, likewise pose queries about other population-level causal quantities. For instance, we could ask for what proportion of cases in the population \\(X\\) has a positive effect: this would be equivalent to asking the value of \\(\\lambda^Y_{01}\\), one element of the \\(\\lambda^Y\\) vector. Or we could ask about the proportion of cases in which \\(X\\) has no effect, which would be asking about \\(\\lambda^Y_{00} + \\lambda^Y_{11}\\). 4.5 Causal Paths To develop richer causal understandings, researchers often seek to describe the causal path or paths through which effects propagate. Consider the DAG in Figure ??, in which \\(X\\) can affect \\(Y\\) through two possible pathways: directly and via \\(M\\). Assume again that all variables are binary, taking on values of \\(0\\) or \\(1\\). As we have seen in Chapter 3, mediation models require causal-type nodes that point into any mediators as well as into the outcome variable. So here we have drawn in a causal-type variable defining \\(M\\)’s response to \\(X\\), \\(\\theta^M\\), and a causal-type variable capturing \\(Y\\)’s response, \\(\\theta^Y\\). Importantly, \\(\\theta^Y\\) defines \\(Y\\)’s response to two parent variables: \\(M\\) and \\(X\\). Suppose that we observe \\(X=1\\) and \\(Y=1\\) in a case. Suppose, further, that we have reasonable confidence that \\(X\\) has had a positive effect on \\(Y\\) in this case. We may nonetheless be interested in knowing whether that causal effect ran through \\(M\\). We will refer to this as a query about a causal path. A causal path query, of course, goes beyond assessing whether some mediating event along the path occurred. We cannot, for instance, establish that the top path in Figure was operative simply by determining the value of \\(M\\) in this case—though that will likely be useful information. Rather, the question of whether the top (mediated) causal path is operative is a composite question of two parts: First, does \\(X\\) have an effect on \\(M\\) in this case? Second, does that effect—the difference in \\(M\\)’s value caused by a change in \\(X\\)—in turn cause a change in \\(Y\\)’s value? In other words, what we want to know is whether the effect of \\(X\\) on \\(Y\\) depends on—will not operate without—the effect of \\(X\\) on \\(M\\).33 Framing the query in this way makes clear that asking whether a causal effect operated via a given path is in fact asking about a specific set of causal effects lying along that path. Figure 4.5: Here \\(X\\) has effects on \\(Y\\) both indirectly through \\(M\\) and directly. As we can show, we can also define a causal-path query as a question about specific nodes on a causal graph. In particular, just as we have defined other questions about causal effects in terms of causal-type nodes, a causal path can also be defined in terms of the values of type nodes: specifically, in the present example, in terms of the nodes \\(\\theta^M\\) and \\(theta^Y\\). To see why, let us first note that there are two combinations of effects that would allow \\(X\\)’s positive effect on \\(Y\\) to operate via \\(M\\): (1) \\(X\\) has a positive effect on \\(M\\), which in turn has a positive effect on \\(Y\\); or (2) \\(X\\) has a negative effect on \\(M\\), which has a negative effect on \\(Y\\). Thus, in establishing whether \\(X\\) affects \\(Y\\) through \\(M\\), the first question is whether \\(X\\) affects \\(M\\) in this case. Whether or not it does is a question about the value of the causal-type node, \\(\\theta^M\\). Let us assume that \\(\\theta^M\\) can take on four possible values corresponding to the four possible responses to \\(X\\): \\(\\theta^M_{10}, \\theta^M_{01}, \\theta^M_{00}, \\theta^M_{11}\\).34 For sequence (1) to operate, \\(\\theta^M\\) must take on the value \\(\\theta^M_{01}\\), representing a positive effect of \\(X\\) on \\(M\\). For sequence (2) to operate, \\(\\theta^M\\) must take on the value \\(\\theta^M_{10}\\), representing a negative effect of \\(X\\) on \\(M\\). \\(\\theta^Y\\), as for our causal-attribution example, defines \\(Y\\)’s response to different combinations of two other variables—here, \\(X\\) and \\(M\\)—since both of these variables point directly into \\(Y\\). Another way to think about this setup is that \\(M\\) is not just a possible mediator of \\(X\\)’s indirect effect; \\(M\\) is also a potential moderator of \\(X\\)’s direct effect. Where \\(X\\) can have both an mediated effect through \\(M\\) and a direct effect, \\(X\\) and \\(M\\) also potentially interact in affecting \\(Y\\). This results in sixteeen possible values for \\(\\theta^Y\\)—again as shown above in Table 2.3. What values of \\(\\theta^Y\\) then are compatible with the operation of the \\(M\\) causal path? Let us first consider this question with respect to sequence (1), in which \\(X\\) has a positive effect on \\(M\\), and that positive effect is necessary for \\(X\\)’s positive effect on \\(Y\\) to occur. For this sequence to operate, \\(\\theta^M\\) must take on the value of \\(\\theta^M_{01}\\). When it comes to \\(\\theta^Y\\), then, what we need to look for types in which \\(X\\)’s effect on \\(Y\\) depends on \\(M\\)’s taking on the value it does as a result of \\(X\\)’s positive effect on \\(M\\). We are thus looking for causal types that represent two kinds of counterfactual causal relations operating on nodes. First, \\(X\\) must have a positive effect on \\(Y\\) when \\(M\\) changes as it should given \\(X\\)’s positive effect on \\(M\\). Second, that change in \\(M\\), generated by a change in \\(X\\), must be necessary for \\(X\\)’s positive effect on \\(Y\\) to operate. The thought experiment here thus imagines a situation in which \\(X\\) changes from \\(0\\) to \\(1\\),35 but \\(M\\) does not change to the value that it should as a result of this change in \\(X\\). We then inspect our types to see if \\(Y\\) would change from \\(0\\) to \\(1\\) in this situation. It is this counterfactual that isolates the causal significance of the path that runs through \\(M\\). It is only if \\(Y\\) would not change to \\(1\\) in this situation that we have identified a causal-type for which the \\(M\\)-mediated path matters. Assuming a positive effect of \\(X\\) on \\(M\\) (\\(\\theta^M=\\theta^M_{01}\\)), we thus need to apply three queries to \\(\\theta^Y\\):36 Is \\(X=1\\) a counterfactual cause of \\(Y=1\\)? Establishing this positive effect of \\(X\\) involves two queries: Where \\(X=0\\), does \\(Y=0\\)? As we are assuming \\(X\\) has a positive effect on \\(M\\), if \\(X=0\\) then \\(M=0\\) as well. We thus look down the \\(X=0, M=0\\) column and eliminate those types in which we do not observe \\(Y=0\\). This eliminates types \\(9\\) through \\(16\\). Where \\(X=1\\), does \\(Y=1\\)? Again, given \\(X\\)’s assumed positive effect on \\(M\\), \\(M=1\\) under this condition. Looking down the \\(X=1, M=1\\) column, we eliminate those types where we do not see \\(Y=1\\). We retain only types \\(2, 4, 6,\\) and \\(8\\). Is \\(X\\)’s effect on \\(M\\) necessary for \\(X\\)’s positive effect on \\(Y\\)? That is, do we see \\(Y=1\\) only if \\(M\\) takes on the value that \\(X=1\\) generates (\\(M=1\\))? To determine this, we inspect the counterfactual condition in which \\(X=1\\) yet \\(M=0\\), and we ask: does \\(Y=0\\)? Of the four remaining types, only \\(2\\) and \\(6\\) pass this test. Under these and only these two values of \\(\\theta^Y\\)—\\(\\theta_{00}^{01}\\) and \\(\\theta_{00}^{11}\\)—we will see a positive effect of \\(X\\) on \\(Y\\) for which the \\(M\\)-mediated path is causally necessary as long as \\(X\\) also has a positive effect on \\(M\\). These two \\(\\theta^Y\\) values are also different from one another in an interesting way. For type \\(\\theta_{00}^{11}\\), \\(X\\)’s effect on \\(Y\\) runs strictly through \\(M\\): if \\(M\\) were to change from \\(0\\) to \\(1\\) without \\(X\\) changing, \\(Y\\) would still change from \\(0\\) to \\(1\\). \\(X\\) is causally important for \\(Y\\) only insofar as it affects \\(M\\). In a case of type \\(\\theta_{00}^{11}\\), then, anything else that similarly affects \\(M\\) would generate the same effect on \\(Y\\) as \\(X\\) does. In type \\(\\theta_{00}^{01}\\), however, both \\(X\\)’s change to \\(1\\) and the resulting change in \\(M\\) are necessary to generate \\(Y\\)’s change to \\(1\\); \\(X\\)’s causal effect thus requires both the mediated and the unmediated pathway. Andhere \\(X\\) itself matters in the counterfactual sense; for a case of type \\(\\theta_{00}^{01}\\), some other cause of \\(M\\) would not generate the same effect on \\(Y\\). We can undertake the same exercise for sequence (2), in which \\(X\\) first has a negative effect on \\(M\\), or \\(\\theta^M=\\theta^M_{10}\\). Here we adjust the three queries for \\(\\theta^Y\\) to take account of this negative effect. Thus, we adjust query 1a so that we are looking for \\(Y=0\\) when \\(X=0\\) and \\(M=1\\). In query 1b, we look for \\(Y=1\\) when \\(X=1\\) and \\(M=0\\). And for query 2, we want types in which \\(Y\\) fails to shift to \\(1\\) when \\(X\\) shifts to \\(1\\) but \\(M\\) stays at \\(1\\). Types \\(\\theta_{01}^{00}\\) and \\(\\theta_{11}^{00}\\) pass these three tests. In sum, we can define a query about causal paths as a query about the value of \\(\\theta\\) terms on the causal graph. For the graph in Figure , asking whether \\(X\\)’s effect runs via the \\(M\\)-mediated path is asking whether one of four combinations of \\(\\theta^M\\) and \\(\\theta^Y\\) hold in case: \\(\\theta^M=\\theta^M_{01}\\) and (\\(\\theta^Y=\\theta_{00}^{01}\\) or \\(\\theta_{00}^{11}\\)) \\(\\theta^M=\\theta^M_{01}\\) and (\\(\\theta^Y=\\theta_{01}^{00}\\) or \\(\\theta_{11}^{00}\\)) It is worth noting how different this formulation of the task of identifying causal pathways is from widespread understandings of process tracing. Scholars commonly characterize process tracing as a method in which we determine whether a mechanism was operating by establishing whether the events lying along that path occurred. As a causal-model framework makes clear, finding out that \\(M=1\\) (or \\(M=0\\), for that matter) does not establish what was going on causally. Observing this intervening step does not by itself tell us what value \\(M\\) would have taken on if \\(X\\) had taken on a different value, or whether this would have changed \\(Y\\)’s value. We need instead to conceive of the problem of identifying pathways as one of figuring out the counterfactual response patterns of the variables along the causal chain. As we will demonstrate later in the book, explicitly characterizing those response patterns as nodes in a causal model helps us think systematically about empirical strategies for drawing the relevant inferences. –&gt; –&gt; –&gt; –&gt; –&gt; References "],
["bayeschapter.html", "Chapter 5 Bayesian Answers 5.1 Bayes Basics 5.2 Bayes applied 5.3 Three principles of Bayesian updating", " Chapter 5 Bayesian Answers We run through the logic of Bayesian updating and show how it is used for answering queries of interest. We illustrate with applications to correlational and process tracing inferences. Bayesian methods are just sets of procedures to figure out how to update beliefs in light of new information. We begin with a prior belief about the probability that a hypothesis is true. New data then allow us to form a posterior belief about the probability of the hypothesis. Bayesian inference takes into account the consistency of the evidence with a hypothesis, the uniqueness of the evidence to that hypothesis, and background knowledge about the problem. In the next section we review the basic idea of Bayesian updating. The following section applies it to the problem of updating on causal estimands given a causal model and data. 5.1 Bayes Basics For simple problems, Bayesian inference accords well with our intuitions. Once problems get slightly more complex however, our intuitions often fail us. 5.1.1 Simple instances Say I draw a card from a deck. The chances it is a Jack of Spades is just 1 in 52. If I tell you that the card is indeed a spade and asked you now what are the chances it is a Jack of Spades, you should guess 1 in 13. If I told you it was a heart you should guess there is no chance it is a Jack of Spades. If I said it was a face card and a spade you should say 1 in 3. All those answers are applications of Bayes’ rule. In each case the answer is derived by assessing what is possible, given the new information, and then assessing how likely the outcome of interest among the states that are possible. In all the cases you calculate: \\[\\text{Probability Jack of Spades | Information} = \\frac{\\text{Is Jack of Spades Consistent with Information?}}{\\text{How many cards are consistent with Information?}} \\] The same logic goes through when things are not quite so black and white. Now consider two slightly trickier examples. Interpreting Your Test Results. Say that you take a test to see whether you suffer from a disease that affects 1 in 100 people. The test is good in the sense that if you have the disease it will say you have it with a 99% probability. If you do not have it, then with a 99% probability, it will say that you do not have it. The test result says that you have the disease. What are the chances you have it? You might think the answer is 99%, but that would be to mix up the probability of the result given the disease with the probability of the disease given the result. In fact the right answer is 50%, which you can think of as the share of people that have the disease among all those that test positive. For example if there were 10,000 people, then 100 would have the disease and 99 of these would test positive. But 9,900 would not have the disease and 99 of these would test positive. So the people with the disease that test positive are half of the total number testing positive. As an equation this might be written: \\[\\text{Probability You have the Disease | Test} = \\frac{\\text{How many people have the disease and test positive?}}{\\text{How many people test positive?}} \\] Two-Child Problem Consider last an old puzzle found described Gardner (1961). Mr Smith has two children, \\(A\\) and \\(B\\). At least one of them is a boy. What are the chances they are both boys? To be explicit about the puzzle, we will assume that the information that one child is a boy is given as a truthful answer to the question “is at least one of the children a boy?” Assuming that there is a 50% probability that a given child is a boy, people often assume the answer is 50%. But surprisingly the answer is 1 in 3. The information provided rules out the possibility that both children are girls and so the right answer is found by readjusting the probability that two children are boys based on this information. As an equation: \\[\\text{Probability both are boys | Not both girls} = \\frac{\\text{Probability both boys}}{\\text{Probability they are not both girls}} = \\frac{\\text{1 in 4}}{\\text{3 in 4}}\\] 5.1.2 Bayes’ Rule for Discrete Hypotheses Formally, all of these equations are applications of Bayes’ rule which is a simple and powerful formula for deriving updated beliefs from new data. The formula is given as: \\[\\begin{eqnarray} \\Pr(H|\\mathcal{D})&amp;=&amp;\\frac{\\Pr(\\mathcal{D}|H)\\Pr(H)}{\\Pr(\\mathcal{D})}\\\\ &amp;=&amp;\\frac{\\Pr(\\mathcal{D}|H)\\Pr(H)}{\\sum_{H&#39;}\\Pr(\\mathcal{D}|H&#39;)\\Pr(H&#39;))} \\end{eqnarray}\\] where \\(H\\) represents a hypothesis and \\(\\mathcal{D}\\) represents a particular realization of new data (e.g., a particular piece of evidence that we might observe). Looking at the formula we see that the posterior belief derives from three considerations. First, the likelihood: how likely are we to have observed these data if the hypothesis were true, \\(\\Pr(\\mathcal{D}|H\\))? Second, how likely were we to have observed these data regardless of whether the hypothesis is true or false, \\(\\Pr(\\mathcal{D})\\)? These first two questions, then, capture how consistent the data are with our hypothesis and how specific the data are to our hypothesis. As shown in the equation above the second question can usefully be reposed as one about all the different ways (alternative Hypotheses, \\(H&#39;\\)) that could give rise to the data. Note, that contrary to some claims, the denominator does not require a listing of all possible hypotheses, just an exhaustive collection of hypotheses. For example we might have the notion of the probability that the accused’s fingerprints would be on the door if she were or were guilty without having to decompose the “not guilty” into a set of hypotheses regarding who else might be guilty. Our posterior belief is further conditioned by the strength of our prior level of confidence in the hypothesis, \\(\\Pr(H)\\). The greater the prior likelihood that our hypothesis is true, the greater the chance that new data consistent with the hypothesis has in fact been generated by a state of the world implied by the hypothesis. 5.1.3 The Dirichlet family and Bayes’ Rule for Continuous Parameters This basic formula extends in a simple way to collections of continuous variables. For example, say we are interested in the value of some parameter vector \\(\\theta\\) (as a vector, \\(\\theta\\) can contain many quantities we are uncertain about), we can calculate this, given a prior probability distribution over possible values of \\(\\theta\\), \\(p\\), and given data \\(D\\) as: \\[p(\\theta|\\mathcal{D})=\\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{\\int_{\\theta&#39;}p(\\mathcal{D|\\theta&#39;})p(\\theta&#39;)d\\theta}\\] Bayes rule requires the ability to express a prior distribution but it does not require that the prior have any particular properties other than being probability distributions. In practice however when we are dealing with continuous parameters, it can be useful to make use of “off the shelf” distributions. In practice we will often be interested in forming beliefs about the share of units that are of a particular type. For this type of question we will make quite heavy use of “Dirichlet” distributions – a family of distributions that capture beliefs about shares. Consider for example the share of people in a population that voted—this is a quantity between 0 and 1. Two people might may both believe that the turnout was around 50% but may differ in how certain they are about this claim. One might claim to have no information and to believe that any turnout rate between 0 and 100% is equally likely, giving an expected turnout of 50%; another might be completely confident that the number if 50% and entertain no other possibilities. We can capture such beliefs quite well by using the Beta distribution—a special case of the Dirichlet. The Beta is a distribution over the \\([0,1]\\) that is governed by two parameters , \\(\\alpha\\) and \\(\\beta\\). In the case in which both \\(\\alpha\\) and \\(\\beta\\) are 1, the distribution is uniform – all values are seen as equally likely. As \\(\\alpha\\) rises large outcomes are seen as more likely and as \\(\\beta\\) rises, lower outcomes are seen as more likely. If both rise proportionately the expected outcome does not change but the distribution becomes tighter. An attractive feature of the Beta distribution is that if one has a prior Beta(\\(\\alpha\\), \\(\\beta\\)) over the probability of some event (e.g. that a coin comes up heads), and then one observes a positive case, the Bayesian posterior distribution is also a Beta with with parameters \\(\\alpha+1, \\beta\\). Thus in a sense if people start with uniform priors and build up knowledge on seeing outcomes, their posterior beliefs should be Beta distributions. Figure 5.1 shows a set of such distributions, starting with one that has greater variance than uniform (this corresponds to the non informative “Jeffrey’s prior”), then uniform, then for a case in which multiple negative and positive outcomes are seen, in equal number, and finally a set of priors with mean of 3/4. Figure 5.1: Beta distributions Dirichlet distributions generalize the Beta to the situation in which there are beliefs not just over a proportion, or a probability, but over collections of probabilities. For example if four outcomes are possible and each is likely to occur with probability \\(\\theta_k\\), \\(k=1,2,3,4\\) then beliefs about these probabilities are distributions over the a three dimensional unit simplex—that is, all 4 element vectors of probabilities that add up to 1. The distribution has as many parameters as there are outcomes and these are traditionally recorded in a vector, \\(\\alpha\\). Similar to the Beta distribution, an uninformative prior (Jeffrey’s prior) has \\(\\alpha\\) parameters of \\((.5,.5,.5, \\dots)\\) and a uniform (“flat”) distribution has \\(\\alpha = (1,1,1,,\\dots)\\). As with the Beta distribution, the Dirichlet updates in a simple way. If you have a Dirichlet prior with parameter \\(\\alpha = (\\alpha_1, \\alpha_2, \\dots)\\) and you observe outcome \\(1\\), for example, then then posterior distribution is also Dirichlet with parameter vector \\(\\alpha&#39; = (\\alpha_1+1, \\alpha_2,\\dots)\\). 5.1.4 Moments In what follows we often refer to the “posterior mean” or the “posterior variance.” These are simply summary statistics of the posterior distribution and can be calculated easily once the posterior is known. For example the posterior mean of a parameter \\(\\theta_1\\)—just one in a collection of parameters stored in \\(\\theta\\)—is simply \\(\\overline{\\theta}_1 | \\mathcal{D} = \\int \\theta_1 p(\\theta | \\mathcal{D}) d\\theta\\). Note importantly that this is calculated using the posterior over the entire vector \\(\\theta\\), there is no notion of updating parameter \\(\\theta_1\\) on its own. Similarly the posterior variance is \\(\\int (\\theta_1 - (\\overline{\\theta}_1 | \\mathcal{D})^2 p(\\theta | \\mathcal{D}) d\\theta\\). 5.1.5 Bayes estimation in practice Although the principle of Bayesian inference is quite simple, in practice calculating posteriors for continuous parameters is computationally complex. In principle with continuous parameters there is an infinity of possible parameter values. Analytic solutions are not, in general, easy to come by and so in practice researchers use some form of sampling. Imagine for instance you were interested in forming a posterior on the share intending to vote democrat, given polling data. (This is not truly continuous, but with large elections it might as well be). One approach is to coarsen the parameter space—we calculate the probability of observing the polling data given possible values \\(\\theta = 0, \\theta = .1, \\theta = .2, \\dots, \\theta = 1\\), and, apply Bayes rule to form a posterior for each of these these possibilities. The downside of the this approach is that it for a decent level of precision it becomes computationally expensive with large parameter spaces and parameter spaces get large quickly. For instance if you are interested in vote shares you might find .4, .5, and .6 too coarse and want posteriors for 0.51 or even 0.505; this would require calculations for 200 parameter values. If you had two parameters that you wanted to slice up each into 200 possible values, you would then have 40,000 parameter pairs to worry about. What’s more, most of those calculations would not be very informative if the real uncertainty all lies in some small (though possibly unknown) range – such as between 40% and 60%. An alternative approach is to use variants of Markov Chain Monte Carlo sampling. Under these approaches parameter vectors are sampled and their likelihood is evaluated. If they have high likelihood then new parameter vectors near them are draw with a high probability. Based on the likelihood associated with these new draws, new draws are made. The result is a chain of draws that build up to approximate the posterior distribution. The output from these procedures is not a set of probabilities for each possible parameter vector but rather a a set of draws of parameter vectors from the posterior distribution. Many algorithms have been developed to achieve these tasks efficiently; in all of our applications we rely on the stan procedures which involve…. 5.2 Bayes applied 5.2.1 Bayesian Inference on Queries In Chapter 2 we described estimands of interest as queries over the values of root nodes in directed acyclic graphs. Once queries are defined in terms of the values of roots then formation of beliefs, given data \\(W\\), about estimands follows immediately from application of Bayes rule. Let \\(Q(u)\\) define the value of the query in context \\(u\\). The updated beliefs about the query are given by the distribution: \\[P(q | W) = \\int_{u:Q(u) = q} P(u|W)du = \\int_{u:Q(u) = q} \\frac{P(W|u)P(u)}{\\int_{u&#39;}P(W|u&#39;)P(u&#39;)du&#39;}du\\] This expression gathers together all the contexts that produce a given value of \\(Q\\) and assesses how likely these are, collectively, given the data.37 For an abstract representation of the relations between assumptions, queries, data, and conclusions, see Figure 1 in Pearl (2012). Return now to Mr Smith’s puzzle. The two “roots” are the sexes of the two children, child \\(A\\) and child \\(B\\). The query here is \\(Q\\): “Are both boys?” which can be written in terms of the roots. The statement “\\(Q=1\\)” is equivalent to the statement (\\(A\\) is a boy &amp; \\(B\\) is a boy). Thus it takes the value \\(q=1\\) in just one context. Statement \\(q=0\\) is the statement (“\\(A\\) is a boy &amp; \\(B\\) is a girl” or “\\(A\\) is a girl &amp; \\(B\\) is a boy” or “\\(A\\) is a girl &amp; \\(B\\) is a girl”). Thus \\(q=0\\) in three contexts. If we assume that each of the two children is equally likely to be a boy or a girl with independent probabilities, then each of the four contexts is equally likely. The result can then be figured out as \\(P(Q=1) = \\frac{1\\times \\frac{1}{4}}{1\\times \\frac{1}{4} + 1\\times \\frac{1}{4}+1\\times \\frac{1}{4}+0\\times \\frac{1}{4}} = \\frac{1}{3}\\). This answer requires summing over only one context. \\(P(Q=0)\\) is of course the complement of this, but using the Bayes formula one can see that it can be found by summing over the posterior probability of three contexts in which the statement \\(Q=0\\) is true. We will often want to think about our causal queries being collections of states of the world — i.e., of unit causal types. Returning to our discussion of queries in Chapter 4, suppose we start with the model \\(X \\rightarrow M \\rightarrow Y\\), and our query is whether \\(X\\) has a positive effect on \\(Y\\). This is a query that is satisfied by four sets of unit types: those in which \\(X\\) has a positive effect on \\(M\\) and \\(M\\) has a positive effect on \\(Y\\), with \\(X\\) being either 0 or 1; and those in which \\(X\\) has a negative effect on \\(M\\) and \\(M\\) has a negative effect on \\(Y\\), with \\(X\\) being either 0 or 1. Our inferences on the query will thus involve gathering these different unit types, and their associated posterior probabilities, together. One interesting feature of Bayesian updating is that we update more strongly in favor of the hypothesis for which the evidence is least damaging to the most-likely ways in which the hypothesis could be true. Suppose our prior belief was that it was much more unlikely that \\(M\\) had a negative effect on \\(Y\\), than that \\(M\\) had a positive effect on \\(Y\\). This makes one of the ways in which \\(X\\) could have a positive effect on \\(Y\\) (the chain of negative effects) much less likely than the other way in which \\(X\\) could have a positive effect on \\(Y\\) (the chain of positive effects). This means that evidence, say, against a chain of negative effects and evidence against a chain of positive effects will not be equally consequential for our query: in particular, we will update more strongly against the query if we find evidence against a chain of positive effects than if we find evidence against a chain of negative effects. Evidence against a chain of positive effects speaks against the most likely way in which the query could be true, whereas evidence against a chain of negative effects speaks against a way the query could be true that we did not think was very likely to begin with. 5.2.2 Simple Bayesian Process Tracing Process tracing in its most basic form seeks to use within case evidence to draw inferences about the case. For example, with a focus on whether \\(X\\) caused \\(Y\\) , data on a “clue”, \\(K\\), is used to make inference about whether or not the outcome in that case was generated by the case’s treatment status. We refer to the within-case evidence gathered during process tracing as clues in order to underline their probabilistic relationship to the causal relationship of interest. Readers familiar with the framework in Collier, Brady, and Seawright (2004) can usefully think of our “clues” as akin to causal process observations, although we highlight that there is no requirement that the clues be generated by the causal process. To make inferences, the analyst looks for clues that will be observed with some probability if the case is of a given type and that will not be observed with some probability if the case is not of that type. It is relatively straightforward to express the logic of process tracing in Bayesian terms, a step that will aid the integration of qualitative with quantitative causal inferences. As noted by others (e.g. Bennett (2008), Beach and Pedersen (2013), Rohlfing (2012)), there is an evident connection between the use of evidence in process tracing and Bayesian inference. . To illustrate, suppose we are interested in regime collapse. We already have \\(X,Y\\) data on one authoritarian regime: we know that it suffered economic crisis (\\(X=1\\)) and collapsed (\\(Y=1\\)). We want to know what caused the collapse. To make progress we will try to draw inferences given a “clue.” Beliefs about the probabilities of observing clues for cases with different causal effects derive from theories of, or evidence about, the causal process connecting \\(X\\) and \\(Y\\). Suppose we theorize that the mechanism through which economic crisis generates collapse runs via diminished regime capacity to reward its supporters during an economic downturn. A possible clue to the operation of a causal effect, then, might be the observation of diminishing rents flowing to regime supporters shortly after the crisis. If we believe the theory, then this is a clue that we might believe to be highly probable for cases of type \\(b\\) that have experienced economic crisis (where the crisis in fact caused the collapse) but of low probability for cases of type \\(d\\) that have experienced crisis (where the collapse occurred for other reasons). To make use of Bayes rule we need to: define our parameters, which are the key quantities of interest provide prior beliefs about the parameters of interest define a likelihood function provide the probability of the data plug these into Bayes’ rule to calculate a posterior on the parameters of interest We discuss each of these in turn. Parameters. The inferential challenge is to determine whether the regime collapsed because of the crisis (\\(b\\) type) or whether it would have collapsed even without it (\\(d\\) type). We do so using further information from the case—one or more clues. We use the variable \\(K\\) to register the outcome of the search for a clue, with \\(K\\)=1 indicating that a specific clue is searched for and found, and \\(K\\)=0 indicating that the clue is searched for and not found. Let \\(j\\in \\{a,b,c,d\\}\\) refer to the type of an individual case. Our hypothesis, in this initial setup, consists simply of a belief about \\(j\\) for the case under examination: specifically whether the case is a \\(b\\) type (\\(j=b)\\). The parameter of interest is the causal type. Prior. We then assign a prior degree of confidence to the hypothesis (\\(p = Pr(H)\\)). This is, here, our prior belief that an authoritarian regime that has experienced economic crisis is a \\(b\\). Likelihood. The likelihood, \\(\\Pr(K=1|H)\\) is the probability of observing the clue, when we look for it in our case, if the hypothesis is true—i.e., here, if the case is a \\(b\\) type. The key feature of a clue is that the probability of observing the clue is believed to depend on the case’s causal type. In order to calculate the probability of the data we will in fact need two such probabilities: we let \\(\\phi_b\\) denote the probability of observing the clue for a case of \\(b\\) type (\\(\\Pr(K=1|j=b)\\)), and \\(\\phi_d\\) the probability of observing the clue for a case of \\(d\\) type (\\(\\Pr(K=1|j=d)\\)). The key idea in many accounts of process tracing is that the differences between these probabilities provides clues with ‘’probative value,’’ that is, the ability to generate learning about causal types. The likelihood, \\(\\Pr(K=1|H)\\), is simply \\(\\phi_b\\). Probability of the data. This is the probability of observing the clue when we look for it in a case, regardless of its type, \\((\\Pr(K=1))\\). More specifically, it is the probability of the clue in a treated case with a positive outcome. As such a case can only be a \\(b\\) or a \\(d\\) type, this probability can be calculated simply from \\(\\phi_b\\) and \\(\\phi_d\\), together with our beliefs about how likely an \\(X=1, Y=1\\) case is to be a \\(b\\) or a \\(d\\) type. This probability aligns (inversely) with Van Evera’s concept of ‘’uniqueness.’’ Inference. We can now apply Bayes’ rule to describe the learning that results from process tracing. If we observe the clue when we look for it in the case, then our posterior belief in the hypothesis that the case is of type b is: \\[\\begin{eqnarray*} \\Pr(j = b |K=1, X=Y=1)= \\frac{\\phi_b p }{\\phi_b p+\\phi_d (1-p)} \\end{eqnarray*}\\] In this exposition we did not make use of a causal model in a meaningful way—we simply need the priors and the clue probabilities. In fact, however, these numbers can be derived from a causal model. To illustrate, imagine a simple causal model in which the \\(X, Y\\) relationship is completely mediated by \\(K\\). In particular, suppose, from background knowledge of the conditional distribution of outcomes given their causes, we have that: \\(\\Pr(K=1 | X=0) = 0\\), \\(\\Pr(K=1 | X=1) = .5\\) \\(\\Pr(Y=1 | K=0) = .5\\), \\(\\Pr(Y=1 | K=1) = 1\\) This data is consistent with a world in which half \\(b\\) and \\(c\\) types in the first step and half \\(b\\) and \\(d\\) types in the second step. Assume that the case at hand is sampled from this world. Then we can calculate that the prior probability, \\(p\\), that \\(X\\) caused \\(Y\\) given \\(X=Y=1\\) is \\(p = \\frac13\\).38 We can also calculate the probability that \\(K=1\\) for a treated \\(b\\) and \\(d\\) case respectively as \\(\\phi_b=1\\) and \\(\\phi_d=0.5\\) (convince yourself of these numbers!). We then get: \\[\\begin{eqnarray*} \\Pr(j = b |K=1, X=Y=1)&amp;=&amp;\\frac{1\\times \\frac13}{1 \\times \\frac13 + 0.5 \\times \\frac23}=0.5 \\end{eqnarray*}\\] We thus shift our beliefs from a prior of \\(\\frac13\\) to a posterior of \\(\\frac12\\). In contrast had we not observed the clue our posterior would have been 0. As should be clear from the above, the inferential leverage in process tracing comes from differences in the probability of observing \\(K=1\\) for different causal types. Thus, the logic described here generalizes Van Evera’s familiar typology of tests by conceiving of the certainty and uniqueness of clues as lying along a continuum. Van Evera’s four tests (“smoking gun,” “hoop,” “straw in the wind,” and “doubly decisive”) represent, in this sense, special cases—particular regions that lie on the boundaries of a “probative-value space.” To illustrate the idea, we represent the range of combinations of possible probabilities for \\(\\phi_b\\) and \\(\\phi_d\\) as a square in Figure and mark the spaces inhabited by Van Evera’s tests. As can be seen, the type of test involved depends on both the relative and absolute magnitudes of \\(\\phi_b\\) and \\(\\phi_d\\). The probative value of a test depends on the difference between them. Thus, a clue acts as a smoking gun for proposition “\\(b\\)” (the proposition that the case is a \\(b\\) type) if it is highly unlikely to be observed if proposition \\(b\\) is false, and more likely to be observed if the proposition is true (bottom left, above diagonal). A clue acts as a “hoop” test if it is highly likely to be found if \\(b\\) is true, even if it still quite likely to be found if it is false. Doubly decisive tests arise when a clue is very likely if \\(b\\) and very unlikely if not. It is, however, also easy to imagine clues with probative qualities lying in the large space amidst these extremes. Figure 5.2: A mapping from the probability of observing a clue if the proposition that a case is a \\(b\\) type is true (\\(\\phi_b\\)) or false (\\(\\phi_d\\)) to a generalization of the tests described in Van-Evera (1997). In this illustration we note that we draw both the priors and the probative value from a causal model. If we altered the model—for example if we had a stronger first stage and so a larger value for \\(\\Pr(K=1|X=0)\\)—this would alter both our prior, \\(p\\), and our calculations of \\(\\phi_d\\). An implication of this is that, although one might be tempted to think of the priors and the probative values as independent quantities, and contemplate how inferences change as priors change (as we did for example in Appendix FLAG REF), keeping probative value fixed, that kind of thought experiment may assume values that are justified by an underlying model. 5.3 Three principles of Bayesian updating FLAG: REDO THESE THREE EXAMPLES WITHOUT PHI 5.3.1 Priors matter The amount of learning that results from a given piece of new data depends strongly on prior beliefs. We saw this already with the example of interpreting our test results above. Figure ?? illustrates the point for process tracing inferences. In each subgraph of Figure ?? , we show how much learning occurs under different scenarios. The horizontal axis indicates the level of prior confidence in the hypothesis and the curve indicates the posterior belief that arises if we do (or do not) observe the clue. As can be seen, the amount of learning that occurs—the shift in beliefs from prior to posterior—depends a good deal on what prior we start out with. For a smoking gun test, the amount of learning is highest for values roughly in the 0.2 to 0.4 range—and then declines as we have more and more prior confidence in our hypothesis. For a hoop test, the amount of learning when the clue is not observed is greatest for hypotheses in which we have middling-high confidence (around 0.6 to 0.8), and minimal for hypotheses in which we have a very high or a very low level of confidence. Figure 5.3: Figure shows how the learning from different types of tests depends on priors regarding the proposition. A smoking gun test has the greatest impact on beliefs when priors are middling low and the clue is observed; a ‘’hoop test’’ has the greatest effect when priors are middling high and the clue is not observed. The implication here is that our inferences with respect to a hypothesis must be based not just on the search for a clue predicted by the hypothesis but also on the plausibility of the hypothesis, based on other things we know. Suppose, for instance, that we fail to observe evidence that we are 90 percent sure we should observe if a hypothesized causal effect has occurred: a strong hoop test is failed. But suppose that the existing literature has given us a very high level of confidence that the hypothesis is right. This high prior confidence, sometimes referred to as a “base rate,” is equivalent to believing that the causal effect exists in a very high proportion of cases. Thus, while any given case with a causal effect has only a 0.1 chance of not generating the clue, the high base rate means that the vast majority of cases that we observe without the clue will nonetheless be cases with causal effects. Thus, the failure of even a strong hoop test, involving a highly certain prediction, should only marginally reduce our confidence in a hypothesis that we strongly expect to be true. A similar line of reasoning applies to smoking gun tests involving hypotheses that prior evidence suggests are very unlikely to be true. Innocent people may be very unlikely to be seen holding smoking guns after a murder. But if a very high proportion of people observed are known to be innocent, then a very high proportion of those holding smoking guns will in fact be innocent—and a smoking-gun clue will be far from decisive. We emphasize two respects in which these implications depart from common intuitions. First, we cannot make general statements about how decisive different categories of test, in Van Evera’s framework, will be. It is commonly stated that hoop tests are devastating to a theory when they are failed, while smoking gun tests provide powerful evidence in favor of a hypothesis. But, in fact the amount learned depends not just on features of the clues but also on prior beliefs. Second, although scholars frequently treat evidence that goes against the grain of the existing literature as especially enlightening, in the Bayesian framework the contribution of such evidence may sometimes be modest, precisely because received wisdom carries weight. Thus, although the discovery of disconfirming evidence—an observation thought to be strongly inconsistent with the hypothesis—for a hypothesis commonly believed to be true is more informative (has a larger impact on beliefs) than confirming evidence, this does not mean that we learn more than we would have if the prior were weaker. % But it is not true as a general proposition that we learn more the bigger the “surprise” a piece of evidence is. %The effect of disconfirming evidence on a hypothesis about which we are highly confident will be smaller than it would be for a hypothesis about which we are only somewhat confident. When it comes to very strong hypotheses, the “discovery” of disconfirming evidence is very likely to be a false negative; likewise, the discovery of supporting evidence for a very implausible hypothesis is very likely to be a false positive. The Bayesian approach takes account of these features naturally.39 5.3.2 Simultaneous, joint updating When we update we often update over multiple quantities. When we see a smoking gun, for instance, we might update our beliefs that the butler did it, but we might also update our beliefs about how likely we are to see smoking guns – maybe they are not so rare as we thought! Intuitively you might think of this updating as happening sequentially – first of all you update over the general proposition, then you update over the particular claim. But in fact you update over both quantities at once. Here we elaborate on the intuition of fully Bayesian process tracing, in which updating occurs over both causal type (\\(j\\)) and beliefs about the probabilities with which clues are observed for each type (\\(\\phi\\) values). The illustration in the text makes clear how updating over type occurs, given beliefs about \\(\\phi\\) values. But how does updating over \\(\\phi\\) occur? Suppose that we observe a case with values \\(X=1, Y=1\\). We begin by defining a prior probability distribution over each parameter. Suppose that we establish a prior categorical distribution reflecting uncertainty over whether the case is a \\(b\\) type (e.g., setting a probability of 0.5 that it is a \\(b\\) and 0.5 that is a \\(d\\) type). We also start with priors on \\(\\phi_b\\) and \\(\\phi_d\\). For concreteness, suppose that we are certain that the clue is unlikely for a \\(d\\) type (\\(\\phi_d=.1\\)), but we are very uncertain about \\(\\phi_b\\); in particular, we have a uniform prior distribution over \\([0,1]\\) for \\(\\phi_b\\). Note that, even though we are very uncertain about \\(\\phi_b\\), the clue still has probative value, arising from the fact that the expected value of \\(\\phi_b\\) is higher than that of \\(\\phi_d\\). Suppose that we then look for the clue in the case and observe it. This observation shifts posterior weight away from a belief that the case is a \\(b\\). See Figure for an illustration. Yet it simultaneously shifts weight toward a higher value for \\(\\phi_b\\) and a lower value for \\(\\phi_d\\). The reason is that the observed clue has a relatively high likelihood both for combinations of parameter values in which the case is a \\(d\\) and \\(\\phi_b\\) is low and for combinations in which the case is a \\(b\\) and \\(\\phi_b\\) is high (or, equivalently, in this example, where \\(\\phi_d\\) is low). The marginal posterior distribution of \\(\\phi_b\\) will thus be shifted upward relative to its prior marginal distribution. The joint posterior distribution will also reflect a dependency between the probability that the case is a \\(b\\) vs. a \\(d\\), on the one hand, and \\(\\phi_b\\) and \\(\\phi_d\\) on the other. Figure 5.4: Joint posteriors distribution on whether a case is a \\(b\\) or \\(d\\) and on the probability of seeing a clue for a \\(b\\) type (\\(\\phi_b\\)). 5.3.3 Posteriors are independent of the ordering of data We often think of learning as a process in which we start off with some set of beliefs—our priors, we gather data, \\(D_1\\), and update our beliefs, forming a posterior; we then observe new data and we update again, forming a new posterior, having treated the previous posterior as a new prior. In such cases it might seem natural that it matters which data we saw first and which later. For instance EXAMPLE In fact though, Bayesian updating is deaf to ordering. If we learn first that the card is a face card and second that it is black, our posteriors that it is a Jack of Spades go from 1 in 52 to 1 in 12 to 1 in 6. If we learn first that the card is black and second that it is a face card, our posteriors that it is a Jack of Spades go from 1 in 52 to 1 in 26 to 1 in 6. We end up in the same places in both cases. And we would ave had the same conclusion if we learned in one go that the card is a black face card. The math of this is easy enough. Our posterior given two sets of data \\(D_1, D_2\\) can be written: \\[p(\\theta | D_1, D_2) = \\frac{p(\\theta, D_1, D_2)}{p(D_1, D_2)} = \\frac{p(\\theta, D_1 | D_2)p(D_2)}{p(D_1 | D_2)p(D_2)}= \\frac{p(\\theta, D_1 | D_2)}{p(D_1 | D_2)}\\] or, equivalently: \\[p(\\theta | D_1, D_2) = \\frac{p(\\theta, D_1, D_2)}{p(D_1, D_2)} = \\frac{p(\\theta, D_2 | D_1)p(D_1)}{p(D_2 | D_1)p(D_1)}= \\frac{p(\\theta, D_2 | D_1)}{p(D_2 | D_1)}\\] In other words our posteriors given both \\(D_1\\) and \\(D_2\\) can be thought of as the result of updating on \\(D_2\\) given we already know \\(D_1\\) or the result of updating on \\(D_1\\) given we already know \\(D_2\\). This fact will be useful in applications. In practice we might assume that we have beliefs based on background data \\(D_1\\), for example regarding general relations between \\(X\\) and \\(Y\\) and a flat prior, and we then update again with new data on \\(K\\). Rather than updating twice, the fact that updating is invariant to order means that we can assume a flat prior and update once given data on \\(X\\), \\(Y\\), and \\(K\\). References "],
["pt.html", "Chapter 6 Process Tracing with Causal Models 6.1 Process tracing and causal models 6.2 Five principles", " Chapter 6 Process Tracing with Causal Models We connect the literature on causal models to qualitative inference strategies used in process tracing. We provide a procedure for inference on case level queries from causal models . In addition we extract a set of implications for process tracing. We show how a key result from the causal models literature provides a condition for when clues may be (or certainly will not be) informative. 6.1 Process tracing and causal models This chapter demonstrates how we can use causal models to conduct confirmatory process tracing: that is, to draw causal inferences about a single case from case-level data. 6.1.1 The intuition We first walk through the basic intuition and then provide a more formal account. When we undertake process tracing, we seek to answer a causal query about a given case. The key insight driving our approach is that the inference about a causal estimand for a case is a claim about what causal types are both likely ex ante (given prior knowledge) and consistent with the data.40 The estimand of interest can be a statement about any number of case-level causal features, including a case-level causal effect, the pathway through which an effect operates, an actual cause, or causal attribution. We will use observations from the case itself to address this query. We do so via a procedure in which we first encode prior knowledge in the form of a causal model, use data to learn about features of the model, and then take what we have learned about the model and map it into our query. Given a causal model, we form posteriors over estimands as follows: Specify all causal types. A causal type, recall, specifies the values that a unit is expected to take, absent any interventions, but also the values it would take given some interventions on some variables. Examples of types might be: Type 1: (\\(X=1\\)) and (\\(Y=1\\) if \\(X=1\\), \\(Y=0\\) if \\(X=0\\)). Type 2: (\\(X=0\\)) and (\\(Y=1\\) if \\(X=1\\), \\(Y=0\\) if \\(X=0\\)). Type 3: (\\(X=1\\)) and (\\(Y=1\\) if \\(X=1\\), \\(Y=1\\) if \\(X=0\\)). Specify priors over causal types. Report how likely you think it is that a given unit is of a particular causal type. In the simplest case one might place 0 weight on some causal types (that might be ruled out by theory, for example) and equal weight on the others. Specify the estimand in terms of causal types. For instance the estimand “\\(Y\\) responds positively to \\(X\\)” can be thought of as a collection of causal types: Q={Type 1, Type 2}.41 Specify the set of causal types that are consistent with the data. For instance if we observe \\(X=1, Y=1\\) we might specify the data-consistent set as {Type 1, Type 3.}. Update. Updating is done then by adding up the prior probabilities on all causal types that are consistent with both the data and the estimand, and dividing this by the sum of prior probabilities on all causal types that are consistent with the data (whether or not they are consistent with the estimand). Figure 6.1: Logic of simple updating on arbitrary estimands. This process is represented graphically with Figure 6.1, where we can think of probabilities as proportionate to areas. Our causal model defines the causal type space. We then proceed by a process of elimination. Only some of the causal types in the model are consistent with prior knowledge. Only some are consistent with the data that we observe. Finally, any query itself maps onto a subset of the possible causal types. The causal types that remain in contention once we have observed the evidence are those at the intersection of consistency with priors and consistency with the data. \\(A\\) represents those types that are also consistent with a given answer to the query (say, \\(X\\) has a positive effect on \\(Y\\)). Thus, our belief about the query before we have seen the data is the probability of all causal types consistent with our priors and with the query (\\(A + B\\)) as a proportion of all types consistent with our priors. Once we have seen the data, we have reduced the permissible types to \\(A + C\\). Our posterior belief on the query is, then, the probabilities of those remaining types that are consistent with the query as a share of the probabilities of all remaining types, or \\(A/(A+C)\\). What we are doing here is straightforward: assessing causal possibilities for their compatibility with both the evidence at hand and our prior knowledge of how the world works. The formalization that we will present ensures that prior knowledge and evidence are all recorded explicitly while forcing logical consistency on the inferences that emerge from them. 6.1.2 A formalization of the general approach More formally, the general approach to inference draws on the components we outlined in chapters 2 to 4: graphical causal models (DAGs), nodal and causal types, and priors. We now show how these elements formally interact with data to generate causal inferences. We continue to focus on a situation with binary variables, though suggest later in the chapter how this can be extended. Though we walk through the procedure for simple models, the approach outlined here can be applies to any causal model with binary variables and to any estimands defined over the model. The process tracing procedure operates as follows: A DAG. We begin with a DAG, or graphical causal model. As we know, a DAG identifies a set of variables and describes the parent-child relations between them, indicating for each variable which other variables are its direct (possible) causes. These relationship, in turn, tell us which (non-descendant) variables a given variable is not independent of given the other variables in the model. Nodal types. Once we have specified a DAG, we have defined the full set of possible nodal types: the types defining the value that a variable will take on given the values of its parents, which we have denoted with \\(\\theta\\) values. At each node, the range and number of nodal types is defined by the number of parents that that node has and the number of values the variables can take on. For instance, assuming all variables to be binary, if \\(Y\\) has parents \\(X\\) and \\(W\\) (so \\(k=2\\)), then there are \\(2^{\\left(2^2\\right)}=16\\)) possible causal types for the \\(Y\\) node. There are \\(2^2\\) possible combinations of values that two binary causal variables can take on—-\\((X=0,W=0), (X=0,W=1), (X=1,W=0), (X=1,W=1)\\)—which implies four possible causal conditions over which \\(Y\\)’s possible responses must be defined. For instance, as we have seen, with two causal variables, we can have \\(\\theta^Y_{0000}\\), where \\(Y\\) is always 0; \\(\\theta^Y_{0001}\\), where \\(Y\\) is 0 unless both \\(X\\) and \\(W\\) are 1; and so on.42 To get the total number of nodal types, we simply raise \\(2\\) (since \\(Y\\) is binary) to the number of causal conditions (4), giving the number of possible patterns of \\(Y\\) values that could be generated across these four conditions (16). (The full set of nodal types for two causal variables in a binary setup is given in 2.3.)43 All variables in a model have nodal types defining the value they take on given the value of their parents, including those variables without substantive parents. Suppose that \\(X\\) and \\(W\\), in this model, have no substantively defined parents. We nonetheless define a nodal type for each of them, which simply captures their exogenous assignment to some value. With \\(X\\) binary, for instance, there are two nodal types, \\(\\theta^X_{0}\\), where \\(X\\) is set to \\(0\\), and \\(\\theta^X_{1}\\), where \\(X\\) is set to \\(1\\). Causal types. We will want to be able to conceive not just of types for individual nodes but of the full collection of nodal types across all nodes in a model. We refer to a unit’s full set of nodal types as its causal type, which we represent as \\(\\theta\\). A causal type is simply a listing that contains one nodal type for each node in the model. For instance, with a model with variable \\(X\\), \\(W\\), and \\(Y\\), each unit has a causal type composed of its nodal types on each of the three nodes.44 Thus, one causal type in this model could be \\(\\theta = (\\theta^X = \\theta^X_1, \\theta^W = \\theta^W_1, \\theta^Y = \\theta^Y_{1101})\\). Another could be \\(\\theta = (\\theta^X = \\theta^X_0, \\theta^W = \\theta^W_1, \\theta^Y = \\theta^Y_{0001})\\). And so on. We show the mapping between nodal and causal types, for a simply \\(X \\rightarrow Y\\) model, in Table 6.1. The column headings represent the \\(8\\) permissible causal types, each expressed simply as a concatenated strings of nodal types. The row headings represent the nodal types. In each interior cell, a \\(1\\) or \\(0\\) indicates whether or not a given nodal type is a component of a given causal type. As can be seen, each causal type has two nodal types that are its components since there are two nodes in this model. Each \\(X\\)-nodal type is part of four causal types since it can be combined with four different \\(Y\\)-nodal types, while each \\(Y\\)-nodal type is part of two causal types since it can be combined with two \\(X\\)-nodal types. Table 6.1: . A mapping between nodal types and causal types for a simple \\(X \\rightarrow Y\\) model. Causal Types \\(\\rightarrow\\) \\(\\theta^X_0\\).\\(\\theta^Y_{00}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{00}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{10}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{10}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{01}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{01}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{11}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{11}\\) Nodal types \\(\\downarrow\\) \\(\\theta^X_0\\) 1 0 1 0 1 0 1 0 \\(\\theta^X_1\\) 0 1 0 1 0 1 0 1 \\(\\theta^Y_{00}\\) 1 1 0 0 0 0 0 0 \\(\\theta^Y_{10}\\) 0 0 1 1 0 0 0 0 \\(\\theta^Y_{01}\\) 0 0 0 0 1 1 0 0 \\(\\theta^Y_{11}\\) 0 0 0 0 0 0 1 1 Priors: Our background beliefs about a causal domain usually will consist of more than just beliefs about which variables have causal connections; they will also usually contain beliefs about what kinds of effects operate between variables. That is, they will contain beliefs about which types are possible or, more generally, are more or less common in the world. We express these beliefs over causal effects as either restrictions on nodal types or as probability distributions over the nodal types. In general, when doing process tracing in this framework, we think of a given case of interest – the one we are studying and seek to learn about – as being drawn at random from a population. Thus, our prior beliefs about a single case – before we do the process tracing – are really beliefs about that population. So, for instance, our prior belief about the probability that inequality has a positive effect on democratization in Mexico in 1999 is our belief about how commonly inequality has a positive effect on democratization in the population of cases that are “like” Mexico in 1999.45 We let \\(\\lambda^j\\) denote our belief about the population distribution of nodal types at node \\(j\\). A \\(\\lambda^j\\) is simply a vector of proportions, one for each possible nodal type, with the proportions adding up to \\(1\\). So, for instance, \\(\\lambda^Y\\) for our current example would be a vector with four values, each of which expresses a proportion for on one of the four nodal types at \\(Y\\). So we might have \\(\\lambda^Y_{0001}=0.1\\), \\(\\lambda^Y_{0011}=0.05\\), and so on – with the \\(\\lambda^Y\\) values summing to \\(1\\) because these values are defined over the full set of possible nodal types for \\(Y\\). We can, in turn, use these population parameters – these beliefs about nodal-type proportions in the population – to create prior probabilities over the causal type for the case at hand. Since causal types are merely combinations of nodal types, and our case has been drawn at random from the population, we can take a set of posited proprtions of nodal types in the population and readily calculate the probability that our case is of any given causal type. To do so, we need to join together \\(\\lambda\\)’s across the nodes in a model. Let us first see how this works in a situation in which we assume that the nodal types are independent of one another. We can think of this as a situation in which there is no confounding that is not captured in the graph – no variable missing from the model that is a common ancestor of multiple nodes in the model. Here, our beliefs over causal types are simply the product of our beliefs over the component nodal types (since the joint probability of independent events is simply the product of their individual probabilities). For instance, one causal type might be “a unit in which \\(X=1\\) and in which \\(Y=1\\) no matter what value \\(X\\) takes.” In this case the probability that a case is of this causal type might be written \\(\\Pr(\\theta^X = \\theta^X_1)\\Pr(\\theta^Y = \\theta^Y_{11}) = \\lambda^X_1\\lambda^Y_{11}\\). The simplest way in which we can express beliefs about the differential probabilities of different causal possibilities is by eliminating nodal types that we do not believe to be possible—setting their parameter values to \\(0\\). Suppose, for instance, that we are examining the effect of ethnic diversity on civil war in a case. We might not know whether ethnic diversity causes civil war in this case, but we might have sufficient background knowledge to believe that ethnic diversity never has a negative effect on civil war: it never prevents a civil war from happening that would have happened in the absence of ethnic diversity. We would thus want to set the parameter value for a negative causal effect to \\(0\\). If we then know nothing about the relative frequencies of the three remaining nodal types for \\(Y\\), we may (following the principle of indifference), frequency of positive effects, null effects with civil war destined to happen, and null effects with civil war never going to happen, assigning a weight of \\(\\frac{1}{3}\\) to each of them. In a situation of unobserved confounding, our beliefs over causal types are still well defined, though they are no longer the simple product of beliefs over nodal types. Let us imagine for instance, in a simple \\(X \\rightarrow Y\\) model, that we believe that some unobserved factor both makes cases more likely to have \\(X = 1\\) and makes it more likely that \\(X\\) has a positive effect on \\(Y\\). This is the same as saying that the probability that \\(\\theta^X = \\theta^X_1\\) is positively correlated with the probability that \\(\\theta^Y = \\theta^Y_{01}\\). Now, our probability that both \\(X=1\\) and \\(X\\) has a positive effect must be calculated using the joint probability formula, \\(\\Pr(A, B) = \\Pr(A)\\Pr(B|A)\\).46 Thus, \\(\\Pr(\\theta^Y = \\theta^Y_{01}, \\theta^X = \\theta^X_1) = \\Pr(\\theta^Y = \\theta^Y_{01})\\Pr(\\theta^X = \\theta^X_1 | \\theta^Y = \\theta^Y_{01})\\). To form priors over causal types in this situation, we need to posit beliefs about a set of more complex, conditional proportions for \\(X\\)’s type. Specifically, we need to posit, for those cases with a positive effect of \\(X\\) on \\(Y\\), what proportion are “assigned” to \\(X=1\\); and, separately, what proportion are assigned to \\(X=1\\) among those cases without a positive effect of \\(X\\) on \\(Y\\). These conditional proportions may, of course, be difficult for the researcher to form beliefs about. Forming a belief about them amounts to saying that we do not know what generates confounding, but we know the correlations it generates in the data. We may wonder how often we will be in that epistemological position. An alternative way to parse the problem, then, is to model the confounding by including the confounder (say, \\(Z\\)) as a new node in the graph. In the above example, \\(Z\\) would point into both \\(X\\) and \\(Y\\). We would then posit population proportions for a set of nodal types for \\(X\\) – representing \\(X\\)’s possible responses to \\(Z\\) – and for \\(Y\\) – representing \\(Y\\)’s possible responses to both \\(X\\) and \\(Z\\). We may find it easier to reason and form beliefs about these more complex nodal types than about the conditional proportions involved in unobserved confounding. The two approaches work out to be analytically equivalent given equivalent underlying beliefs, so the choice between them will be a matter of researcher preference.47 Importantly, in process tracing, we are focused on drawing case-level inferences and, as such, we treat the population-level parameters as given and fixed. In general, these parameters derive from our beliefs about how the world works, and those beliefs will typically be uncertain. The key point, however, is that in process tracing, the population parameters serve as an input into the analysis, conditioning our inferences from the evidence; but we do not update on these population-level beliefs once we see the data from a single case. Importantly, as we show later in the book, we do update on population-level inferences in the more general setup that we introduce in Chapter 8 for analyzing mixed data in multiple cases. We also show in Chapter 15 how we can test the sensitivity of conclusions to the values at which we set population parameters. Interestingly, as we also show, process-tracing inferences, including uncertainty about conclusions, are unaffected by the level of uncertainty we might have about population parameters; we thus do not specify this uncertainty for the purposes of process tracing. The relationship between causal types, nodal types, and the correlation among nodal types is captured in what we call a parameter matrix. We show a parameter matrix for a simple \\(X \\rightarrow Y\\) model with no unobserved confounding in Table ??. Here each column label (except the last) represents the probability that a case is of a given causal type. Each row label represents a population-level parameter: a belief about the proportions of different nodal types in the population. We indicate a set of possible parameter values in the final column. Table 6.2: . A mapping between nodal types and causal types for a simple \\(X \\rightarrow Y\\) model (with no unobserved confounding). Causal types \\(\\rightarrow\\) \\(\\theta^X_0,\\theta^Y_{00}\\) \\(\\theta^X_1,\\theta^Y_{00}\\) \\(\\theta^X_0,\\theta^Y_{10}\\) \\(\\theta^X_1,\\theta^Y_{10}\\) \\(\\theta^X_0,\\theta^Y_{01}\\) \\(\\theta^X_1,\\theta^Y_{01}\\) \\(\\theta^X_0,\\theta^Y_{11}\\) \\(\\theta^X_1,\\theta^Y_{11}\\) Parameter values (population proportions) Population parameters \\(\\downarrow\\) \\(\\lambda^X_1\\) 0 1 0 1 0 1 0 1 0.5 \\(\\lambda^X_0\\) 1 0 1 0 1 0 1 0 0.5 \\(\\lambda^Y_{00}\\) 1 1 0 0 0 0 0 0 0.2 \\(\\lambda^Y_{10}\\) 0 0 1 1 0 0 0 0 0.2 \\(\\lambda^Y_{01}\\) 0 0 0 0 1 1 0 0 0.4 \\(\\lambda^Y_{11}\\) 0 0 0 0 0 0 1 1 0.2 To start with the first two rows, these represent the population proportions of each of \\(X\\)’s nodal types. For instance, \\(\\lambda^X_{0}\\) is our belief about the proportion of cases in the population that are of nodal type \\(\\theta^X_{0}\\). The first row, \\(\\lambda^X_{1}\\), represents our belief about the inverse: the proportion of cases in the population of type \\(\\theta^X_{1}\\). We posit beliefs about these parameters in the final column, indicating that we think that half of cases in the population are “assigned” to \\(X=0\\) and half to \\(X=1\\). Note that, since there are only two possible nodal types for \\(X\\), and their proportions must sum to 1, there is actually just one degree of freedom here: once we’ve specified one of these parameter values, the other is defined as well. The last four rows represent the proportion of cases in the population with different \\(Y\\)-nodal types: in order, the proportion in which \\(X\\) has no effect on \\(Y\\), with \\(Y\\) fixed at \\(0\\); the proportion in which \\(X\\) has a negative effect; the proportion in which \\(X\\) has a positive effect; and the proportion in which \\(X\\) has no effect, with \\(Y\\) fixed at \\(1\\). Again, in the last column, we provide possible values for these proportions, the four of which must also sum to \\(1\\). Here we are stating that positive \\(X \\rightarrow Y\\) effects are twice as common in the population as the other three nodal types, which we set at equal prevalence. The interior cells indicate whether a given population parameter enters into the prior probability of a given causal type. Thus, for instance, to calculate the prior probability of the causal type \\(\\theta^X_1, \\theta^Y_{10}\\), we need to multiply the two parameters values corresponding to the \\(1\\)’s in this causal type’s column: \\(\\lambda^X_1\\) by \\(\\lambda^Y_{10}\\). Given the parameter values we have assigned for this example, then, the prior on this causal type is simply \\(0.5 \\times 0.2 = 0.1\\). The prior probability that a case is of a given causal type thus comes directly from our beliefs about how nodal types are distributed in the population. All we know before we study a case is whatever we know about cases “like” it in general. It is then these causal-type probabilities – which represent probabilities that a given case is of a particular causal type – that we will update on once we see the data for this case. We show the somewhat more complex situation of unobserved confounding in Table ??. It is the first four rows that allow for unobserved confounding—the correlations across types. In a potential outcomes framework, we could think of these rows as capturing differential “assignment propensities” for \\(X\\). Here, we allow for different probabilities of \\(X\\)’s type being \\(\\theta^X_1\\) depending on what \\(Y\\)’s type is. Thus, \\(\\lambda^X_0 | \\theta^Y= \\theta^Y_{01}\\) is the proportion of \\(\\theta^X_0\\) types among cases with \\(\\theta^Y_{01}\\) type: put differently, it is the probability of \\(X\\) being assigned to \\(0\\) when \\(X\\) has a positive effect on \\(Y\\). The second row represents the inverse proportion: the proportion of a \\(\\theta^X_1\\) types among \\(\\theta^Y_{01}\\) types. The next two rows then capture the proportions of the \\(X\\)-types among all other \\(Y\\)-types (i.e., among those cases for which \\(X\\) does not have a positive effect on \\(Y\\)). Unobserved confounding in this setup takes the form of a difference in the proportions of a given \\(X\\) type among different \\(Y\\) types. Thus, if \\(\\lambda^X_1, | \\theta^Y_{01}\\) is not the same as \\(\\lambda^X_1 | \\theta^Y \\neq \\theta^Y_{01}\\), we have unobserved confounding. Imagine, for instance, if we are studying the effect of faster economic growth (\\(X\\)) on democratization (\\(Y\\)), and we believe that there is some unobserved factor that both makes some countries’ economies grow more quickly and also makes economic growth more likely to have a positive effect on democratization. This belief amounts to a belief that the probability of a case being assigned to \\(X=1\\) is higher if \\(Y\\)’s nodal type is \\(\\theta^Y_{01}\\) than if it is not. In other words, in terms of the rows in Table ??, we believe here that \\(\\lambda^X_1 | \\theta^Y=\\theta^Y_{01}\\) is greater than \\(\\lambda^X_1 | \\theta^Y \\neq \\theta^Y_{01}\\). To illustrate, we provide parameter values along these lines in the final column. Again, however, a researcher might prefer to specify the confounder (say, \\(Z\\)) as a node in the model. The rows in the parameter matrix would then be a set of population parameters defined as proportions of unconditional nodal types, with four \\(X\\)-types representing possible responses to \\(Z\\), and 16 \\(Y\\) types, representing \\(Y\\)’s possible responses to \\(X\\) and \\(Z\\). Table 6.3: . A mapping between nodal types and causal types for a simple \\(X \\rightarrow Y\\) model with unobserved confounding. Causal Types \\(\\rightarrow\\) \\(\\theta^X_0,\\theta^Y_{00}\\) \\(\\theta^X_1,\\theta^Y_{00}\\) \\(\\theta^X_0,\\theta^Y_{10}\\) \\(\\theta^X_1,\\theta^Y_{10}\\) \\(\\theta^X_0,\\theta^Y_{01}\\) \\(\\theta^X_1,\\theta^Y_{01}\\) \\(\\theta^X_0,\\theta^Y_{11}\\) \\(\\theta^X_1,\\theta^Y_{11}\\) Population parameters \\(\\downarrow\\) \\(\\lambda^X_0 | \\theta^Y= \\theta^Y_{01}\\) 0 0 0 0 1 0 0 0 \\(\\lambda^X_1 | \\theta^Y=\\theta^Y_{01}\\) 0 0 0 0 0 1 0 0 \\(\\lambda^X_0| \\theta^Y \\neq \\theta^Y_{01}\\) 1 0 1 0 0 0 1 0 \\(\\lambda^X_1 | \\theta^Y \\neq \\theta^Y_{01}\\) 0 1 0 1 0 0 0 1 \\(\\lambda^Y_{00}\\) 1 1 0 0 0 0 0 0 \\(\\lambda^Y_{10}\\) 0 0 1 1 0 0 0 0 \\(\\lambda^Y_{01}\\) 0 0 0 0 1 1 0 0 \\(\\lambda^Y_{11}\\) 0 0 0 0 0 0 1 1 One special kind of prior that we might wish to set is to disallow a particular (conditional) type altogether. For instance, if studying the effect of we may believe that Possible data types. A data type is a particular pattern of data that we could potentially observe for a given case. More specifically, a data type is a set of values, one for each node in a model. For instance, in our \\(X, W, Y\\) setup, \\(X=1, W=0, Y=0\\) would be one data type. Importantly, each possible causal type maps into a single data type. One intuitive way to think about why this is the case is that a causal type tells us (a) the values to which all exogenous variables in a model are assigned and (b) how all endogenous variables respond to their parents. Given these two components, only one set of node values is possible. For example, causal type \\(\\theta = (\\theta^X = \\theta^X_1, \\theta^W = \\theta^W_0, \\theta^Y = \\theta^Y_{0100})\\) imples data \\(X=1, W=0, Y=1\\). There is no other set of data that can be generated by this causal type. Equally importantly, however, the mapping from causal types to data types is not one-to-one. More than one causal type can generate the same case-level data pattern. For instance, the causal type \\(\\theta = (\\theta^X = \\theta^X_1, \\theta^W = \\theta^W_0, \\theta^Y = \\theta^Y_{1101})\\) will also generate the data type, \\(X=1, W=0, Y=1\\). Thus, observing this data type leaves us with ambiguity about the causal type by which it was generated. A full mapping between causal types and data types can be summarized by an “ambiguity matrix.” In Table 6.4, we provide an example of such a matrix, derived directly from the parameter matrix in Table ??. Here, the rows represent causal types and the columns (except for the last) represent data types. The notation for data types is straightforward, with for instance \\(X0Y0\\) meaning that \\(X=0, Y=0\\) has been observed. In the interior cells, the \\(1\\)’s and \\(0\\)’s indicate whether or not a given data type could arise from a given causal type. We can readily see here that each causal type can generate only one data type. We can also see the ambiguity of the data, however, since each data type can be generated by two causal types. For instance, if we observe \\(X=1, Y=1\\), we know that the case is either of causal type \\(\\theta^X_1,\\theta^Y_{01}\\) or of causal type \\(\\theta^X_1,\\theta^Y_{11}\\) – but do not know which. Table 6.4: . An ambiguity matrix, mapping from data types to causal types for a simple \\(X \\rightarrow Y\\) model. Data types \\(\\rightarrow\\) X0Y0 X1Y0 X0Y1 X1Y1 Priors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_0,\\theta^Y_{00}\\) 1 0 0 0 0.1 \\(\\theta^X_1,\\theta^Y_{00}\\) 0 1 0 0 0.1 \\(\\theta^X_0,\\theta^Y_{10}\\) 0 0 1 0 0.1 \\(\\theta^X_1,\\theta^Y_{10}\\) 0 1 0 0 0.1 \\(\\theta^X_0,\\theta^Y_{01}\\) 1 0 0 0 0.2 \\(\\theta^X_1,\\theta^Y_{01}\\) 0 0 0 1 0.2 \\(\\theta^X_0,\\theta^Y_{11}\\) 0 0 1 0 0.1 \\(\\theta^X_1,\\theta^Y_{11}\\) 0 0 0 1 0.1 In the last column, we provide prior probabilities for each of the causal types. These have been calculated directly from the parameter matrix (Table ??). To see how the calculation works, start with a causal type in the parameter matrix – say, \\(\\theta^X_0,\\theta^Y_{01}\\). We go down that causal type’s column and select the rows with \\(1\\)’s, representing the parameters for the included nodal types, \\(\\lambda^X_0\\) and \\(\\lambda^Y_{01}\\). As we want the joint probability of these two nodal types (and a parameter matrix is constructed such that the rows represent independent events),48 we simply multiply together the values for these included parameters: \\(0.5 \\times 0.4 = 0.2\\). As noted, our prior belief about whether the case at hand is of a given causal type is a straightforward function of our beliefs about how prevalent each of the component nodal types is in the population. As models get more complex, the numbers of causal and data types simply multiply. In Table 6.5, we show the ambiguity matrix for a simple mediation model (\\(X \\rightarrow M \\rightarrow Y\\)). Here, the causal types are combinations of three nodal types, one for each variable in the model. Similarly, the data types have three elements, one for each variable. We now have 8 data types and 32 causal types. Table 6.5: . An ambiguity matrix, mapping from data types to causal types for a simpe mediation model, \\(X \\rightarrow M \\rightarrow Y\\). Data types \\(\\rightarrow\\) X0M0Y0 X1M0Y0 X0M1Y0 X1M1Y0 X0M0Y1 X1M0Y1 X0M1Y1 X1M1Y1 Priors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_0,\\theta^M_{00},\\theta^Y_{00}\\) 1 0 0 0 0 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{00}\\) 0 1 0 0 0 0 0 0 0.02 \\(\\theta^X_0,\\theta^M_{10},\\theta^Y_{00}\\) 0 0 1 0 0 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{00}\\) 0 1 0 0 0 0 0 0 0.02 \\(\\theta^X_0,\\theta^M_{01},\\theta^Y_{00}\\) 1 0 0 0 0 0 0 0 0.04 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{00}\\) 0 0 0 1 0 0 0 0 0.04 \\(\\theta^X_0,\\theta^M_{11},\\theta^Y_{00}\\) 0 0 1 0 0 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{00}\\) 0 0 0 1 0 0 0 0 0.02 \\(\\theta^X_0,\\theta^M_{00},\\theta^Y_{10}\\) 0 0 0 0 1 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{10}\\) 0 0 0 0 0 1 0 0 0.02 \\(\\theta^X_0,\\theta^M_{10},\\theta^Y_{10}\\) 0 0 1 0 0 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{10}\\) 0 0 0 0 0 1 0 0 0.02 \\(\\theta^X_0,\\theta^M_{01},\\theta^Y_{10}\\) 0 0 0 0 1 0 0 0 0.04 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{10}\\) 0 0 0 1 0 0 0 0 0.04 \\(\\theta^X_0,\\theta^M_{11},\\theta^Y_{10}\\) 0 0 1 0 0 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{10}\\) 0 0 0 1 0 0 0 0 0.02 \\(\\theta^X_0,\\theta^M_{00},\\theta^Y_{01}\\) 1 0 0 0 0 0 0 0 0.04 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{01}\\) 0 1 0 0 0 0 0 0 0.04 \\(\\theta^X_0,\\theta^M_{10},\\theta^Y_{01}\\) 0 0 0 0 0 0 1 0 0.04 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{00}\\) 0 1 0 0 0 0 0 0 0.04 \\(\\theta^X_0,\\theta^M_{01},\\theta^Y_{01}\\) 1 0 0 0 0 0 0 0 0.08 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{01}\\) 0 0 0 0 0 0 0 1 0.08 \\(\\theta^X_0,\\theta^M_{11},\\theta^Y_{01}\\) 0 0 0 0 0 0 1 0 0.04 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{01}\\) 0 0 0 0 0 0 0 1 0.04 \\(\\theta^X_0,\\theta^M_{00},\\theta^Y_{11}\\) 0 0 0 0 1 0 0 0 0.02 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{11}\\) 0 0 0 0 0 1 0 0 0.02 \\(\\theta^X_0,\\theta^M_{10},\\theta^Y_{11}\\) 0 0 0 0 0 0 1 0 0.02 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{11}\\) 0 0 0 0 0 1 0 0 0.02 \\(\\theta^X_0,\\theta^M_{01},\\theta^Y_{11}\\) 0 0 0 0 1 0 0 0 0.04 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{11}\\) 0 0 0 0 0 0 0 1 0.04 \\(\\theta^X_0,\\theta^M_{11},\\theta^Y_{11}\\) 0 0 0 0 0 0 1 0 0.02 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{11}\\) 0 0 0 0 0 0 0 1 0.02 Again, the ambiguities arising from data patterns are apparent. For instance, if we observe \\(X=1, M=0, Y=0\\), we see that there are four causal types that could have generated this pattern. To unpack the situation a bit, these data tell us that \\(\\theta^X = \\theta^X_1\\). But they do not tell us whether \\(M\\)’s type is such that \\(X\\) has a negative effect on \\(M\\) (\\(\\theta^M_{10}\\)) or \\(X\\) has no effect with \\(M\\) fixed at \\(0\\) (\\(\\theta^M_{00}\\)). Similarly, we do not know whether \\(M\\) has a positive effect on \\(Y\\) (\\(\\theta^Y_{01}\\)) or no effect with \\(Y\\) fixed at \\(0\\) (\\(\\theta^Y_{00}\\)). This leaves four combinations of nodal types—four causal types—that are consistent with the data. Our priors here derive from a set of parameter values, much like in the previous example, in which the \\(X\\) types are equally common (0.5 each); a positive effect of \\(X\\) on \\(M\\) is twice as common (0.4) as the other \\(M\\) types (all set to 0.2); and a positive effect of \\(M\\) on \\(Y\\) is twice as common (0.4) as all other \\(Y\\) types (all at 0.2). We can then easily see why we thus get priors on some causal types are higher than those on others: for instance, the two causal types with priors of 0.08 both have two positive effects (at the \\(X \\rightarrow Y\\) and \\(M \\rightarrow Y\\) stages) while the causal types with priors of 0.02 include no positive effects at either stage. Table 6.6: Ambiguity matrix for X -&gt; M -&gt; Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows. X0M0Y0 X1M0Y0 X0M1Y0 X1M1Y0 X0M0Y1 X1M0Y1 X0M1Y1 X1M1Y1 prior X0M00Y00 1 0 0 0 0 0 0 0 0.02 X1M00Y00 0 1 0 0 0 0 0 0 0.02 X0M10Y00 0 0 1 0 0 0 0 0 0.02 X1M10Y00 0 1 0 0 0 0 0 0 0.02 X0M01Y00 1 0 0 0 0 0 0 0 0.04 X1M01Y00 0 0 0 1 0 0 0 0 0.04 X0M11Y00 0 0 1 0 0 0 0 0 0.02 X1M11Y00 0 0 0 1 0 0 0 0 0.02 X0M00Y10 0 0 0 0 1 0 0 0 0.02 X1M00Y10 0 0 0 0 0 1 0 0 0.02 X0M10Y10 0 0 1 0 0 0 0 0 0.02 X1M10Y10 0 0 0 0 0 1 0 0 0.02 X0M01Y10 0 0 0 0 1 0 0 0 0.04 X1M01Y10 0 0 0 1 0 0 0 0 0.04 X0M11Y10 0 0 1 0 0 0 0 0 0.02 X1M11Y10 0 0 0 1 0 0 0 0 0.02 X0M00Y01 1 0 0 0 0 0 0 0 0.04 X1M00Y01 0 1 0 0 0 0 0 0 0.04 X0M10Y01 0 0 0 0 0 0 1 0 0.04 X1M10Y01 0 1 0 0 0 0 0 0 0.04 X0M01Y01 1 0 0 0 0 0 0 0 0.08 X1M01Y01 0 0 0 0 0 0 0 1 0.08 X0M11Y01 0 0 0 0 0 0 1 0 0.04 X1M11Y01 0 0 0 0 0 0 0 1 0.04 X0M00Y11 0 0 0 0 1 0 0 0 0.02 X1M00Y11 0 0 0 0 0 1 0 0 0.02 X0M10Y11 0 0 0 0 0 0 1 0 0.02 X1M10Y11 0 0 0 0 0 1 0 0 0.02 X0M01Y11 0 0 0 0 1 0 0 0 0.04 X1M01Y11 0 0 0 0 0 0 0 1 0.04 X0M11Y11 0 0 0 0 0 0 1 0 0.02 X1M11Y11 0 0 0 0 0 0 0 1 0.02 Updating on types given the data. Once we observe actual data in a case, we can then update on the probabilities assigned to each causal type. The logic is simple. When we observe a set of data from a case, we place \\(0\\) probability on all causal types that could not have produced these data; we then scale up the probabilities on all causal types that could have. We can see how this works within an ambiguity matrix. Let’s return to the ambiguity matrix in Table 6.4. We start out with a set of probability weights on all rows (causal types). Now, suppose that we observe the data \\(X=1, Y=1\\), i.e., data type \\(X1Y1\\). We then look down the \\(X1Y1\\) column, and we know that all rows with a \\(0\\) in them represent causal types that could not have generated these data. These causal types are thus excluded. What is left are two rows: \\(\\theta^X_1, \\theta^Y_{01}\\) and \\(\\theta^X_1, \\theta^Y_{11}\\). Returning now to the probabilities, we put 0 weight on all of the excluded rows; and then we scale up the remaining probabilities so that they sum to 1 (preserving the ratio between them). The priors of 0.2 and 0.1 in the retained rows scale up to \\(\\frac{2}{3}\\) and \\(\\frac{1}{3}\\), which become our posterior probabilities on the causal types. We display an updated ambiguity matrix, with excluded data types and causal types removed, in Table 6.7. Before we see any data on the case at hand, then, we believe (based on our beliefs about the population to which the case belongs) that there is a 0.2 probability that the case is one in which \\(X\\) is assigned to \\(1\\) and has a positive effect on \\(Y\\); and 0.1 probability that it’s a case in which \\(X\\) gets assigned to \\(1\\) and has no effect on \\(Y\\) with \\(Y\\) fixed at \\(1\\). Seeing the \\(X=1, Y=1\\) data, we now believe that there is a 0.667 probability that the case is of the former type, and a 0.333 probability that it is of the latter type. Table 6.7: . An updated version of the ambiguity matrix in Table 6.4, after observing \\(X=1, Y=1\\) in a case. Data types \\(\\rightarrow\\) X1Y1 Priors on causal types Posteriors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_1,\\theta^Y_{01}\\) 1 0.2 0.6667 \\(\\theta^X_1,\\theta^Y_{11}\\) 1 0.1 0.3333 We can also see how this works for our \\(X \\rightarrow M \\rightarrow Y\\) model, and the ambiguity matrix in Table 6.5. If we observe the data \\(X=1, M=0, Y=0\\), for instance, this exercise would yield the updated ambiguity matrix in Table (tab:ambigmedupdate). Here we have eliminated all rows (causal types) with a \\(0\\) in the relevant data-type column (\\(X1M0Y0\\)) and formed the posteriors by scaling up the priors in the retained rows. Table 6.8: . An updated version of the ambiguity matrix in Table 6.5, after observing \\(X=1, M=0, Y=0\\) in a case. Data types \\(\\rightarrow\\) X1M0Y0 Priors on causal types Posteriors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{00}\\) 1 0.02 0.1667 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{00}\\) 1 0.02 0.1667 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{01}\\) 1 0.04 0.3333 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{01}\\) 1 0.04 0.3333 A notable feature of the logic of single-case process tracing is that the relative probabilities on the retained causal types never change. If we start out believing that causal type \\(A\\) is twice as likely as causal type \\(B\\), and both \\(A\\) and \\(B\\) are retained once we see the data, then \\(A\\) will be twice as likely as \\(B\\) in our posteriors. All updating occurs by eliminating causal types from consideration and zeroing in on those that remain. type X1M0Y0 prior posterior X1M00Y00 1 0.02 0.1667 X1M10Y00 1 0.02 0.1667 X1M00Y01 1 0.04 0.3333 X1M10Y01 1 0.04 0.3333 A similar logic applies if partial data are observed: that is, if we do not collect data for all nodes in the model. The one difference is that, now, rather than reducing to one column we entertain the possibility of any data type consistent with the observed data. In general, more than one data type will be consistent with partial data. For instance, suppose that we observe \\(X=1, Y=0\\) but do not observe \\(M\\)’s value. These are data that are consistent with both the data type \\(X1M0Y0\\) and the data type \\(X1M1Y0\\) (since the unobserved \\(M\\) could be either \\(0\\) or \\(1\\)). We thus retain both of these data-type columns as well as all causal types consistent with either of these data types. This gives the updated ambiguity matrix in Table 6.9. We note that, with these partial data, we are not able to update as strongly. For instance, for the causal type \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{00}\\), instead of updating to a posterior probability of 0.1667, we update to a posterior of only 0.0833 – because there is a larger set of causal types with which these partial data are consistent. Table 6.9: . An updated version of the ambiguity matrix in Table 6.5, after observing partial data in case: \\(X=1, Y=0\\), with \\(M\\) unobserved. Data types \\(\\rightarrow\\) X1M0Y0 X1M1Y0 Priors on causal types Posteriors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{00}\\) 1 0 0.02 0.0833 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{00}\\) 1 0 0.02 0.0833 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{00}\\) 0 1 0.04 0.1667 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{00}\\) 0 1 0.02 0.0833 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{10}\\) 0 1 0.04 0.1667 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{10}\\) 0 1 0.02 0.0833 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{01}\\) 1 0 0.04 0.1667 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{01}\\) 1 0 0.04 0.1667 type X1M0Y0 X1M1Y0 prior posterior X1M00Y00 1 0 0.02 0.0833 X1M10Y00 1 0 0.02 0.0833 X1M01Y00 0 1 0.04 0.1667 X1M11Y00 0 1 0.02 0.0833 X1M01Y10 0 1 0.04 0.1667 X1M11Y10 0 1 0.02 0.0833 X1M00Y01 1 0 0.04 0.1667 X1M10Y01 1 0 0.04 0.1667 Updating on estimands. We now have a posterior probability for each causal type for the case at hand. The causal question we are interested in answering, our estimand, may not be about causal types per se. It is about an estimand that can be expressed as a combination of causal types. For instance, suppose we are working with the model \\(X \\rightarrow M \\rightarrow Y\\); and that our question is, “Did \\(X=1\\) cause \\(Y=1\\)?”. This question is asking both: Does \\(X=1\\) in this case? Does \\(X\\) have a positive effect on \\(Y\\) in this case? The causal types that qualify are those, and only those, in which the answer to both is “yes.” Meeting condition (1) requires that \\(\\theta^X=\\theta^X_1\\). Meeting condition (2) requires that \\(\\theta^M\\) and \\(\\theta^Y\\) are such that \\(X\\) has an effect on \\(M\\) that yields a positive effect of \\(X\\) on \\(Y\\). This could occur via a positive \\(X \\rightarrow M\\) effect linked to a positive \\(M \\rightarrow Y\\) effect or via a negative \\(X \\rightarrow M\\) effect linked to a negative \\(M \\rightarrow Y\\) effect. Thus, the qualifying causal types in this model are: \\(\\theta^X_1, \\theta^M_{01}, \\theta^Y_{01}\\) \\(\\theta^X_1, \\theta^M_{10}, \\theta^Y_{10}\\) Our prior on the estimand—what we believe before we collect data on the case at hand—is given simply by summing up the prior probabilities on each of the causal types that correspond to the estimand. Note that we must calculate the prior from the full ambiguity matrix, before excluding types for inconsistency with the data. Returning to the full ambiguity matrix in Table 6.5, we see that the priors on these two types (given the population parameters assumed there) are 0.08 and 0.02, respectively, giving a prior for the estimand of 0.1. The posterior on any estimand is, likewise, given by summing up the posterior probabilities on each of the causal types that correspond to the estimand, drawing of course from the updated ambiguity matrix. For instance, if we observe the data \\(X=1, M=1, Y=1\\), we update to the ambiguity matrix in Table 6.10. Our posterior on the estimand, “Did \\(X=1\\) cause \\(Y=1\\)?” is the sum of the posteriors on the above two causal types. Since \\(\\theta^X_1, \\theta^M_{10}, \\theta^Y_{10}\\) is excluded by the data, this just leaves the posterior on \\(\\theta^X_1, \\theta^M_{01}, \\theta^Y_{01}\\), 0.4444, which is the posterior belief on our estimand. If we observe only the partial data, \\(X=1, Y=1\\), then we update to the ambiguity matrix in Table 6.11. Now both corresponding causal types are included, and we sum their posteriors to get the posterior on the estimand: \\(0.0769 + 0.3077 = 0.3846\\). FLAG: Briefly discuss other estimand(s) one could do, though don’t show in detail here. Will do pathways in Chap. 7. type X1M1Y1 prior posterior X1M01Y01 1 0.08 0.4444 X1M11Y01 1 0.04 0.2222 X1M01Y11 1 0.04 0.2222 X1M11Y11 1 0.02 0.1111 Table 6.10: . An updated version of the ambiguity matrix in Table 6.5, after observing \\(X=1, M=1, Y=1\\) in a case. Data types \\(\\rightarrow\\) X1M1Y1 Priors on causal types Posteriors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{01}\\) 1 0.08 0.4444 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{01}\\) 1 0.04 0.2222 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{11}\\) 1 0.04 0.2222 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{11}\\) 1 0.02 0.1111 Table 6.11: . An updated version of the ambiguity matrix in Table 6.5, after observing partial data in case: \\(X=1, Y=0\\), with \\(M\\) unobserved. Data types \\(\\rightarrow\\) X1M0Y0 X1M1Y0 Priors on causal types Posteriors on causal types Causal types \\(\\downarrow\\) \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{10}\\) 1 0 0.02 0.0769 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{10}\\) 1 0 0.02 0.0769 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{01}\\) 0 1 0.08 0.3077 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{01}\\) 0 1 0.04 0.1538 \\(\\theta^X_1,\\theta^M_{00},\\theta^Y_{11}\\) 0 1 0.02 0.0769 \\(\\theta^X_1,\\theta^M_{10},\\theta^Y_{11}\\) 0 1 0.02 0.0769 \\(\\theta^X_1,\\theta^M_{01},\\theta^Y_{11}\\) 1 0 0.04 0.1538 \\(\\theta^X_1,\\theta^M_{11},\\theta^Y_{11}\\) 1 0 0.02 0.0769 For more complex models and estimands, it can be more difficult to eyeball the corresponding causal types. In practice, therefore, we use a function in the gbiqq package to do this for us. X0.M10.Y10, X1.M10.Y10, X0.M01.Y10, X1.M01.Y10, X0.M10.Y01, X1.M10.Y01, X0.M01.Y01, X1.M01.Y01 This completes the abstract representation of the process tracing procedure. We now build up the intuition by walking through the procedure for simple mediation and moderation models. 6.1.3 Illustration with code XMY &lt;- make_model(&quot;X -&gt; M -&gt; Y&quot;) %&gt;% set_parameters (c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2)) query_model(model = XMY, queries = list(PC = &quot;Y[X=1] &gt; Y[X=0]&quot;), subsets = list(TRUE, &quot;X==1 &amp; Y==1&quot;, &quot;X==1 &amp; Y==1 &amp; M==0&quot;, &quot;X==1 &amp; Y==1 &amp; M==1&quot;), using = &quot;parameters&quot;) Query Subset Using mean PC All parameters 0.200 PC X==1 &amp; Y==1 parameters 0.385 PC X==1 &amp; Y==1 &amp; M==0 parameters 0.250 PC X==1 &amp; Y==1 &amp; M==1 parameters 0.444 6.2 Five principles 6.2.1 Classic qualitative tests are special cases of updating on a model The approach we have described here updates on the model given data on all variables, and from the model makes inferences to estimands. This procedure appears different to the approach described, for example, in Collier, Brady, and Seawright (2004) and in Chapter 5, in which one seeks specific evidence that is directly informative about causal propositions: “clues” that are arise with different probabilities if one proposition or another is true. In fact however the approaches are deeply connected. This “probative value of clues” approach can indeed be justified by reference to more fully elaborated models of the world. To see this we can write down the probability of observing \\(K=1\\) conditional on causal type \\(X\\), using the \\(\\phi\\) notation from Humphreys and Jacobs (2015) and introduced in Chapter 5. Here \\(\\phi_{jx}\\) refers to the probability of observing a clue in a case of type \\(j\\) when \\(X=x\\). Starting with our prior distribution over the lower-level causal types (the \\(\\lambda\\)’s), we can derive, for an \\(X=1\\) case, the probability of seeing the clue if the case is of type \\(b\\) (positive effect) or of type \\(d\\) (no effect, \\(Y\\) always \\(1\\)): \\[\\begin{equation} \\begin{split} \\phi_{b1} &amp; = \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y}}{\\lambda_{01}^{K}\\lambda_{01}^{Y}+\\lambda_{10}^{K}\\lambda_{10}^{Y}}\\\\ \\phi_{d1} &amp; = \\frac{\\lambda_{11}^{Y}(\\lambda_{01}^{K}+\\lambda_{11}^{K})+\\lambda_{11}^{K}\\lambda_{01}^{Y}}{\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}} \\end{split} \\label{eqn:phisfromlambdas} \\end{equation}\\] These quantities allow for easy mapping between our prior beliefs about our causal query—as expressed in the lower level model—and the classic process-tracing tests in Van Evera (1997). Figure 6.2 illustrates. In each panel, we manipulate a prior for one or more of the lower-level causal effects, keeping all other priors flat, and we see how probative value changes. As the curves for \\(\\phi_b\\) and \\(\\phi_d\\) diverge, probative value is increasing since there is an increasing difference between the probability of seeing the clue if \\(X\\) has a positive effect on \\(Y\\) and the probability of seeing the clue if \\(X\\) has no effect. In the left panel, we see that as we place a lower prior probability on \\(K\\)’s being negatively affected by \\(X\\),49 seeking \\(K=1\\) increasingly takes on the quality of a hoop test for \\(X\\)’s having a positive effect on \\(Y\\). The clue, that is, increasingly becomes something we must see if \\(X\\) positively affects \\(Y\\), with the clue remaining moderately probable if there is no effect. Why? The less likely we believe it is that \\(K=0\\) was caused by \\(X=1\\), the less consistent the observation of \\(K=0\\) is with \\(X\\) having a positive causal effect on \\(Y\\) via \\(K\\) (since, to have such an effect, if \\(X=1\\) and \\(K=0\\), would precisely have to mean that \\(X=1\\) caused \\(K=0\\)). In the second graph, we simultaneously change the prior probabilities of zero effects at both stages in the sequence: of \\(K\\) and \\(Y\\) being \\(1\\) regardless of the values of \\(X\\) and \\(K\\), respectively.50 We see here that, as the probabilities of zero effects jointly diminish, seeking \\(K=1\\) increasingly becomes a smoking-gun test for a positive effect of \\(X\\) on \\(Y\\): the probability of seeing the clue if the case is a \\(d\\) type diminishes. The reason is that, as zero effects at the lower level become less likely, it becomes increasingly unlikely that \\(K=1\\) could have occurred without a positive effect of \\(X\\) on \\(K\\), and that \\(Y=1\\) could have occurred (given that we have seen \\(K=1\\)) without a posiitve effect of \\(K\\) on \\(Y\\). Figure 6.2: The probability of observing \\(K\\) given causal type for different beliefs on lower-level causal effects. In the left figure, priors on all lower-level causal effects are flat except for the probability that \\(X\\) has a negative effect on \\(K\\). If we believe that it is unlikely that \\(X\\) has a negative effect on \\(K\\), \\(K\\) becomes a `hoop’ test for the proposition that a case is of type \\(b\\). The righthand figure considers simultaneous changes in \\(\\lambda_{11}^K\\) and \\(\\lambda_{11}^Y\\)—the probabilities that \\(K=1\\) regardless of \\(X\\), and that \\(Y=1\\) regardless of \\(K\\), with flat distributions on all other lower-level effects. With \\(\\lambda_{11}^K\\), \\(\\lambda_{11}^Y\\) both close to 0, \\(K\\) becomes a ‘smoking gun’ test for the proposition that \\(X\\) has a positive effect on \\(Y\\) (\\(b\\) type). 6.2.2 Conditional independence alone does not provide probative value 6.2.3 Uncertainty does not alter inference for single case causal inference In the procedure described for process tracing in this chapter (and different to what we introduce in Chapter 8) we assume that \\(\\lambda\\) is known and we do not place uncertainty around it. This might appear somewhat heroic, but in fact for single case inference it is without loss of generality. The expected inferences we would make for any estimand accounting for priors is the same as the inferences we if we use the expectation only. To see this, let \\(\\pi_j\\) denote the probability of observing causal type \\(j\\) and \\(p(D)\\) te probability of observing data realization \\(D\\). Say that \\(j \\in D\\) if type \\(j\\) produces data type \\(D\\) and say \\(j \\in E\\) if casual type \\(j\\) is an element of the estimand set of interest. For instance in an \\(X \\rightarrow Y\\) model, if we observe \\(X=Y=1\\) then \\(D\\) consists of causal types \\(D={(\\theta^X_1, \\theta^Y_{01}), (\\theta^X_1, \\theta^Y_{11})})\\) and the estimand set for “\\(X\\) has a positive effect on \\(Y\\)” consists of \\(E={(\\theta^X_1, \\theta^Y_{01}), (\\theta^X_0, \\theta^Y_{01})})\\). The posterior on an estimand \\(E\\) given data \\(D\\) given prior over \\(\\pi\\), \\(p(\\pi)\\) is: \\[\\Pr(E | D) = \\int_\\pi \\frac{\\sum_{j \\in E \\cap D}\\pi_j}{\\sum_{j \\in D}\\pi_j} f(\\pi)d\\pi\\] However, since for any \\(\\pi\\), \\(\\sum_{j \\in D}\\pi_j = p(D)\\) we have: \\[\\Pr(E | D) = \\int_\\pi \\sum_{j \\in E \\cap D}\\pi_j f(\\pi)d\\pi/p(D) = \\sum_{j \\in E \\cap D} \\overline{\\pi}_j/p(D)\\] 6.2.4 Probative value requires \\(d-\\)connection As we have argued, causal estimands can be expressed as the values of exogenous nodes in a causal graph. Case-level causal effects and causal paths can be defined in terms of response-type nodes; average effects and notable causes in terms of population-level parameter nodes (e.g., \\(\\pi\\) or \\(\\lambda\\) terms); and questions about actual causes in terms of exogenous conditions that yield particular endogenous values (conditioning on which makes some variable a counterfactual cause). We thus define causal inference more generally as the assessment of the value of one or more unobserved (possibly unobservable) exogenous nodes on a causal graph, given observable data. To think through the steps in this process, it is useful to distinguish among three different features of the world, as represented in our causal model: there are the things we want to learn about; the things we have already observed; and the things we could observe. As notation going forward, let: \\(\\mathcal Q\\) denote the exogenous variables that define our query; we generally assume that \\(\\mathcal Q\\) cannot be directly observed so that its values must be inferred \\(\\mathcal W\\) denote a set of previously observed nodes in the causal model, and \\(\\mathcal K\\) denote a set of additional variables—clues—that we have not yet observed but could observe. Now suppose that we seek to design a research project to investigate a causal question. How should the study be designed? Given that there are some features of the world that we have already observed, which additional clues should we seek to collect to shed new light on our question? In terms of the above notation, what we need to figure out is whether a given \\(\\mathcal K\\) might be informative about—might provide additional leverage on—\\(\\mathcal Q\\) given the prior observation of \\(\\mathcal W\\). To ask whether one variable (or set of variables) is informative about another is to ask whether the two (sets of) variables are, on average, correlated with one another, given whatever we already know. Likewise, if two variables’ distributions are fully independent of one another (conditional on what else we have observed), then knowing the value of one variable can provide no new information about the value of the other. Thus, asking whether a set of clues, \\(\\mathcal K\\), is informative about \\(\\mathcal Q\\) given the prior observation of \\(\\mathcal W\\), is equivalent to asking whether \\(\\mathcal K\\) and \\(\\mathcal Q\\) are conditionally independent given \\(\\mathcal W\\). That is, \\(\\mathcal K\\) can be informative about \\(\\mathcal Q\\) given \\(\\mathcal W\\) only if \\(\\mathcal K\\) and \\(\\mathcal Q\\) are not conditionally independent of one another given \\(\\mathcal W\\). As we have shown, as long as we have built \\(\\mathcal Q\\), \\(\\mathcal K\\), and \\(\\mathcal W\\) into our causal model of the phenomenon of interest, we can answer this kind of question by inspecting the structure of the model’s DAG. In particular, what we need to go looking for are relationships of \\(d\\)-separation. The following proposition, with only the names of the variable sets altered, is from Pearl (2009) (Proposition 1.2.4): Proposition 1: If sets \\(\\mathcal Q\\) and \\(\\mathcal K\\) are \\(d\\)-separated by \\(\\mathcal W\\) in a DAG, \\(\\mathcal G\\), then \\(\\mathcal Q\\) is independent of \\(\\mathcal K\\) conditional on \\(\\mathcal W\\) in every distribution compatible with \\(\\mathcal G\\). Conversely, if \\(\\mathcal Q\\) and \\(\\mathcal K\\) are not \\(d\\)-separated by \\(\\mathcal W\\) in DAG \\(\\mathcal W\\), then \\(\\mathcal Q\\) and \\(\\mathcal K\\) are dependent conditional on \\(\\mathcal W\\) in at least one distribution compatible with \\(\\mathcal G\\). We begin with a causal graph and a set of nodes on the graph (\\(W\\)) that we have already observed. Given what we have already observed, a collection of clue nodes, \\(\\mathcal K\\), will be uninformative about the query nodes, \\(\\mathcal Q\\), if \\(\\mathcal K\\) is \\(d\\)-separated from \\(\\mathcal Q\\) by \\(\\mathcal W\\) on the graph. When \\(\\mathcal W\\) \\(d\\)-separates \\(\\mathcal K\\) from \\(\\mathcal Q\\), this means that what we have already observed already captures all information that the clues might yield about our query. On the other hand, if \\(\\mathcal K\\) and \\(\\mathcal Q\\) are \\(d\\)-connected (i.e., not \\(d\\)-separated) by \\(W\\), then \\(K\\) is possibly informative about \\(Q\\).\\(K\\) is not \\(d\\)-separated from \\(\\mathcal Q\\) by \\(\\mathcal W\\).51 Note, moreover, that under quite general conditions (referred to in the literature as the faithfulness of a probability distribution) then there are at least some values of \\(\\mathcal W\\) for which \\(\\mathcal K\\) will be informative about \\(\\mathcal Q\\).52 Let us examine Proposition 1 in practice. We begin with the simplest case possible, and then move on to more complex models. The very simplest probabilistic causal graph has \\(X\\) influencing \\(Y\\), with \\(X\\) determined by a coin flip. Assuming that there is some causal heterogeneity—that is, it is unknown in any particular case whether \\(X\\) causes \\(Y\\)—we also include a response-type variable, \\(Q\\), pointing into \\(Y\\), as shown in Figure . Here, \\(Q^Y\\) determines the value of \\(Y\\) that will be generated by \\(X\\). Asking about the causal effect of \\(X\\) in a case thus means learning the value of \\(Q^Y\\) in that case. As will be recalled, in a binary setup with one causal variable, a response-type variable can take on one of four values, \\(q^Y_{00}\\), \\(q^Y_{10}\\), \\(q^Y_{01}\\) and \\(q^Y_{11}\\),53 corresponding to the four possible causal types in this setting. Figure 6.3: A simple causal setup in which the effect of \\(X\\) on \\(Y\\) in a given case depends on the case’s response type for \\(Y\\). Let us assume that we have observed nothing yet in this case and then ask what clue(s) might be informative about \\(Q^Y\\), the node of interest. The other two nodes in the graph are \\(X\\) and \\(Y\\): these are thus the possible clues that we might go looking for in our effort to learn about \\(Q^Y\\) (i.e., they are the possible members of \\(\\mathcal K\\)). First, can we learn about \\(Q^Y\\) by observing \\(X\\)? We can answer this question by asking whether \\(X\\) is \\(d\\)-connected to \\(Q^Y\\) on the graph given what we have already observed (which is nothing). We can see visually that there is no active path from \\(X\\) to \\(Q^Y\\): the only path between \\(X\\) and \\(Q\\) is blocked by colliding arrow heads. Thus, \\(X\\) and \\(Q^Y\\) are \\(d\\)-separated, meaning that \\(X\\) will not be informative about \\(Q^Y\\): observing the value that a causal variable takes on in a case—having seen nothing else in the case—tells us nothing whatsoever about that variable’s effect on the outcome. If we want to know whether a case is of a type in which the presence of natural resources would cause civil war, observing only that the case has natural resources does not help answer the question. –&gt; What, then, if we instead were to observe only \\(Y\\)? Is \\(Y\\) \\(d\\)-connected to \\(Q\\) given what we have already observed (which, again, is nothing)? It is: the arrow from \\(Q^Y\\) to \\(Y\\) is an active path. Observing only the outcome in a case does tell us something about causal effects. Returning to the natural resources and civil war example, observing only that a country has had a civil is informative about the case’s causal type (the value of \\(Q^Y\\)). In particular, it rules out the possibility that this is a case in which nothing could cause a civil war: that is, it excludes \\(q^Y_{00}\\) (i.e., \\(c\\)-type) as a possible value of \\(Q^Y\\). Suppose now, having observed \\(Y\\), that we were to consider also observing \\(X\\). Would we learn anything further about \\(Q^Y\\) from doing so? We have already seen that observing \\(X\\) alone yields no information about \\(Q^Y\\) because the two nodes are unconditionally \\(d\\)-separated, the path between them blocked by the colliding arrowheads at \\(Y\\). However, as we have seen, observing a collider variable (or one of its descendants) unblocks the flow of information, generating relations of conditional dependence across the colliding arrowheads. Here, \\(X\\) and \\(Q^Y\\) are \\(d\\)-connected by \\(Y\\): thus, if we have already observed \\(Y\\), then observing \\(X\\) does confer additional information about \\(Q^Y\\). Knowing only that a country has natural resources tells us nothing about those resources’ effect on civil war in that country. But if we already know that the country has a civil war, then learning that the country has natural resources helps narrow down the case’s possible response types. Having already used the observation of \\(Y=1\\) to rule out the possibility that \\(Q^Y=q^Y_{00}\\), observing \\(X=1\\) together with \\(Y=1\\) allows us to additionally rule out the possibility that natural resources prevent civil war, i.e., that \\(Q^Y=q^Y_{01}\\).54 Finally, what if we observe \\(X\\) first and are considering whether to seek information about \\(Y\\)? Would doing so be informative? \\(X\\) does not \\(d-\\)separate \\(Q^Y\\) from \\(Y\\); thus, observing \\(Y\\) will be informative about \\(Q^Y\\). In fact, observing \\(Y\\) if we have already seen \\(X\\) is more informative than observing \\(Y\\) alone. The reasoning follows the logic of collision discussed just above. If we observe \\(Y\\) having already seen \\(X\\), not only do we reap the information about \\(Q^Y\\) provided by \\(Y\\)’s correlation with \\(Q^Y\\); we simultaneously open up the path between \\(X\\) and \\(Q^Y\\), learning additionally from the conditional dependence between \\(X\\) and \\(Q^Y\\) given \\(Y\\). We put Proposition 1 to work in a slightly more complex set of models in Figure . Here we investigate the informativeness of a clue that is neither \\(X\\) nor \\(Y\\). Each graph in Figure has four variables: \\(X\\); \\(Y\\); a possible clue, \\(K\\); and a response-type variable, \\(Q\\). We draw all 34 possible graphs with variables \\(X\\), \\(Y\\), \\(K\\), and \\(Q\\) for causal models in which (a) all variables are connected to at least one other variable, (b) \\(X\\) causes \\(Y\\) either directly or indirectly, and (c) \\(Q\\) is a direct cause of \\(Y\\) but is not caused by any other variable in the model and is thus exogenous. The title of each panel reports \\(K\\)’s conditional informativeness using principles of \\(d\\)-separation: it tells us when \\(K\\) is possibly informative about \\(Q\\) depending on whether \\(X\\), \\(Y\\), both or none are observed.55 Figure 6.4: All connected directed acyclic graphs over \\(X,Y,K,Q\\), in which \\(Q\\) is an exogenous variable that directly causes \\(Y\\), and \\(X\\) is a direct or indirect cause of \\(Y\\). The title of each graph indicates the conditions under which \\(K\\) can be informative about (i.e., is not \\(d\\)-separated from) \\(Q\\), given the prior observation of \\(X\\), \\(Y\\), both, or neither (…). 6.2.5 Probative value The results show us not just what kinds of variables can be informative about a case’s response-type but also what combinations of observations yield leverage on case-level causal effects. A number of features the graphs are worth highlighting: Clues at many stages. Process tracing has focused a great deal on observations that lie “along the path” between suspected causes and outcomes. What we see in Figure , however, is that observations at many different locations in a causal model can be informative about causal effects. We see here that \\(K\\) can be informative when it is pre-treatment (causally prior to \\(X\\)—e.g. panel (3)), post-treatment but pre-outcome (that is, “between” \\(X\\) and \\(Y\\) as, e.g., in panel (20)), an auxiliary effect of \\(X\\) that itself has no effect on \\(Y\\) (e.g., in panel (19)), post-outcome (after \\(Y\\)—e.g., in panel (15)), or a joint effect of both the suspected cause and the outcome (e.g., panel (31)). Mediator Clues. While clues that lie in between \\(X\\) and \\(Y\\) may be informative, they can only be informative under certain conditions. For instance, when a clue serves only as a mediator in our model (i.e., its only linkages are being caused by \\(X\\) and being affected by \\(Y\\)) and \\(Q\\) only affects \\(Y\\), as in panels (20) and (21), the clue is only informative about \\(Q\\) if we have also observed the outcome, \\(Y\\). Of course, this condition may commonly be met—qualitative researchers usually engage in retrospective research and learn the outcome of the cases they are studying early on—but it is nonetheless worth noting why it matters: in this setup, \\(K\\) is unconditionally \\(d\\)-separated from \\(Q\\) by the collision at \\(Y\\); it is only by observing \\(Y\\) (the collider) that the path between \\(K\\) and \\(Q\\) becomes unblocked. (As we saw above, the very same is true for observing \\(X\\); it is only when we know \\(Y\\) that \\(X\\) is informative about \\(Q\\).) In short, observations along causal paths are more helpful in identifying causal effects to the extent that we have measured the outcome. Importantly, this is not the same as saying that mediator clues are only informative about causal effects where we have observed the outcome. Observing \\(Y\\) is necessary for the mediator to be informative about a \\(Q\\) term that is connected only to \\(Y\\). Observing a mediator without the outcome, however, could still be informative about the overall effect of \\(X\\) on \\(Y\\) by providing leverage on how the mediator responds to \\(X\\), which is itself informative about \\(X\\)’s effect on \\(Y\\) via the mediator.56 Moreover, observing the mediator could be informative without the observation of \\(Y\\) if, for instance, \\(Q\\) also points into \\(K\\) itself or into a cause of \\(K\\). As we discuss below, the clue then is informative as a “symptom” of the case’s response type, generating learning that does not hinge on observing the outcome. Symptoms as clues. Some clues may themselves be affected by \\(Q\\): that is to say, they may be symptoms of the same conditions that determine causal effects in a case. For instance, in our illustrative model involving government survival, government sensitivity functions as a response-type variable for the effect of a free press (\\(X\\)) on government removal (\\(Y\\)): a free press only generates government removal when the government is non-sensitive to public opinion. Sensitivity to public opinion thus represents our query variable, \\(Q\\), if we seek to learn whether a free press causes government removal in a case. While it may not be possible to observe or otherwise measure the government’s sensitivity, there may be consequences of government sensitivity that are observable: for instance, whether government officials regularly consult with civil-society actors on policy issues. While consultations would not be part of the causal chain generating the free press’s effect, observing consultations (or the lack of them) would be informative about that effect because consultations are a symptom of the same conditions that enable the effect. We see that \\(K\\) is a child or descendant of \\(Q\\) in several of the graphs in Figure : \\(Q\\) directly causes \\(K\\) in panels (7) through (14), (17), (18), (25)-(30), (33), and (34); \\(Q\\) causes (K) only indirectly through \\(X\\) in panels (22) through (24); \\(Q\\) causes (K) only indirectly through \\(Y\\) in panels (15), (16), and (31); and \\(Q\\) causes \\(K\\) only indirectly through \\(X\\) and through \\(Y\\) in panel (32). We can then use the principle of \\(d\\)-separation to figure out when the symptom clue is potentially informative, given what we have already observed. It is easy to see that \\(K\\) is potentially informative, no matter what we have already observed, if \\(K\\) is directly affected by \\(Q\\); there is nothing we could observe that would block the \\(Q \\rightarrow K\\) path. Thus, \\(Q\\)’s “symptom” can, in this setup, contain information about type above and beyond that contained in the \\(X\\) and \\(Y\\) values. However, where \\(Q\\) affects \\(K\\) only through some other variable, observing that other variable renders \\(K\\) uninformative by blocking the \\(Q\\)-to-\\(K\\) path. For instance, where \\(Q\\) affects \\(K\\) indirectly through \\(X\\), once we observe \\(X\\), we already have all the information about \\(Q\\) that would be contained in \\(K\\). Surrogates as clues. Clues may be consequences of the outcome, as in graphs (15) and (16). If \\(K\\) is a consequence only of \\(Y\\), then it will contain no new information about \\(Q\\) where \\(Y\\) is already known. However, in situations where the outcome has not been observed, \\(K\\) can act as a “surrogate” for the outcome and thus yield leverage on \\(Q\\) (Frangakis and Rubin (2002)). A researcher might, for instance, seek to understand causal effects on an outcome that is difficult to directly observe: consider, for instance, studies that seek to explain ideational change. Ideas themselves, the \\(Y\\) in such studies, are not directly observable. However, their consequences—such as statements by actors or policy decisions—will be observable and can thus serve as informative surrogates for the outcome of interest. Clues may similarly serve as surrogates of a cause, as in graphs (19) and (22). Here \\(X\\) causes \\(K\\), but \\(K\\) plays no role in the causal process generating \\(Y\\). \\(K\\) is of no help if we can directly measure \\(X\\) since the latter \\(d\\)-separates \\(K\\) from \\(Q\\). But if an explanatory variable cannot be directly measured—consider, e.g., ideas or preferences as causes—then its consequences, including those that have no relationship to the outcome of interest, can provide leverage on the case-level causal effect. Clues can also be a consequence of both our suspected cause and the outcome of interest, thus serving as what we might call “double surrogates,” as in panels (31) and (32). Here \\(X\\) is a direct cause of \\(Y\\), and \\(K\\) is a joint product of \\(X\\) and \\(Y\\). A double surrogate can be informative as long as we have not already observed both \\(X\\) and \\(Y\\). Where data on either \\(X\\) or \\(Y\\) are missing, there is an open path between \\(K\\) and \\(Q\\). If we have already observed both, however, then there is nothing left to be learned from \\(K\\). Instruments as clues. Clues that are causally prior to an explanatory variable, and have no other effect on the outcome, can sometimes be informative. Consider, for instance, graph (3). Here \\(K\\) is the only cause of \\(X\\). It can thus serve as a proxy. If we have seen \\(X\\), then \\(X\\) blocks the path between \\(K\\) and \\(Q\\), and so \\(K\\) is unhelpful. \\(K\\) can be informative, though, if we have not observed \\(X\\). Note that informativeness here still requires that we observe \\(Y\\). Since \\(Y\\) is a collider for \\(Q\\) and the \\(K \\rightarrow X \\rightarrow\\) chain, we need to observe \\(Y\\) in order to \\(d\\)-connect \\(K\\) to \\(Q\\). A rather different setup appears in graph (5), where both \\(K\\) and \\(Q\\) cause \\(X\\). Now the conditions for \\(K\\)’s informativeness are broader. Observing \\(X\\) still makes \\(K\\) uninformative as a proxy for \\(X\\) itself. However, because \\(X\\) is a collider for \\(K\\) and \\(Q\\), observing \\(X\\) opens up a path from \\(K\\) to \\(Q\\), rendering a dependency between them. Still, we have to observe at least one of \\(X\\) or \\(Y\\) for the instrument to be informative here. This is because both of \\(K\\)’s paths to \\(Q\\) run through a collision that we need to unblock by observing the collider. For one path, the collider is \\(X\\); for the other path, the collider is \\(Y\\).57 Other patterns involving instrumentation are also imaginable, though not graphed here. For example, we might have a causal structure that combines instrumentation and surrogacy. Suppose that \\(X\\) is affected by \\(Q\\) and by an unobservable variable \\(\\theta_X\\); and that \\(\\theta_X\\) has an observable consequence, \\(K\\). Then \\(K\\), though not a cause of \\(X\\), is a “surrogate instrument” (Hernán and Robins 2006) as it is a descendant of an unobserved instrument, \\(U\\), and thus allows us to extract inferences similar to those that we could draw from a true instrument. Confounders as clues. In several of the graphs, \\(K\\) is a confounder in that it is a direct cause of both \\(X\\) and \\(Y\\) (panels (4), (6), (12), and (14)). Let us focus on graph (4), which isolates \\(K\\)’s role as a confounder. Here \\(K\\) can be informative via two possible paths. First, if \\(X\\) is not observed but \\(Y\\) is, then \\(K\\) is \\(d\\)-connected to \\(Q\\) along the path \\(K \\rightarrow X \\rightarrow Y \\leftarrow Q\\). \\(K\\) is in this sense serving as a proxy for \\(X\\), with its path to \\(Q\\) opened up by the observation of the collider, \\(Y\\). Second, with \\(Y\\) observed, \\(K\\) can provide information on \\(Q\\) via the more direct collision, \\(K \\rightarrow Y \\leftarrow Q\\). If \\(X\\) is observed, then the first path is blocked, but the second still remains active. As with any pre-outcome variable, for a confounder clue to provide purchase on \\(Y\\)’s response type, \\(Y\\) itself must be observed. In a sense, then, the role of confounders as clues in case-level inference is the mirror image of the role of confounders as covariates in cross-case correlational inference. In a correlational inferential framework, controlling for a variable in \\(K\\)’s position in graph (5) renders the \\(X, Y\\) correlation (which we assume to be observed) informative about \\(X\\)’s average causal effect. When we use confounders as evidence in within-case inference, it is our observations of other variables that determine how informative the confounder itself will be about \\(X\\)’s causal effect. It is important to be precise about the kinds of claims that one can make from graphs like those in Figure {fig:34graphs}. The graphs in this figure allow us to identify informativeness about an unobserved node \\(Q\\) that is a parent of \\(Y\\). This setup does not, however, capture all ways in which clues can be informative about the causal effect of \\(X\\) on \\(Y\\) or about other causal estimands of interest. For instance, as noted above, even if a clue is uninformative about a \\(Q\\) node pointing into \\(Y\\), it may still help establish whether \\(X\\) causes \\(Y\\): the statement that \\(X\\) causes \\(Y\\) will for some graphs be a statement about a collection of nodes that form the set of query variables \\(\\mathcal Q\\). This is the case, for instance, in any graph of the form \\(X \\rightarrow M \\rightarrow Y\\), where we are interested not just in \\(Y\\)’s response to \\(M\\) (the mediator) but also in \\(M\\)’s response to \\(X\\). Of interest, thus, are not just a \\(Q^Y\\) response-type node pointing into \\(Y\\) but also a \\(Q^M\\) response-type node that is a parent of \\(M\\). Observations that provide leverage on either \\(Q\\) term will thus aid an inference about the overall causal effect. A clue \\(K\\) that is \\(d-\\)separated from \\(Q^Y\\) may nevertheless be informative about \\(X\\)’s effect on \\(Y\\) if it is not \\(d-\\)separated from \\(Q^M\\); this opens up a broader range of variables as informative clues. Additionally, as our discussion in Chapter 2 makes clear, estimands other than the case-level causal effect—such as average causal effects, actual causes, and causal paths—involve particular features of context: particular sets of exogenous nodes as members of our query set, \\(\\mathcal Q\\). Thus, even for the same causal model, informativeness will be defined differently for each causal question that we seek to address. The broader point is that we can identify what kinds of observations may address our estimand if we can place that estimand on a causal graph and then assess the graph for relationships of \\(d\\)-separation and -connection. Further, we emphasize that a DAG can only tell us when a clue may be informative (conditional some prior observation): \\(d-\\)connectedness is necessary but not sufficient for informativeness. This fact derives directly from the rules for drawing a causal graph: the absence of an arrow between two variables implies that they are not directly causally related, while the presence of an arrow does not imply that they always are. As we saw in our analysis of the government-removal example in Chapter 2, whether variables connected to one another by arrows in the original DAG were in fact linked by a causal effect depended on the context. Likewise, whether a clue \\(K\\) is in fact informative may depend on particular values of \\(\\mathcal W\\)—the variables that have already been observed. As a simple example, let \\(q = k_1w + (1-w)k_2\\), where \\(W\\) is a variable that we have already observed and \\(K_1\\) and \\(K_2\\) are clues that we might choose to observe next. Here, if \\(w=1\\) then learning \\(K_1\\) will be informative about \\(Q\\), and learning \\(K_2\\) will not; but if \\(w=0\\), then \\(K_1\\) will be uninformative (and \\(K_2\\) informative). In general, then, graphical analysis alone can help us exclude unhelpful research designs, given our prior observations and a fairly minimal set of prior beliefs about causal linkages. This is no small feat. But identifying those empirical strategies that will yield the greatest leverage requires engaging more deeply with our causal model, as we explore next. References "],
["application-process-tracing-with-a-causal-model.html", "Chapter 7 Application: Process Tracing with a Causal Model 7.1 Inequality and Democratization: The Debate 7.2 A Structural Causal Model 7.3 Results 7.4 Pathways 7.5 Model definition and inference in code 7.6 Concluding thoughts", " Chapter 7 Application: Process Tracing with a Causal Model We apply the causal-model-based approach to process tracing to a major substantive issue in comparative politics: the relationship between inequality and democratization. We illustrate how one can convert theories in this domain into relatively simple causal models. Drawing on data from Haggard and Kaufman (2012), we use qualitative restrictions on causal types together with flat priors to draw inferences about the probabilty with which inequality facilitated or hindered democratization. In this chapter, we demonstrate how causal-model-based process-tracing works using real data. We undertake this illustration on a substantive issue that has been of central interest to students of comparative politics for decades: the causes of democratization. As the literature and range of arguments about democratization are vast, we focus on just a piece of the debate—specifically on causal claims about the relationship between economic inequality and democratization, with particular attention to the work of Boix (2003), Acemoglu and Robinson (2005), and Haggard and Kaufman (2012). In this chapter, we demonstrate process tracing with causal models, while in a later chapter we demonstrate the integration of process-tracing with correlational analysis. Our focus in this chapter is on using process tracing to assess the case-level causal effect of inequality on democracy. 7.1 Inequality and Democratization: The Debate Sociologists, economists, and political scientists have long theorized and empirically examined the relationship between inequality and democracy (e.g., Dahl (1973), Bollen and Jackman (1985), Acemoglu and Robinson (2005), Boix (2003), Ansell and Samuels (2014)). In recent years, the work of Boix (2003), Acemoglu and Robinson (2005), and Ansell and Samuels (2014) represent major theoretical advances in specifying when and how inequality might generate transitions to democracy (as well as its persistence, which we bracket here). The first and third of these books also provide large-n cross-national and historical tests of their theories’ key correlational predictions. Haggard and Kaufman (2012), moreover, derive causal process observations from a large number of “Third Wave” cases of democratization in order to examine these theories’ claims about the centrality of distributional issues to regime change. We provide a very condensed summary of the core logic of Boix (2003) and Acemoglu and Robinson (2005) before seeking to translate that logic into a causal model for the purposes of process tracing, using a transformed version of Haggard and Kaufman’s causal-process data. We briefly summarize the core logics of and differences among these three sets of arguments here, bracketing many of their moving parts to focus on the basic theorized relationship between inequality and democracy. Both Boix’s and Acemoglu and Robinson’s theories operate within a Meltzer-Richard (Meltzer and Richard (1981)) framework in which, in a democracy, the median voter sets the level of taxation-and-transfer and, since mean income is higher than median income, benefit from and vote for a positive tax rate, implying redistribution from rich to poor. The poorer the median voter, the more redistribution she will prefer. Democracy, with its poorer median voter, thus implies greater redistribution than (rightwing) authoritarianism—a better material position from the poor at the expense of the rich elite. Thus, in each of these approaches, struggles over political regimes are conflicts over the distribution of material resources. In Boix’s model, the poor generally prefer democracy for its material benefits. When they mobilize to demand regime change, the rich face a choice as to whether to repress or concede, and they are more likely to repress as inequality is higher since, all else equal, they have more to lose from democracy. Thus, with the poor always preferring democracy over rightwing authoritarianism, inequality reduces the prospects for democratization. In Acemoglu and Robinson’s model, inequality simultaneously affects the expected net gains to democracy for both rich and poor. At low levels of inequality, democracy is relatively unthreatening to the elite, as in Boix, but likewise of little benefit to the poor. Since regime change is costly, the poor do not mobilize for democracy when inequality is low, and democratization does not occur. At high levels of inequality, democracy is of great benefit to the poor but has high expected costs for the elite; thus, democratization does not occur because the elite repress popular demands for regime change. In Acemoglu and Robinson’s model, democracy emerges only when inequality is at middling levels: high enough for the poor to demand it and low enough for the rich to be willing to concede it. Ansell and Samuels, finally, extend the distributive politics of regime change in two key ways. First, they allow for a two-sector economy, with a governing elite comprising the landed aristocracy and an urban industrial elite excluded from political power under authoritarian institutions. Total inequality in the economy is a function of inequality in the landed sector, inequality in the industrial sector, and the relative size of each. Second, authoritarian (landed) elites can tax the industrial bourgeoisie, thus giving the industrial elite an incentive to seek constraints on autocratic rule. Third, in Ansell and Samuels’ model, rising industrial inequality means a rising industrial elite, generating a larger gap between them and industrial workers, though the industrial masses are richer than the peasantry. A number of results follow, of which we highlight just a couple. Rising land inequality reduces the likelihood of bourgeois rebellion by giving the landed elite greater repressive capacities and increasing their expected losses under democracy. As industrial inequality rises, however, the industrial elite have more to lose to confiscatory taxation and thus greater incentive to push for partial democracy (in which they have the ability to constrain the government, though the poor remain politically excluded) as well as greater resources with which to mobilize and achieve it. Full democracy, brought on by joint mass and bourgeois rebellion, is most likely as the industrial sector grows in relative size, giving the urban masses more to lose to autocratic expropriation and more resources with which to mobilize and rebel. These three theoretical frameworks thus posit rather differing relationships between inequality and democracy. Taking these theoretical logics as forms of background knowledge, we would consider it possible that inequality reduces the likelihood of democracy or that it increases the likelihood of democracy. Yet one feature that all three theories have in common is a claim that distributional grievances drive demands for regime change. Moreover, in both Boix and Acemoglu and Robinson, less economically advantaged groups are, all else equal, more likely to demand democracy the worse their relative economic position. Ansell and Samuels’ model, on the other hand, suggests that relative deprivation may cut both ways: while poorer groups may have more to gain from redistribution under democracy, better-off groups have more to fear from confiscatory taxation under autocracy. In all three frameworks, mobilization by groups with material grievances is critical to transitions to democracy: elites do not voluntarily cede power. In their qualitative analysis of “Third Wave” democratizations, Haggard and Kaufman point to additional factors, aside from inequality, that may generate transitions. Drawing on previous work on 20th century democratic transitions (e.g., Huntington (1993), Linz and Stepan (1996)), they pay particular attention to international pressures to democratize and to elite defections. 7.2 A Structural Causal Model We now need to express this background knowledge in the form of a structural causal model. Suppose that we are interested in the case-level causal effect of inequality on democratization of a previously autocratic political system. Suppose further, to simplify the illustration, that we conceptualize both variables in binary terms: inequality is either high or low, and democratization either occurs or does not occur. This means that we want to know, for a given case of interest, whether high inequality (as opposed to low inequality) causes democracy to emerge, prevents democracy from emerging, or has no effect (i.e., with democratization either occurring or not occurring independent of inequality). We can represent this query in the simple, high-level causal model shown in Figure 7.1. Here, the question, “What is the causal effect of high inequality on democratization in this case?” is equivalent to asking what the value of \\(\\theta^D\\) is in the case, where the possible values are \\(\\theta_{00}^D, \\theta_{01}^D, \\theta_{10}^D\\), and \\(\\theta_{11}^D\\). We assume here that the case’s response type, \\(\\theta^D\\), is not itself observable, and thus we are in the position of having to make inferences about it. Drawing on the grammar of causal graphs discussed in Chapter 5, we can already identify possibilities for learning about \\(\\theta^D\\) from the other nodes represented in this high-level graph. Merely observing the level of inequality in a case will tell us nothing since \\(I\\) is not \\(d-\\)connected to \\(\\theta^D\\) if we have observed nothing else. On the other hand, only observing the outcome—regime type—in a case can give us information about \\(\\theta^D\\) since \\(D\\) is \\(d-\\)connected to \\(\\theta^D\\). For instance, if we observe \\(D=1\\) (that a case democratized), then we can immediately rule out \\(\\theta_{00}^D\\) as a value of \\(\\theta^D\\) since this type does not permit democratization to occur. Further, conditional on observing \\(D\\), \\(I\\) is now \\(d-\\)connected to \\(\\theta^D\\): in other words, having observed the outcome, we can additionally learn about the case’s type from observing the status of the causal variable. For example, if \\(D=1\\), then observing \\(I=1\\) allows us additionally to rule out the value \\(\\theta_{10}^D\\) (a negative causal effect). Now, observing just \\(I\\) and \\(D\\) alone will always leave two response types in contention. For instance, seeing \\(I=D=1\\) (the case had high inequality and democratized) would leave us unsure whether high inequality caused the democratization in this case (\\(\\theta^D=\\theta_{01}^D\\)) or the democratization would have happened anyway (\\(\\theta^D=\\theta_{11}^D\\)). This is a limitation of \\(X, Y\\) data that we refer to in Humphreys and Jacobs (2015) as the “fundamental problem of type ambiguity.” Note that this does not mean that we will be left indifferent between the two remaining types. Learning from \\(X, Y\\) data alone—narrowing the types down to two—can be quite significant, depending on our priors over the distribution of types. For example, if we previously believed that a \\(\\theta_{00}^D\\) type (cases in which democracy will never occur, regardless of inequality) was much more likely than a \\(\\theta_{11}^D\\) type (democracy will always occur, regardless of inequality) and that positive and negative effects of inequality were about equally likely, then ruling out the \\(\\theta_{00}^D\\) and \\(\\theta_{10}^D\\) values for a case will shift us toward the belief that inequality caused democratization in the case.58 Figure 7.1: Simple democracy, inequality model Nonetheless, we can increase the prospects for learning by theorizing the relationship between inequality and democratization. Given causal logics and empirical findings in the existing literature, we can say more than is contained in Figure 7.1 about the possible structure of the causal linkages between inequality and democratization. And we can embed this prior knowledge of the possible causal relations in this domain in a lower-level model that is consistent with the high-level model that most simply represents our query. If we were to seek to fully capture them, the models developed by Boix, Acemoglu and Robinson, and Ansell and Samuels would, each individually, suggest causal graphs with a large number of nodes and edges connecting them. Representing all variables and relationships jointly contained in these three models would take an extremely complex graph. Yet there is no need to go down to the lowest possible level—to generate the most detailed graph—in order to increase our empirical leverage on the problem. We represent in Figure one possible lower-level model consistent with our high-level model. Drawing on causal logics in the existing literature, we unpack the nodes in the high-level model in two ways: We interpose a mediator between inequality and democratization: mobilization (\\(M\\)) by economically disadvantaged groups expressing material grievances. \\(M\\) is a function of both \\(I\\) and of its own response-type variable, \\(\\theta^M\\), which defines its response to \\(I\\). In inserting this mediator, we have extracted \\(\\theta^M\\) from \\(\\theta^D\\), pulling out that part of \\(D\\)’s response to \\(I\\) that depends on \\(M\\)’s response to \\(I\\). We specify a second influence on democratization, international pressure (\\(P\\)). Like \\(\\theta^M\\), \\(P\\) has also been extracted from \\(\\theta^D\\); it represents that part of \\(D\\)’s response to \\(I\\) that is conditioned by international pressures. Figure 7.2: A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type. In representing the causal dependencies in this graph, we allow for inequality to have (in the language of mediation analysis) both an “indirect” effect on democratization via mobilization and a “direct” effect. The arrow running directly from \\(I\\) to \\(D\\) allows for effects of inequality on democratization beyond any effects running via mobilization of the poor, including effects that might run in the opposite direction. (For instance, it is possible that inequality has a positive effect on democratization via mobilization but a negative effect via any number of processes that are not explicitly specified in the model.) The graph also implies that there is no confounding: since there is no arrow running from another variable in the graph to \\(I\\), \\(I\\) is assigned as-if randomly. The lower-level graph thus has two exogenous, response-type nodes that will be relevant to assessing causal effects: \\(\\theta^M\\) and \\(\\theta^{D_{lower}}\\). \\(\\theta^M\\), capturing \\(I\\)’s effect on \\(M\\), ranges across the usual four values for a single-cause, binary setup: \\(\\theta_{00}^M, \\theta_{01}^M, \\theta_{10}^M\\), and \\(\\theta_{11}^M\\). \\(\\theta^{D_{lower}}\\) is considerably more complicated, however, because this node represents \\(D\\)’s response to three causal variables: \\(I\\), \\(M\\), and \\(P\\). One way to put this is that the values of \\(\\theta^{D_{lower}}\\) indicate how inequality’s direct effect will depend on mobilization (and vice versa), conditional on whether or not there is international pressure. We need more complex notation than that introduced in Chapter 5 in order to represent the possible response types here. In Chapter 5, we needed four subscripts on \\(\\theta\\) to represent the potential outcomes for the four combinations of values that two binary variables can take on. With a third binary variable, we now require eight. Table 7.1: Interpreting the eight subscripts on \\(\\theta^D\\) given 3 parents. subscript position Interpretation 1 Value of D when: I = 0 &amp; P = 0 &amp; M = 0 2 Value of D when: I = 1 &amp; P = 0 &amp; M = 0 3 Value of D when: I = 0 &amp; P = 1 &amp; M = 0 4 Value of D when: I = 1 &amp; P = 1 &amp; M = 0 5 Value of D when: I = 0 &amp; P = 0 &amp; M = 1 6 Value of D when: I = 1 &amp; P = 0 &amp; M = 1 7 Value of D when: I = 0 &amp; P = 1 &amp; M = 1 8 Value of D when: I = 1 &amp; P = 1 &amp; M = 1 Thus, to illustrate, \\(\\theta^D_{01011010}\\) represents a type in which inequality always (regardless of pressure) has a positive direct effect on democratization when \\(M=0\\) and always has a negative direct effect on democratization when \\(M=1\\); in which pressure itself never has an effect; and in which mobilization has a positive effect when inequality is low but a negative effect when inequality is high. The result is \\(2^8=256\\) possible response types for \\(D\\). With 4 response types for \\(M\\), we thus have 1024 possible combinations of causal effects between named variables in the lower-level graph. How do these lower-level response types map onto the higher-level response types that are of interest? In other words, which combinations of lower-level types represent a positive, negative, or zero causal effect of inequality on democratization? To define a causal effect of \\(I\\) in this setup, we need to define the “joint effect” of two variables as being the effect of changing both variables simultaneously (in the same direction, unless otherwise specified). Thus, the joint effect of \\(I\\) and \\(M\\) on \\(D\\) is positive if changing both \\(I\\) and \\(M\\) from \\(0\\) to \\(1\\) changes \\(D\\) from \\(0\\) to \\(1\\). We can likewise refer to the joint effect of an increase in one variable and a decrease in another. Given this definition, a positive causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns: Linked positive mediated effects. \\(I\\) has a positive effect on \\(M\\); and \\(I\\) and \\(M\\) have a joint positive effect on \\(D\\) when \\(P\\) takes on whatever value it takes on in the case. Linked negative mediated effects \\(I\\) has a negative effect on \\(M\\); and \\(I\\) and \\(M\\) have a joint negative effect on \\(D\\) when \\(P\\) takes on whatever value it takes on in the case. Positive direct effect \\(I\\) has no effect on \\(M\\) and \\(I\\) has a positive effect on \\(D\\) at whatever value \\(M\\) is fixed at and whatever value \\(P\\) takes on in the case. If we start out with a case in which inequality is high and democratization has not occurred (or inequality is low and democratization has occurred), we will be interested in the possibility of a negative causal effect. A negative causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns: Positive, then negative mediated effects \\(I\\) has a positive effect on \\(M\\); and \\(I\\) and \\(M\\) have a joint negative effect on \\(D\\) when \\(P\\) takes on whatever value it takes on in the case. Negative, then joint negative mediated effects \\(I\\) has a negative effect on \\(M\\); and jointly increasing \\(I\\) while decreasing \\(M\\) generates a decrease in \\(D\\) when \\(P\\) takes on whatever value it takes on in the case. Negative direct effects \\(I\\) has no effect on \\(M\\) and \\(I\\) has a negative effect on \\(D\\) at whatever value \\(M\\) is fixed at and whatever value \\(P\\) takes on in the case. Finally, all other response patterns yield no effect of inequality on democratization. Thus, for a case in which \\(I=D=1\\), our query amounts to assessing the probability that \\(\\theta^M\\) and \\(\\theta^D_{lower}\\) jointly take on values falling into conditions 1, 2, or 3. And for a case in which \\(I \\neq D\\), where we entertain the possibility of a negative effect, our query is an assessment of the probability of conditions 4, 5, and 6. 7.2.1 Forming Priors We now need to express prior beliefs about the probability distribution from which values of \\(\\theta^M\\) and \\(\\theta^D_{lower}\\) are drawn. We place structure on this problem by drawing a set of beliefs about the likelihood or monotonicity of effects and interactions among variables from the theories in Boix, Acemoglu and Robinson, and Ansell and Samuels. As a heuristic device, we weight more heavily those propositions that are more widely shared across the three works than those that are consistent with only one of the frameworks. We intend this part of the exercise to be merely illustrative of how one might go about forming priors from an existing base of knowledge; there are undoubtedly other ways in which one could do so from the inequality and democracy literature. Specifically, the belief that we embed in our priors about \\(\\theta^M\\) is: Monotonicity of \\(I\\)’s effect on \\(M\\): In Acemoglu and Robinson, inequality should generally increase the chances of—and, in Boix, should never prevent—mobilization by the poor. Only in Ansell and Samuels’ model does inequality have a partial downward effect on the poor’s demand for democracy insofar as improved material welfare for the poor increases the chances of autocratic expropriation; and this effect is countervailed by the greater redistributive gains that the poor will enjoy under democracy as inequality rises.59 Consistent with the weight of prior theory on this effect, in our initial run of the analysis, we rule out negative effects of \\(I\\) on \\(M\\). We are indifferent in our priors between positive and null effects and between the two types of null effects (mobilization always occurring or never occurring, regardless of the level of inequality). We thus set our prior on \\(\\theta^M\\) as: \\(p(\\theta^M=\\theta^M_{10})=0.0\\), \\(p(\\theta^M=\\theta^M_{00})=0.25\\), \\(p(\\theta^M=\\theta^M_{11})=0.25\\), and \\(p(\\theta^M=\\theta^M_{01})=0.5\\). We relax this monotonicity assumption, to account for the Ansell and Samuels logic, in a second run of the analysis. For our prior on democracy’s responses to inequality, mobilization, and international pressure (\\(\\theta^D_{lower}\\)), we extract the following beliefs from the literature: Monotonicity of direct \\(I\\) effect: no positive effect: In none of the three theories does inequality promote democratization via a pathway other than via the poor’s rising demand for it. In all three theories, inequality has a distinct negative effect on democratization via an increase in the elite’s expected losses under democracy and thus its willingness to repress. In Ansell and Samuels, the distribution of resources also affects the probability of success of rebellion; thus higher inequality also reduces the prospects for democratization by strengthening the elite’s hold on power. We thus set a zero prior probability on all types in which \\(I\\)’s direct effect on \\(D\\) is positive for any value of \\(P\\). Monotonicity of \\(M\\)’s effect: no negative effect: In none of the three theories does mobilization reduce the prospects of democratization. We thus set a zero probability on all types in which \\(M\\)’s effect on \\(D\\) is negative at any value of \\(I\\) or \\(P\\). Monotonicity of \\(P\\)’s effect: no negative effect: While international pressures are only discussed in Haggard and Kaufman’s study, none of the studies considers the possibility that international pressures to democratize might prevent democratization that would otherwise have occurred. We thus set a zero probability on all types in which \\(P\\)’s effect is negative at any value of \\(I\\) or \\(M\\). In all, this reduces the number of nodal types for \\(D\\) from 256 to just 20. For all remaining, allowable types, we set flat priors. In Table we show, for the remainig 20 allowable types, how international pressure moderates the effects of inequality (direct) and mobilization. We use the following notation to characterize conditioning effects of pressure on the effect of another variable, \\(X\\): \\(N\\): \\(P\\) has no moderating effect \\(O_X\\): \\(P=1\\) creates an “opportunity” for \\(X\\) to have an effect that it does not have at \\(P=0\\); at \\(P=1\\) and \\(X=0\\), \\(D\\) takes on the value it does when \\(X=0\\) and \\(X\\) has an effect, but does not take on this value when \\(P=0\\) and \\(X=0\\) \\(C_X\\): \\(P=1\\) is a causal “complement” to \\(X\\), allowing \\(X\\) to have an effect it did not have at \\(P=0\\); at \\(P=1\\) and \\(X=1\\), \\(D\\) takes on the value it does when \\(X=1\\) and \\(X\\) has an effect, but does not take on this value when \\(P=0\\) and \\(X=1\\) \\(S_X\\): \\(P=1\\) “substitutes” for \\(X\\), generating the outcome that \\(X=1\\) was necessary to generate at \\(P=0\\); at \\(P=1\\) and \\(X=0\\), \\(D\\) takes on the value it does when \\(X=1\\) and \\(X\\) has an effect, but does not take on this value when \\(P=0\\) and \\(X=0\\) \\(E_X\\): \\(P\\) “eliminates” \\(X\\)’s effect, preventing \\(X=1\\) from generating the outcome it generates when \\(P=0\\); at \\(P=1\\) and \\(X=1\\), \\(D\\) does not take on the value it does when \\(X=1\\) and \\(X\\) has an effect, but does take on this value when \\(P=0\\) and \\(X=1\\) FLAG: Types are given in gbiqq ordering but with this notation we need to check if interpretation is still consistent with cell contents (suspect not as P,M ordering may be reversed) Table 7.2: Table of allowable types. The table characterizes the nature of the interaction, using the notation explained in the main text. We impose flat priors across all non-excluded types. Nodal Type \\(\\theta^D\\) Nature of interaction Prior probability \\(\\theta_{00000000}\\) \\(N\\) 0.05 \\(\\theta_{00000010}\\) \\(C_M, O_I\\) 0.05 \\(\\theta_{00100010}\\) \\(N\\) 0.05 \\(\\theta_{00001010}\\) \\(O_I\\) 0.05 \\(\\theta_{00101010}\\) \\(S_M, O_I\\) 0.05 \\(\\theta_{10101010}\\) \\(N\\) 0.05 \\(\\theta_{00000011}\\) \\(C_M\\) 0.05 \\(\\theta_{00100011}\\) \\(C_M, E_I\\) 0.05 \\(\\theta_{00110011}\\) \\(N\\) 0.05 \\(\\theta_{00001011}\\) \\(C_M, S_M, O_I\\) 0.05 \\(\\theta_{00101011}\\) \\(S_M, E_I, O_I\\) 0.05 \\(\\theta_{10101011}\\) \\(C_M, E_I, O_I\\) 0.05 \\(\\theta_{00111011}\\) \\(S_M, O_I\\) 0.05 \\(\\theta_{10111011}\\) \\(N\\) 0.05 \\(\\theta_{00001111}\\) \\(N\\) 0.05 \\(\\theta_{00101111}\\) \\(S_M, E_I\\) 0.05 \\(\\theta_{10101111}\\) \\(E_I\\) 0.05 \\(\\theta_{00111111}\\) \\(S_M\\) 0.05 \\(\\theta_{10111111}\\) \\(S_M, E_I\\) 0.05 \\(\\theta_{11111111}\\) \\(N\\) 0.05 Since \\(P\\) conditions the effect of \\(I\\), we must also establish a prior on the distribution of \\(P\\). In this analysis, we set the prior probability of \\(P=1\\) to 0.5, implying that before seeing the data we think that international pressures to democratize are present half the time. 7.3 Results We can now choose nodes other than \\(I\\) or \\(D\\) to observe from the lower-level model. Recall that our query is about the joint values of \\(\\theta^M\\) and \\(\\theta^{D_{lower}}\\). By the logic \\(d-\\)separation, we can immediately see that both \\(M\\) and \\(P\\) may be informative about these nodes when \\(D\\) has already been observed. Conditional on \\(D\\), both \\(M\\) and \\(P\\) are \\(d-\\)connected to both \\(\\theta^M\\) and \\(\\theta^{D_{lower}}\\). Let us see what learn, then, if we search for either mobilization of the lower classes or international pressure or both, and find either clue either present or absent. We consider four distinct situations, corresponding to four possible combinations of inequality and democratization values that we might be starting with. In each situation, the nature of the query changes. Where we start with a case with low inequality and no democratization, asking if inequality caused the outcome is to ask if the lack of inequality caused the lack of democratization. Where we have high inequality and no democratization, we want to know if democratization was prevented by high inequality (as high inequality does in Boix’s account). For cases in which democratization occurred, we want to know whether the lack or presence of inequality (whichever was the case) generated the democratization. Inference is done by applying Bayes rule to the observed data given the priors. Different “causal types” are consistent or inconsistent with possible data observations. Conversely the observation of data lets us shift weight towards causal types that are consistent with the data and away from those that are not. A case that displays \\(D=1\\) when \\(P=M=I=0\\) cannot be of type \\(\\theta_{00000000}\\), thus observation of \\(P=M=I=0, D=1\\) would force a shift in weight away from this (and other) nodal types and onto nodal type \\(\\theta_{11111111}\\) (and other compatible types) . 7.4 Pathways FLAG: Do pathways as estimands ADD ESTIMAND: DOES INEQUALITY REDUCE THE EFFECT OF MOBILIZATION. 7.4.1 Cases with incomplete data We consider first causal relations for cases that did not democratize. These cases are not included in Haggard and Kaufman (2012). The results for cases that did not democratize (at the time in question) are presented in tables 7.3 and Table 7.4. Each table shows, for one kind of case, our posterior beliefs on the causal responsibility of \\(I\\) for the outcome for different search strategies. 7.4.1.1 \\(I=0, D=0\\): Non democracy with low inequality To begin with \\(I=0, D=0\\) cases, did the lack of inequality cause the lack of democratization (as, for instance, at the lefthand end of the Acemoglu and Robinson inverted \\(U\\)-curve)? Table 7.3: No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects. P M posterior NA NA 0.128 0 NA 0.088 1 NA 0.231 NA 0 0.150 0 0 0.107 1 0 0.250 NA 1 0.000 0 1 0.000 1 1 0.000 We start out, based on the \\(I\\) and \\(D\\) values and our model, believing that there is a 0.128 chance that low inequality prevented democratization. We then see that our beliefs shift most dramatically if we go looking for mobilization and find that it was present. The reason is that any positive effect of \\(I\\) on \\(D\\) has to run through the pathway mediated by \\(M\\) because we have excluded a positive direct effect of \\(I\\) on \\(D\\) in our priors. Moreover, since we do not allow \\(I\\) to have a negative effect on \\(M\\), observing \\(M=1\\) when \\(I=0\\) must mean that \\(I\\) has no effect on \\(M\\) on this case, and thus \\(I\\) cannot have a positive effect on \\(D\\) (regardless also of what we find if we look for \\(P\\)). If we do not observe mobilization when we look for it, we now think it is somewhat more likely that \\(I=0\\) caused \\(D=0\\) since it is still possible that high inequality could cause mobilization. We also see that observing whether there is international pressure has a substantial effect on our beliefs. When we observe \\(M=1\\) (or don’t look for \\(M\\) at all), the presence of international pressure increases the likelihood that low inequality prevented democratization. Intuitively, this is because international pressure, on average across types, has a positive effect on democratization; so pressure’s presence creates a greater opportunity for low inequality to counteract international pressure’s effect and prevent democratization from occurring that otherwise would have (if there had been high inequality and the resulting mobilization). 7.4.1.2 \\(I=1, D=0\\): Non democracy with high inequality In cases with high inequality and no democratization, the question is whether high inequality prevented democratization via a negative effect, as theorized by Boix. That negative effect has to have operated via inequality’s direct effect on democratization since our monotonicity restrictions allow only positive effects via mobilization. Here, the consequence of observing \\(P\\) is similar to what we see in the \\(I=0, D=0\\) case: seeing international pressure greatly increases our confidence that high inequality prevented democratization, while seeing no international pressure moderately reduces that confidence. There is, returning to the same intuition, more opportunity for high inequality to exert a negative effect on democratization when international pressures are present, pushing toward democratization. Table 7.4: Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects. P M posterior NA NA 0.438 0 NA 0.340 1 NA 0.615 NA 0 0.394 0 0 0.263 1 0 0.571 NA 1 0.475 0 1 0.393 1 1 0.667 Here, however, looking for \\(M\\) has more modest effect than it does in an \\(I=0, D=0\\) case. This is because we learn less about the indirect pathway from \\(I\\) to \\(D\\) by observing \\(M\\): as we have said, we already know from seeing high inequality and no democratization (and under our monotonicity assumptions) that any effect could not have run through the presence or absence of mobilization. However, \\(M\\) provides some information because it, like \\(P\\), acts as moderator for \\(I\\)’s direct effect on \\(D\\) (since \\(M\\) is also pointing into \\(D\\)). As we know, learning about moderators tells us something about (a) the rules governing a case’s response to its context (i.e., its response type) and (b) the context it is in. Thus, in the first instance, observing \\(M\\) together with \\(I\\) and \\(D\\) helps us eliminate types inconsistent with these three data points. For instance, if we see \\(M=0\\), then we eliminate any type in which \\(D\\) is 0, regardless of \\(P\\)’s value, when \\(M=0\\) and \\(I=1\\). Second, we learn from observing \\(M\\) about the value of \\(M\\) under which \\(D\\) will be responding to \\(I\\). Now, because \\(M\\) is itself potentially affected by \\(I\\), the learning here is somewhat complicated. What we learn most directly from observing \\(M\\) is the effect of \\(I\\) on \\(M\\) in this case. If we observe \\(M=1\\), then we know that \\(I\\) has no effect on \\(M\\) in this case; whereas if we observe \\(M=0\\), \\(I\\) might or might not have a positive effect on \\(M\\). Learning about this \\(I \\rightarrow M\\) effect then allows us to form a belief about how likely \\(M\\) would be to be 0 or 1 if \\(I\\) changed from \\(0\\) to \\(1\\); that is, it allows us to learn about the context under which \\(D\\) would be responding to this change in \\(I\\) (would mobilization be occurring or not)? This belief, in turn, allows us to form a belief about how \\(D\\) will respond to \\(I\\) given our posterior beliefs across the possible types that the case is. The net effect, assuming that we have not observed \\(P\\), is a small upward effect in our confidence that inequality mattered if we see no mobilization, and a small downward effect if we see mobilization. Interestingly, if we do observe \\(P\\), the effect of observing \\(M\\) reverses: observing mobilization increases our confidence in inequality’s effect, while observing no mobilization reduces it. 7.4.2 Inferences for cases with observed democratization We now turm to cases in which democratization has occurred—the category of cases that Haggard and Kaufman examine. For these cases we use data from Haggard and Kaufman (2012) to show the inferences we would draw using this procedure and the actual observations made for a set of 8 cases. Haggard and Kaufman consider only cases that democratized, so all cases in this table have the value \\(D=1\\). We show here how confident we would be that the level inequality caused democratization if (a) we observed only the cause and effect (\\(I\\) and \\(D\\)); (b) we additionally observed either the level of mobilization by disadvantaged classes or the level of international pressure; and (c) if we observed both, in addition to \\(I\\) and \\(D\\). Note that countries labels are marked in the “full data” cells in the lower right quadrant, but their corresponding partial data cells can be read by moving to the left column or the top row (or to the top left cell for the case with no clue data). In coding countries’ level of inequality, we rely on Haggard and Kaufman’s codings using the Gini coefficient from the Texas Inequality dataset. In selecting cases of democratization, we use the codings in Cheibub, Gandhi, and Vreeland (2010), one of two measures used by Haggard and Kaufman. Our codings of the \\(M\\) and \\(P\\) clues come from close readings of the country-specific transition accounts in Haggard, Kaufman, and Teo (2012), the publicly shared qualitative dataset associated with Haggard and Kaufman (2012). We code \\(M\\) as \\(1\\) where the transition account refers to anti-government or anti-regime political mobilization by economically disadvantaged groups, and as \\(0\\) otherwise. For \\(P\\), we code \\(P=0\\) is international pressures to democratize are not mentioned in the transition account. The main estimates refer to analyses with only qualitative, monotonicity restrictions on our priors. We also show in square brackets the estimates if we allow for a negative effect of inequality on mobilization but believe it to be relatively unlikely. 7.4.2.1 \\(I=0, D=1\\): Low inequality democracies In a case that had low inequality and democratized, did low inequality cause democratization, as Boix’s thesis would suggest? Looking at the first set of cases in Table 7.5, did Mexico, Albania, Taiwan, and Nicaragua democratize because they had relatively low inequality? Based only on observing the level of inequality and the outcome of democratization, we would place a 0 probability on inequality having been a cause. What can we learn, then, from our two clues? We are looking here for a negative effect of \\(I\\) on \\(D\\), which in our model can only run via a direct effect, not through mobilization. Thus, the learning from \\(M\\) is limited for the same reason as in an \\(I=1, D=0\\) case. And \\(M\\) is modestly informative as a moderator for the same reasons and in the same direction, with observing mobilization generally reducing our confidence in inequality’s negative effect relative to observing no mobilization. In our four cases, if we observe the level of mobilization, our confidence that inequality mattered goes up slightly (to 0) in Mexico and Taiwan, where mobilization did not occur, and goes down slightly in Albania and Nicaragua (to 0) where mobilization did occur. Table 7.5: Four cases with low inequality and democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on \\(M\\) or \\(P\\). Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects. Case M: Mobilization? P: Pressure? No clues M only P only M and P Mexico (2000) 0 0 0.438 0.475 0.615 0.667 Taiwan (1996) 0 1 0.438 0.475 0.34 0.393 Albania (1991) 1 0 0.438 0.394 0.615 0.571 Nicaragua (1984) 1 1 0.438 0.394 0.34 0.263 Looking for the international pressure clue is, however, highly informative, though the effect runs in the opposite direction as in an \\(I=1, D=0\\) case. It is observing the absence of international pressure that makes us more confident in low inequality’s effect. Since democratization did occur, the presence of international pressure makes it less likely for low inequality to have generated the outcome since international pressure could have generated democratization by itself. Once we bring this second clue into the analysis, Mexico and Taiwan sharply part ways: seeing no international pressure in Mexico, we are now much more confident that inequality mattered for the Mexican transition (0); seeing international pressure in Taiwan, we are now substantially less confident that inequality mattered to the Taiwanese transition (NA). Similarly, observing \\(P\\) sharply differentiates the Albanian and Nicaraguan cases: seeing no international pressure in the Albanian transition considerably boosts our confidence in inequality’s causal role there (1), while observing international pressure in the Nicaraguan transition strongly undermines our belief in an inequality effect there (NA). 7.4.2.2 \\(I=1, D=1\\): High inequality democracies Where we see both high inequality and democratization, the question is whether high inequality caused democratization via a positive effect. Considering the second set of cases in Table , did high inequality cause Mongolia, Sierra Leone, Paraguay, and Malawi to democratize? Observing only the level of inequality and the democratization outcome, we would have fairly low confidence that inequality mattered, with a posterior on that effect of 1. Let us see what we can learn if we also observe the level of mobilization and international pressure. As in an \\(I=0, D=0\\) case, \\(M\\) can now be highly informative since this positive effect has to run through mobilization. Here it is the observation of a lack of mobilization that is most telling: high inequality cannot have caused democratization, given our model, if inequality did not cause mobilization to occur. There is no point in looking for international pressure since doing so will have no effect on our beliefs. Thus, when we observe no mobilization by the lower classes in Mongolia and Paraguay, we can be certain (given our model) that high inequality did not cause democratization in these cases. Moreover, this result does not change if we also go and look for international pressure: neither seeing pressure nor seeing its absence shifts our posterior away from 1. Table 7.6: Four cases with high inequality and democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on \\(M\\) or \\(P\\). Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects. Case M: Mobilization? P: Pressure? No clues M only P only M and P Mongolia (1990) 0 0 0.128 0 0.231 0 Paraguay (1989) 0 1 0.128 0 0.088 0 Sierra Leone (1996) 1 0 0.128 0.15 0.231 0.25 Malawi (1994) 1 1 0.128 0.15 0.088 0.107 If we do see mobilization, on the other hand—as in Sierra Leone and Malawi—we are slightly more confident that high inequality was the cause of democratization (1). Moreover, if we first see \\(M=1\\), then observing international pressure can add much more information; and it substantially differentiates our conclusions about the causes of Sierra Leone’s and Malawi’s transitions. Just as in an $I=0, D=1 case, it is the absence of international pressure that leaves the most “space” for inequality to have generated the democratization outcome. When we see the absence of pressure in Sierra Leone, our confidence that high inequality was a cause of the transition increases to 1; seeing pressure present in Malawi reduces our confidence in inequality’s effect to NA. 7.5 Model definition and inference in code How is this model defined and used in practice? Using the gbiqq package we can set this model up, along with restrictions, in a few lines a follows. model &lt;- make_model(&quot;I -&gt; M -&gt; D &lt;- P; I -&gt; D&quot;) %&gt;% set_restrictions(&quot;(M[I=1] &lt; M[I=0])&quot;) %&gt;% set_restrictions(&quot;(D[I=1] &gt; D[I=0]) | (D[M=1] &lt; D[M=0]) | (D[P=1] &lt; D[P=0])&quot;) We can then calculate conditional inferences as follows: conditional_inferences(model, query = &quot;D[I=0]==0&quot;, given = &quot;D==1 &amp; I==1&quot;) 7.6 Concluding thoughts Haggard and Kaufman set out to use causal process observations to test inequality-based theories of democratization against the experiences of “Third Wave” democratizations. Their principal test is to examine whether they see evidence of distributive conflict in the process of democratization, defined largely as the presence or absence of mobilization prior to the transition. They secondarily look for other possible causes, specifically international pressure and splits in the elite. In interpreting the evidence, Haggard and Kaufman generally treat the absence of mobilization as evidence against inequality-based theories of democratization as a whole (p. 7). They also see the presence of distributive mobilization in cases with high inequality and democratization as evidence against the causal role of inequality (p. 7). These inferences, however, seem only loosely connected to the logic of the causal theories under examination. Haggard and Kaufman express concern that inequality-oriented arguments point to “cross-cutting effects” (p. 1) of inequality, but do not systematically work through the implications of these multiple pathways for empirical strategy. Our analysis suggests that a systematic engagement with the underlying models can shift that interpretation considerably. Under the model we have formulated, where inequality is high, the absence of mobilization in a country that democratized is indeed damning to the notion that inequality mattered. However, where inequality is low—precisely the situation in which Boix’s theory predicts that we will see democratization—things are more complicated. If we assume that inequality cannot prevent mobilization, then observing no mobilization does not work against the claim that inequality mattered for the transition; indeed, it slightly supports it, at least given what we think is a plausible model-representation of arguments in the literature. Observing the absence of inequality in such a case, however, can undercut an inequality-based explanation if (and only if) we believe it is possible that inequality might prevent mobilization that would otherwise have occurred. Further, in cases with high inequality and democratization, it is the absence of mobilization by the lower classes that would least consistent with the claim that inequality mattered. Observing mobilization, in contrast, pushes in favor of an inequality-based explanation. Moreover, it is striking that Haggard and Kaufman lean principally on a mediator clue, turning to evidence of international pressure and elite splits (moderators, or alternative causes) largely as secondary clues to identify “ambiguous” cases. As we have shown, under a plausible model given prior theory, it is the moderator clue that is likely to be much more informative. Of course, the model that we have written down is only one possible interpretation of existing theoretical knowledge. It is very possible that Haggard and Kaufman and other scholars in this domain hold beliefs that diverge from those encoded in our working model. The larger point, however, is that our process tracing inferences will inevitably depend—and could depend greatly—on our background knowledge of the domain under examination. Moreover, formalizing that knowledge as causal model can help ensure that we are taking that prior knowledge systematically into account—that the inferences we draw from new data are consistent with the knowledge that we bring to the table. The analysis also has insights regarding case selection. Haggard and Kaufman justify their choice of only \\(D=1\\) cases as a strategy “designed to test a particular theory and thus rests on identification of the causal mechanism leading to regime change” (p. 4). Ultimately, however, the authors seem centrally concerned with assessing whether inequality, as opposed to something else, played a key causal role in generating the outcome. As the results above demonstrate, however, there is nothing special about the \\(D=1\\) cases in generating leverage on this question. The tables for \\(D=0\\) show that, given the model, the same clues can shift beliefs about as much for \\(D=0\\) as for \\(D=1\\) cases. We leave a more detailed discussion of this kind of issue in model-based case-selection for Chapter REFERENCE. Finally we emphasize that all of the inference in this chapter depends on a model that is constrained by theoretical insights but not one that is trained by data. Although we are able to make many inferences using this model, given the characteristics of a case of interest, we have no empirical grounds to justify these inferences. In Chapter REFERENCE we show how this model can be trained with broader data from multiple cases and in Chapter REFERENCE we illustrate how the model itself can be put into question. References "],
["mixing.html", "Chapter 8 Integrated inferences 8.1 There’s only ever one case 8.2 General procedure 8.3 Illustration 8.4 Illustrated inferences 8.5 Considerations 8.6 Conclusion", " Chapter 8 Integrated inferences We argue that mixed methods can be thought of as the analysis of single cases with vector valued variables. Reconceptualizing as large n is useful prmarily for computation reasons and often comes with hidden independence assumptions. We illustrate the single case approach and provide a set of models for the many case approach. The main goal of this chapter is to generalize the model developed in Chapter 6 to research situations in which we have data on multiple cases. In doing so we generalize the model in Humphreys and Jacobs (2015) to one that in which rather than the probative value of clues being assumed, they are derived from a causal structure. We start however with a conceptual point: the exact structure introduced in Chapter 6 for single case analysis can be used as is for multi-case analysis. To see this you should think of the the nodes as vector-valued, and the estimands as just a particular summary of the vector-valued case level causal effects. Thought of this way the conceptual work for mixed methods inference from models has been done already and our goal here is more technical—how to exploit assumptions on independence across cases to generate simpler theories of repeated phenomena. 8.1 There’s only ever one case Conceptualized correctly, there is no difference at all between the data types or the inference used in within-case and cross-case inference. The reason is not, as King, Keohane, and Verba (1994) suggest, that all causal inference is fundamentally correlational, even in seemingly single case studies. Nor is the point that, looked at carefully, single “case studies” can be disaggregated into many cases. The intuition runs in the opposite direction: fundamentally, model-based inference always involves comparing a pattern of data with the logic of the model. Looked at carefully, studies with multiple cases can be conceptualized of as single-case studies: the drawing of inferences from a single collection of clues. The key insight is that, when we move from a causal model with one observation to a causal model with multiple observations, all that we are doing is replacing nodes with a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors). To illustrate the idea that multi-case studies are really single-case studies with vector valued variables, consider the following situation. There are two units studied, drawn from some population, a binary treatment \\(X\\) is assigned independently with probability .5 to each case; an outcome \\(Y\\) along with clue variable \\(K\\) is observable. We suppose \\(X\\) can affect \\(Y\\) and in addition there is a background, unobserved, variable \\(\\theta\\) (causal type) that takes on values in \\(\\{a,b,c,d\\}\\), that affects both \\(K\\) and \\(Y\\). We will assume that \\(\\theta\\) is not independently assigned and that the two units are more likely to have the same values of \\(\\theta\\) than different values. For simplicity, we will suppose that for any given case \\(K=1\\) whenever \\(X\\) causes \\(Y\\), and \\(K=1\\) with a 50% probability otherwise. Thus, \\(K\\) is informative about a unit’s causal type. Note that we have described the problem at the unit level. We can redescribe it at the population level however as a situation in which a treatment vector \\(X\\) can take on one of four values, \\((0,0), (0,1), (1,0), (1,1)\\) with equal probability (or more strictly: as determined by \\(\\theta\\)). \\(\\theta\\) is also a vector with two elements that can take on one of 16 values \\((a,a), (a,b),\\dots (d,d)\\) as determined by \\(U_\\theta\\). In this case we will assume that the 16 possibilities are not equally likely, which captures the failure of independence in the unit level assignments. \\(Y\\) is a vector that reflects the elements of \\(\\theta\\) and \\(X\\) in the obvious way (e.g \\(X=(0,0), \\theta=(a,b)\\) generates outcomes \\(Y=(1,0)\\); though it is immediately obvious that representing nodes in vector forms allows for more general vector-level mappings to allow for SUTVA violations. \\(K\\) has the same domain as \\(X\\) and \\(Y\\), and element \\(K[j]=1\\) if \\(\\theta[j]=b\\). Note that to describe the estimand, the Sample Average Treatment Effect, we also need to consider operations and queries defined at the vector level. In practice we consider three operations, one in which both units have \\(X\\) forced to 0 and two in which one nit has \\(X\\) set to 0 and the other has \\(X\\) set to 1. Thus we are interested in the average effect of changing one unit to treatment while the other is held in control. Note also that before our estimands were binary—of the form: is it a \\(b\\) type?–and our answer was a probability; now our estimand is categorical and our answer is a distribution (what is the probability the SATE is 0, what is the probability the SATE is .5, etc…) Represented in this way we can use the tools of Chapters 6 and 7 to fully examine this seemingly multi-case study. In the below we examine a situation in which we consider the value of observing \\(K\\) on one case — in this set up this is equivalent to observing part of the vector \\(K\\) and making inferences on the full vector \\(\\theta\\). 8.2 General procedure In practice however thinking of nodes as capturing the outcomes on all units leads to enormous complexity. For example an exogeneous variable \\(X\\) which takes on values of 0 or 1 at random for 10 units has \\(2^{10}\\) types in this conceptualization, rather than just two when thought of at the case level. We reduce complexity however by thinking of models as operating on units and learning about models by observing multiple realizations of processes covered by the model, rather than just one. Thinking about it this way is not free however as it requires invoking some kind of independence assumptions — that outcomes in two units do not depend on each other. If we cannot stand by that assumption, then we have to build independence failures into our models. With multiple cases we… A DAG. As for process tracing, we begin with a graphical causal model specifying possible causal linkages between nodes. Nodal types. Just as in process tracing, the DAG and variable ranges define the set of possible nodal types in the model—the possible ways in which each variable is assigned (if exogenous) or determined by its parents (if endogenous). Causal types. And, again, a full set of nodal types gives rise to a full set of causal types, encompassing all possible combinations of nodal types across all nodes in the model. Priors. The first difference between single- and multiple-case inference lies in how we set priors on causal types. In process tracing, we set parameter values for each nodal type (or conditional nodal type, for unobserved confounding). Our parameters—e.g., \\(\\lambda^X_0\\), \\(\\lambda^Y_{01}\\)—represent our beliefs about the proportions of these types in the population. When we only observe a single data type—data on a single case—we do not have sufficient information to learn about the distribution of types in the population. And so we treat these population-level beliefs as fixed parameters, rather than priors that we update on. (What we update on, in process tracing, is our priors on whether a given case is of a particular type or set of types.) Likewise, uncertainty about those population-level parameters has no effect on our inferences for a single case. When we get to observe data on multiple cases, however, we have the opportunity to learn both about the cases at hand and about the population. Moreover, our level of uncertainty about population-level parameters will shape our inferences. We thus want our parameters (the \\(\\lambda\\)’s) to be drawn from a prior distribution — a distribution that expresses our uncertainty and over which we can update once we see the data. While different distributions may be appropriate to the task in general, uncertainty over proportions (of cases, events, etc.) falling into a set of discrete categories is described by a Dirichlet distribution, as discussed in Chapter 5. As will be recalled, the parameters of a Dirichlet distribution (the \\(\\alpha\\)’s) can be thought of as conveying both the relative expected proportions in each category and our degree of uncertainty. To first examine a situation with no observed confounding, we need to specify a prior distribution for each set of nodal types. For a simple \\(X \\rightarrow Y\\) model, we have two parameter sets: one for \\(X\\)’s types and one for \\(Y\\)’s types. For \\(X\\)’s types, we specify \\(\\alpha^X_0\\) and \\(\\alpha^X_1\\), corresponding to the nodal types \\(\\theta^X_0\\) and \\(\\theta^X_1\\), respectively. For \\(Y\\)’s types, we specify \\(\\alpha^Y_{00}\\), \\(\\alpha^Y_{10}\\), \\(\\alpha^Y_{01}\\), and \\(\\alpha^Y_{11}\\), corresponding to the nodal types \\(\\theta^Y_{00}\\), etc. So, for instance, setting \\(\\alpha^Y_{00}=1\\), \\(\\alpha^Y_{10}=1\\), \\(\\alpha^Y_{01}=1\\), and \\(\\alpha^Y_{11}=1\\) yields a uniform distribution in which all share allocations of types in the population are equally likely. Setting \\(\\alpha^Y_{00}=3\\), \\(\\alpha^Y_{10}=3\\), \\(\\alpha^Y_{01}=3\\), and \\(\\alpha^Y_{11}=3\\) puts more weight on share allocations in which the shares are relatively equal. Setting \\(\\alpha^Y_{00}=5\\), \\(\\alpha^Y_{10}=5\\), \\(\\alpha^Y_{01}=10\\), and \\(\\alpha^Y_{11}=5\\) puts greater weight positive causal effects than the other three types. And we can express greater certainty about that weighting by setting higher absolute alpha values, such as \\(\\alpha^Y_{00}=50\\), \\(\\alpha^Y_{10}=50\\), \\(\\alpha^Y_{01}=100\\), \\(\\alpha^Y_{11}=5\\). NEED A DISCUSSION OF RESTRICTIONS APPROACH TO SETTING PRIORS A specified parameter set for node \\(j\\), then, allows for a range of possible allocations of nodal types in the population, or \\(\\lambda^j\\)’s while making some \\(\\lambda^j\\)’s more likely than others. A set of \\(\\alpha^j\\)’s, in other words, gives us a prior distribution over nodal types at node \\(j\\). Thus, under the parameter set for \\(Y\\) (\\(\\alpha^Y_{00}=3\\), \\(\\alpha^Y_{10}=3\\), \\(\\alpha^Y_{01}=3\\), \\(\\alpha^Y_{11}=3\\)), a relatively equal allocation like \\(\\lambda^Y_{00}=0.26\\), \\(\\lambda^Y_{10}=0.24\\), \\(\\lambda^Y_{01}=0.2\\), \\(\\lambda^Y_{11}=0.3\\) will have a higher probability than a highly skewed allocation like \\(\\lambda^Y_{00}=0.1\\), \\(\\lambda^Y_{10}=0.1\\), \\(\\lambda^Y_{01}=0.7\\), \\(\\lambda^Y_{11}=0.1\\). Likewise, \\(\\alpha^Y_{00}=5\\), \\(\\alpha^Y_{10}=5\\), \\(\\alpha^Y_{01}=10\\), \\(\\alpha^Y_{11}=5\\) puts more prior weight on any \\(\\lambda^Y\\) with relatively more positive effects and an even distributon among other types than on a \\(\\lambda^Y\\) with an even spread across all types or than one skewed towards negative effects. Meanwhile, the tighter distribution given by (\\(\\alpha^Y_{00}=50\\), \\(\\alpha^Y_{10}=50\\), \\(\\alpha^Y_{01}=100\\), \\(\\alpha^Y_{11}=5\\)) implies even steeper differences in those probabilities. For a model with any number of nodes, we can then imagine a draw of one \\(\\lambda^j\\) from its prior distribution for each node, giving a full \\(\\lambda\\) vector. Any particular \\(\\lambda\\) vector, in turn, implies a probability distribution over causal types (\\(\\theta\\)). With the help of a parameter matrix (mapping from parameters to causal types), we can then, just as with process tracing, calculate the prior probability that a case is of any particular causal type, given the parameter (\\(\\lambda\\)) values we have drawn. Implicitly, then, our prior distribution over \\(\\lambda\\) gives rise in turn to a prior distribution over the causal type shares in the population. Where there is unobserved confounding, we now need parameter sets corresponding to the correlated types, as with the setup for process tracing. Thus, if we believe the likelihood of \\(X=1\\) is correlated with whether or not \\(X\\) has a positive effect on \\(Y\\), we will need two parameter sets (rather than one) for \\(X\\): one for \\(X\\)’s value when \\(\\theta^Y = \\theta^Y_{01}\\) and one for \\(X\\)’s value when \\(\\theta^Y \\neq \\theta^Y_{01}\\). For each of these parameter sets, we specify two \\(\\alpha\\) parameters representing our beliefs about \\(X\\)’s assignment. We can draw \\(\\lambda\\) values for these conditional nodal types from the resulting Dirichlet distributions, as above, and can then calculate causal type probabilities in the usual way. Event probabilities. We now need to build a likelihood function that can map from beliefs about the world to data: i.e., that can tell us how likely we are to see a given data pattern—across multiple cases—under a given distribution of causal types in the population. The first step in building the likelihood function is to calculate event probabilities: the probability of observing a case of a particular data type given a particular population-level distribution of causal type shares (that is, given a \\(\\lambda\\) draw). We assume, for now, that we deploy the same data strategy for each case, collecting data on all nodes. We denote an event probability for a given data pattern for variables \\(X, Y, \\dots\\) as \\(w_{x, y, \\dots}\\). For instance, the probability of observing \\(X=0, Y=1\\) in a case (given \\(\\lambda\\)) is \\(w_{01}\\). An ambiguity matrix, just as for process tracing, tells us which causal types are consistent with a particular data type, as observed for a single case. To calculate the probability of the data given a distribution of causal types, we simply add together the probabilities of all of the causal types with which it is consistent. See, for instance, the parameter matrix and the ambiguity matrix in Tables 8.1 and 8.2. We have indicated a single draw of \\(\\lambda\\) values (population type shares) in the parameter matrix, and these have been used to calculate the priors on causal types provided in the ambiguity matrix. Let’s now calculate the event probability for each data type. Starting with \\(X=0, Y=0\\), we can read off the ambiguity matrix that the consistent causal types are (\\(\\theta^X_0, \\theta^Y_{00}\\)) and (\\(\\theta^X_0, \\theta^Y_{01}\\)). The event probability, \\(w_{00}\\), is then given by adding together the probabilities of these two causal types, \\(0.1 + 0.2 = 0.3\\). All four event probabilities, for the four data types, are then calculated in the same way: \\(w_{00} = 0.1 + 0.2 = 0.3\\) \\(w_{10} = 0.1 + 0.1 = 0.2\\) \\(w_{01} = 0.1 + 0.2 = 0.2\\) \\(w_{11} = 0.2 + 0.1 = 0.3\\) As any case must be of one and only one data type, the full set of event probabilities for a single \\(\\lambda\\) draw must naturally sum to \\(1\\). Table 8.1: A parameter matrix for a simple \\(X ightarrow Y\\) model (with no unobserved confounding), indicating a single draw of \\(\\lambda\\) values from the prior distribution. X0.Y00 X1.Y00 X0.Y10 X1.Y10 X0.Y01 X1.Y01 X0.Y11 X1.Y11 Shares X0 1 0 1 0 1 0 1 0 0.4 X1 0 1 0 1 0 1 0 1 0.6 Y00 1 1 0 0 0 0 0 0 0.3 Y10 0 0 1 1 0 0 0 0 0.2 Y01 0 0 0 0 1 1 0 0 0.2 Y11 0 0 0 0 0 0 1 1 0.3 Table 8.2: An ambiguity matrix for a simple \\(X ightarrow Y\\) model (with no unobserved confounding), showing the priors over causal types arising from a single draw of \\(\\lambda\\) from its prior distribution. X0Y0 X1Y0 X0Y1 X1Y1 prior X0Y00 1 0 0 0 0.12 X1Y00 0 1 0 0 0.18 X0Y10 0 0 1 0 0.08 X1Y10 0 1 0 0 0.12 X0Y01 1 0 0 0 0.08 X1Y01 0 0 0 1 0.12 X0Y11 0 0 1 0 0.12 X1Y11 0 0 0 1 0.18 For a case in which only partial data are observed, we follow the same basic logic as with partial process-tracing data. We retain all columns (data types) in the ambiguity matrix that are consistent with the partial data. So, for instance, if we observe only \\(Y=1\\), we woudl retain both the \\(X=0, Y=1\\) column and the \\(X=1, Y=1\\) column. We then calculate the event probability by summing causal-type probabilities for all causal types that could have produced these partial data — i.e., all those with a \\(1\\) in either column. Likelihood. Now that we know the probability of observing each data pattern in a single case given \\(\\lambda\\), we can use these event probabilities to aggregate up to the likelihood of observing a data pattern across multiple cases (given \\(\\lambda\\)). With discrete variables, we can think of a given multiple-case data pattern simply as a set of counts: for, say, \\(X, Y\\) data, we will observe a certain number of \\(X=0, Y=0\\) cases (\\(n_{00}\\)), a certain number of \\(X=1, Y=0\\) cases (\\(n_{10}\\)), a certain number of \\(X=0, Y=1\\) cases (\\(n_{01}\\)), and a certain number of \\(X=1, Y=1\\) cases (\\(n_{11}\\)). A data pattern, given a particular set of variables observed (a search strategy), thus has a multinomial distribution. The likelihood of a data pattern under a given search strategy, in turn, takes the form of a multinomial distribution conditional on the number of cases observed and the event probabilities for each data type, given a \\(\\lambda\\) draw. Let us assume now that we have a 3-node model, with \\(X, Y\\), and \\(M\\) all binary. Let \\(n_{XYK}\\) denote an 8-element vector recording the number of cases in a sample displaying each possible combination of \\(X,Y,K\\) data, thus: \\(n_{XYM}=(n_{000},n_{001},n_{100},\\dots ,n_{111})\\). The elements of \\(n_{XYK}\\) sum to \\(n\\), the total number of cases studied. Likewise, let the event probabilities for data types given \\(\\lambda\\) be registered in a vector, \\(w_{XYK}=(w_{000},w_{001},w_{100},\\dots ,w_{111})\\). The likelihood of a data pattern, \\(\\mathcal D\\) is then: \\[ \\Pr(\\mathcal{D}|\\lambda) = \\text{Multinom}\\left(n_{XYK}|n, w_{XYK}\\right) \\\\ \\] In other words, the likelihood of observing a particular data pattern given \\(\\lambda\\) is given by the corresponding value of the multinomial distribution given the event probabilities. What if we have a mixture of search strategies? Suppose, for instance, that we have collected \\(X,Y\\) data on a set of cases, and that we have additionally collected data on \\(M\\) for a random subset of these. We can think of this as conducting quantitative analysis on a large sample and conducting in-depth process tracing on a subsample. We then can summarize our data in two vectors, the 8-element \\(n_{XYM}\\) vector for the cases with process tracing, and a 4-element vector \\(n_{XY*} = (n_{00*},n_{10*},n_{01*},n{11*}\\) for the partial data on those cases with no process tracing. Likewise, we now have two sets of event probabilities: one for the cases with complete data, \\(w_{XYM}\\), and a 4-element vector for those with partial data, \\(w_{XY*}\\). Let \\(n\\) denote the total number of cases examined, and \\(k\\) the number for which we have data on \\(K\\). Now, assuming that each observed case represents an independent, random draw from the population, we can form the likelihood function as a product of multinomial distributions: \\[ \\Pr(\\mathcal{D}|\\theta) = \\text{Multinom}\\left(n_{XY*}|n-k, w_{XY*}\\right) \\times \\text{Multinom}\\left(n_{XYK}|k, w_{XYK}\\right) \\\\ \\] Thus, assuming that each observed case represents an independent, random draw from the population, we can form the likelihood function as a product of multinomial distributions. For instance, for a model with \\[ \\Pr(\\mathcal{D}|\\theta) = \\text{Multinom}\\left(n_{XY*}|n-k, w_{XY*}\\right) \\times \\text{Multinom}\\left(n_{XYK}|k, w_{XYK}\\right) \\\\ \\] 8.2.1 Estimation Say a data strategy seeks data on \\(X\\) and \\(Y\\) in 2 cases and seeks data on \\(K\\) if ever \\(X=Y=1\\). The probability of each data type is as given in table below: type: prob: \\(X0Y0\\) \\(\\lambda^X_0(\\lambda^Y_{00}+\\lambda^Y_{01}))\\) \\(X0Y1\\) \\(\\lambda^X_0(\\lambda^Y_{11}+\\lambda^Y_{10}))\\) \\(X1Y0\\) \\(\\lambda^X_1(\\lambda^Y_{00}+\\lambda^Y_{10}))\\) \\(X1M0Y1\\) \\(\\lambda^X_1(\\lambda^M_{00}+\\lambda^M_{10})(\\lambda^Y_{11}+\\lambda^Y_{10}))\\) \\(X1M1Y1\\) \\(\\lambda^X_1(\\lambda^M_{11}+\\lambda^M_{01})(\\lambda^Y_{11}+\\lambda^Y_{01}))\\) The two observations can be thought of as a multinomial draw from these five event types. Alternatively they can also be thought of as the product of a draw from a strategy in which a set of units is drawn with observations on \\(X,Y\\) only and another set is drawn with observations on \\(X, M,Y\\). In the single multinomial view we have the probability of seeing data with \\(X=Y=0\\) in one case and \\(X=1, M=0, Y=1\\) in another is: \\(2P(X=0, Y=0)P(X=1, M=0, Y=1)\\) In the conditional strategy view we have \\(2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\\) In the two strategy view we have \\(P(X=0, Y=0)P(X=1, M=0, Y=1)\\) which is the same up to a constant. Say rather than conditioning \\(X=Y=1\\) to examine \\(M\\) one of the two cases were chosen at random to observe \\(M\\) and it just so happend to be be a case with \\(X=Y=1\\): type: prob: \\(X0Y0\\) \\(.5\\lambda^X_0(\\lambda^Y_{00}+\\lambda^Y_{01}))\\) \\(X0Y1\\) \\(.5\\lambda^X_0(\\lambda^Y_{11}+\\lambda^Y_{10}))\\) \\(X1Y0\\) \\(.5\\lambda^X_1(\\lambda^Y_{00}+\\lambda^Y_{10}))\\) \\(X1Y1\\) \\(.5\\lambda^X_1(\\lambda^Y_{11}+\\lambda^Y_{01}))\\) \\(X0M0Y0\\) \\(.5\\lambda^X_0(\\lambda^M_{00}+\\lambda^M_{01}))(\\lambda^Y_{00}+\\lambda^Y_{01}))\\) \\(X0M1Y0\\) \\(.5\\lambda^X_0(\\lambda^M_{11}+\\lambda^M_{10}))(\\lambda^Y_{00}+\\lambda^Y_{10}))\\) … \\(X1M0Y1\\) \\(\\lambda^X_1(\\lambda^M_{00}+\\lambda^M_{10})(\\lambda^Y_{11}+\\lambda^Y_{10}))\\) \\(X1M1Y1\\) \\(\\lambda^X_1(\\lambda^M_{11}+\\lambda^M_{01})(\\lambda^Y_{11}+\\lambda^Y_{01}))\\) In the single multinomial view we have the probability of seeing data with \\(X=Y=0\\) in one case and \\(X=1, M=0, Y=1\\) in another is now: \\(2P(X=0, Y=0)P(X=1, M=0, Y=1)\\) In the conditional strategy view we have \\(2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\\) In the two strategy view we have \\(P(X=0, Y=0)P(X=1, M=0, Y=1)\\) which is the same up to a constant. 8.3 Illustration Consider a generalization of the models introduced in Chapter 6 in which a treatment \\(X\\) is a cause of both \\(K\\) and \\(Y\\), and outcome \\(Y\\) is a product of both \\(X\\) and \\(K\\). Though \\(K\\) is both a mediator and a moderator for the effect of \\(X\\). There are now 16 nodal types for \\(Y\\), 4 for \\(K\\) and 2 for \\(X\\), yielding 32 causal types. To allow for the possibility of non-random selection of \\(X\\) we will assume that the assignment probability for \\(X\\) depends on \\(U^Y\\). This is a feature shared also in the baseline model when we specify \\(\\pi\\) as a function of types \\(a\\),\\(b\\),\\(c\\),\\(d\\). Our piors requires specifying: A distribution over the 15-dimensional simplex representing possible values of \\(\\lambda^Y\\)–which in turn determine types \\(u^Y\\). A distribution over the 3-dimensional vector representing possible values of \\(\\lambda^K\\), which in turn determine types \\(u^K\\). The model is restricted in various ways. We assume now confounding in the assignemnt of \\(X\\). Less obviously we implicitly assume that \\(K\\) is independent of \\(\\theta^Y\\) conditional on \\(X\\). With these elements in hand, however, all we need now is to provide a mapping from these fundamental parameters to the parameters used in the baseline model to form the likelihood. The key transformation is the identification of causal types resulting from the 64 combinations of \\(\\lambda^Y\\) and \\(\\lambda^K\\). These are shown below. TABLE TO SHOW CAUSAL TYPES Consider the following matrices of values for \\(u_Y\\) and \\(u_K\\), where \\(\\lambda_{pq}^{rs}\\) is the probability that \\(u^Y = t_{pq}^{rs}\\), meaning that \\(Y\\) would take the value \\(p\\) when \\(X=0, K=0\\), \\(q\\) when \\(X=0, K=1\\), \\(r\\) when \\(X=1, K=0\\), and \\(s\\) when \\(X=1, K=1\\). Similarly \\(\\lambda_{w}^{z}\\) is the probability that \\(u^K\\) takes value \\(t_{w}^{z}\\) meaning that \\(K\\) takes the value \\(w\\) when \\(X=0\\) and \\(z\\) when \\(X=1\\). TABLE TO SHOW CONDITIONAL PROBABILITIES OF K GIVEN X=1 AND TYPE These types are the transformed parameters; the probability of a type is just the sum of the probabilities of the fundamental types that compose it, formed by taking the product of the \\(\\lambda^Y\\) and \\(\\lambda^K\\) values marked in the rows and columns of table . Similarly \\(\\phi_{tx}\\) can be constructed as the probability of observing \\(K\\) conditional on this type (again, sums of products of probabilities associated with cells in table ). For instance, using the row and column indices in exponents (GIVE FULL LABELS) from table : \\[\\phi_{b1}=\\frac{\\lambda_K^2(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^6+\\lambda_Y^8)+\\lambda_K^4(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^{10}+\\lambda_Y^{12})}{ \\lambda_K^1(\\lambda_Y^3+\\lambda_Y^4+\\lambda_Y^7+\\lambda_Y^8)+\\lambda_K^2(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^6+\\lambda_Y^8)+\\lambda_K^3(\\lambda_Y^3+\\lambda_Y^4+\\lambda_Y^11+\\lambda_Y^{12})+\\lambda_K^4(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^{10}+\\lambda_Y^{12})}\\] With these transformed parameters in hand, the likelihood is exactly the same as that specified in the baseline model. 8.4 Illustrated inferences 8.4.1 XY model Consider the simple model in which \\(X\\) causes \\(Y\\) without confounding. Assuming flat priors on types, what inferences do we draw from different sorts of (small) datasets. Do we learn more about effects from two cases that are the same, two cases that differ on X and Y only or two cases that differ on both. The results are given in table 8.3. ## recompiling to avoid crashing R session Table 8.3: Inferences for different data observations in a simple X-&gt;Y model data a b c d ate bd bc 00 0.20 0.30 0.31 0.20 0.09 1.51 0.96 01 0.30 0.20 0.19 0.31 -0.10 0.65 1.04 11 0.20 0.30 0.20 0.31 0.09 0.97 1.52 01, 01 0.34 0.16 0.17 0.34 -0.17 0.49 0.98 11, 11 0.17 0.34 0.17 0.33 0.17 1.03 2.02 01, 11 0.23 0.23 0.16 0.37 0.00 0.62 1.41 10, 11 0.25 0.25 0.25 0.25 0.00 0.98 1.00 00, 11 0.17 0.37 0.23 0.23 0.20 1.58 1.64 11, 11, 11 0.14 0.36 0.14 0.36 0.22 1.01 2.65 We note a number of features: \\(X=1, Y=1\\) data does not discriminate between \\(\\theta^Y_{01}\\) and \\(\\theta^Y_{11}\\) and so while more of this data puts greater weight on both \\(\\lambda^Y_{01}\\) and \\(\\lambda^Y_{11}\\), it does nothing to discriminate between them. Similarly \\(X=0, Y=0\\) data does not discriminate between \\(\\theta^Y_{01}\\) and \\(\\theta^Y_{00}\\) though it puts greater weight (uniformly) on \\(\\lambda^Y_{01}\\) and \\(\\lambda^Y_{00}\\), For this reason, greatest weight is placed on \\(\\theta^Y_{01}\\) when data on both \\(X=Y=0\\) and \\(X=Y=1\\) cases are found. The fractions suggest a common formula: \\[\\lambda^Y|n_{xy} \\sim Dirichlet\\left(1+\\frac{n_{01} + n_{10}}2, 1+\\frac{n_{00} + n_{11}}2, 1+\\frac{n_{00} + n_{10}}2, 1+\\frac{n_{01} + n_{11}}2\\right)\\] Posterior mean on ATE is then \\(\\frac{n_{00} + n_{11} - n_{01} - n_{10}}n\\). data a b c d ate bd bc 111 0.12 0.13 0.32 0.43 0.01 0.31 0.41 111, 111 0.11 0.14 0.28 0.48 0.03 0.29 0.50 000, 111 0.12 0.16 0.36 0.36 0.04 0.45 0.44 111, 111, 111 0.10 0.15 0.25 0.50 0.05 0.29 0.60 8.5 Considerations 8.5.1 The identification problem model &lt;- make_model(&quot;X1 -&gt; M1 -&gt; Y &lt;- M2 &lt;- X2&quot;) # restrict such that *only* M1 OR M2 could cause Y -- can we create a DD test? / achieve identification 8.5.2 Continuous data We can similarly shift from binary to continuous variable values through an expansion of the causal types. Suppose that \\(Y\\) can take on \\(m\\) possible values. With \\(k\\) explanatory variables, each taking on \\(r\\) possible values, we then have \\(m^{r^k}\\) causal types and, correspondingly, very many more elements in \\(\\phi\\). Naturally, in such situations, researchers might want to reduce complexity by placing structure onto the possible patterns of causal effects and clue probabilities, such as assuming a monotonic function linking effect sizes and clue probabilities. 8.5.3 Measurement error We have assumed no measurement error; in applications there could be considerable interest in measurement error. On one hand clue information may contain information about possible mismeasurement on \\(X\\) and \\(Y\\); on the other hand there might interest in whether measured clues adequately capture those features of a causal process that is thought to be measureable. The probability of different types of measurement error can be included among the set of parameters of interest, with likelihood functions adjusted accordingly. Suppose, for instance, that with probability \\(\\epsilon\\) a \\(Y=0\\) case is recorded as a \\(Y=1\\) case (and vice versa). Then the event probability of observing an \\(X=1\\),\\(Y=1\\) case, for example, is \\(\\epsilon \\lambda_a \\pi_a + (1-\\epsilon) \\lambda_b \\pi_b + \\epsilon \\lambda_c \\pi_c + (1-\\epsilon) \\lambda_d \\pi_d\\). %If instead there were measurement error on \\(X\\) but not on \\(Y\\), then the event probability would be: \\(\\epsilon \\lambda_a (1-\\pi_a) + (1-\\epsilon) \\lambda_b \\pi_b + \\epsilon \\lambda_d (1-\\pi_d) + (1-\\epsilon) \\lambda_d \\pi_d\\). Similar expressions can be derived for measurement error on \\(X\\) or \\(K\\). Specifying the problem in this way allows us both to take account of measurement error and learn about it. 8.5.4 Spillovers Spillovers may also be addressed through an appropriate definition of causal types. For example a unit \\(i\\) that is affected either by receiving treatment or via the treatment of a neighbor, \\(j\\), might have potential outcomes \\(Y_i(X_i,X_j)=\\max(X_i,X_j)\\) while another type that is not influenced by neighbor treatment status has \\(Y_i(X_i,X_j)=\\max(X_i)\\). With such a set-up, relevant clue information might discriminate between units affected by spillovers and those unaffected. 8.5.5 Clustering and other violations of independence 8.5.6 Parameteric models 8.6 Conclusion ADD REFERENCE TO TABLE 1 OF FOR MIXED DATA “Ability and Achievement” Otis Duncan References "],
["mixingapp.html", "Chapter 9 Mixed-Method Application: Inequality and Democracy Revisited 9.1 A trained model 9.2 Data 9.3 Inference 9.4 Prior / posterior comparison for multiple estimands 9.5 Discussion", " Chapter 9 Mixed-Method Application: Inequality and Democracy Revisited In chapter 7 we drew inferences from a ‘theory based’ democracy and inequality model on data. Here we train the model on data before making case level inferences, allowing for the possibility of confounding in the assignment of inequality. In this case we update our beliefs over the population parameters and not just over the case level parameters. Thus we simultaneously learn about our thoery and use our theory to learn about cases. The data informed inferences are, on the whole, weaker than the theory based inferences of Chapter 7. 9.1 A trained model We now apply these ideas on mixed method research to our model of democracy and inequality. The key difference to the exercise in in Chapter 7 is that whereas there we took the model as given, and sought to draw inferences to case given the model, the model now becomes an object that we both learn from and learn about. In essence we use the data on many cases to update our beliefs about the general model and then use this \"trained’ model to then make inferences about cases. Along with this change in goals come some changes in the structure of the model: Instead of specifying beliefs about causal types, \\(\\theta\\), we now need to specify a belief over the distribution of causal types, \\(\\lambda\\). Concretely whereas in the simple process tracing model we specified that inequality has a positive effect on mobilization for some share of units, we now specify a distribution over the share of units for which inequality could have a positive effect, ranging anywhere from 0 to 100% As we train the model we allow now for the possibility that there is unobserved confounding – in particular the possibility that inequality is less (or more!) likely in places in which inequality would induce mobilization. We do not impose confounding in one direction of another, but we do allow for the possibility of confounding. As a result we may have correlations in our posteriors over beliefs regarding confounding and beliefs about effects. Model definition in code We can now define the model compactly using gbiqq as follows. model &lt;- make_model(&quot;I -&gt; M -&gt; D &lt;- P; I -&gt; D&quot;, add_priors = FALSE) %&gt;% set_restrictions(c( &quot;(M[I=1] &lt; M[I=0])&quot;, &quot;(D[I=1] &gt; D[I=0]) | (D[M=1] &lt; D[M=0]) | (D[P=1] &lt; D[P=0])&quot;)) %&gt;% set_confound(list(I = &quot;(M[I=1] == 1) &amp; (M[I=0] == 1)&quot;)) %&gt;% set_priors() This model, with confounding, is represented graphically as in Figure ?? 9.2 Data We use the same data as before, based on the analysis in Haggard and Kaufman (2012), but now rather than implementing analysis case by case, the joint distribution of the data will become important for training the model. Here is a snapshot of the data: Case P I M D Afghanistan NA 1 NA 0 Albania 0 0 1 1 Algeria NA 0 NA 0 Angola NA 1 NA 0 Argentina 0 0 1 1 Bangladesh 0 0 0 1 Note that although data is gathered on \\(D\\) and \\(I\\) in all cases, data on mobilization, \\(M\\), and external pressure, \\(P\\), was only gathered for some (in fact, for those cases in which there was democratization). \\(M\\) in particular derives from qualitative research that assessed whether mobilization took place in those cases that democratized. The raw correlations berween variables is as shown in Table ??. Note that some of these correlations are missing because data was only gathered on some variables conditional on the values of others. For those quantities where do see correlations they are not especially strong. There is a weak relation between inequality and democratization – though this is consistent with inequality having different types of effect. the strongest correlation here is between \\(P\\) and \\(M\\), which are assumed to be uncorrelated in the model, though this correlation is also quite weak. ## Warning in cor(data[, -1], use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is ## zero P I M D P 1.000 0.157 -0.177 NA I 0.157 1.000 0.114 -0.154 M -0.177 0.114 1.000 NA D NA -0.154 NA 1.000 9.3 Inference With data and model in hand we can update our model to get posteriors on the distribution of all admissible causal types. In practice this is done by constructing a stan model that maps from a set of parameters to a distribution on causal types which in turn provide a likelihood function for observable data. (Using the gbiqq package the posterior is calculated by gbiqq(model, data)) The parameters param_mat &lt;- get_parameter_matrix(model) dim(param_mat) ## [1] 29 240 9.3.1 Did inequality cause democracy? 9.3.2 Did inequality prevent democracy? We see that inequality appears more likely to prevent democratization than to cause it. We are most confident that inequality played a preventative role in those cases in which there was mobilization but still no democratization. We are most confident that inequality had a positive effect in those cases in which we observe mobilization—but even then the probability that inequality made the difference is small. 9.4 Prior / posterior comparison for multiple estimands Estimands can be calculated for both the prior and posterior distributions. Inequality causes democratization: Table 9.1: Prior Query Subset Using mean sd not named All priors 0.049 0.047 not named D==1 &amp; I==1 priors 0.130 0.123 not named D==1 &amp; I==1 &amp; M ==1 priors 0.158 0.139 not named D==1 &amp; I==1 &amp; M==0 priors 0.000 0.000 Table 9.1: Posterior Query Subset Using mean sd not named All posteriors 0.038 0.028 not named D==1 &amp; I==1 posteriors 0.123 0.089 not named D==1 &amp; I==1 &amp; M ==1 posteriors 0.167 0.117 not named D==1 &amp; I==1 &amp; M==0 posteriors 0.000 0.000 Inequality prevents democratization: Table 9.2: Prior Query Subset Using mean sd not named All priors 0.266 0.086 not named D==0 &amp; I==1 priors 0.466 0.155 not named D==0 &amp; I==1 &amp; M ==1 priors 0.490 0.177 not named D==0 &amp; I==1 &amp; M==0 priors 0.403 0.133 Table 9.2: Posterior Query Subset Using mean sd not named All posteriors 0.237 0.055 not named D==0 &amp; I==1 posteriors 0.353 0.088 not named D==0 &amp; I==1 &amp; M ==1 posteriors 0.410 0.144 not named D==0 &amp; I==1 &amp; M==0 posteriors 0.308 0.077 FLAG: Multiple estimands. 9.5 Discussion References "],
["elements-of-design.html", "Chapter 10 Elements of Design 10.1 Model, inquiry, data strategy, answer strategy 10.2 Evaluating a mixed method design 10.3 Illustration of in code", " Chapter 10 Elements of Design A fully specified causal model includes the information needed to assess the properties of a research design that seeks to learn from or learn about the model. We talk through how to go from defining causal models to “declaring” research designs and use this framework in later chapters to inform decisions about details of design choices. So far we have described a way to think about causal models, a way to specify causal estimands, and a Bayesian approach to inference, given models and estimands. Together with a strategy for data gathering these elements are enough to fully characterize a research design. If in addition we provide criterea for evaluating a design we have enough to be able to simulate the behavior of a research design and assess whether a design is up to the task fo answering the questions we want to answer. Once we have a method to assess the performance of a given design we can can start asking what kind of design is optimal, given some beliefs about the world (see Blair et al. (2019) for more on this general approach to design declaration and diagnosis). In the next chapters we use this approach to assess a set of design choices including choices regarding the clues about which data is sought, the types of cases for which data is sought, and the number of cases for which different types of data is sought. In the remainder of this chapter we discuss a simple evaluative criterion for a design and give examples for design declaration for a simple single case process tracing design and a mixed methods design. 10.1 Model, inquiry, data strategy, answer strategy We use the MIDA approach (model, inquiry, data strategy, answer strategy) approach to declare a simple process tracing design with an arbitrary model. Model. We will define a model as introduced in Chapter 2. We outline approaches for this in section X below. Inquiry As discussed in Chapter 4, an inquiry is a question asked of a model. This is typically a question about the distribution of a variable in some controlled or natural condition, or some summary of such distributions. We refer to the quantity being targeted by a query as the estimand. Data strategy. The data strategy describes how data will be gathered. In typical DeclareDesign applications this includes both randomization and data gathering (sampling) strategies. We focus on data gathering though we highlight that randomization strategies can be implemented via a modification of the confounds allowed by the model. A sampling strategy might indicate a sequence of conditional data gathering schemes, for instance: gather data on \\(X\\) and \\(Y\\) for 100 cases, then gather data on \\(M\\) for all cases in which \\(X=Y\\). Note that in some cases we might want to think of the estimand as being defined after the data strategy. This would be the case for instance if we chose a case and we seek to work out some feature about that case rather than about the population. Answer strategy. The answer strategy combines the observed data with a causal model to generate an updated model from which inferences can be drawn. Importantly, the model used in the answer strategy does not need to be the same model as assumed at the model step since we could imagine analysts coming to the data with quite different models in mind. Of course any model used in the answer strategy should generally involve the same variables as in the model itself. A design is a concatenation of these four steps. The concatenated lets us examine instances of the application of a design. A single instance would involve a single draw of a true parameter vector from the distribution given in the model definition a calculation of the value of an estimand given this true parameter draw the generation of a dataset given the model implied by step 1 and the data strategy the generation of an answer to the inquiry generated from the realized data from 3. and the answer strategy With the observation of multiple instances we get to assess the distribution of our answers — and our uncertainty around these – over repeated draws, and each time we get to see how well the answer we get maps onto the assumed truth in that draw. 10.1.1 Defining a model For this we need to define our model. In doing so we declare the set of variables we are interested in and the relations of independence between them. In defining the variables we generally also define the ranges of the variables—indicating, for example whether they are binary, categorical, or continuous. In defining the edges we identify the set of parents of any node. Which nodes How much detail? Be aware of implications of uneveness in the level of detail Justifying conditional independence claims Justifying restrictions Refer back to our discussion of the inequality and democracy model in section ??. There we discussed a theory that unpacked a higher level model but that did not place any restrictions on functional forms — that is, on causal types. We might imagine such restrictions being justified by theory, however. Drawing on Boix (2003), for instance, we might theorize that inequality can have a negative effect on democratization by giving the elite more to lose from majority rule, making autocrats less willing to hand over power. Inequality’s positive effect, we might further posit, derives from the fact that it gives the poor more to gain from the redistribution that democratization would enable (Acemoglu and Robinson (2005)). However, this positive effect can only unfold to the extent that the masses are able to mobilize, and the capacity to mobilize will hinge on ethnic homogeneity. Ethnic homogeneity thus defines the causal possibilities in regard to \\(I\\)’s effect on \\(D\\). First, homogeneity is necessary for a positive effect of inequality. Second, by enabling mobilization around distributional demands, ethnic homogeneity rules out a net negative effect of inequality (as inequality’s mobilizing effects will balance out elite fears of expropriation). Third, by making mass mobilization easier in general, ethnic homogeneity makes possible mobilization and democratization without inequality. Under ethnic heterogeneity, on the other hand, inequality can have a negative effect, or it can no effect at all with autocracy entrenched. Put differently, under ethnic homogeneity, inequality’s effect can only correspond to a \\(b\\) type or a \\(d\\) type, while under heterogeneity the effect can only be of type \\(a\\) or type \\(c\\). \\(E\\) thus allows us to partition the range of causal possibilities that model (a) had lumped together under \\(\\theta^D\\). Now we can capture this logic with a functional equation in which \\(\\theta^D\\) now takes on just two possible values (0 or 1), rather than four: \\[\\begin{equation} D=IE(1-\\theta^D)+\\theta^DE+(1-E)(1-I)\\theta^D \\end{equation}\\] We can work through the arithmetic to observe \\(E\\)’s causal-partitioning effect. Whether \\(E\\) is 0 or 1 determines whether we are in a world of posiitve effects (\\(b\\) types) and democratization regardless (\\(d\\) types), or a world of negative effects (\\(a\\) types) and autocracy regardless (\\(c\\) types). Note that the righthand side is a sum of three expressions. We can think of \\(E\\) as a “switch” that turns these expressions “on” or “off.” When \\(E=1\\), the third expression goes to 0, leaving only the first two in play “on.” Now, \\(\\theta^D\\) determines whether \\(I\\) has a positive effect (when \\(\\theta^D=0\\)) or no effect with \\(D\\) fixed at \\(1\\) (when \\(\\theta^D=1\\)). Conversely, when \\(E=0\\), the first two expressions both go to \\(0\\), and \\(\\theta^D\\) determines whether \\(I\\) will have a negative effect (when \\(\\theta^D=1\\)) or no effect with \\(D\\) stuck at 0 (when \\(\\theta^D=0\\)). A declaration of structures of unobserved confounding. Priors. Beliefs about the distribution of shocks. When defined as part of the model we think of these priors as being the priors from the vantage point of someone assessing a design and they need not be the same as the priors used in the analysis. REFELCTING THE LITERATURE GATHERING DISCPILINARY PRIORS EMPIRICAL APPROACH 10.1.2 Choosing estimands Estimands are causal statements Estimands under controlled conditions, estimands under observational conditions Estimands that depend on the cases Sample and population level estimands / post stratification 10.1.3 Selecting a data strategy Data strategies can involve: how many cases to examine which cases to examine which clues to examine in which cases Conditional strategies, dynamic strategies 10.1.4 Answer strategy We assume throughout that researchers draw inferences by updating on causal models as described in previous chapters. 10.2 Evaluating a mixed method design The observation that theories vary in their precision points to a method for describing the learning that is attributable to a lower-level theory relative to a higher level theory. When a lower-level theory represents a disaggregation, the lower-level theory identifies a set of potentially observable variables that are not listed by the the higher-level theory. This allows one to assess the gains in precision (for some collection of unobserved variables) that can arise from learning the values of additional observables in the lower-level theory. Suppose that the contribution of a lower-level theory is to allow for inferences from new data \\(K\\) about some set of query variables \\(Q\\), after we have already observed variables \\(W\\) from the higher-level model. Then we can use the expected squared error from the mean posterior estimate as a measure of precision for collection \\(Q\\), as a measure of loss: \\[E_{k, q} \\left(\\left( \\int q&#39; P(q&#39; | k, w)dq&#39; - q\\right)^2\\right)\\] where the expectation is taken over the joint distribution of \\(K\\) and \\(Q\\), given \\(W\\). This is an expected loss—or the Bayes risk. The inner term \\(P(q&#39;|k, w)\\) is the posterior distribution on \\(q&#39;\\) given observation of \\(k\\) and \\(w\\). Another way to think of the gains is as the expected reduction in the variance of the Bayesian posterior: how certain do you expect you will be after you make use of this new information? In fact these two quantities are equivalent (see for example Scharf (1991)). Moreover, it is easy to see that whenever inferences are sensitive to \\(K\\), the expected variance of the posterior will be lower than the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of \\(K\\), given what is already known from observation of \\(W\\).60 \\[Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))\\] The contribution of a theory can then be defined as the mean reduction in Bayes risk: \\[\\text{Gains from theory} = 1- \\frac{E_{K|W}(Var(Q|K,W))}{Var(Q|W)}\\] This is a kind of \\(R^2\\) measure (see also Gelman and Pardoe (2006)). 10.2.1 Other loss functions Other loss functions could be used, including functions that take account of the costs of collecting additional data,61 or to the risks associated with false diagnoses.62 For illustration say that it is known that \\(X=1, Y=1\\) and that, given this information (playing the role of \\(W\\)), the posterior probability that a unit is of type \\(b\\) (and not type \\(d\\)) is \\(p\\). Say then that a theory specifies that \\(K\\) will take a value 1 with probability \\(\\phi_j\\) if the unit is of type \\(j\\). Then what is the value added of this theory? Define \\(Q\\) here as the query regarding whether the unit is a \\(b\\) type. Then the prior variance, \\(Var(Q|W)\\), is simply \\(p(1-p)^2 +(1-p)p^2 = p(1-p)\\). To calculate \\(E_{K|W}(Var(Q|K,W))\\), note that the posterior if \\(K\\) is observed is \\(\\frac{\\phi_bp}{\\phi_bp+\\phi_d(1-p)}\\). Let us call this \\(\\hat{q}_K\\), and the belief when \\(K\\) is not observed \\(\\hat{q}_{\\overline{K}}\\). In that case the expected error is: \\[\\text{Expected Error} = p\\phi_b\\left(1-\\hat{q}_K\\right)^2+(1-p)\\phi_d\\hat{q}_K^2+p(1-\\phi_b)\\left(1-\\hat{q}_{\\overline{K}}\\right)^2+(1-p)(1-\\phi_d)\\hat{q}_{\\overline{K}}^2\\] where the four terms are the errors when \\(K\\) is seen for a \\(b\\) type, when \\(K\\) is seen for a \\(d\\) type, when \\(K\\) is not seen for a \\(b\\) type, and when \\(K\\) is not see for a \\(d\\) type. Defining \\(\\rho_K = (p\\phi_b+(1-p)\\phi_d)\\) as the probability of observing \\(K\\) given the prior, we can write the posterior variance as: \\[\\text{Expected Posterior Variance} = \\rho_K\\hat{q}_K(1-\\hat{q}_K)+(1-\\rho_K)\\hat{q}_{\\overline{K}}(1-\\hat{q}_{\\overline{K}})\\] With a little manipulation, both of these expressions simplify to: \\[\\text{Expected Posterior Variance} =p(1-p)\\left(\\frac{\\phi_b\\phi_d}{\\phi_bp+\\phi_d(1-p)} + \\frac{(1-\\phi_b)(1-\\phi_d)}{(1-\\phi_b)p+(1-\\phi_d)(1-p)}\\right)\\] The gains are then: \\[\\text{Gains} =1- \\frac{\\phi_b\\phi_d}{\\phi_bp+\\phi_d(1-p)} - \\frac{(1-\\phi_b)(1-\\phi_d)}{(1-\\phi_b)p+(1-\\phi_d)(1-p)}\\] Let’s consider the same question using a particular model and calculate these quantities given this model. model &lt;- make_model(&quot;X -&gt; Y; K -&gt; Y&quot;) %&gt;% set_parameters(c( .5, .5, .5, .5, 0,.5,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,.5)) The probability that \\(K=1\\) given \\(X=Y=1\\) is: prob_K &lt;- query_distribution(model, query = &quot;K[Y=1]==1&quot;, using = &quot;parameters&quot;, subset = &quot;X==1 &amp; Y==1&quot;) The inferences conditional on the observation of \\(K\\) are: inferences &lt;- gbiqq::query_model( model, queries = &quot;Y[X=1]&gt;Y[X=0]&quot;, using = &quot;parameters&quot;, subsets = list(&quot;X==1 &amp; Y==1&quot;, &quot;X==1 &amp; Y==1 &amp; K==1&quot;, &quot;X==1 &amp; Y==1 &amp; K==0&quot;)) The expected error is then reduced from 0 to 0 10.2.2 Other measures of a gain from a theory Other natural measures of gains from theory might include the simple correlation between \\(K\\) and \\(Q\\), or entropy-based measures (see Zhang and Srihari (2003) for many more possibilities). For this problem the correlation is given by (see appendix): \\[\\rho_{KQ} = \\frac{(\\phi_b+\\phi_d)(1-2p)(p(1-p))^{.5}}{ (p\\phi_b+(1-p)\\phi_d)(1-(p\\phi_b+(1-p)\\phi_d)))^{.5}}\\] One might also use a measure of “mutual information” from information theory: \\[I(Q,K) = \\sum_q \\sum_k P(q,k)\\log\\left(\\frac{P(q,k)}{P(q)P(k)}\\right)\\] To express this mutual information as a share of variation explained, we could divide \\(I(Q,K)\\) by the entropy of \\(Q\\), \\(H(Q)\\) where \\(H(Q) = -\\sum_qP(q)\\log(P(q))\\). The resulting ratio can be interpreted as 1 minus the ratio of the entropy of \\(Q\\) conditional (on \\(K\\)) to the unconditional entropy of \\(Q\\). For this example, Figure shows gains as a function of \\(\\phi_b\\) given a fixed value of \\(\\phi_d\\). The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely. Figure 10.1: The solid line shows gains in precision (reduced posterior variance) for different values of \\(\\phi_b\\) given \\(\\phi_d=0.25\\) and \\(p=.5\\) for the example given in the text. Additional measures of probative value are also provided including \\(|\\phi_b - \\phi_d|\\), the correlation of \\(K\\) and \\(Q\\), and the reduction in entropy in \\(Q\\) due to mutual information in \\(Q\\) and \\(K\\). 10.3 Illustration of in code In the gbiqq package there is a single function that lets you declare a full design in one go by letting you supply arguments to declare a model, an inquiry, a data strategy, and an answer strategy # A single function can be used to declare a model, an inquiry, a data strategy, and an answer strategy my_design &lt;- gbiqq_designer( model = make_model(&quot;X -&gt; M -&gt; Y&quot;), inquiry = list(ATE = &quot;Y[X=1] - Y[X=0]&quot;), data_strat = list(n_obs = 5, vars = list(c(&quot;X&quot;, &quot;Y&quot;), &quot;M&quot;), probs = list(1, .5), subsets = list(NULL, &quot;X==1 &amp; Y==0&quot;), n = NULL), answer_strat = NULL ) With this model in hand you can use it to draw likely data, run analyses, an drun diagnostics. sample_data &lt;- draw_data(my_design) sample_estimands &lt;- draw_estimands(my_design) sample_estimates &lt;- get_estimates(my_design, df) References "],
["clue-selection-as-a-decision-problem.html", "Chapter 11 Clue Selection as a Decision Problem 11.1 A strategic approach 11.2 Clue selection for the running example 11.3 Clue selection for the Democracy model 11.4 Conclusion", " Chapter 11 Clue Selection as a Decision Problem We draw out the implications of the causal model approach for clue selection strategies. We introduce a tool for generating an optimal decision tree for clue selection given. Consider now the problem of determining what qualitative data to gather on a case. Evidently it makes sense to gather information on clues that have large probative value, but whether or not clues have probative value can depend on what clues have already been collected: Finding out that the Butler had no motive may be informative for the claim that he is innocent, but it may not be useful if you already know he had no opportunity. In our running example, we can see many situations where researchers have a choice of observations that could be informative, and situations in which the informativeness of an observation can depend on what is already known. In Figure , we showed how one can use the structural equations to provide a set of conditional causal graphs that let one see easily what caused what at different values of the root nodes \\(S\\) and \\(X\\). Each of these plots graphs a particular context. We can thus readily see which collection of root nodes constitutes a given query, or estimand. Turning things around, we can see, given a query, which nodes are informative of the probability that the query is true.63 For example, suppose one can see that \\(X=0\\) and \\(Y=0\\) but does not know the causal effect of \\(X\\) on \\(Y\\). This is equivalent to saying that we know that we are in either panel \\(A\\) or \\(B\\) but we do not know which one. Defining the query in terms of root nodes, the question becomes \\(S \\stackrel{?}{=} 1\\), or \\(P(S=1|X=0,Y=0)\\); the difference between the contexts in the two panels is that \\(S=0\\) when, and only when, \\(X=0\\) causes \\(Y=0\\) . Given the structural equation for \\(S\\), \\(P(S|X=0,Y=0) = P(S|X=0)\\), and given independence of \\(X\\) and \\(S\\), \\(P(S=1|X=0)= \\pi^S\\). Figuring out \\(S\\) fully answers the query.64 We can also see instances in this example of how existing data can make clues uninformative. Say one wanted to know if \\(X\\) causes \\(C\\) in a case. As we can see from inspection of the panels, this query is equivalent to asking whether \\(S=1\\) (as \\(X\\) causes \\(C\\) only in those two panels (\\(B\\) and \\(D\\)) where \\(S=1\\). Data on \\(R\\) is unconditionally informative about this query as \\(R\\) is not \\(d-\\)separated from \\(S\\). For example, \\(R=1\\) implies \\(S=0\\). However, if \\(C\\) and \\(X\\) are already known, then \\(R\\) is no longer informative because \\(C\\) and \\(X\\) together d-separate \\(R\\) from \\(S\\).65 The running example also lets us demonstrate how informative clues can be found in many different places in a graph. Informative spouses Spouses—parents of the same child—can inform on one another. As we have seen in other examples, when an outcome has multiple causes, knowing the value of one of those causes helps assess the effect(s) of the other(s). For example, here, \\(S\\) and \\(X\\) are both parents of \\(C\\); \\(S\\) is thus informative for assessing whether \\(X\\) causes \\(C\\). Indeed this query, written in terms of roots, is simply \\(P(S)\\): \\(X\\) causes \\(C\\) if and only if \\(S=1\\). Likewise, \\(S\\) causes \\(C\\) (negatively) if and only if \\(X=1\\). Pre-treatment clues. Did the absence of media reports on corruption (\\(R=0\\)) cause government survival (\\(Y=0\\))? Look to the pre-treatment clue, \\(X\\): \\(X=0\\) is a smoking gun establishing that the absence of a report produced government survival. Or, substantively, if there were a free press, then a missing report would never be a cause of survival since it would occur only in the absence of corruption, which would itself be sufficient for survival. More broadly, this example illustrates how knowledge of selection into treatment can be informative about treatment effects. Post-outcome clues. Suppose we observe the presence of a free press (\\(X=1\\)) and want to know if it caused a lack of corruption (\\(C=0\\)), but cannot observe the level of corruption directly. Observing \\(Y\\)—which occurs after the outcome—is informative here: if \\(X=1\\), then \\(X\\) causes \\(C\\) (negatively) if and only if \\(Y=0\\). When an outcome is not observed, a consequence of that outcome can be informative about its value and, thus, about the effect of an observed suspected cause. Mediators as clues: We see a politically sensitive government (\\(S=1\\)) and its survival (\\(S=0\\)). Did the government survive because of its sensitivity to public opinion? Here, the mediation clue \\(C\\) is helpful: a lack of corruption, \\(C=0\\), is evidence of \\(S\\)’s negative effect on \\(Y\\). –&gt; And of course, different clues can be informative in different ways for different types of estimand. Needed then is a systematic way for identifying what clues to look for, and perhaps, in what order to look for them. 11.1 A strategic approach The representation of inference problems as one of querying a Bayesian model points to a relatively simple method for answering this question, at least for small problems. Consider first a situation where one has access to data \\(W\\) and wants to know the expected probative value of all possible collections of data one could gather. This can be done as follows: Define the model. Define a query on the model. Define a data strategy — for instance: figure out the value of \\(C\\). Given existing data figure out the probability of different realizations of the data and for each possible realization calculate the posterior variance. Then calculate the expected posterior variance by taking an average of these variances with weights given by the probability of observing the clue pattern in question. Repeat steps 3-4 for different data strategies. This procedure then returns expected posterior variance associated with different data strategies. A still more sophisticated strategy would determine which clues to search for later given findings from clues that are sought first. This reflects the possibility that a given clues \\(K_2\\) may be informative if another clue \\(K_1\\) turns up positive but not if it comes out negative. We provide some tools for both of these approaches and illustrate them below for hte running example and the democracy application. 11.2 Clue selection for the running example Lets return to the running example and assess the informativeness of different clue strategies. This model is formally defined as follows: model &lt;- make_model(&quot;S -&gt; C -&gt; Y &lt;- R &lt;- X; X -&gt; C -&gt; R&quot;) %&gt;% set_restrictions(labels = list(C = &quot;C1110&quot;, R = &quot;R0001&quot;, Y = &quot;Y0001&quot;), keep = TRUE) %&gt;% set_priors() Using this model we can ask how likely different data realizations are and what we would infer from each possible data realization, given existing data. We illustrate assuming we already know that \\(X=0\\) and \\(Y=0\\). Application of the function conditional_inferences produces a matrix with the results. We reproduce these as Table 11.1. Table 11.1: Inferences given different data patterns. S X C R Y posterior prob NA 0 NA NA 0 0.5 0.50 0 0 NA NA 0 1.0 0.25 1 0 NA NA 0 0.0 0.25 NA 0 1 NA 0 0.5 0.50 0 0 1 NA 0 1.0 0.25 1 0 1 NA 0 0.0 0.25 NA 0 NA 0 0 0.5 0.50 0 0 NA 0 0 1.0 0.25 1 0 NA 0 0 0.0 0.25 NA 0 1 0 0 0.5 0.50 0 0 1 0 0 1.0 0.25 1 0 1 0 0 0.0 0.25 Each inference has an associated posterior variance and so it is easy to assess the expected reduction in variance from seeking any kind of clue. See Table ??. Strategy Given Prior belief Prior Uncertainty Posterior Uncertainty S X==0 &amp; Y==0 0.5 0.25 0 C X==0 &amp; Y==0 0.5 0.25 0.25 R X==0 &amp; Y==0 0.5 0.25 0.25 C, R X==0 &amp; Y==0 0.5 0.25 0.25 C, S X==0 &amp; Y==0 0.5 0.25 0 S, R X==0 &amp; Y==0 0.5 0.25 0 C, S, R X==0 &amp; Y==0 0.5 0.25 0 The implication of the analysis is that if we know \\(X=0\\) and \\(Y=0\\) and we are interested in finding out whether\\(X=0\\) because \\(Y\\) is 0 we should look for evidence on \\(S\\). Given this simple model, knowledge of \\(S\\) is enough to answer the question at hand and no othe information is useful at all. 11.2.1 Dynamic Strategies The clue collection strategies described above assume that researchers identify the full set of clues to be gathered in advance and do not alter their in Given \\(n\\) nodes, a data collection strategy will be of the form: \\[\\sigma = \\{K_1, (K_2|K_1 = 1), (K_2|K_1 = 0), (K_3|K_1=1, K_2 =0)\\dots\\}\\] where each \\(K_j\\) is en element of the nodes on the graph, or is the empty set. Each of these strategies has an associated expected reduction in variance as well as an associated expected cost. Such a strategy vector specifies the first clue, and then subsequent clues condition on what was found from previous searches. To illustrate with the running example we imagine a situation in which it is known that \\(Y=1\\) and we are interested in whether \\(Y=0\\) because of \\(S\\) (though we don’t know at the outset what the value of \\(S\\) is). We consider strategies in which we first seek information on one node and then, conditional on what we find, we look for data on one other node (or not). With five nodes, one already known, there are \\(4 \\times 4^2\\) strategies of this form. Say that the government was not replaced, we want to know is this because the government was sophisticated. If we learn that the government was not sophisticated, then this answers the question in the negative. If we learn that the government was sophisticated then we can infer that this was the cause if we learn that there was a free press (or if we learn that there was no corruption). For each strategy we can then assess the expected variance reduction; in addition, if collecting different clues comes at different costs—but collection depends on past findings—then we can also calculated the expected costs of each strategy. Strategy Step 1 Step 2 if 0 Step 2 if 1 Expected variance Expected Cost 1 S None None 0.167 1 2 S X X 0 2.5 3 S None X 0 2 Figure ?? plots a collection of strategies based on two criteria—the variance reduction and the expected number of clues sought, which could be an indicator for cost. One can see a frontier of optimal strategies, depending on how these two desiderata trade-off against each other. FOr the figure we imagined that \\(X\\) is the most costly to collect, followed by \\(C\\), then \\(S\\), then \\(Y\\), then \\(R\\). The cheapest strategy among those that minimize variance involves gathering \\(C\\) only. The lowest variance strategy that minimizes costs gathering \\(Y\\) only. 11.3 Clue selection for the Democracy model With a model in hand we are also in a position to assess what we could learn from different data stratgies and what we would infer upon discovery of different data. Table 11.2: Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization. I P M D posterior prob 1 NA NA 1 0.128 0.196 1 0 NA 1 0.231 0.054 1 1 NA 1 0.088 0.142 1 NA 0 1 0.000 0.029 1 0 0 1 0.000 0.004 1 1 0 1 0.000 0.025 1 NA 1 1 0.150 0.167 1 0 1 1 0.250 0.050 1 1 1 1 0.107 0.117 We show in Table how uncertainty is likely to be reduced with different research designs. We show these reductions here for the two kinds of cases in which democratization does occur. The first row displays the variance on our posterior belief about the effect if \\(I\\) on \\(D\\) before we observe anything at all. The second row shows what happens to that uncertainty when we observe just cause and outcome, \\(I\\) and \\(D\\). The next four rows show the results for four possible choices in regard to process tracing: looking for neither \\(M\\) nor \\(P\\) (which is identical to doing no process tracing at all); looking for \\(P\\); looking for \\(M\\); and looking for both. The clearest message here is that, if we had to choose between clues, we should observe \\(P\\): given our model (including our priors on the types), we reduce our uncertainty more by learning about an alternative cause than by learning about a mediator. We also see that the mediator is much more informative when the causal effect we are looking for is one that could have operated via the mediator, as compared to when the mediator is informative only as a moderator of the cause’s direct effects. Table 11.3: Variances and expected variances given different clue seeking stratgies for cases in which we have observed inequality and democratization. given prior_estimand prior_var E_post_var I==1 &amp; D==1 0.128 0.112 0.112 I==1 &amp; D==1 0.128 0.111 0.107 I==1 &amp; D==1 0.128 0.111 0.109 I==1 &amp; D==1 0.128 0.111 0.105 Table 11.4: Variances and expected variances given different clue seeking stratgies for cases in which we have observed low inequality and democratization. strategy given prior_estimand prior_var E_post_var None I==0 &amp; D==1 0.438 0.246 0.246 P I==0 &amp; D==1 0.438 0.246 0.229 M I==0 &amp; D==1 0.438 0.246 0.245 P and M I==0 &amp; D==1 0.438 0.246 0.225 To come: applied case-level analyses involving causal pathways, actual causes, and notable causes. FLAG: DISTINGUISH ROWS IN THIS TABLE FLAG: ADD DYNAMIC 11.4 Conclusion Explicit statement of a causal model—including prior beliefs over roots—allows one to assess what will be inferred from all possible observations. This opens the way for simple strategies for assessing what data is most valuable, and in what order it should be gathered. We are conscious that here we are pushing the basic logic to the limits. In practice researchers will often find it difficult to describe a model in advance and to place beliefs on nodes. Moreover the collection of new data could easily give rise to possibilities and logics that were not previously contemplated. Nothing here seeks to deny these facts; the claim here is a simpler one: insofar as one can specify a model before engaging in data gathering, the model provides a powerful tool to assess what data is most useful to gather. With larger graphs, continuous variables, and more stochastic components, it may not be feasible to graph every possible context; but the strategy for inference remains the same.↩ Graphically what is important is that \\(S\\) is informative not because it is \\(d-\\)connected with \\(Y\\), but because it is \\(d-\\)connected to the query variable—here, simply, to itself.↩ We can come to the same conclusion by reasoning with the graphs: if \\(X=0\\) and \\(C=1\\), we know we are in subfigure \\(A\\) or \\(B\\), and \\(X\\) causes \\(C\\) only in panel \\(B\\). However, \\(R\\) is of no help to us in distinguishing between the two contexts as it takes the same value in both graphs.↩ "],
["wide.html", "Chapter 12 Going wide and going deep 12.1 Intuitions: Does a sufficiently large \\(N\\) always trump \\(K\\)? 12.2 Evaluating strategies 12.3 Varieties of mixing 12.4 Probative value of clues 12.5 Effect Heterogeneity 12.6 Uncertainty Regarding Assignment Processes 12.7 Uncertainty regarding the probative value of clues", " Chapter 12 Going wide and going deep Researchers often need to choose between collecting data on more cases or collecting more data within cases. We discuss the tradeoffs and communicate an intuition that clue data, even on a small number of cases, can be informative even when there is \\(X, Y\\) data on a very large number of cases, but only if it provides information that cannot be gathered from \\(X,Y\\) data, such as selection into treatment. Simulations suggest that going deep is especially valuable for observational research, situations with homogeneous treatment effects, and, of course, when there is strong probative value. How does this approach guide researchers in making choices about research designs? We address this question with a focus on characterizing the kind of learning that emerges from different combinations of investment in the collection of correlational as compared with process-tracing data under different research conditions. We report the results here of simulation-based experiments designed to tell us under what research conditions different mixes of methods can be expected to yield more accurate inferences. We also discuss, at a high level, the implications of the framework for strategies of qualitative case-selection. 12.1 Intuitions: Does a sufficiently large \\(N\\) always trump \\(K\\)? We begin by considering the learning that occurs upon observing outcomes from varying numbers of cases given different \\(XY\\) data ranging from small to quite large. The goal here is to build up intuitions on how beliefs change given different observations and hw this affects posterior variance. We address the question is a very controlled setting in which a researcher is confronted with balanced \\(X,Y\\) data that exhibits no correlation the researcher can seek a doubly decisive clue on cases in the \\(X=Y=1\\) cell though not known in advance, it turns out that each time the researcher finds evidence suggesting that the case in question is a \\(b\\) type the selection probabilities are either unknown or known with near certainty In this case, we can expect that seeing evidence of \\(b\\) types will shift the researcher to increase her beliefs on the average causal effect. But how strong will these shifts be and how does this depend on the amount of XY data available? Does the signal from the \\(XY\\) data drown out any signal from the \\(K\\) data? Intuitions to answer these questions can be gathered from the simulations reported in Figure . For these simulations we varied the size of the \\(XY\\) data from 5 observations in each cell to 5000. The key features of the simulations are: When assignment propensities are unknown—as for example with observational data—the clue information shifts beliefs independent of how many \\(XY\\) cases there are. The key insight is that the clue information provides inforation on assignment propensities which are informative about the share of each type in each cell and these shares determine treatment effects no matter how large or small the cells are. When assignment propensities are known with large data there is a lot of learning over the distribution of types in a population (at least up to differences in types rather than the distribution of fundamental types). Clue information shifts beliefs about the types of the particular cases for which clue data is gathered but has almost no effect on estimates of the population estimand. Not visible from the figure however: in the case with large \\(N\\) and known propensities, observation on many \\(b\\) types in the \\(X=Y=1\\) cell, while not changing estimates of average treatment effects (\\(\\lambda_b- \\lambda_s\\)) does affect beliefs on heterogneity, because the data is more consistent with a world with many \\(a\\)s and \\(b\\)s than one with many \\(c\\)s and \\(d\\)s. For example if there were 10,000 data points in each \\(X,Y\\) and clue information on 20 cases in the \\(X=Y=1\\) cell suggest that these are all \\(b\\) types, then the conclusion would be that 95% of the cases are \\(a\\) and \\(b\\) types, in equal proportion. model &lt;- make_model(&quot;X -&gt; Y &lt;- K&quot;) %&gt;% set_restrictions(&quot;(Y[X=0, K=1]==1) | (Y[X=0, K=0]==0)&quot;) %&gt;% set_parameters(c(0.001, .999, .5, .5, .25, .25, .25, .25)) Query Subset Using mean sd ATE All parameters 0.499 NA ATE K==1 parameters 0.500 NA ATE K==0 parameters -0.500 NA ATE All priors -0.001 0.342 ATE K==1 priors 0.505 0.223 ATE K==0 priors -0.501 0.223 N=8 N=40 N=80 Clues on 4 cases 0.377 0.454 0.477 Clues on 8 cases 0.399 0.461 0.479 model_confound &lt;- make_model(&quot;X -&gt; Y &lt;- K&quot;) %&gt;% set_restrictions(&quot;(Y[X=0, K=1]==1) | (Y[X=0, K=0]==0)&quot;) %&gt;% set_parameters(c(0.001, .999, .5, .5, .25, .25, .25, .25)) %&gt;% set_confound(list(X = &quot;(Y[X=1]==1)&quot;)) draw_parameters(model_confound) ## X-1.X0 X-1.X1 K.K0 K.K1 X.X0 X.X1 ## 0.735537 0.264463 0.696762 0.303238 0.361783 0.638217 ## Y.Y1000 Y.Y1010 Y.Y1001 Y.Y1011 ## 0.525786 0.444407 0.027870 0.001937 kable( query_model(model_confound, queries = list(ATE = &quot;Y[X=1] - Y[X=0]&quot;), using = rep(c(&quot;parameters&quot;, &quot;priors&quot;), each = 3), subsets = list(TRUE, &quot;K==1&quot;, &quot;K==0&quot;))) Query Subset Using mean sd ATE All parameters 0.499 NA ATE K==1 parameters 0.500 NA ATE K==0 parameters -0.500 NA ATE All priors -0.007 0.346 ATE K==1 priors 0.499 0.222 ATE K==0 priors -0.493 0.222 x &lt;- (get_types(model_confound, query = &quot;Y[X=1] - Y[X=0]&quot;, join_by = &quot;|&quot;)$types) N=8 N=40 N=80 Clues on 4 cases 0.372 0.451 0.468 Clues on 8 cases 0.403 0.457 0.474 12.2 Evaluating strategies As a metric of the returns from different research strategies we calculate the expected inaccuracy in the estimation of the average treatment effect, as given in equation . \\[\\begin{equation} \\mathcal{L}=\\mathbb{E}_\\theta(\\mathbb{E}_{\\mathcal{D}|\\theta}(\\tau(\\theta)-\\hat{\\tau}(\\mathcal{D}))^2) \\label{Loss} \\end{equation}\\] where \\(\\tau(\\theta)\\) is the value of \\(\\lambda_b-\\lambda_a\\) (the average treatment effect) given \\(\\theta\\), and \\(\\hat{\\tau}(\\mathcal{D})\\) is the estimate of this treatment effect (the mean posterior value) that is generated following some realization of data \\(\\mathcal{D}\\). Thus, if some \\(\\theta\\) characterized the true state of the world, then \\(\\mathbb{E}_{\\mathcal{D}|\\theta}(\\tau^\\theta-\\hat{\\tau})^2\\) is the expected error in estimation of the causal effect given different realizations of the data, \\(\\mathcal{D}\\), that could obtain in this state of the world. \\(\\mathcal{L}\\) is then the expected value of these errors given prior beliefs over possible values of \\(\\theta\\). Note that, while we focus on errors on estimated average causal effects, similar exercises could assess how cross- and within-case observations distinctively contribute to other estimands | including the causal explanations for individual cases and the validity of causal theories | as well as to learning about inferential assumptions themselves (assignment and clue probabilities). For all simulations, prior distributions are drawn with parameters as described in the Supplementary Materials (, Table ??). Priors on the type distribution are drawn from a Dirichlet distribution; priors for each of the \\(\\pi\\) and \\(\\phi\\) values are drawn independently from Beta distributions. We note that, while by construction priors on each parameter are independent, this will not generally be the case for posterior distributions. In most cases we simulate the prior distribution using 5200 draws of each parameter. For most experiments we then systematically vary the prior distribution for one parameter of the research situation between two extreme positions. We then calculate the expected posterior from each possible data realization and, in turn, the expected loss in estimates of treatment effects for a range of levels of investment in qualitative and quantitative evidence. A few further features of the experiments below are worth noting. First, our illustrations focus on learning about population-level causal effects; however, the model can yield results about the benefits of alternative research designs for estimating a wide range of other quantities of interest, such as case-specific causal explanations or clue probabilities. Second, while we focus on the search for a single clue in each case, the analysis can be extended to the case of an arbitrarily large set of clues. Third, in many of these experiments, the probative values are set at doubly decisive levels for all \\(\\phi\\) parameters, and thus focus on the very optimistic case of maximally informative process tracing. Fourth, we illustrate tradeoffs at low levels of \\(n\\), but the model can be employed to make choices for arbitrarily large numbers of cases. Finally, we note that some results may be sensitive to the choice of priors. The results below should thus be understood as an illustration of the utility of the BIQQ framework for guiding research choices, rather than as a set of more general prescriptive design rules. 12.3 Varieties of mixing What are the marginal gains from additional pieces of correlational and process-tracing evidence for the accuracy of causal estimates? Figure displays results, plotting the errors associated with different mixes of correlational and process data. Qualitative and quantitative data can act as partial substitutes for assessing causal effects. The relative marginal gains from going wider and going deeper vary with the study design. Optimal strategies might involve going deep in a subsample of cases only. 12.4 Probative value of clues 12.5 Effect Heterogeneity 12.6 Uncertainty Regarding Assignment Processes 12.7 Uncertainty regarding the probative value of clues "],
["caseselection.html", "Chapter 13 Case selection as a Decision Problem 13.1 Explorations 13.2 Diagnosing case-selection strategies: procedure 13.3 Evaluating types of strategies 13.4 Different models, different strategies 13.5 Different queries, different strategies 13.6 In code 13.7 Compare multiple data strategies 13.8 Experiments 13.9 Chapter Appendix: Accounting for case selection", " Chapter 13 Case selection as a Decision Problem With a causal model in hand, together with priors over parameters, you can assess in advance what conclusions you will draw from different observations and assess what kinds of observations are most worth seeking. We draw out the implicaitons of this idea for case selection. A critical decision for scholars employing mixed methods is to determine which cases are most valuable for within-case analysis. A host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of \\(X\\), \\(Y\\) data. Perhaps the most common strategy is to select cases in which \\(X=1\\) and \\(Y=1\\) and look to see whether in fact \\(X\\) caused \\(Y\\) in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases “on the regression line” or, for some purposes, cases “off the regression line” (e.g., Lieberman (2005)). Some scholars suggest ensuring variation in \\(X\\) (most prominently, King, Keohane, and Verba (1994)), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on \\(X\\) or \\(Y\\) (Fearon and Laitin (2008), Herron and Quinn (2016)). Which cases you should choose will likely depend on the purposes to which you want to put them. A matching strategy for instance—selecting cases that are comparable on many features but that differ on \\(X\\)—replicates at a small scale the kind of inference done by matching estimators with large-\\(n\\) data. The strategy emphasize the inferences to be made from \\(X,Y\\) variation rather than inferences drawn specifically from within case information beyond what is available in the measurement of \\(X\\) and \\(Y\\). (Citations needed.) Other treatments seek to use qualitative information to check assumptions made in \\(X, Y\\) analysis: for example, is the measurement of \\(X\\) and \\(Y\\) reliable in critical cases? (Citations needed) For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the \\(X,Y\\) inferences: for example influential cases that have unusually extreme values on \\(X\\) and \\(Y\\).^[Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.] Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims. A third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman’s “model-building” mode of “nested analysis” (Lieberman (2005)). Here it may be cases off the regression line that are of interest. Weller and Barnes (CITE article) on case selection focus on (a) X/Y relations and (b) whether the cases are useful for hypothesis generation. In what follows, we focus on a simpler goal: given existing \\(X, Y\\) data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of \\(X\\) on \\(Y\\)? The basic insight of this chapter is simple enough: the optimal strategy for case selection for a model-based analysis can be determined by the model and the query, just as we saw for the optimal clue-selection strategy in Chapter ??. Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as: go where the probative value is, and sample from \\(X\\) and \\(Y\\) values in proportion to their occurrence in the population, invest in collections of cases that provide complementary learning. Beyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with \\(X, Y, K\\) data. For the general intuition, recall that the probative value of a process-tracing test hinges on the difference in clue likelihoods associated with the alternative hypotheses in play for a given case. Recall that for different values of \\(X\\) and \\(Y\\) cell, we want to use process tracing to help us distinguish between two specific types that are consistent with the \\(X, Y\\) pattern. Which types are in question varies across \\(X,Y\\) combinations. Table illustrates. Thus, in the \\(X=0, Y-0\\) cell, what would be most useful is a clue that has high probative value in distinguishing between an untreated (\\(X=0\\)) \\(b\\) type and and an untreated \\(c\\) type. For a case in the \\(X=1, Y=0\\) cell, on the other hand, what matters is how well the clue can discriminate between treated (\\(X=1\\)) \\(a\\) and \\(d\\) types. In our notation, it is the difference in \\(phi_{jx}\\) values for that indicates these cell-specific degrees of leverage. To illustrate, consider a situation in which for a given clue we have \\(\\phi_{b1}\\)=0.566; \\(\\phi_{d1}\\)=0.5;67 \\(\\phi_{b0}\\)=0.5^68; and \\(\\phi_{c0}\\)=0.169. In this situation, searching for the clue in \\(X=Y=1\\) cases will yield no leverage since the clue does not discriminate between the two types (\\(b\\) and \\(d\\)) that need to be distinguished given \\(X=Y=1\\). Here there is no additional learning about \\(\\lambda_b\\) that can be gained from looking for the clue. In contrast, \\(X=0, Y=0\\) cases will be informative since the clue is much better at distinguishing between \\(b\\) and \\(c\\) types—the two types in contention for this kind of case. Thus, although process tracing here does not provide information on the prevalence of positive causal effects (\\(b\\) types) for an \\(X=Y=1\\) case, it does provide information when \\(X=Y=0\\). While it is common practice for mixed-method researchers to perform their process tracing “on the regression line,” the BIQQ framework suggests that the gains to process tracing for different \\(X\\) and \\(Y\\) values in fact depend on the particular constellations of \\(\\phi\\) values for the potentially available clues. More generally, the framework allows one to assess the expected gains from any given case-selection strategy ex ante once priors have been specified. 13.1 Explorations Most closely related to our analysis in this chapter is the contribution of Herron and Quinn (2016), who build on Seawright and Gerring (2008). While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches. As we do, Herron and Quinn (2016) focus on a situation with binary \\(X,Y\\) data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, \\(Z_i\\) is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to “treatment”, or an \\(X=1\\) value). Herron and Quinn (2016) assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly. Our setup differs from that in Herron and Quinn (2016) in a few ways. Herron and Quinn (2016) paramaterize differently, though this difference is not important.70 Perhaps the most important difference between our analysis and that in Herron and Quinn (2016) is that we connect the inference strategy to process-tracing approaches. Whereas Herron and Quinn (2016) assume that causal types can be read directly, we assume that these are inferred imperfectly from clues. As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of \\(X\\). And, as in the baseline model, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases. Here we assume that the case selection decision is made after observing the \\(XY\\) distribution and we explore a range of different possible contingency tables. In Herron and Quinn (2016) the distribution from which the contingency tables are drawn is fixed, though set to exhibit an expected observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large \\(XY\\) data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases. Another important difference, is that in many of their analyses, Herron and Quinn (2016) take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has beliefs about the true treatment effect that correspond to their priors, and for whom there is therefore no expected bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher’s beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)? Finally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by Seawright and Gerring (2008)) and evaluating them, we define a strategy as the particular distribution over \\(XY\\) cells to be examined and proceed to examine every possible strategy given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies—those strategies that perform similarly—emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies. Despite these various differences, our results will agree in key ways with those in Herron and Quinn (2016). 13.2 Diagnosing case-selection strategies: procedure We describe here the procedure through which we can use gbiqq to diagnose and compare case-selection strategies given (i) a causal model, (ii) any data we have already observed, and (iii) the causal query we seek to answer. The general intuition is that we can use the causal model and any previously observed data to estimate what observations we are more or less likely to make under a given case-selection strategy, and then figure out how far off from the (under the model) true estimand we can expect to be under the strategy, given whatever causal question we seek to answer. To illustrate the procedure, suppose that we want to estimate the average treatment effect of \\(X\\) on \\(Y\\) in a population and have initially observed \\(X\\) and \\(Y\\) data on 6 cases. The initial data are summarized in Table ??. event count X0Y0 2 X1Y0 1 X0Y1 1 X1Y1 2 As we can see, two cases lie in each of the on-diagonal cells of an \\(X,Y\\) table, those where \\(X=Y\\), and one case lies in each of the off-diagonal cells, those where \\(X \\neq Y\\). Suppose that we are now considering gathering process-tracing evidence for 1 of these cases to inform our estimate of the ATE. There are many different case-selection strategies we might pursue, and each of these can give rise to different possible data and thus to different possible conclusions. What should we do? DAG. We start, as always, with a DAG representing our beliefs about which variables we believe to be direct causes of other variables. For the current illustration, suppose that we are operating with a simple mediation model, \\(X \\rightarrow M \\rightarrow Y\\). Given data. If we have already observed something in a set of cases, we can use this information to condition our strategy for searching for further information. For instance, if we have observed \\(X\\)’s and \\(Y\\)’s value in a set of cases, we might select cases for process tracing based on their values of \\(X\\) and \\(Y\\). Further, what we have already observed in the cases may constrain what possible data we could end up with once we have collected the additional (process tracing) data. Priors. As when conducting mixed-method inference, we can set qualitative restrictions and/or differential quantitative weights on the (possibly conditional) nodal types in the model. And we can indicate our uncertainty over the latter, by setting the \\(\\alpha\\) parameters of the relevant Dirichlet distributions. For the current example, let us define restrictions at both the \\(M\\) and \\(Y\\) nodes, positing beliefs that \\(X\\) never has a negative effect on \\(M\\) and that \\(M\\) never has a negative effect on \\(Y\\). Let us further assume that we have flat priors over the remaining nodal types and posit similar assignment propensities for all types (no unobserved confounding). Query. We define our query. This might, for instance, be the share of cases in the population in which \\(X\\) has a positive effect on \\(Y\\); or it might be \\(X\\)’s average effect on \\(Y\\). We can use the general procedure to identify case-selection strategies for any causal query that can be defined on a DAG. And, importantly, the optimal case-selection strategy may depend on the query. For instance, the best case-selection strategy for estimating the average causal effect of \\(X\\) on \\(Y\\) may not be the same as the best strategy for figuring out for what proportion of the population \\(X\\) has a positive effect on \\(Y\\). Define one or more strategies. A strategy is defined, generically, as the search for data on a given set of nodes, in a given number of cases randomly selected conditional on some information we already have about potential cases. Let us assume here that our strategy will involve uncovering \\(M\\)’s value in 1 case—but we are wondering how to choose this case. Consider four possible strategies, conditional on the \\(X\\) and \\(Y\\) values that we already know. We could do process tracing on a randomly selected \\(X=1, Y=1\\) case, a randomly selected \\(X=0, Y=0\\) case, the \\(X=1, Y=0\\) case, or the \\(X=0, Y=1\\) case. We itemize this set of possible strategies in the first column in Table ??. Possible data. For each strategy, there are multiple possible sets of data that we could end up observing. In particular, the data we could end up with will be the \\(X,Y\\) patterns we have already observed plus either \\(M=0\\) or \\(M=1\\) in the case that our strategy leads us to select for process tracing. We represent the data possibilities (showing just the possible \\(M\\) values) in the second column in Table ??. Thus, for instance, for a strategy in which we choose a random \\(X=1, Y=1\\) case, we could end up observing the initial \\(X,Y\\) pattern plus \\(M=0\\) in one of the \\(X=1, Y=1\\) cases, or the initial \\(X,Y\\) pattern plus \\(M=1\\) in one of the \\(X=1, Y=1\\) cases. Probability of the data. We now calculate a probability of each possible data realization, given the model and the data (the \\(X\\)’s and \\(Y\\)’s) that we have already observed. In practice, we do this in gbiqq via simulation. Starting with the model together with our priors, we update our beliefs about \\(\\lambda\\) based on the initial \\(X,Y\\) data. This posterior now represents our prior for the purposes of the process tracing; it represents what we believe about causal-type share allocations in the population, having seen the \\(X,Y\\) data only. We then use this posterior to draw a series of \\(\\lambda\\) values. Given that the ambiguity matrix gives us the mapping from causal types to data realizations, we can calculate for each \\(lambda\\) draw the probability of each data possibility given that particular \\(\\lambda\\) and the strategy. We then average across repeated \\(\\lambda\\) draws. (Since \\(\\lambda\\)’s are being drawn from our prior, we are automatically weighting more heavily those \\(\\lambda\\)’s that we believe to be most likely.) We show the data probabilities in the third column of Table ??: one probability for each data-possibility given each strategy. Posterior variance on estimate given the data. For each data possibility, we can then use gibiqq to ask what inference we would get from each data possibility, given whatever query we seek to answer. What we are in fact interested in for the purposes of case selection is the variance of the posterior. We indicate in the fourth column of Table ?? the posterior variance for each possible data realization. Expected posterior variance under each strategy. The quantity of ultimate interest is the posterior variance that we expect to end up with under each strategy. Calculating this expectation is now elementary as we have both the posterior variance arising from each data possibility and the probability of each data possibility (given our prior beliefs and the data already observed). The expected posterior variance is simply an average of the posterior variances under each data possibility, weighted by the probability of each data possibility. The final column provides the expected posterior variances, one for each strategy. FLAG: Write code and generate results table, caseselect1. Columns are: Strategy, Possible Data [with row split into M=0, M=1], Probability of Data (for each M value), Posterior Variance (for each M value), Expected Posterior Variance (one value per strategy, weighted average over M values). Strategies are select an \\(X=0, Y=0\\) case; select an \\(X=1, Y=1\\) case; select an \\(X=1, Y=0\\) case; select an \\(X=0, Y=1\\) case. We see that XXXXXXX We can similarly diagnose strategies for a plan to process-trace any number of the cases for which we have \\(X,Y\\) data. In Table ??, we show results for six of the possible strategies we might employ for collecting \\(M\\) for 3 cases. We compare going deeper into two \\(X=Y=1\\) cases and one \\(X=Y=0\\) case; two \\(X=Y=1\\) cases and an \\(X=0, Y=1\\) case; two \\(X=Y=1\\) cases and an \\(X=1, Y=0\\) case; and an \\(X=0, Y=1\\), an \\(X=1, Y=0\\), and an \\(X=1, Y=1\\) case; two \\(X=Y=0\\) cases and one \\(X=Y=1\\) case; two \\(X=Y=0\\) cases and an \\(X=0, Y=1\\) case; two \\(X=Y=0\\) cases and an \\(X=1, Y=0\\) case; and an \\(X=0, Y=1\\), an \\(X=1, Y=0\\), and an \\(X=1, Y=0\\) case. FLAG: Write code and generate results table, caseselect3. Same structure as caseselect1. We can now see that…… 13.3 Evaluating types of strategies The qualitative case-selection literature has identified a range of possible strategies for choosing cases for in-depth analysis. These include, for instance, selecting for variation on \\(X\\) (KKV), selecting for variation on \\(Y\\), selecting cases on the regression line (Seawright and Gerring, Lieberman), and selecting off the regression line (Seawright and Gerring, Seawright 2017, Lieberman).[These authors view selection off the regression line as best for arriving at inductive insight. We address this strategy primarily to show the contrast with the on-the-line strategy.] While we have not seen it advocated elsewhere, we might add to this list the strategy of selecting cases that are representative in their \\(X,Y\\) values of the larger set of cases from which we are selecting. (FLAG: insert proper citations). While it is difficult to clearly distinguish these strategies from each other with a small initial set of \\(X, Y\\) cases, we can do so readily if we start with a larger set of \\(X,Y\\) cases. In Table ??, we show the results of diagnoses of each of these five classes of strategies, assuming that we will be process-tracing 6 cases. In each diagnosis we start with 500 \\(X, Y\\) cases, with 100 \\(X=Y=0\\) cases, 200 \\(X=Y=1\\) cases, 130 \\(X=0, Y=1\\) cases, and 70 \\(X=1, Y=0\\) cases. The regression line here represents a positive association. We work with the same model and priors as above. Importantly, this means that the results we show here are not general evaluations of these strategies, but contingent on this particular model and set of priors. And that is precisely our point: optimal case-selection will always hinge on our model, a claim that we demonstrate further below. FLAG: Create Table (caseselectlots) with these diagnoses. Do not represent data possibilities in the table (too many). Strategies are: Variation in \\(X\\): Randomly select 3 \\(X=0\\) cases and 3 \\(X=1\\) cases Variation in \\(Y\\): Randomly select 3 \\(Y=0\\) cases and 3 \\(Y=1\\) cases On the regression line: Randomly select 3 from the two \\(X=Y=0\\) cell and 3 from the \\(X=Y=1\\) cell. Off the regression line: Randomly select 3 from the \\(X=0, Y=1\\) cell and 3 from the \\(X=1, Y=0\\) cell. Representativeness: Randomly select 6 cases We can see that…. 13.4 Different models, different strategies 13.5 Different queries, different strategies 13.6 In code # We define a model model &lt;- make_model(&quot;X-&gt;M-&gt;Y&quot;) %&gt;% set_restrictions(c(&quot;(Y[M=1]&lt;Y[M=0])&quot;, &quot;(M[X=1]&lt;M[X=0])&quot;)) %&gt;% set_parameter_matrix() %&gt;% set_parameters(type = &quot;flat&quot;) # We imagine some preexisting data we have observed data &lt;- data.frame(X = c(0,0,0,1,1,1), M = NA, Y = c(0,0,1,0,1,1)) # We make a matrix showing how many cases there are of each data typema given &lt;- collapse_data(data, model) # We can then imagine what data we might observe if we examine M inside some subset of cases possible_data &lt;- make_possible_data(model, given = given, vars = list(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;), within = TRUE, N = list(1,1,1,1), condition = list(&quot;X==0 &amp; Y==0&quot;, &quot;X==1 &amp; Y==0&quot;, &quot;X==0 &amp; Y==1&quot;, &quot;X==1 &amp; Y==1&quot;)) Diagnosis: diagnosis_X1Y1 &lt;- diagnose_strategies( reference_model = reference_model, analysis_model = model, given = given, queries = &quot;Y[X=1]&gt;Y[X=0]&quot;, estimates_database = estimates_database_n1_X1Y1, possible_data = possible_data) 13.7 Compare multiple data strategies We now apply the procedure to a set of strategies. ## Warning in kable_markdown(x, padding = padding, ...): ## The table should have a header (column names) || || || || @all here is where we need most backup help right now — these examples show the basic analysis — interest is in understanding how posterior variance differs for difference data strategies given difference givens and different background models. Steps – Do wrapper to make it a little faster to generate results from a new strategy / mode in one line (that uses saved results when possible) Extend current example to all four “gather M for 1 case” strategies and all 8 “gather M for 2 cases” (could be two of the same sort or two different) Then do the same for (a) a model with monotonicity of X to M and M to Y assumed and (b) a X -&gt; Y &lt;- M model with complementarity of X and M assumed – (LM) I don’t understand how model (a) is different from the model above. Even better if possible to graph some of the results, even with confidnece intervals. You can see the previous graphs we had in K1.pdf - K4.pdf in “6 Book” @lily — strategies that involve “look at M in each of teh X/Y cells” will involve using the full data strategies of make_possible_data with condition = list(\"X==0 &amp; Y==0\", \"X==0 &amp; Y==1\") etc OLD TEXT: In this example, we see that the researcher would expect to be better off—in the sense of having less posterior uncertainty—by focusing her process-tracing efforts where a greater share of the population of cases lies: on the regression line. Taking one observation in each of three cells has her devoting much of her effort to a case that is relatively unrepresentative of the population she wishes to learn about. In the experiments that follow, we implement this kind of simulation for all possible clue strategies—for a fixed number of clues sought—and report the expected posterior variance. 13.8 Experiments In all of the graphs, we start with 16 “quantitative” cases: cases for which we have observed an \\(X\\) and a \\(Y\\) value. We are then choosing some subset of these cases for process tracing. Within each group of nine graphs, we are sampling a fixed number of cases for process tracing: 1 case in the first set, 2 cases in the second set, 3 cases in the third set, and 4 cases in the final set. We treat process tracing in a case as the search for one clue in that case, though this “one clue” could be conceived of as a collection of clues that jointly have a given probative value. Within each set of graphs, we see how different case selection strategies fare as we vary two features of the research situation. Moving down the rows of graphs, we vary the distribution of the 16 cases over an \\(XY\\) table. In the first row, the 16 cases are spread evenly across the 4 \\(X,Y\\) cells; in the second row, \\(X\\) and \\(Y\\) are positively correlated; in the third row, \\(Y=1\\) is observed only in cases with \\(X=1\\) (i.e., the \\(X=0, Y=1\\) cell is empty). Moving across the columns of graphs, we vary the probative value of the clue that we are looking for in the process tracing. What is changing is for what kind of a case—in terms of its \\(X,Y\\) values—the clue is most probative (i.e., doubly decisive). Where the clue is doubly decisive, finding the clue present or finding it absent both nail down the type of the case. Wherever the clue is not doubly decisive, it is assumed to be a “hoop test” for an \\(a\\) type (in the \\(X=1, Y=0\\) and \\(X=0, Y=1\\) cells) and for a \\(b\\) type (in the \\(X=0, Y=1\\) cells). This means that not finding the clue rules out the case’s being an \\(a\\) or a \\(b\\) type in the relevant cells; thus, the clue is still informative, but less so than if it were doubly decisive. In the first column, the clue is doubly decisive for all kinds of cases. In the second column, the clue is doubly decisive only for \\(X=1\\) cases, and a hoop test otherwise. In the third column, the clue is doubly decisive only for \\(Y=1\\) cases, and a hoop test otherwise. And in the final column, the clue is doubly decisive only “on the regression line” consistent with a positive effect, and a hoop test otherwise. Each case selection strategy is indicated on each graph using the same four-digit pattern that we used to indicate data realizations in the example above. Throughout, what we seek to estimate is the average causal effect of \\(X\\) on \\(Y\\) (or \\(\\lambda_b-\\lambda_a\\)). The strategy’s vertical placement on the graph indicates the “loss”, i.e., expected posterior uncertainty, associated with the strategy. Thus, a lower placement indicates greater learning. Around each dot, we also provide 90 percent simulation error bars, though these are hidden by the dots themselves when the simulation error is very small, as it is in most cases. In all simulations, we assume that there is no confounding, and we start (before seeing the \\(X, Y\\) pattern) with flat priors over the distribution of causal types in the population. In the graphs with more than one process tracing case, we also color-code and group together families of strategies as indicated in the figure caption. We begin with the simplest problem, where only one case is to be chosen for process tracing. In this situation, as seen in our first set of figures, two principle emerge. First, it is better select cases in the \\(X,Y\\) conditions where the probative value lies. More informative clues generate more learning; so if probative value varies across types of cases, this should have direct implications for case selection. Second, it is better to select cases from the largest cells. The second principle is perhaps less obvious, but it derives from a sampling logic: learning about a case drawn randomly from a cell gives you information about that cell and so the larger the cell the more cases there is learning about. Perhaps just as important is what does not emerge as a principle: all else equal there is no reason to focus on either the \\(X=Y=1\\) cases or on the diagonal (\\(X=Y\\)): all four cells are symmetric (ceteris paribus) in that they all exhibit an ambiguity between \\(a\\)’s and \\(b\\)’s on the one hand and \\(c\\)’s and \\(d\\)’s on the other. There is thus nothing intrinsically informative about the cases on the diagonal. We do see that choosing on the diagonal is beneficial when \\(X\\) is positively correlated with \\(Y\\), but this is because the population of cases is concentrated along the diagonal; this is an illustration of the representativeness principle, not of some special feature of the diagonal. The figure shows gains from different strategies involving process tracing in 1 case. In each simulation, we start with 16 “quantitative” cases. Moving down the rows of graphs, we vary the distribution of these cases over an \\(XY\\) table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. More interesting patterns start emerging once we can choose two cases. We see the same two basic principles matter—go for probative value and for representativeness—but we now also see that there are complementarities in learning between different types of cases. In fact, for symmetric problems—as in the upper left panel, where the cases are evenly spread out and probative value is strong everywhere—we see a ranking between four families: first cases that fix \\(Y\\) (at 0 or 1) and spread on \\(X\\), second cases that fix \\(X\\) and spread on \\(Y\\), third cases that are on or off the diagonal, and fourth cases that focus on a single cell. Seeking clues on the diagonal does emerge as a good strategy (see for example the left figure in the middle row), but this appears to arise because the diagonal is data dense, not because there are particular complementarities of probative value there. In the bottom left figure for example, we see that in the case where there is data for \\(X=1, Y=0\\) but little data for \\(X=0, Y=1\\) , selecting cases distributed over the \\(Y=0\\) cells is about as informative as selecting on the diagonal. Selecting off the diagonal is one of the worst strategies here and is dominated by selecting all data from a dense cell. Skip now to our graph of results where 4 cases are selected. As in previous figures we see strong gains for strategies that select cases proportionate to the size off cells. This is mot clear in the top left figure where one case per (equally sized) cell is the best strategy. More subtly it can be seen in the bottom left where the 2-0-1-1 strategy dominates — this is a strategy that spreads roughly proportionately even if that means leaving some cells unrepresented. Strikingly in the base case various hybrid strategies do quite well, likely reflecting the fact that are as close to optimal spreads as possible. On and off diagonal strategies do poorly unless there is a strong diagonal, in which case these can dominate spreading across cells. Overall relatively simple patterns emerge though these differ in some ways from text book suggestions. First focusing on probative value is key. Second seeking larger cells and balancing cases across cells appears fruitful. Third, and less intuitively, some combinations appear to gain more leverage than others. On and off diagonal strategies for example seem weaker in general than strategies that fix \\(X\\) or that fix \\(Y\\). Strategies that fix \\(Y\\) and allow variation on \\(X\\) seem strong, again, ceteris paribus. Perhaps the most striking result from the simulations is that the optimal choice depends on many features. A simple rule, or even these core principles, may not get identify the right strategy. Ye the right strategy can be calculated, at least if one is willing to lay out beliefs on causal structure and probative value. Gains from different strategies involving process tracing in 2 cases. We treat process tracing in a case as the search for one clue in that case, though this “clue” could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 “quantitative” cases. Moving down the rows of graphs, we vary the distribution of these cases over an \\(XY\\) table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. Families of strategies are grouped and color-coded as follows: red=maximally dispersing across cells; yellow= Gains from different strategies involving process tracing in 3 cases. We treat process tracing in a case as the search for one clue in that case, though this “clue” could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 “quantitative” cases. Moving down the rows of graphs, we vary the distribution of these cases over an \\(XY\\) table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. Gains from different strategies involving process tracing in 4 cases. We treat process tracing in a case as the search for one clue in that case, though this “clue” could be conceived of as a collection of clues that jointly have the probative value indicated. Simulations invole 16 units distributed over an \\(XY\\) table in three patterns (rows) and variation over the probative value of different clues (columns). 13.9 Chapter Appendix: Accounting for case selection 13.9.1 Independent case selection strategy We have focused on cases in which the researcher examines a fixed number of cases for clue information. An alternative strategy that produces a simpler likelihood is one in which each case is selected for within-case data gathering with some independent probability. The likelihood below introduces a case selection probability \\(\\kappa_{xy}\\) that covers this case and allows for the possibility that selection probabilities are different for different \\(X,Y\\) combinations. Thus we assume that \\(X\\), \\(Y\\) data is observed for all cases under study, but that \\(K\\) data may be sought for only a subset of these (we use the wildcard symbol ’‘\\(*\\)’’ to denote that the value of the clue is unknown). We let \\(n_{xyk}\\) denote the number of cases of each type. Then, again assuming data is independently and identically distributed, the likelihood is: \\[\\Pr(\\mathcal{D}|\\theta)= {\\text{Multinomial}}(n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*} | w_{000}, w_{001},w_{00*},w_{010}, w_{010},w_{01*}, w_{100}, w_{101},w_{10*},w_{110},w_{111} ,w_{11*})\\] where the event probabilities are now given by: \\[{\\left( \\begin{array}{c} w_{000} \\\\ w_{001} \\\\ \\vdots \\\\ w_{11*} \\end{array} \\right)= \\left( \\begin{array}{c} \\lambda_b(1-\\pi_b)\\kappa_{00}(1-\\phi_{b0}) + \\lambda_c(1-\\pi_c)\\kappa_{00}(1-\\phi_{c0})\\\\ \\lambda_b(1-\\pi_b)\\kappa_{00}\\phi_{b0} + \\lambda_c(1-\\pi_c)\\kappa_{00}\\phi_{c0}\\\\ \\vdots \\\\ \\lambda_b\\pi_{b}(1-\\kappa_{11}) + \\lambda_d\\pi_{d}(1-\\kappa_{11}) \\end{array} \\right)}\\] Note we use a Greek symbol for the case selection probabilities to highlight that these may also be unknown and be an object of inquiry, entering into the vector of parameters, \\(\\theta\\). While we have assumed in the canonical model that \\(X,Y\\) cases are selected at random, this need not be the case. Say instead that each case of type \\(j\\) is selected into the study with probability \\(\\rho_j\\). In that case, assuming independent selection of cases for qualitative analysis, the likelihood function is now: \\[\\Pr(\\mathcal{D}|\\theta) = {\\text{Multinomial}}(n, w)\\] where: \\[n = (n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*})\\] and the event probabilities, \\(w\\), are now, given by: \\[\\left( \\begin{array}{c} w_{000} \\\\ w_{001} \\\\ \\vdots \\\\ w_{11*} \\end{array} \\right)= \\left( \\begin{array}{c} \\frac{\\rho_b \\lambda_b}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_b)\\kappa_{00}(1-\\phi_{b0}) + \\frac{\\rho_c \\lambda_c}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_c)\\kappa_{00}(1-\\phi_{c0})\\\\ \\frac{\\rho_b \\lambda_b}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_b)\\kappa_{00}\\phi_{b0}+ \\frac{\\rho_c \\lambda_c}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_c)\\kappa_{00}\\phi_{c0}\\\\ \\vdots \\\\ \\frac{\\rho_b \\lambda_b}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}\\pi_{b}(1-\\kappa_{11})+ \\frac{\\rho_d \\lambda_d}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_{11}}\\pi_{d}(1-\\kappa_{11}) \\end{array} \\right)\\] Note we have used a Greek symbol for the selection probabilities to highlight that these probabilities may be unknown and could enter into the set of parameters of interest, \\(\\theta\\). 13.9.2 Conditional random case selection Finally consider the likelihood for a design in which a researcher selects to search for clues as a function of the \\(X,Y\\) data. This is a somewhat harder case because the size of each \\(X,Y\\) group is stochastic. Let \\(n_{xy} = n_{xy0}+n_{xy1}+n_{xy*}\\) denote the number of cases with particular values on \\(X\\) and \\(Y\\), and let \\(n_{XY}=(n_{00},n_{01},n_{10},n_{11})\\) denote the collection of \\(n_{xy}\\) values. Say now that conditional on the \\(X,Y\\) observations, a researcher sets a target of \\(k_{xy}(n_{XY})\\) cases for clue examination (note here that the number of clues sought for a particular \\(X,Y\\) combination can be allowed to depend on what is observed across all \\(X\\), \\(Y\\) combinations). Then the likelihood is: \\[\\text{Multinomial}(n_{XY}|w_{XY})\\prod_{x\\in\\{0,1\\},y \\in\\{0,1\\}}\\text{Binom}(n_{xy1}|k_{xy}(n_{xy}), \\psi_{xy1})\\] The multinomial part of this expression gives the probability of observing the particular \\(X,Y\\) combinations; the event probabilities for these depend on \\(\\lambda\\) and \\(\\pi\\) only | for example \\(w_{11} = \\lambda_b \\pi_b+\\lambda_d \\pi_d\\). The subsequent binomials give the probability of observing the clue patterns conditional on searching for a given number of clues (\\(k_{xy}(n_{xy})\\)) and given an event probability \\(\\psi_{xy1}\\) for seeing a clue given that the clue is sought for an \\(x,y\\) combination; thus for example: \\[ \\psi_{111} = \\frac{\\lambda_b \\pi_b}{\\lambda_b \\pi_b+\\lambda_d \\pi_d} \\phi_{b1} + \\frac{\\lambda_d \\pi_d}{\\lambda_b \\pi_b+\\lambda_d \\pi_d} \\phi_{d1}\\] References "],
["justifying-models.html", "Chapter 14 Justifying models 14.1 Bounds on probative value 14.2 The possibility of identification of probative value from experimental data 14.3 Learning across populations 14.4 Different models for different sites 14.5 Causal discovery", " Chapter 14 Justifying models We outline strategies to reduce reliance on unfounded beliefs about the probative value of clues. The approach we have described to inference always involve updating beliefs given data. But to get off the ground researchers need to be able to state priors on all parameters. In many applications the problem of stating priors can be more fundamental than for many Bayesian applications for two reasons. First the beliefs are beliefs over the distribution of individual level effects and not just the beliefs over average effects. This puts us up against the fundamental problem of causal inference (Holland cite, Dawid cite FLAG). Second, the beliefs can do a lot of work—especially in small \\(n\\) applications. Indeed for the the process tracing described chapters 6 and 7 [FLAG ADD REFS] the inferences are little more than conditional applications of a model. We see two broad responses to this problem. One is emphasize the contingent nature of claims. As we outlined in Chapter 4, some causal models might reasonably reflect actual beliefs about the world—for example one might, be convinced that a treatment was randomly assigned, that there is no interference, and that units are independently sampled from a distribution of types. All of these beliefs may be unwise. But if held, then the simple DAG in chapter 4 (REF) can be taken to represents beliefs about the world rather than a model of the world, in the sense of a simplified representation. But as we noted in Chapter 4, for an even modestly more complex situation, it seems inevitable that the model being used is truly a model and not a faithful summary of beliefs. Owning the model in this way results in a useful reposing of the question: the question becomes not whether the assumptions are correct but whether the model is useful (Clarke and Primo 2012). That is the subject of Chapter 15. Here we focus on more positive steps that might be taken to underpin a model. We highlight first how the type of approach used in Chapters 8 and 9 can be used to justify a process tracing model on the basis of a mixed methods model. These applications presuppose knowledge of a DAG however. There are two further responses to this concern. One is to try to generate the DAG itself from data or a combination of data and theory. We discuss this approach here. Another is to assess the importance of DAG assumptions – which we address in Chapter 15. 14.1 Bounds on probative value Classic treatments of process tracing make use of Causal Process Observations — observations that are taken to be indicative of a particular causal process in operation. We introduced in Chapter 5 (as well as in FLAG CITE humphreysjacobs) quantities such as \\(\\phi_{b}\\)—the probability that \\(K=1\\) given \\(X\\) caused \\(Y\\) and \\(X=Y=1\\), or \\(\\phi_{d}\\)—–the probability that \\(K=1\\) given \\(X\\) did not cause \\(Y\\) and \\(X=Y=1\\). These accounts do not guide much guidance however regarding where these quantities come from — given that causal types are unobservable how can one justify a belief about the probability of some observation given a causal type. Is it even possible to justify such beliefs? The grounded approach we described provides an answer to this puzzle. In short, knowledge of the structure of a causal model, together with data on exchangeable units, can be enough to place bounds on possible values of \\(\\phi_{b}, \\phi_{d}\\). We illustrate the basic idea and then review some results in this area. Imagine a fortunate situation in which (a) it is known that the true causal model has the form \\(X \\rightarrow M \\rightarrow Y\\) and (b) we have a lot of experimental data on the conditional distribution of \\(M\\) given \\(X\\) and of \\(Y\\) given \\(M\\) for exchangeable units (meaning that we can treat our unit of interest as if it were a draw from this set). Let us define: \\(\\tau_1 = \\Pr(M=1 | X=1) - \\Pr(M=1 | X=0)\\) \\(\\rho_1 = \\Pr(M=1 | X=1) - \\Pr(M=0 | X=0)\\) \\(\\tau_2 = \\Pr(Y=1 | M=1) - \\Pr(Y=1 | M=0)\\) \\(\\rho_2 = \\Pr(Y=1 | M=1) - \\Pr(Y=0 | M=0)\\) These are all quantities that can be calculated from the data. The \\(\\tau\\)s are average treatment effects and the \\(\\rho\\)s are indicators for how common the \\(Y=1\\) outcome is. We are interested in the probability of observing \\(M=1\\) given \\(X=Y=1\\): \\[\\phi_{b1} = \\frac{\\lambda_{b}^K\\lambda_{b}^Y}{\\lambda_{b}^K\\lambda_{b}^Y + \\lambda_{a}^K\\lambda_{a}^Y}\\] Noting that \\(\\tau_j = \\lambda_{b_j} - \\lambda_{a_j}\\): \\[\\phi_{b1} = \\frac{\\lambda_{b}^K\\lambda_{b}^Y}{\\lambda_{b}^K\\lambda_{b}^Y + (\\lambda_{b}^K-\\tau_1)(\\lambda_{b}^Y - \\tau_2)}\\] which we can see is decreasing in \\(\\lambda_{b}^j\\) (this may seem counterintuitive, but the reason is that with \\(\\tau^j\\) fixed, lower \\(\\lambda_{b}^j\\) also means lower \\(\\lambda_{a}^j\\) which means less ambiguity about how \\(X\\) affects \\(Y\\) (i.e. through positive or negative effects on \\(K\\)). The lowest permissible value of \\(\\lambda_{b_j}\\) is \\(\\tau_j\\), yielding \\(\\phi_{b1} = 1\\). The highest value obtainable by \\(\\lambda_{b_j}\\) is when \\(\\lambda_{a_j} = \\frac{1-\\tau_j+\\rho_j}2\\) and so \\(\\lambda_{b_j} = \\frac{1+\\tau_j+\\rho_j}2\\). In this case: \\[\\phi_{b1} = \\frac{(1+\\tau_1+\\rho_1)(1+\\tau_2+\\rho_2)}{(1+\\tau_1+\\rho_1)(1+\\tau_2+\\rho_2) + (1-\\tau_1+\\rho_1)(1-\\tau_2+\\rho_2)}= \\frac{(1+\\tau_1+\\rho_1)(1+\\tau_2+\\rho_2)}{2(1+\\rho_1)(1+\\rho_2) + 2\\tau_1\\tau}\\] And so: \\[\\frac{(1+\\tau_1+\\rho_1)(1+\\tau_2+\\rho_2)}{2(1+\\rho_1)(1+\\rho_2) + 2\\tau_1\\tau_2} \\leq \\phi_{b1} \\leq 1\\] These are the bounds on \\(\\phi_{b1}\\). We can calculate bounds on \\(\\phi_{d1}\\) in a similar way (though of course the bounds on \\(\\phi_{b1}\\) and \\(\\phi_{d1}\\) are not independent). \\[\\phi_{d1} = \\frac{\\lambda_{b}^K\\lambda_{d}^Y}{(\\lambda_{a}^K + \\lambda_{b}^K + \\lambda_{c}^K)\\lambda_{d}^Y+ \\lambda_{c}^K\\lambda_{a}^Y}\\] Figure ?? illustrates how “smoking gun” and “hoop” tests might each be justified with knowledge of \\(\\tau_j, \\rho_j\\). For the smoking gun, \\(\\phi_{b1}\\) is .5 because \\(\\lambda_a^j = \\lambda_b^j\\) so half of the upper level \\(b\\) types work through a positive effect on \\(M\\) and half through a negative effect on \\(M\\). \\(\\phi_{d1}\\), on the other hand, is low here \\(d\\) types mostly arise because of \\(c\\) types in the first step and \\(a\\) types in the second, and hence most commonly with \\(M=1\\). Whether the bounds map into useful probative value depends in part on whether causal effects are better identified in the first or the second stage. We can see this in Figure 14.1. The key difference between the panels is that \\(\\phi_d\\) is constrained to be low in the first panel but not in the second. For intuition note that a higher level \\(d\\) type will exhibit \\(M=1\\) if it is formed via \\(db\\), \\(bd\\),or \\(dd\\) and it will exhibit \\(M=0\\) if it is formed via \\(ca\\), \\(cd\\), \\(ad\\). The weak second stage makes it possible that there are no second stage d types, only a and b types. The stronger first stage makes it possible that there are no first stage \\(c\\) types. In that case the higher level d types are formed uniquely of \\(db\\) types – which always exhibit \\(M=1\\) if \\(X=1\\). This is not possible however for the data assume in the first panel. In the first panel the the higher value on \\(\\rho_2\\) means that there must be at least .25 d types. And the weak first stage means that there must at least .5 a and c types combined. Thus there must be a set of cases in which \\(M\\) is not observed even though we have an upper level d type. Figure 14.1: Probative value with different first and second stage relations In short we emphasize that difficult as it might seem at first it is possible to put relatively tight bounds on probative value for causal types with access to experimental data on exchangeable units. 14.2 The possibility of identification of probative value from experimental data While it is possible to calculate bounds on probative value, it can be simpler to calculate bounds on estimands directly. These bounds can be justified with reference to background data in the same ways as the bounds on probative value. Following Dawid, Humphreys, and Musio (2019) we again imagine we had access to infinite experimental data on the effect of \\(X\\) on \\(Y\\) and we want to know for a case (exchangeable with any other in this population) with \\(X=Y=1\\), whether \\(X=1\\) caused \\(Y=1\\). Call this the “probability of causation.” Say we knew the marginal distributions: \\(\\Pr(Y=1|X=1) = .75\\) \\(\\Pr(Y=1|X=0) = .25\\) The we could represent this knowledge as Markovian transition matrix from \\(X\\) to \\(Y\\) like this: \\[P=\\left( \\begin{array}{cc} 0.50 &amp; 0.50 \\\\ 0.25 &amp; 0.75 \\end{array}\\right)\\] In this case, from results in Dawid, Musio, and Murtas (2017), we can place bounds directly on the probability that \\(X\\) caused \\(Y\\), viz: \\[\\frac13 \\leq PC \\leq \\frac23 \\] For intuition note that \\(P\\) implies a causal effect of .25 and so the lowest value of \\(\\lambda_b\\) consistent with \\(P\\) arises when \\(\\lambda_b = .25\\) and \\(\\lambda_a = 0\\), in which case \\(\\lambda_c = .25\\) and \\(\\lambda_d = .5\\). In this case \\(\\lambda_b/(\\lambda_b+ \\lambda_d)=\\frac{1}{3}\\). The highest consistent value of \\(\\lambda_b\\) arises when \\(\\lambda_b = .5\\) and \\(\\lambda_a = .25\\), in which case \\(\\lambda_c = 0\\) and \\(\\lambda_d = .25\\). In this case \\(\\lambda_b/(\\lambda_b+ \\lambda_d)=\\frac{2}{3}\\). Defining \\(\\tau\\) and \\(\\rho\\) as before, the more general formula for the case with \\(\\rho&gt;0\\) is: \\[\\frac{2\\tau}{1+\\tau+\\rho} \\leq PC \\leq \\frac{1+\\tau-|\\rho|}{1+\\tau+\\rho} \\] Say now we have access to auxiliary data \\(K\\) and plan to make inferences based on \\(K\\). We will suppose first that \\(K\\) is a mediator, as above, and second that \\(K\\) is a moderator. 14.2.1 Mediator Say now that in addition we know from experimental data, that \\(K\\) mediates the relationship between \\(X\\) and \\(Y\\); indeed we will assume that we have a case of complete mediation, such that, conditional on \\(K\\), \\(Y\\) does not depend on \\(X\\). Say the transition matrices from \\(X\\) to \\(K\\) and \\(K\\) to \\(Y\\) are: \\[P^{xk}=\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 1/2 &amp; 1/2\\end{array}\\right), P^{ky}=\\left( \\begin{array}{cc} 1/2 &amp; 1/2 \\\\ 0 &amp; 1\\end{array}\\right)\\] Even without observing \\(K\\), this information is sufficient to place a prior on PC of \\(p=\\frac13\\). To see this, note that we can calculate: \\(\\lambda_a^K =0\\), \\(\\lambda_b^K = \\frac{1}{2}\\), \\(\\lambda_c^K = \\frac{1}{2}\\), \\(\\lambda_d^K = 0\\) \\(\\lambda_a^Y =0\\), \\(\\lambda_b^Y=\\frac{1}{2}\\), \\(\\lambda_c^Y=0\\), \\(\\lambda_d^Y=\\frac{1}{2}\\) and so: \\(\\lambda_b^u = \\lambda_b^K\\lambda_b^Y = \\frac{1}4\\) \\(\\lambda_d^u = \\lambda_d^Y\\) \\(p = \\frac{\\lambda_b^u}{\\lambda_b^u + \\lambda_d^u} = \\frac{1}3\\). whence: \\(\\phi_{b1} = 1\\) \\(\\phi_{d1} = \\lambda_d^K + \\lambda_b^K = \\frac{1}{2}\\) More generally we can calculate the lower bound on the probability that \\(X\\) caused \\(Y\\) as the product of the lower bounds that \\(X\\) caused \\(M\\) and that \\(M\\) caused \\(Y\\), and similarly for the upper bound, using the same formula as before. Signing things so that \\(\\tau^j\\geq 0\\), \\(j \\in {1,2}\\): \\[\\frac{2\\tau_1}{1+\\tau_1+\\rho_1}\\frac{2\\tau_2}{1+\\tau_2+\\rho_2} \\leq PC \\leq \\frac{1+\\tau_1-|\\rho_1|}{1+\\tau_1+\\rho_1}\\frac{1+\\tau_2-|\\rho_2|}{1+\\tau_2+\\rho_2} \\] We have undertaken essentially the same operations as above except that now we are placing bounds on a substantive estimand of interest rather than first placing bounds on probative value of a clue and then turning to Bayes rule to place bounds on the estimand. 14.2.2 Moderator Consider now a situation in which our case is drawn from a set of cases for which \\(X\\) and \\(K\\) were each randomly assigned. Say then that the transition matrices, conditional on \\(K\\) look as follows: \\[P^{K=0}=\\left( \\begin{array}{cc} 0 &amp; 1 \\\\ 0.5 &amp; 0.5 \\end{array}\\right), P^{K=1}=\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array}\\right)\\] In this case we can now identify PC, even before observing \\(K\\). If \\(K=0\\), PC is 0—there are no cases with positive effects in this condition. If \\(K=1\\) PC = 1. We have a prior that \\(K=1\\) of .5 and after observing \\(X=Y=1\\) we raise this to \\(2/3\\). Thus our prior belief on \\(PC\\) — before seeing \\(K\\)— is \\(2/3 * 1 + 1/3 * 0 = 2/3\\). How about \\(\\phi_{b1}\\) and \\(\\phi_{d1}\\)? Here positive effects only arise when \\(K=1\\) and so \\(\\phi_{b1} = 1\\). \\(Y=1\\) without being cause by \\(X\\) only if \\(K=0\\) and so \\(\\phi_{b0} = 0\\). Thus we have a double decisive clue. 14.2.3 Case level bounds from mixed data 14.3 Learning across populations Now consider strategies to learn about clues from observing patterns in different populations. We first consider a situation in which we believe the same model holds in multiple sites but in which learning about the model requires combining data about different parts of the model from multiple studies. model &lt;- make_model(&quot;X -&gt; Y &lt;- Z -&gt; K&quot;) We imagine we have access to three types of data; Study 1 is an experiment looking at the effects of \\(X\\) on \\(Y\\), ancillary data on \\(K\\) is collected but \\(Z\\) is not observed Study 2 is a factorial study examining the joint effects of \\(X\\) and \\(Z\\) on \\(Y\\), \\(K\\) is not observed Study 3 is an RCT looking at the relation between \\(Z\\) and \\(K\\). \\(X\\) and \\(Y\\) are not observed. Tables 14.1 - 14.3 show conditional inferences for the probability that \\(X\\) caused \\(Y\\) in \\(X=Y=1\\) cases conditional on \\(K\\) for each study, analyzed individually Table 14.1: Clue is uninformative in Study 1 Subset mean sd X == 1 &amp; Y == 1 &amp; K == 1 0.5 0.153 X == 1 &amp; Y == 1 &amp; K == 0 0.5 0.146 Table 14.2: Clue is also uninformative in Study 2 (factorial) Subset mean sd X == 1 &amp; Y == 1 &amp; K == 1 0.546 0.110 X == 1 &amp; Y == 1 &amp; K == 0 0.546 0.112 Table 14.3: Clue is also uninformative in Study 3 (experiment studying \\(K\\)) Subset mean sd X == 1 &amp; Y == 1 &amp; K == 1 0.5 0.154 X == 1 &amp; Y == 1 &amp; K == 0 0.5 0.152 In no case is \\(K\\) informative. In study 1 data on \\(K\\) is not available, in study 2 it is available but researchers do not know, quantitatively, how it relates to \\(Z\\). In the third study the \\(Z,K\\) relationship is well understood but the joint relation between \\(Z,X\\), and \\(Y\\) is not understood. Table 14.4 shows the inferences when the data are combined with joint updating across all parameters. Table 14.4: Clue is informative after combining studies linking \\(K\\) to \\(Z\\) and \\(Z\\) to \\(Y\\) Subset mean sd X == 1 &amp; Y == 1 &amp; K == 1 0.663 0.081 X == 1 &amp; Y == 1 &amp; K == 0 0.519 0.099 X == 1 &amp; Y == 1 &amp; K == 1 &amp; Z == 1 0.713 0.101 X == 1 &amp; Y == 1 &amp; K == 0 &amp; Z == 1 0.713 0.101 X == 1 &amp; Y == 1 &amp; K == 1 &amp; Z == 0 0.509 0.104 X == 1 &amp; Y == 1 &amp; K == 0 &amp; Z == 0 0.509 0.104 Here fuller understanding of the model lets researchers use information on \\(Z\\) to update on values for \\(Z\\) and in turn update on the likely effects of \\(X\\) on \\(Y\\). Rows 3-6 highlight that the updating works through inferences on \\(Z\\) and there are no gains when \\(Z\\) is known, as in Study 2. In this example Studies 2 and 3 can be thought of as helper experiments for Study 1. Study 2 might be thought of as a mechnism study whereas Study 3 is more like a measurement study. 14.4 Different models for different sites In the last example we assumed that the same model operated in the same wayat all sites. This is a strong assumption, though sometimes justifiable (for instance if sites were randomly allocated across studies). If the same model does not operate at different sites it might still be possible to update in this way. For this, however, we need to be able to specify how sites differ Consider a problem where the models partially differ across sites: for instance we believe that although treatment effects are different in two sites yet the mechanisms linking treatment to outcomes are the same. As a simple example we might imagine that \\(X\\) is differentially likely to produce \\(M\\) in two sites, but if it does the relation between \\(M\\) and \\(Y\\) is common across sites. # In this model you are more likely to have an M=1 type regardless if Y = 1 regardless # This produces a positive confound model_1 &lt;- make_model(&quot;X-&gt;M-&gt;Y&quot;) %&gt;% set_confound(confound = list(M = &quot;(Y[M=1] ==1) &amp; (Y[M=0]==1)&quot;)) %&gt;% set_parameters(c(.1, 0, .2, .7, .5, .5, .7, 0, .2, .1, .2, .2, .4, .2)) plot_dag(model_1) if(do_diagnosis){ df_1 &lt;- simulate_data(model_1, n = 20000, using = &quot;parameters&quot;) posterior_1 &lt;- gbiqq(model_1, df_1, stan_model = fit) # In this model you are more likely to have an M=1 regardless if Y = 1 regardless # This produces a negative confound model_2 &lt;- make_model(&quot;X-&gt;M-&gt;Y&quot;) %&gt;% set_confound(confound = list(M = &quot;(Y[M=1] ==1) &amp; (Y[M=0]==1)&quot;)) %&gt;% set_parameters(c(.7, .1, .2, 0, .5, .5, 0, .1, .2, .7, .2, .2, .4, .2)) df_2 &lt;- simulate_data(model_2, n = 20000, using = &quot;parameters&quot;) posterior_2 &lt;- gbiqq(model_2, df_2, stan_model = fit) out1 &lt;- query_model(posterior_1, using=&quot;posteriors&quot;, queries = list(`X on M` = &quot;M[X=1] - M[X=0]&quot;, `M on Y` = &quot;Y[M=1] - Y[M=0]&quot;)) out2 &lt;- query_model(posterior_2, using=&quot;posteriors&quot;, queries = list(`X on M` = &quot;M[X=1] - M[X=0]&quot;, `M on Y` = &quot;Y[M=1] - Y[M=0]&quot;)) write_rds(list(posterior_1, posterior_2, out1, out2), &quot;saved/same_mechanism.rds&quot;) } same_mechanism &lt;- read_rds(&quot;saved/same_mechanism.rds&quot;) kable(same_mechanism[[3]]) Query Subset Using mean sd X on M All posteriors 0.204 0.006 M on Y All posteriors 0.239 0.027 kable(same_mechanism[[4]]) Query Subset Using mean sd X on M All posteriors 0.099 0.006 M on Y All posteriors 0.189 0.068 # The marginal effect of X on M will be different in the two cases # The effect of M on Y is the same however, though it is confounded Under the model there is possibly a difference in the effect of \\(X\\) on \\(Y\\) in the 14.4.1 Observational and experimental Let us imagine a second case in which one wants to update based on 14.5 Causal discovery We start with a model with three variables, \\(X,M,Y\\) where \\(X\\) affects \\(Y\\) directly and indirectly through \\(M\\). We simulate data from this model – assuming monotonicity but otherwise a flat distribution on types, and then try to recover the structure from this model. In this case the data structure did not impose restrictions on the skeleton. The true graph can however be recovered with knowledge of the temporal ordering of variables. Next we consider the model in which X causes Y through M but not directly. In this case we have a restriction — specifically there is no arrow pointing directly from \\(X\\) to \\(Y\\). Again we impose monotonicity, draw data, and try to recover the model: Again we have the correct skeleton and knowledge of timing is enough to recover the graph. Finally we consider the model in which \\(Y\\) has two causes that do not influence each other. Again we impose monotonicity, draw data, and try to recover the model: Figure 14.2: DAGs from Data References "],
["evaluation.html", "Chapter 15 Evaluating models 15.1 Tools for evaluating models 15.2 Evaluating the Democracy-Inequality model 15.3 Prior check 15.4 Monotonic restrictions", " Chapter 15 Evaluating models Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good. Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these may be wrong. More generally there is a literature on probabilistic causal models that assess the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see Parsons (2001). Under this kind of approach a researcher might willing to say that they think some probability \\(p\\) is not plausibly greater than .5, but unwilling to make a statement about their beliefs about where in the \\(0\\) to \\(0.5\\) range it lies. Such incomplete statements can be enough to rule our classes of conclusion. 15.1 Tools for evaluating models DO ALL THIS WITH PIMD APPLICATION Check conditional independencies Show for whether there’s a direct effect of X on Y Check confounding assumptions approach 2 – say actual confound is q~=0; but model assumes q = 0. Draw data from priors, draw data; given data type (001, 100 etc) plot (a) the posterior distribution under no confounding nad (b) the distribution of estimands that gave rise to the data. Change qual to quant priors in PIMD model A graph showing how some conclusions changes as we relax one of the restrictions. How much do our conclusions depend on qual restrictions? How do conclusions differ if we drop all restrictions Check fit Compare out of sample fit between model with posteriors and priors. Reality checks – posterior fits. Predict data from the posterior. Look at whether the data you see are similar to the data you actually have. Compare likelihoods of the data under different models 15.2 Evaluating the Democracy-Inequality model 15.3 Prior check In a second iteration of the analysis, we show what happens if we loosen the monotonicity restriction on \\(I\\)’s effect on \\(M\\). Here we consider negative effects of \\(I\\) on \\(M\\) unlikely, rather than impossible, and we consider null and positive effects somewhat likely. We refer to these priors as “quantitative priors” in the sense that they place a numerical value on beliefs rather than a logical restriction. Here, we set our prior on \\(\\theta^M\\) as: \\(p(\\theta^M=\\theta^M_{10})=0.1\\), \\(p(\\theta^M=\\theta^M_{00})=0.25\\), \\(p(\\theta^M=\\theta^M_{11})=0.25\\), and \\(p(\\theta^M=\\theta^M_{01})=0.4\\). We show the results for the inferences given different findings in tables and . The mapping into expected posterior variance associated with each strategy is shown by the numbers in parentheses in Table . The results differ in various modest ways. However, the biggest difference we observe is in the degree to which the mobilization clue matters when we are looking for negative effects of inequality. As discussed, if we assumed monotonic positive effects of inequality on mobilization and monotonic positive effects of mobilization on inequality, then the mediator clue is uninformative about the indirect pathway since that pathway can only generate a positive effect. However, if we allow for the possibility of a negative effect of inequality on mobilization, we now make \\(M\\) informative as a mediator even when the effect of inequality that we are interested in is negative: it is now possible that inequality has a negative effect on democratization via a negative effect on mobilization, followed by a positive effect of mobilization on democratization. So now, observing whether mobilization occurred adds information about whether a negative effect could have occurred via the mobilization pathway. Moreover, it is possible for the two effects of observing \\(M\\) on our beliefs to work in opposite ways. What we learn from observing \\(M\\) about the \\(I \\rightarrow M \\rightarrow D\\) pathway may push in a different direction from what we learn from observing \\(M\\) about the direct \\(I \\rightarrow D\\) pathway. We see this dynamic at work in a case with low inequality and democratization. Where we are only learning about \\(M\\) as a moderator of \\(I\\)’s direct effect (monotonicity assumption in place), observing \\(M=0\\) shifts our beliefs in favor of \\(I\\)’s negative effect. But where we are learning about \\(M\\) as both mediator and moderator, observing \\(M=0\\) shifts our beliefs against \\(I\\)’s negative effect. The reason for this latter result is straightforward: if \\(I=0\\) and we then see \\(M=0\\), then we have just learned that inequality’s possible indirect negative effect, running via the mobilization pathway, has not in fact occurred; and this has a considerable downward effect on our beliefs in an overall negative effect of inequality. This learning outweighs the small positive impact of observing \\(M=0\\) on our confidence that \\(I\\) had a direct negative effect on \\(D\\). We see these differences most clearly in the cases of Albania (as compared to Mexico) and Nicaragua (as compared to Taiwan). Under priors fully constrained to monotonic causal effects, we saw that the mediator clue, \\(M\\), made only a small difference to our inferences. However, if we allow for a negative effect of \\(I\\) on \\(M\\), even while believing it to be unlikely, observing mobilization in Albania and Nicaragua makes us substantially more confident that inequality mattered, and differentiates our conclusions about these cases more sharply from our conclusions about Mexico and Taiwan, respectively. 15.4 Monotonic restrictions Compare fit between model with and without monotonic restrictions References "],
["final-words.html", "Chapter 16 Final Words 16.1 General lessons 16.2 Worries about what you have to put in 16.3 Limits on what you can get out 16.4 A world of models: Practical steps forward for collective cumulation", " Chapter 16 Final Words The central idea of this book is that many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account of how and under what conditions causal relations play out. The focus on design based inference that has grown in strength over the last decade has made it possible to dispense with such models, but the cost has been a narrowing of questions to focus on varients of the average causal effect. A shift in focus towards treating causal models as estimands holds out the promise of addressing a wide set of questions in an integrated way: Case level questions: did \\(X\\) affect \\(Y\\) in this instance? Process questions: did \\(X\\) affect \\(Y\\) in this instance? Transportation questions: what are the implications of the study of this treatment in this place for that treatment in that place? In addition the models provide a clear procedure for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw. Even when quantities are not identified they can be used to assess when conclusions are consistent with the data for different priors. In providing a structure for these tasks we also gain insight into problems with how we were thinking of these things before… Includes that priors and probative value can’t be independent. 16.1 General lessons Sometimes understanding and applying teh causal model is enoug, without need for additional qualitative data. 16.2 Worries about what you have to put in Well defined nodes? Do DAGs actually capture causal processes that qualitative researchers see – qualitative researchers see that the domino 2 fell the moment it was hit by domino 1. How do we express this in a DAG? Acyclic really? THe first assumption made in the construction of causal models in this book is the underlying DAG. One can specify a DAG without making any substantive claims about function forms or patterns of confounding. Yet even the DAG presents worries. Theoretically deeper models. 16.3 Limits on what you can get out Complexity. We have sought to use non-parametric models.. To maintain simplicity we have largely focused on models with binary nodes. At first blush this class of causal models appears very simple. In fact however we quickly learn that even with a small set of nodes produces a dizzying variety of causal types. Identification. In modelling complete structures we see clearly how much easier it is to define problems than it is to solve them. Many of the quantities we care about are easily shown to not be identified by the causal models we employ. The causes of effect estimand is perhaps the most obvious of these. Limits of qualitative data under ignorable assignments. You generally cannot conclude all that much about population quantiries from only a small number of cases when causal effects are identified. 16.4 A world of models: Practical steps forward for collective cumulation "],
["examplesappendix.html", "Chapter 17 Analysis of canonical models with gbiqq 17.1 \\(X\\) causes \\(Y\\), no confounding 17.2 \\(X\\) causes \\(Y\\), with unmodelled confounding 17.3 \\(X\\) causes \\(Y\\), with confounding modelled 17.4 Simple mediation model 17.5 Simple moderator model 17.6 An IV model 17.7 A model that allows application of the frontdoor criterion 17.8 A model with a violation of sequential ignorability 17.9 Learning from a collider 17.10 A model mixing observational and experimental data 17.11 Transportation of findings across contexts", " Chapter 17 Analysis of canonical models with gbiqq We walk through a set of canonical models and show how to define and analyze them using gbiqq. 17.1 \\(X\\) causes \\(Y\\), no confounding In the simplest \\(X\\) causes \\(Y\\) model the ATE is identified but the “probability of causation” (PC) is not: we can however generally place bounds on PC. The model can be written: model &lt;- make_model(&quot;X -&gt; Y&quot;) plot_dag(model) This sparse definition assumes that there is no confounding and no constraints on the ways \\(X\\) relates to \\(Y\\). You can see the parameter matrix, which confirms this, showing the mapping from parameters to causal types: Table 17.1: Parameter matrix for X causes Y model without confounding X0.Y00 X1.Y00 X0.Y10 X1.Y10 X0.Y01 X1.Y01 X0.Y11 X1.Y11 X0 1 0 1 0 1 0 1 0 X1 0 1 0 1 0 1 0 1 Y00 1 1 0 0 0 0 0 0 Y10 0 0 1 1 0 0 0 0 Y01 0 0 0 0 1 1 0 0 Y11 0 0 0 0 0 0 1 1 We can simulate data using the bare bones model and assuming a “true model” in which there is a true positive effect of 0.5. data &lt;- simulate_data(model, n = 1000, parameters = c(.5, .5, .2, .1, .6, .1)) The kinds of inferences on the probability that \\(X\\) has a positive effect on \\(Y\\) given different data is calculated as follows: updated &lt;- gbiqq(model, data) We can then ask questions about particular estimands like this: ATE &lt;- &quot;Y[X=1] - Y[X=0]&quot; PC &lt;- &quot;Y[X=1] &gt; Y[X=0]&quot; results &lt;- gbiqq::query_model( updated, queries = list(ATE = ATE, ATE = ATE, PC = PC, PC = PC), using = list(&quot;priors&quot;, &quot;posteriors&quot;)) Query Subset Using mean sd ATE All priors 0.00 0.32 ATE All posteriors 0.53 0.03 PC All priors 0.25 0.20 PC All posteriors 0.63 0.06 We see from the posterior variance on PC that PC is not identified (or more precisely they key feature is that this distribution does not tighten even with very large N). For more intuition we graph the posteriors: We find that they do not converge but they do place positive mass in the right range. Within this range, the shape of the posterior depends on the priors only. 17.2 \\(X\\) causes \\(Y\\), with unmodelled confounding An \\(X\\) causes \\(Y\\) model with confounding can be written: model &lt;- make_model(&quot;X -&gt; Y&quot;) %&gt;% set_confound(list(X = &quot;(Y[X=1]&gt;Y[X=0])&quot;, X = &quot;(Y[X=1]&lt;Y[X=0])&quot;, X = &quot;(Y[X=1] ==1)&quot;)) plot_dag(model) The parameter matrix here has more parameters than nodal types, reflecting the conditional assignment probabilities of \\(X\\) – \\(X\\) can have different assignment probabilities for different nodal types for \\(Y\\). Table 17.2: Parameter matrix for X causes Y model with arbitrary confounding X0.Y00 X1.Y00 X0.Y10 X1.Y10 X0.Y01 X1.Y01 X0.Y11 X1.Y11 X0 0 0 0 0 0 0 1 0 X1 0 0 0 0 0 0 0 1 X0 0 0 1 0 0 0 0 0 X1 0 0 0 1 0 0 0 0 X0 0 0 0 0 1 0 0 0 X1 0 0 0 0 0 1 0 0 X0 1 0 0 0 0 0 0 0 X1 0 1 0 0 0 0 0 0 Y00 1 1 0 0 0 0 0 0 Y10 0 0 1 1 0 0 0 0 Y01 0 0 0 0 1 1 0 0 Y11 0 0 0 0 0 0 1 1 With the possibility of any type of confounding, the best we can do is place “Mansky bounds” on the average causal effect. To see this, let’s plot a histogram of our posterior on average causal effects, given lots of data: data &lt;- simulate_data( model, n = 1000, parameters = c(.5, .5, .5, .5, .5, .5, .5, .5, .1, .1, .7, .1)) updated &lt;- gbiqq(model, data, refresh = 0) ## Prior distribution added to model The key thing here is that the posterior on the ATE has shifted, as it should, but it is not tight, even with large data. In fact the distribution of the posterior covers one unit of the range between -1 and 1. 17.3 \\(X\\) causes \\(Y\\), with confounding modelled Say now we have a theory that the relationship between \\(X\\) and \\(Y\\) is confounded by unobserved variable \\(C\\). Although \\(C\\) is unobserved we can still include it in the model and observe the confounding it generates by estimating the model on data generated by the model but assuming that we cannot observe \\(C\\). model &lt;- make_model(&quot;C -&gt; X -&gt; Y &lt;- C&quot;) %&gt;% set_restrictions(c( &quot;(Y[X=1] &lt; Y[X=0]) | (Y[C=1] &lt; Y[C=0])&quot;, &quot;(X[C=1] &lt; X[C=0])&quot;)) %&gt;% set_parameters(type = &quot;prior_mean&quot;) The ATE estimand in this case is given by: Query Subset Using mean ATE All parameters 0.333 In the first column below we run a regression using data generated from this model but with \\(C\\) unobserved. The second column shows what we would estimate if were able to observe \\(C\\). Dependent variable: Y (1) (2) X 0.443*** 0.328*** (0.009) (0.009) C 0.336*** (0.009) Constant 0.276*** 0.166*** (0.006) (0.007) Observations 10,000 10,000 R2 0.197 0.296 Adjusted R2 0.196 0.296 Residual Std. Error 0.448 (df = 9998) 0.420 (df = 9997) F Statistic 2,446.000*** (df = 1; 9998) 2,103.000*** (df = 2; 9997) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Our posteriors over the effect of \\(X\\) on \\(Y\\) and the effect of the unobserved confounder (\\(C\\)) on \\(Y\\) have a joint distributed with negative covariance. To illustrate we will use the same data but assume priors from model where we do not restrict the relationship between \\(C\\) and \\(Y\\) and show the joint distribution of our posteriors. model &lt;- make_model(&quot;C -&gt; X -&gt; Y &lt;- C&quot;) %&gt;% set_restrictions(&quot;(X[C=1] &lt; X[C=0])&quot;) 17.4 Simple mediation model We define a simple mediation model and illustrate learning about whether \\(X=1\\) caused \\(Y=1\\) from observations of \\(M\\). model &lt;- make_model(&quot;X -&gt; M -&gt; Y&quot;) %&gt;% set_confound(confound = list(X = &quot;M[X=1]==1&quot;)) %&gt;% set_parameters(c(.5, .5, .2, .8, .2, 0, .8, 0, .2, 0, .8, 0)) plot_dag(model) Data and estimation: data &lt;- simulate_data(model, n = 1000, using = &quot;parameters&quot;) updated &lt;- gbiqq(model, data) result &lt;- gbiqq::query_model( updated, queries = list(COE = &quot;c(Y[X=1] &gt; Y[X=0])&quot;), subsets = c(&quot;X==1 &amp; Y==1&quot;, &quot;X==1 &amp; Y==1 &amp; M==0&quot;, &quot;X==1 &amp; Y==1 &amp; M==1&quot;), using = &quot;posteriors&quot;) Query Subset Using mean sd COE X==1 &amp; Y==1 posteriors 0.913 0.192 COE X==1 &amp; Y==1 &amp; M==0 posteriors 0.039 0.106 COE X==1 &amp; Y==1 &amp; M==1 posteriors 0.914 0.192 Note that observation of \\(M=0\\) results in a 0 probability for the posterior that \\(X\\) caused \\(Y\\), while observation of \\(M=1\\) has only a modest positive effect. The mediator thus provides a hoop test for the proposition that \\(X\\) caused \\(Y\\). 17.5 Simple moderator model We define a simple model with a moderator and illustrate how updating about COE is possible using the value of a mediator as a clue. model &lt;- make_model(&quot;X -&gt; Y; Z -&gt; Y&quot;) plot_dag(model) data &lt;- simulate_data( model, n = 1000, parameters = c(.5, .5, .5, .5, .02, .02, .02, .02, .02, .02, .02, .02, .02, .70, .02, .02, .02, .02, .02, .02)) posterior &lt;- gbiqq(model, data) result &lt;- gbiqq::query_model( updated, queries = list(COE = &quot;Y[X=1] &gt; Y[X=0]&quot;), subsets = list(&quot;X==1 &amp; Y==1&quot;, &quot;X==1 &amp; Y==1 &amp; Z==0&quot;, &quot;X==1 &amp; Y==1 &amp; Z==1&quot;), using = &quot;posteriors&quot;) Query Subset Using mean sd COE X==1 &amp; Y==1 posteriors 0.795 0.044 COE X==1 &amp; Y==1 &amp; Z==0 posteriors 0.404 0.143 COE X==1 &amp; Y==1 &amp; Z==1 posteriors 0.883 0.037 As an exercise, define a model where, learning about a model with moderators allows you to tighten bounds on COE even without observing the value of the mediator. 17.6 An IV model We define a simple mediation model and illustrate learning about whether \\(X=1\\) caused \\(Y=1\\) from observations of \\(M\\). model &lt;- make_model(&quot;X -&gt; M -&gt; Y&quot;) %&gt;% set_confound(confound = list(M = &quot;Y[M=1]==1&quot;)) plot_dag(model) result &lt;- gbiqq::query_model( updated, queries = list(ATE = &quot;c(Y[M=1] - Y[M=0])&quot;), subsets = list(TRUE, &quot;M[X=1] &gt; M[X=0]&quot;, &quot;M==0&quot;, &quot;M==1&quot;), using = &quot;posteriors&quot;) Query Subset Using mean sd ATE All posteriors 0.590 0.063 ATE M[X=1] &gt; M[X=0] posteriors 0.592 0.071 ATE M==0 posteriors 0.571 0.120 ATE M==1 posteriors 0.607 0.036 We calculate the average causal effect for all and for the compliers and conditional on values of \\(M\\). 17.7 A model that allows application of the frontdoor criterion 17.8 A model with a violation of sequential ignorability 17.9 Learning from a collider Pearl describes a model similar to the following as a case for which controlling for covariate \\(W\\) induces bias in the estimation of the effect of \\(X\\) on \\(Y\\), which could otherwise be esimated without bias. model &lt;- make_model(&quot;X -&gt; Y &lt;- S; S -&gt; W&quot;) %&gt;% set_confound(list(X = &quot;W[S=1]&gt;W[S=0]&quot;)) %&gt;% set_parameters(parameters = c(.1, .9, .5, .5, .9, .1, .1,.1,.7,.1, .2, 0,0,0, 0,0,0,0, .6,0,0,0, 0,0,0,.2)) plot_dag(model) data &lt;- simulate_data(model, n = 20000) data$S &lt;- NA The true effect of \\(X\\) on \\(Y\\) is .3 but the PC is quite different for units with \\(W=0\\) and \\(W=1\\): Query Subset Using mean Y(1)-Y(0) All parameters 0.300 Y(1)-Y(0) X==1 &amp; Y==1 parameters 0.600 Y(1)-Y(0) X==1 &amp; Y==1 &amp; W==0 parameters 0.083 Y(1)-Y(0) X==1 &amp; Y==1 &amp; W==1 parameters 0.744 These are the quantities we seek to recover. The ATE can be gotten fairly precisely in a simple regression. But controlling for \\(W\\) introduces bias both for the unconditional and the conditional effects of \\(X\\): Dependent variable: Y (1) (2) (3) X 0.300*** 0.302*** 0.019** (0.007) (0.006) (0.009) W 0.373*** -0.004 (0.006) (0.010) X:W 0.571*** (0.012) Constant 0.198*** 0.013** 0.200*** (0.006) (0.006) (0.007) Observations 20,000 20,000 20,000 R2 0.084 0.230 0.306 Adjusted R2 0.084 0.230 0.306 Residual Std. Error 0.468 (df = 19998) 0.429 (df = 19997) 0.407 (df = 19996) F Statistic 1,845.000*** (df = 1; 19998) 2,989.000*** (df = 2; 19997) 2,945.000*** (df = 3; 19996) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 How does the Bayesian model do, with and without data on \\(W\\)? Inferences that do not use \\(W\\) get ATE right on average, but PC is not identified and statements about PC conditional on \\(W\\) are not possible: Without \\(W\\): Table 17.3: Collider excluded from model Query Subset Using mean sd Y(1)-Y(0) All posteriors 0.301 0.007 Y(1)-Y(0) X==1 &amp; Y==1 posteriors 0.798 0.056 Y(1)-Y(0) X==1 &amp; Y==1 &amp; W==0 posteriors 0.792 0.066 Y(1)-Y(0) X==1 &amp; Y==1 &amp; W==1 posteriors 0.793 0.065 We see including the collider does not induce error in estimation of the ATE, even though it does in a regression framework. It provides an ability to make different PC case level claims given W, but these are nevertheless far off in this example because we still do not have identification. With \\(W\\): Table 17.4: Collider included in model Query Subset Using mean sd Y(1)-Y(0) All posteriors 0.302 0.006 Y(1)-Y(0) X==1 &amp; Y==1 posteriors 0.792 0.039 Y(1)-Y(0) X==1 &amp; Y==1 &amp; W==0 posteriors 0.626 0.121 Y(1)-Y(0) X==1 &amp; Y==1 &amp; W==1 posteriors 0.839 0.039 17.10 A model mixing observational and experimental data We imagine that node \\(R\\) indicates whether a unit was assigned to be randomly assigned to treatment assignment (\\(X=Z\\) if \\(R=1\\)) or took on its observational value (\\(X=O\\) if \\(R=0\\)). We assume the exclusion restriction that entering the experimental sample is not related to \\(Y\\) other than through assignment of \\(X\\). model &lt;- make_model(&quot;R -&gt; X; O -&gt;X; Z -&gt; X; X -&gt; Y&quot;, add_priors = FALSE) %&gt;% set_restrictions(&quot;(X[R=1, Z=0]!=0) | (X[R=1, Z=1]!=1) | (X[R=0, O=0]!=0) | (X[R=0, O=1]!=1)&quot;) %&gt;% set_priors() %&gt;% set_confound(list(O = &quot;(Y[X=1] &gt; Y[X=0])&quot;, O = &quot;(Y[X=1] &lt; Y[X=0])&quot;, O = &quot;(Y[X=1] == 1)&quot;)) plot_dag(model) The parameter matrix has just one type for \\(X\\) since \\(X\\) really operates here as a kind of switch, inheriting the value of \\(Z\\) or \\(O\\) depending on \\(R\\). Parameters allow for complete confounding between \\(O\\) and \\(Y\\) by \\(Z\\) and \\(Y\\) are unconfounded. model &lt;- set_parameters(model, c(.2, .8, .8, .2, .2, .8, .8, .2, .5, .5, .5, .5, 1, .2, .2, .4, .2)) The estimands: Query Subset Using mean ATE All parameters 0.2 ATE R==0 parameters 0.2 ATE R==1 parameters 0.2 The priors: Query Subset Using mean sd ATE All priors -0.006 0.326 ATE R==0 priors -0.006 0.326 ATE R==1 priors -0.006 0.326 Data: data &lt;- simulate_data(model, n = 600) # Uncomment if data on $O$ is not available for cases assigned to $R=1$. # data$O[data$R == 1] &lt;- NA The true effect is .2 but naive analysis on the observational data would yield a srtongly upwardly biased estimate. The gbiqq estimates are: posterior &lt;- gbiqq(model, data) Query Subset Using mean sd ATE All posteriors 0.19 0.048 ATE R==0 posteriors 0.19 0.048 ATE R==1 posteriors 0.19 0.048 Did observational data improve the estimates from the experimental data? posterior &lt;- gbiqq(model, data[data$R==1,]) Query Subset Using mean sd ATE All posteriors 0.227 0.053 ATE R==0 posteriors 0.227 0.053 ATE R==1 posteriors 0.227 0.053 A key quantity of interest from this model is the average effect of treatment conditional on being in treatment in the observational group. We have: Query Subset Using mean sd ATE R==1 &amp; X==0 posteriors 0.190 0.048 ATE R==1 &amp; X==1 posteriors 0.190 0.048 ATE R==0 &amp; X==0 posteriors -0.122 0.072 ATE R==0 &amp; X==1 posteriors 0.414 0.060 17.11 Transportation of findings across contexts We study the effect of \\(X\\) on \\(Y\\) in country 1 and want to make inferences to country 2, Our problem however is that countries differ in terms of some feature, \\(W\\), that is distributed differently in the two countries and that affects \\(Y\\) via some mechanism \\(K\\). [[We assume that we have an experiment in country 1 but only observational variation in country 2.]] For instance, \\(X\\) is cash and \\(Y\\) is welfare. \\(W\\) is background levels of conflict which affects welfare via security \\(K\\), possibly differently in both countries. Although they differ, we have the following encompassing theory for both countries. model &lt;- make_model(&quot;W -&gt; K -&gt; Y &lt;- X&quot;) "],
["references.html", "References", " References "]
]
