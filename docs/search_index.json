[["mixing.html", "Chapter 9 Integrated inferences 9.1 Theres only ever one case 9.2 General procedure 9.3 Illustrated inferences 9.4 Considerations 9.5 Rules of thumb for reasoning about learning 9.6 Conclusion", " Chapter 9 Integrated inferences We extend the analysis of Chapter 7 to cases with population data. In these cases we get to learn about the distribution of causal effects and are able to update the models we use for case level inference. The main goal of this chapter is to generalize the model developed in Chapter 7 to research situations in which we have data on multiple cases. In doing so we generalize the model in Humphreys and Jacobs (2015) to one that in which rather than the probative value of clues being assumed, they are derived from a causal structure. We start however with a conceptual point: the exact structure introduced in Chapter 6 for single case analysis can be used as is for multi-case analysis. To see this you should think of the the nodes as vector-valued, and the estimands as just a particular summary of the vector-valued case level causal effects. Thought of this way the conceptual work for mixed methods inference from models has been done already and our goal here is more technicalhow to exploit assumptions on independence across cases to generate simpler theories of repeated phenomena. 9.1 Theres only ever one case Conceptualized correctly, there is no difference at all between the data types or the inference used in within-case and cross-case inference. The reason is not, as King, Keohane, and Verba (1994) suggest, that all causal inference is fundamentally correlational, even in seemingly single case studies. Nor is the point that, looked at carefully, single case studies can be disaggregated into many cases. The intuition runs in the opposite direction: fundamentally, model-based inference always involves comparing a pattern of data with the logic of the model. Looked at carefully, studies with multiple cases can be conceptualized of as single-case studies: the drawing of inferences from a single collection of clues. The key insight is that, when we move from a causal model with one observation to a causal model with multiple observations, all we need to do is replace nodes with a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors). In practice of course we do more than this: we introduce independence assumptions which allow us to work with a much more simplified parameter space than we would have if we operated as if we had only one case. To illustrate the idea that multi-case studies are really single-case studies with vector valued variables, consider the following situation (described first at the unit level). There are two units studied, drawn from some population, a binary treatment \\(X\\) is assigned independently with probability .5 to each case; an outcome \\(Y\\) along with clue variable \\(K\\) is observable. We suppose \\(X\\) can affect \\(Y\\) and in addition there is a background, unobserved, variable \\(\\theta\\) (causal type) that takes on values in \\(\\{a,b,c,d\\}\\), that affects both \\(K\\) and \\(Y\\). We will assume that \\(\\theta\\) is not independently assigned and that the two units are more likely to have the same values of \\(\\theta\\) than different values. For simplicity, we will suppose that for any given case \\(K=1\\) whenever \\(X\\) causes \\(Y\\), and \\(K=1\\) with a 50% probability otherwise. Thus, \\(K\\) is informative about a units causal type. Note that we have described the problem at the unit level. We can redescribe it at the population level however as a situation in which a treatment vector \\(X\\) can take on one of four values, \\((0,0), (0,1), (1,0), (1,1)\\) with equal probability (or more strictly: as determined by \\(\\theta\\)). \\(\\theta\\) is also a vector with two elements that can take on one of 16 values \\((a,a), (a,b),\\dots (d,d)\\) as determined by \\(U_\\theta\\). In this representation we could assume that the 16 possibilities are not equally likely, which captures the failure of independence in the unit level assignments. \\(Y\\) is a vector that reflects the elements of \\(\\theta\\) and \\(X\\) in the obvious way (e.g \\(X=(0,0), \\theta=(a,b)\\) generates outcomes \\(Y=(1,0)\\); though it is immediately obvious that representing nodes in vector forms allows for more general vector-level mappings to allow for SUTVA violations. \\(K\\) has the same domain as \\(X\\) and \\(Y\\), and element \\(K[j]=1\\) if \\(\\theta[j]=b\\). Note that to describe the estimand, the Sample Average Treatment Effect, we also need to consider operations and queries defined at the vector level. In practice we consider three operations, one in which both units have \\(X\\) forced to 0 and two in which one unit has \\(X\\) set to 0 and the other has \\(X\\) set to 1. Thus we are interested in the average effect of changing one unit to treatment while the other is held in control. Note also that before our estimands were binaryof the form: is it a \\(b\\) type?and our answer was a probability; now our estimand is categorical and our answer is a distribution (what is the probability the SATE is 0, what is the probability the SATE is .5, etc) Represented in this way we can use the tools of Chapters 6 and 7 to fully examine this seemingly multi-case study. 9.2 General procedure In practice however thinking of nodes as capturing the outcomes on all units leads to enormous complexity. For example an exogeneous variable \\(X\\) which takes on values of 0 or 1 at random for 10 units has \\(2^{10}\\) types in this conceptualization, rather than just two when thought of at the case level. We reduce complexity however by thinking of models as operating on units and learning about models by observing multiple realizations of processes covered by the model, rather than just one. Thinking about it this way is not free however as it requires invoking independence assumptions  that outcomes in two units do not depend on each other. If we cannot stand by that assumption, then we have to build independence failures into our models. Taking this step the procedure we now use in the mixed methods works as follows: INTROUDCE CONCEPTS AND THEN ILLUSTRATE A DAG. As for process tracing, we begin with a graphical causal model specifying possible causal linkages between nodes. Our chain model for instance has DAG: \\(X \\rightarrow M \\rightarrow Y\\). Nodal types. Just as in process tracing, the DAG and variable ranges define the set of possible nodal types in the modelthe possible ways in which each variable is assigned (if exogenous) or determined by its parents (if endogenous). Causal types. And, again, a full set of nodal types gives rise to a full set of causal types, encompassing all possible combinations of nodal types across all nodes in the model. Priors. The first difference between single- and multiple-case inference lies in how we set priors on causal types. In process tracing, we set parameter values for each nodal type (or conditional nodal type, for unobserved confounding). Our parameterse.g., \\(\\lambda^X_0\\), \\(\\lambda^Y_{01}\\)represent our beliefs about the proportions of these types in the population. When we only observe a single data typedata on a single casewe do not have sufficient information to learn about the distribution of types in the population. And so we treat these population-level beliefs as fixed parameters, rather than priors that we update on. (What we update on, in process tracing, is our priors on whether a given case is of a particular type or set of types.) Likewise, uncertainty about those population-level parameters has no effect on our inferences for a single case. When we get to observe data on multiple cases, however, we have the opportunity to learn both about the cases at hand and about the population. Moreover, our level of uncertainty about population-level parameters will shape our inferences. We thus want our parameters (the \\(\\lambda\\)s) to be drawn from a prior distribution  a distribution that expresses our uncertainty and over which we can update once we see the data. While different distributions may be appropriate to the task in general, uncertainty over proportions (of cases, events, etc.) falling into a set of discrete categories is usefully described by a Dirichlet distribution, as discussed in Chapter 6. The parameters of a Dirichlet distribution (the \\(\\alpha\\)s) can be thought of as conveying both the relative expected proportions in each category and our degree of uncertainty. ADD MULTILEVEL GRAPH Box: Setting priors For a model with no unobserved confounding, setting priors requires specifying a prior distribution for each set of nodal types. Parameters are provided as vectors of positive numbers with one number for each nodal type. These numbers correspond to the \\(\\alpha\\) parameters of a Dirichlet distribution. The relative size of each number governs the the relative probability of each nodal type. The absolute sizes govern the certainty over the types. To wit. For a simple \\(X \\rightarrow Y\\) model, we have two parameter sets: one for \\(X\\)s types and one for \\(Y\\)s types. For \\(X\\)s types, we specify \\(\\alpha^X_0\\) and \\(\\alpha^X_1\\), corresponding to the nodal types \\(\\theta^X_0\\) and \\(\\theta^X_1\\), respectively. A distribution of the form (\\(\\alpha^X_0=100, \\alpha^X_1=100)\\) implies a lot of confidence that a given unit has \\(X=1\\) with probability .5. A distribution of the form (\\(\\alpha^X_0=.1, \\alpha^X_1=.1)\\) implies that either \\(X=1\\) with a high probability (for all units) or \\(X=0\\) with a low probability (for all units), but we are not sure which. For \\(Y\\)s types, we specify \\(\\alpha^Y_{00}\\), \\(\\alpha^Y_{10}\\), \\(\\alpha^Y_{01}\\), and \\(\\alpha^Y_{11}\\), corresponding to the nodal types \\(\\theta^Y_{00}\\), etc. So, for instance: \\(\\alpha^Y_{00}=1\\), \\(\\alpha^Y_{10}=1\\), \\(\\alpha^Y_{01}=1\\), and \\(\\alpha^Y_{11}=1\\) yields a uniform distribution in which all share allocations of types in the population are equally likely. \\(\\alpha^Y_{00}=3\\), \\(\\alpha^Y_{10}=3\\), \\(\\alpha^Y_{01}=3\\), and \\(\\alpha^Y_{11}=3\\) puts more weight on share allocations in which the shares are relatively equal. \\(\\alpha^Y_{00}=5\\), \\(\\alpha^Y_{10}=5\\), \\(\\alpha^Y_{01}=10\\), and \\(\\alpha^Y_{11}=5\\) puts greater weight positive causal effects than the other three types. Unobserved confounding. When there is unobserved confounding, we need parameter sets that allow for a joint distribution over nodal types. Thus, if we believe the likelihood of \\(X=1\\) is correlated with whether or not \\(X\\) has a positive effect on \\(Y\\), we will need two parameter sets (rather than one) for \\(X\\): one for \\(X\\)s value when \\(\\theta^Y = \\theta^Y_{01}\\) and one for \\(X\\)s value when \\(\\theta^Y \\neq \\theta^Y_{01}\\). For each of these parameter sets, we specify two \\(\\alpha\\) parameters representing our beliefs about \\(X\\)s assignment. We can draw \\(\\lambda\\) values for these conditional nodal types from the resulting Dirichlet distributions, as above, and can then calculate causal type probabilities in the usual way. Distributions over causal types. For a model with any number of nodes, we can then imagine a draw of one \\(\\lambda^j\\) from its prior distribution for each node, giving a full \\(\\lambda\\) vector. Any particular \\(\\lambda\\) vector, in turn, implies a probability distribution over causal types (\\(\\theta\\)). With the help of a parameter matrix (mapping from parameters to causal types), we can then, just as with process tracing, calculate the prior probability that a case is of any particular causal type, given the parameter (\\(\\lambda\\)) values we have drawn. Implicitly, then, our prior distribution over \\(\\lambda\\) gives rise in turn to a prior distribution over the causal type shares in the population. Event probabilities. We now need to build a likelihood function that can map from beliefs about the world to data: i.e., that can tell us how likely we are to see a given data patternacross multiple casesunder a given distribution of causal types in the population. The first step in building the likelihood function is to calculate event probabilities: the probability of observing a case of a particular data type given a particular population-level distribution of causal type shares (that is, given a \\(\\lambda\\) draw). We assume, for now, that we deploy the same data strategy for each case, collecting data on all nodes. We denote an event probability for a given data pattern for variables \\(X, Y, \\dots\\) as \\(w_{x, y, \\dots}\\). For instance, the probability of observing \\(X=0, Y=1\\) in a case (given \\(\\lambda\\)) is \\(w_{01}\\). An ambiguity matrix, just as for process tracing, tells us which causal types are consistent with a particular data type, as observed for a single case. To calculate the probability of the data given a distribution of causal types, we simply add together the probabilities of all of the causal types with which it is consistent. See, for instance, the parameter matrix and the ambiguity matrix in Tables 9.1 and 9.2. We have indicated a single draw of \\(\\lambda\\) values (population type shares) in the parameter matrix, and these have been used to calculate the priors on causal types provided in the ambiguity matrix. Lets now calculate the event probability for each data type. Starting with \\(X=0, Y=0\\), we can read off the ambiguity matrix that the consistent causal types are (\\(\\theta^X_0, \\theta^Y_{00}\\)) and (\\(\\theta^X_0, \\theta^Y_{01}\\)). The event probability, \\(w_{00}\\), is then given by adding together the probabilities of these two causal types, \\(0.1 + 0.2 = 0.3\\). All four event probabilities, for the four data types, are then calculated in the same way: \\(w_{00} = 0.1 + 0.2 = 0.3\\) \\(w_{10} = 0.1 + 0.1 = 0.2\\) \\(w_{01} = 0.1 + 0.2 = 0.2\\) \\(w_{11} = 0.2 + 0.1 = 0.3\\) As any case must be of one and only one data type, the full set of event probabilities for a single \\(\\lambda\\) draw must naturally sum to \\(1\\). Table 9.1: A parameter matrix for a simple \\(X ightarrow Y\\) model (with no unobserved confounding), indicating a single draw of \\(\\lambda\\) values from the prior distribution. X0.Y00 X1.Y00 X0.Y10 X1.Y10 X0.Y01 X1.Y01 X0.Y11 X1.Y11 Shares.param_names Shares.param_value Shares.param_set Shares.node Shares.nodal_type Shares.gen Shares.priors X.0 1 0 1 0 1 0 1 0 X.0 0.4 X X 0 1 1 X.1 0 1 0 1 0 1 0 1 X.1 0.6 X X 1 1 1 Y.00 1 1 0 0 0 0 0 0 Y.00 0.3 Y Y 00 2 1 Y.10 0 0 1 1 0 0 0 0 Y.10 0.2 Y Y 10 2 1 Y.01 0 0 0 0 1 1 0 0 Y.01 0.2 Y Y 01 2 1 Y.11 0 0 0 0 0 0 1 1 Y.11 0.3 Y Y 11 2 1 Table 9.2: An ambiguity matrix for a simple \\(X ightarrow Y\\) model (with no unobserved confounding), showing the priors over causal types arising from a single draw of \\(\\lambda\\) from its prior distribution. X0Y0 X1Y0 X0Y1 X1Y1 prior X0Y00 1 0 0 0 0.12 X1Y00 0 1 0 0 0.18 X0Y10 0 0 1 0 0.08 X1Y10 0 1 0 0 0.12 X0Y01 1 0 0 0 0.08 X1Y01 0 0 0 1 0.12 X0Y11 0 0 1 0 0.12 X1Y11 0 0 0 1 0.18 For a case in which only partial data are observed, we follow the same basic logic as with partial process-tracing data. We retain all columns (data types) in the ambiguity matrix that are consistent with the partial data. So, for instance, if we observe only \\(Y=1\\), we would retain both the \\(X=0, Y=1\\) column and the \\(X=1, Y=1\\) column. We then calculate the event probability by summing causal-type probabilities for all causal types that could have produced these partial data  i.e., all those with a \\(1\\) in either column. Likelihood. Now that we know the probability of observing each data pattern in a single case given \\(\\lambda\\), we can use these event probabilities to aggregate up to the likelihood of observing a data pattern across multiple cases (given \\(\\lambda\\)). With discrete variables, we can think of a given multiple-case data pattern simply as a set of counts: for, say, \\(X, Y\\) data, we will observe a certain number of \\(X=0, Y=0\\) cases (\\(n_{00}\\)), a certain number of \\(X=1, Y=0\\) cases (\\(n_{10}\\)), a certain number of \\(X=0, Y=1\\) cases (\\(n_{01}\\)), and a certain number of \\(X=1, Y=1\\) cases (\\(n_{11}\\)). A data pattern, given a particular set of variables observed (a search strategy), thus has a multinomial distribution. The likelihood of a data pattern under a given search strategy, in turn, takes the form of a multinomial distribution conditional on the number of cases observed and the event probabilities for each data type, given a \\(\\lambda\\) draw. Let us assume now that we have a 3-node model, with \\(X, Y\\), and \\(M\\) all binary. Let \\(n_{XYK}\\) denote an 8-element vector recording the number of cases in a sample displaying each possible combination of \\(X,Y,K\\) data, thus: \\(n_{XYM}=(n_{000},n_{001},n_{100},\\dots ,n_{111})\\). The elements of \\(n_{XYK}\\) sum to \\(n\\), the total number of cases studied. Likewise, let the event probabilities for data types given \\(\\lambda\\) be registered in a vector, \\(w_{XYK}=(w_{000},w_{001},w_{100},\\dots ,w_{111})\\). The likelihood of a data pattern, \\(\\mathcal D\\) is then: \\[ \\Pr(\\mathcal{D}|\\lambda) = \\text{Multinom}\\left(n_{XYK}|n, w_{XYK}\\right) \\\\ \\] In other words, the likelihood of observing a particular data pattern given \\(\\lambda\\) is given by the corresponding value of the multinomial distribution given the event probabilities. What if we have a mixture of search strategies? Suppose, for instance, that we have collected \\(X,Y\\) data on a set of cases, and that we have additionally collected data on \\(M\\) for a random subset of these. We can think of this as conducting quantitative analysis on a large sample and conducting in-depth process tracing on a subsample. We then can summarize our data in two vectors, the 8-element \\(n_{XYM}\\) vector for the cases with process tracing, and a 4-element vector \\(n_{XY*} = (n_{00*},n_{10*},n_{01*},n_{11*}\\) for the partial data on those cases with no process tracing. Likewise, we now have two sets of event probabilities: one for the cases with complete data, \\(w_{XYM}\\), and a 4-element vector for those with partial data, \\(w_{XY*}\\). Let \\(n\\) denote the total number of cases examined, and \\(k\\) the number for which we have data on \\(K\\). Now, assuming that each observed case represents an independent, random draw from the population, we can form the likelihood function as a product of multinomial distributions: \\[ \\Pr(\\mathcal{D}|\\theta) = \\text{Multinom}\\left(n_{XY*}|n-k, w_{XY*}\\right) \\times \\text{Multinom}\\left(n_{XYK}|k, w_{XYK}\\right) \\\\ \\] BOX 9.2.0.1 Likelihood and sampling Say a data strategy seeks data on \\(X\\) and \\(Y\\) in 2 cases and seeks data on \\(M\\) if ever \\(X=Y=1\\). The probability of each data type is as given in table below: type: prob: \\(X1M0Y1\\) \\(\\lambda^X_1(\\lambda^M_{00}+\\lambda^M_{10})(\\lambda^Y_{11}+\\lambda^Y_{10})\\) \\(X1M1Y1\\) \\(\\lambda^X_1(\\lambda^M_{11}+\\lambda^M_{01})(\\lambda^Y_{11}+\\lambda^Y_{01})\\) \\(X0Y0\\) \\(\\lambda^X_0(\\lambda^M_{00}+\\lambda^M_{01})(\\lambda^Y_{00}+\\lambda^Y_{01}) + \\lambda^X_0(\\lambda^M_{10}+\\lambda^M_{11})(\\lambda^Y_{00}+\\lambda^Y_{10})\\) \\(X0Y1\\) \\(\\lambda^X_0(\\lambda^M_{00}+\\lambda^M_{01})(\\lambda^Y_{10}+\\lambda^Y_{11}) + \\lambda^X_0(\\lambda^M_{10}+\\lambda^M_{11})(\\lambda^Y_{01}+\\lambda^Y_{11})\\) \\(X1Y0\\) \\(\\lambda^X_1(\\lambda^M_{00}+\\lambda^M_{10})(\\lambda^Y_{00}+\\lambda^Y_{01}) + \\lambda^X_1(\\lambda^M_{01}+\\lambda^M_{11})(\\lambda^Y_{00}+\\lambda^Y_{10})\\) The two observations can be thought of as a multinomial draw from these five event types. Alternatively they can also be thought of as the product of a draw from a strategy in which a set of units is drawn with observations on \\(X,Y\\) only and another set is drawn with observations on \\(X, M, Y\\). In the single multinomial view we have the probability of seeing data with \\(X=Y=0\\) in one case and \\(X=1, M=0, Y=1\\) in another is: \\(2P(X=0, Y=0)P(X=1, M=0, Y=1)\\) In the conditional strategy view we have \\(2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\\) In the two strategy view we have \\(P(X=0, Y=0)P(X=1, M=0, Y=1)\\) which is the same up to a constant. Say rather than conditioning \\(X=Y=1\\) to examine \\(M\\) one of the two cases were chosen at random to observe \\(M\\) and it just so happend to be be a case with \\(X=Y=1\\): type: prob: \\(X0Y0\\) \\(0.5\\lambda^X_0(\\lambda^M_{00}+\\lambda^M_{01})(\\lambda^Y_{00}+\\lambda^Y_{01}) + 0.5\\lambda^X_0(\\lambda^M_{10}+\\lambda^M_{11})(\\lambda^Y_{00}+\\lambda^Y_{10})\\) \\(X0Y1\\) \\(0.5\\lambda^X_0(\\lambda^M_{00}+\\lambda^M_{01})(\\lambda^Y_{10}+\\lambda^Y_{11}) + 0.5\\lambda^X_0(\\lambda^M_{10}+\\lambda^M_{11})(\\lambda^Y_{01}+\\lambda^Y_{11})\\) \\(X1Y0\\) \\(0.5\\lambda^X_1(\\lambda^M_{00}+\\lambda^M_{10})(\\lambda^Y_{00}+\\lambda^Y_{01}) + 0.5\\lambda^X_1(\\lambda^M_{01}+\\lambda^M_{11})(\\lambda^Y_{00}+\\lambda^Y_{10})\\) \\(X1Y1\\) \\(0.5\\lambda^X_1(\\lambda^M_{00}+\\lambda^M_{10})(\\lambda^Y_{10}+\\lambda^Y_{11}) + 0.5\\lambda^X_1(\\lambda^M_{11}+\\lambda^M_{01})(\\lambda^Y_{11}+\\lambda^Y_{01})\\) + \\(X0M0Y0\\) \\(0.5\\lambda^X_0(\\lambda^M_{00}+\\lambda^M_{01}))(\\lambda^Y_{00}+\\lambda^Y_{01})\\) \\(X0M1Y0\\) \\(0.5\\lambda^X_0(\\lambda^M_{11}+\\lambda^M_{10}))(\\lambda^Y_{00}+\\lambda^Y_{10})\\)  \\(X1M1Y1\\) \\(0.5\\lambda^X_1(\\lambda^M_{11}+\\lambda^M_{01})(\\lambda^Y_{11}+\\lambda^Y_{01})\\) In the single multinomial view we have the probability of seeing data with \\(X=Y=0\\) in one case and \\(X=1, M=0, Y=1\\) in another is now: \\(2P(X=0, Y=0)P(X=1, M=0, Y=1)\\) In the conditional strategy view we have \\(2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\\) In the two strategy view we have \\(P(X=0, Y=0)P(X=1, M=0, Y=1)\\) which is the same up to a constant. 9.2.1 Estimation WRITE GENERAL APPROACH WEITE BAYES RULE AND REFER BACK TO CHAPTER 9.2.2 Query Describe and illustrate 9.3 Illustrated inferences 9.3.1 Chain model show table with columns: * 14 columns: 10 nodel types, high abcd 4 * no data, * 1 data point on X=1 Y=1 only * 1 data point on X=1 Y=1 M = 1 * 10 data point on X=1 Y=1 only * 10 data point on X=1 Y=1 M = 1 9.4 Considerations 9.4.1 The identification problem Do you require identification to make progress What do you learn if things are not identified model &lt;- make_model(&quot;X1 -&gt; M1 -&gt; Y &lt;- M2 &lt;- X2&quot;) # restrict such that *only* M1 OR M2 could cause Y -- can we create a DD test? / achieve identification 9.4.2 Continuous data We can similarly shift from binary to continuous variable values through an expansion of the causal types. Suppose that \\(Y\\) can take on \\(m\\) possible values. With \\(k\\) explanatory variables, each taking on \\(r\\) possible values, we then have \\(m^{r^k}\\) causal types and, correspondingly, very many more elements in \\(\\phi\\). Naturally, in such situations, researchers might want to reduce complexity by placing structure onto the possible patterns of causal effects and clue probabilities, such as assuming a monotonic function linking effect sizes and clue probabilities. 9.4.3 Measurement error Model: \\[X \\rightarrow Y \\rightarrow Y_{\\text{measured}} \\leftarrow \\text{bad coding}\\] What if we are naive What if we are aware of risks but don;t know how bad bad coding is What if we observed bad coding in a subsample We have assumed no measurement error; in applications there could be considerable interest in measurement error. On one hand clue information may contain information about possible mismeasurement on \\(X\\) and \\(Y\\); on the other hand there might interest in whether measured clues adequately capture those features of a causal process that is thought to be measureable. The probability of different types of measurement error can be included among the set of parameters of interest, with likelihood functions adjusted accordingly. Suppose, for instance, that with probability \\(\\epsilon\\) a \\(Y=0\\) case is recorded as a \\(Y=1\\) case (and vice versa). Then the event probability of observing an \\(X=1\\),\\(Y=1\\) case, for example, is \\(\\epsilon \\lambda_a \\pi_a + (1-\\epsilon) \\lambda_b \\pi_b + \\epsilon \\lambda_c \\pi_c + (1-\\epsilon) \\lambda_d \\pi_d\\). %If instead there were measurement error on \\(X\\) but not on \\(Y\\), then the event probability would be: \\(\\epsilon \\lambda_a (1-\\pi_a) + (1-\\epsilon) \\lambda_b \\pi_b + \\epsilon \\lambda_d (1-\\pi_d) + (1-\\epsilon) \\lambda_d \\pi_d\\). Similar expressions can be derived for measurement error on \\(X\\) or \\(K\\). Specifying the problem in this way allows us both to take account of measurement error and learn about it. 9.4.4 Spillovers Spillovers may also be addressed through an appropriate definition of causal types. For example a unit \\(i\\) that is affected either by receiving treatment or via the treatment of a neighbor, \\(j\\), might have potential outcomes \\(Y_i(X_i,X_j)=\\max(X_i,X_j)\\) while another type that is not influenced by neighbor treatment status has \\(Y_i(X_i,X_j)=\\max(X_i)\\). With such a set-up, relevant clue information might discriminate between units affected by spillovers and those unaffected. 9.4.5 Clustering and other violations of independence 9.4.6 Parameteric models 9.5 Rules of thumb for reasoning about learning Learning requires uncertainty. And expected learning goes up as you become more uncertain about what youll find. If your causal model puts a very high probability on X having a positive effect on M, and you already know Xs value, you should expect to learn very little from observing M since youre very likely to see exactly the M value you expect given X. (Currently in Chap. 12)((And we want to make research design choices based on expected learning, not based on the mere possibility of learning: yes, our beliefs will shift if we look for M and find the unexpected value. But because that data-realization is highly unlikely, we expect the learning from observing M to be minimal. Pure within-case (or n=1) learning requires informative priors about the nodes to be observed. For instance, in a chain model, where we want to go and observe M, its not enough to have an informative prior about the X-&gt;Y relationship. We need an informative prior about the X-&gt;M or M-&gt;Y link in order to learn from M. For instance, are positive X-&gt;M effects more common than negative ones? Learning is possible in the absence of informative priors if we have n&gt;1 because it is sometimes possible to draw causal information from correlations. For instance, even if we have no idea what the distribution of X-&gt;M effects are, observing some correlation (or no correlation) between M and X across multiple cases provides information about the likelihood of an X-&gt;Y effect (even if it doesnt tell us about the direction of the effect). If there are different ways a query can be satisfied, evidence against one of those ways is evidence against the query as a whole. Say we have a two-path model  with one direct and one indirect path  and we want to know if X affects Y. We observe a mediator, M, along the indirect path in a set of cases. If the M data pattern is inconsistent with an indirect effect, then this is also evidence against an overall effect. In general, finding evidence against one way the effect can happen reduces our confidence in the effect happening at all. Prior beliefs structure the learning from new data. When confronted with new data, a prior will condition updating as though the prior were trying to preserve itself. (There are elements of this point in Chap 13  you learn more about heterogeneity if ATE is known.)((Say, were working with a chain model, suppose we have a lot of prior X,Y data consistent with a positive ATE of X on Y. We then process trace an X=1, Y=1 case and an X=0, Y=0 case, observing M=1 in both, so M is uncorrelated with X. Weve just found evidence against an effect of X on Y, so this will reduce our posterior on the ATE. But the ATE prior also has a way to preserve itself: it can force a downward updating of our beliefs about the prevalence negative effects. If we havent observed any cases in which negative effects might operate, there is little to constrain that downward updating. The overall result will be updating on positive effects (downwardly) together a some mix of updating on the ATE (downwardly) and updating on negative effects (downwardly).((A corollary is that learning about a kind of case that we directly observe can get transmitted to a kind of case that we dont directly observe via a constraint on our beliefs imposed by the model or priors. This is just a special instance of priors generating probative value: our priors on the ATE makes evidence about positive effects probative about negative effects. If we had flat priors on the ATE, learning about positive effects would have no impact on our beliefs about negative effects. A parallel example arises with a two-path model and observation of a mediator, M, along the indirect path in a set of cases. Suppose we find an M data pattern inconsistent with any kind of indirect effect: what happens to our beliefs about the ATE? In general, finding evidence against one way the effect can happen should reduce our confidence in the effect happening at all. However, if we have started out with a strong prior on the ATE but flat priors over whether the effect is a direct or indirect one, then our updating will tend to conserve our ATE beliefs by updating our beliefs about direct effects. So evidence against the indirect effect will function as evidence for direct effects as well as evidence against a total effect. A further implication for process tracing is that there will generally be sharp limits to what we can learn about overall effects if we study mediators along only some of the theorized pathways, especially if we already have some prior information about effects. The difficulty is that whatever we learn from the mediators we do observe, about the pathways they lie along, will get offset by shifts in our beliefs about other pathways, generated by the constraint in our prior knowledge about the overall effect. Suppose you already have some belief that economic development makes democracy more likely, and you think there may be two mechanisms: one running through a rising middle class and one operating through a more robust and organized working class. Suppose then that I show you that the organization of the working class does not vary with per capita GDP. Rationally, you should then reduce your confidence in the working-class pathway. However, you should also increase your confidence in the operation of the middle-class pathway  because (a) you have prior reason to believe that the overall development \\(\\rightarrow\\) democracy effect exists and (b) we have not observed a mediator along the middle-class pathway. On balance, then, learning about just one pathway will not have a large impact on beliefs about the overall effect of GDP on democratization. The larger lesson here is that, if our process tracing strategy involves the examination of mediators to learn about total \\(X \\rightarrow Y\\) effects, then how much we stand to learn depends on whether we are collecting diagnostic evidence along all plausible pathways connecting \\(X\\) to \\(Y\\). To be clear, we do not need to collect mediator clues on all possible pathways. If we have strong priors that one or more possible pathways are very unlikley, then we might safely be able to avoid collecting observations along those pathways without substantially reducing the prospects for learning. Also, the point that we are making here applies to using mediator data to answer queries about the effect of \\(X\\) on \\(Y\\). If instead we want to know whether a particular pathway is operating, then collecting evidence just on that pathway might be highly informative. Stronger priors impose a stronger constraint on learning. If weve observed only a small amount of X,Y data in the above chain-model example, then observing the M data in the on-the-regression-line cases will have a weaker impact on our beliefs about negative effects, and a bigger impact on our beliefs about ATE. Ditto for the 2-path model example. However, where prior data/beliefs on the ATE are strong, well learn less about the ATE, and more about negative effects (or the direct path). If there are different ways in which a query can be satisfied, evidence against the likelier way is stronger evidence against the query than is evidence against an unlikelier way. In the 2-path model, if we started out thinking that the indirect effect was more likely than the direct effect, then evidence against the direct effect will have a bigger impact on our beliefs about the overall model. It is difficult to get empirical leverage on very unlikely queries. And queries may be unlikelier than they appear. Suppose we start with the 2-path model, and want to know if X has a positive effect on Y that rests on a chain of positive effects via M. And suppose, importantly, that we begin with flat priors over all nodal types. Our intuitions likely tell us that this is exactly the kind of question for which an observation of M is the perfect empirical strategy. And that intuition is, in a sense, correct: we can indeed learn about the query by observing M. Seeing M=1 in an X=Y=1 case, for instance. would be evidence consistent with the query while M=0 would be inconsistent. Fine. ((But we will only learn a little from this observation. The reason is that the query itself has a very low prior probability. It may actually not be obvious at first glance just how unlikely our query is to be true. (After all, the model has two causal paths, and were asking if positive effects run through one of them, right? Not quite.) Seeing this requires us to think about the joint probabilities implied by the query. First, the query requires X to have a positive effect on M, which we think theres only a 25% chance of. In addition, the query puts a very narrow constraint on Ys possible nodal types: Y has to have a nodal type in which M has a positive effect on Y when X does not change, and in which X does not have a positive effect on Y unless M changes from 0 to 1. This pair of conditions is met by only 2 of Ys 16 nodal types, implying a 12.5% chance. The prior on the query is thus 0.25 x 0.125 = 0.03125. Thus, while observing M=0 takes the probability of the query down to 0%, we started out very close to 0%! And observing M=1 results in only a small uptick, to about 6% because there remain many type combinations consistent with M=1 but that do not fit through the needle-eye of this query. 9.6 Conclusion ADD REFERENCE TO TABLE 1 OF FOR MIXED DATA Ability and Achievement Otis Duncan "]]
