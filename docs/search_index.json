[["intro.html", "Chapter 1 Introduction 1.1 The Case for Causal Models 1.2 Key contributions 1.3 The Road Ahead", " Chapter 1 Introduction :::: {.headerbox data-latex=&quot;&quot;} ::: {.center data-latex=&quot;&quot;} ::: We describe the book's general approach, preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences, and provide a roadmap for the rest of the book. :::: Here is the key idea of this book. Quantitative social scientists spend a lot of time trying to understand causal relations between variables by looking across large numbers of cases to see how outcomes differ when potential causes differ. This strategy relies on variation in causal conditions across units of analysis, and the quality of the resulting inferences depends in large part on what forces give rise to that variation. Qualitative social scientists, like historians, spend a lot of time looking at a smaller set of cases and seek to learn about causal relations by examining evidence of causal processes in operation within these cases. Qualitative scholars rely on theories of how things work, theories that specify what should be observable within a case if indeed an outcome were generated by a particular cause. These two approaches seem to differ in what they seek to explain---individual-level or population-level outcomes; in the forms of evidence they require---cross-case variation or within-case detail; and in what they need to assume---knowledge of assignment processes or knowledge of causal processes. The central theme of this book is that this distinction, though culturally real (Goertz and Mahoney (2012)), is neither epistemologically deep nor analytically helpful. Social scientists can work with causal models that simultaneously exploit cross-case variation and within-case detail, that address both case-level and population-level questions, and that both depend on, and contribute to developing, theories of how things work.1 We describe an approach to doing this in which researchers form causal models, update those models using data, and then query the models to get answers to particular causal questions. This framework is very different from standard statistical approaches in which researchers focus on selecting the best estimator to estimate a particular estimand of interest. In a causal models framework, the model itself gets updated, not the estimate: we begin by learning about processes, and only then draw inferences about particular causal relations of interest, either at the case level or at the population level. We do not claim that a causal-model-based approach is the best or only strategy suited to addressing causal questions. There are plenty of settings in which other approaches would likely work better. But we do think that the approach holds considerable promise --- allowing researchers to combine disparate data in a principled way to ask a vast range of causal questions, helping integrate theory and empirics in a compelling way, and providing coherent guidance on research design --- and that it should have a place in the applied researcher's toolkit. Our goals in this book are to motivate this approach; provide an introduction to the theory of structural causal models; provide practical guidance for setting up, updating, and querying causal models; and show how the approach can inform key research-design choices, especially case-selection and data-collection strategies. 1.1 The Case for Causal Models There are three closely related motivations for embracing a causal models approach. One is is a concern over the limits of design-based inference. A second is an interest in integrating qualitative knowledge with quantitative approaches. A third is an interest in better connecting empirical strategies to theory. 1.1.1 The limits to design-based inference To caricature positions a bit, consider the difference between an engineer and a skeptic. The engineer tackles problems of causal inference using models: theories of how the world works, generated from past experiences and applied to the situation at hand. They come with prior beliefs about a set of mechanisms operating in the world and, in a given situation, will ask whether the conditions are in place for a known mechanism to operate effectively. The skeptic, on the other hand, maintains a critical position, resisting the importation of beliefs that are not supported by evidence in the case at hand. The engineer's approach echoes what was until recently a dominant orientation among social scientists. At the turn of the current century, much analysis---both empirical and theoretical---took the form of modelling processes (&quot;data generating processes&quot;) and then interrogating those models. Over the last two decades, however, skeptics have raised a set of compelling concerns about the assumption-laden nature of standard regression analysis, while also clarifying how valid inferences can be made with limited resort to models in certain research situations. The result has been a growth in the use of design-based inference techniques that, in principle, allow for model-free estimation of causal effects (see Dunning (2012), Gerber, Green, and Kaplan (2004), Druckman et al. (2011), Palfrey (2009) among others). These include lab, survey, and field experiments and natural-experimental methods exploiting either true or &quot;as-if&quot; randomization by nature. With the turn to experimental and natural-experimental methods has come a broader conceptual shift, with a growing reliance on the &quot;potential outcomes&quot; framework which provide a clear language for thinking about causation (see Rubin (1974), Splawa-Neyman et al. (1990) among others) without having to invoke fully specfied models of data-generating processes. The ability to estimate average effects and to characterize uncertainty---for instance calculating \\(p\\)-values and standard errors---without resort to models is an extraordinary development. In Fisher (2017)'s terms, with these tools, randomization processes provide a &quot;reasoned basis for inference,&quot; placing empirical claims on a powerful footing. Excitement about the strengths of these approaches has been mixed with various concerns regarding how the approach shapes inquiry. We highlight two. The first concern---raised by many in recent years (e.g., Thelen and Mahoney (2015))---is about design-based inference's scope of application. While experimentation and natural experiments represent powerful tools, the range of research situations in which model-free inference is possible is inevitably limited. For a wide range of causal conditions of interest both to social scientists and to society, controlled experimentation is impossible, and true or &quot;as-if&quot; randomization is absent. Moreover, limiting our focus to those questions for which, or situations in which, exogeneity can be established &quot;by design&quot; would represent a dramatic narrowing of social science's ken. To be clear, this is not an argument against experimentation or design-based inference when these can be used; rather it is an argument for why social science needs a broader set of tools. The second concern is more subtle. The great advantage of design-based inference is that it liberates researchers from the need to rely on models to make claims about causal effects. The risk is that, in operating model-free, researchers end up learning about effect sizes but not about models. Yet often the model is the thing we want to learn about. Our goal as social scientists is to come to grips with how the world works, not simply to collect propositions about the effects that different causes have had on different outcomes in different times and places. It is through models that we derive an understanding of how things might work in contexts and for processes and variables that we have not yet studied. Thus, our interest in models is intrinsic, not instrumental. By taking models out of the equation, as it were, we limit the potential for learning about the world. 1.1.2 Qualitative and mixed-method inference Recent years have seen the elucidation of the inferential logic behind &quot;process tracing&quot; procedures used in qualitative political science and other disciplines. On our read of this literature, the logic of process tracing in these accounts depends on a particular form of model-based inference.2 While process tracing as a method has been around for more than three decades (e.g., George and McKeown (1985)), its logic has been most fully laid out by qualitative methodologists in political science and sociology over the last 15 years (e.g., A. Bennett and Checkel (2015), George and Bennett (2005), H. Brady and Collier (2010), Hall (2003), Mahoney (2010)). Whereas King, Keohane, and Verba (1994) sought to derive qualitative principles of causal inference within a correlational framework, qualitative methodologists writing in the wake of &quot;KKV&quot; have emphasized and clarified process-tracing's &quot;within-case&quot; inferential logic: in process tracing, explanatory hypotheses are tested based on observations of what happened within a case, rather than on observation of covariation of causes and effects across cases. The process-tracing literature has also advanced increasingly elaborate conceptualizations of the different kinds of probative value that within-case evidence can yield. For instance, qualitative methodologists have explicated the logic of different test types (&quot;hoop tests&quot;, &quot;smoking gun tests&quot;, etc.) involving varying degrees of specificity and sensitivity (D. Collier (2011), Mahoney (2012)).3 Other scholars have expressed the leverage provided by process-tracing evidence in Bayesian terms, moving from a set of discrete test types to a more continuous notion of probative value (Fairfield and Charman (2017), A. Bennett (2015), Humphreys and Jacobs (2015)).4 Yet, conceptualizing the different ways in which probative value might operate leaves a fundamental question unanswered: what gives within-case evidence its probative value with respect to causal relations? We do not see a clear answer to this question in the current process-tracing literature. Implicitly---but worth rendering explicit---the probative value of process-tracing evidence depends on researcher beliefs that come from outside of the analysis in question. We enter a research situation with a model of how the world works, and we use this model to make inferences given observed patterns in the data --- while at the same time updating those models based on the data. A key aim of this book is to demonstrate the role that models can --- and, in our view, must --- play in drawing case-level causal inferences and to clarify conditions under which these models can be defended. To do so we draw on an approach to specifying causal models developed originally in computer science and that predates this work in qualitative methodology. The broad approach, described in Cowell et al. (1999) and Pearl (2009), is consistent with the potential outcomes framework, and provides rules for updating on population and case level causal queries from different types of data. In addition to clarifying the logic of qualitative inference, we will argue that such causal models can also enable the systematic integration of qualitative and quantitative forms of evidence. Social scientists are increasingly developing mixed-method research designs, research strategies that combine quantitative with qualitative forms of evidence (Small 2011). A typical mixed-methods study includes the estimation of causal effects using data from many cases as well as a detailed examination of the processes taking place in a few cases. Now-classic examples of this approach include Lieberman's study of racial and regional dynamics in tax policy (Lieberman (2003)); Swank's analysis of globalization and the welfare state (Swank (2002)); and Stokes' study of neoliberal reform in Latin America (Stokes (2001)). Major recent methodological texts provide intellectual justification of this trend toward mixing, characterizing small-\\(n\\) and large-\\(n\\) analysis as drawing on a single logic of inference and/or as serving complementary functions (King, Keohane, and Verba (1994); D. Collier, Brady, and Seawright (2004)). The American Political Science Association now has an organized section devoted in part to the promotion of multi-method investigations, and the emphasis on multiple strategies of inference research is now embedded in guidelines from many research funding agencies (Creswell and Garrett 2008). However, while scholars frequently point to the benefits of mixing correlational and process-based inquiry (e.g., D. Collier, Brady, and Seawright (2010), p.~181), and have sometimes mapped out broad strategies of multi-method research design (Lieberman (2005), Seawright and Gerring (2008)), they have rarely provided specific guidance on how the integration of inferential leverage should unfold. In particular, the literature has not supplied specific principles for aggregating findings---whether mutually reinforcing or contradictory---across different modes of analysis.5 As we aim to demonstrate in this book, however, grounding inference in causal models provides a very natural way of combining information of the \\(X,Y\\) variety with information about the causal processes connecting \\(X\\) and \\(Y\\). The approach that we develop here can be readily addressed both to the case-oriented questions that tend to be of interest to qualitative scholars and to the population-oriented questions that tend to motivate quantitative inquiry. As will become clear, when we structure our inquiry in terms of causal models, the conceptual distinction between qualitative and quantitative inference becomes hard to sustain. Notably, this is not because all causal inference depends fundamentally on covariation but because in a causal-model-based inference, what matters for the informativeness of a piece of evidence is how that evidence alters beliefs about a model, and in turn, a query. While the apparatus that we present is formal, the approach---in asking how pieces of evidence drawn from different parts of a process map on to a base of theoretical knowledge---is arguably most closely connected to process tracing in its core logic. 1.1.3 Connecting theory and empirics The relationship between theory and empirics has been a surprisingly uncomfortable one in political science. In a recent intervention, for instance, Clarke and Primo (2012) draw attention to and critique political scientists' widespread reliance on the &quot;hypothetico-deductive&quot; (H-D) framework, in which a theory or model is elaborated, empirical predictions derived, and data sought to test these predictions and the model from which they derive. Clarke and Primo draw on decades of scholarship in the philosophy of science pointing to deep problems with the H-D framework, including with the idea that the truth of a model logically derived from first principles can be tested against evidence. In fact the relationship between theory and evidence in social inquiry is often surprisingly unclear both in qualitative and quantitative work. We can perhaps illustrate it best, however, by reference to qualitative work, where the centrality of theory to inference has been most emphasized. In process tracing, theory is what justifies inferences. In their classic text on case study approaches, George and Bennett (2005) describe process tracing as the search for evidence of &quot;the causal process that a theory hypothesizes or implies&quot; (6). Similarly, Hall (2003) conceptualizes the approach as testing for the causal-process-related observable implications of a theory; Mahoney (2010) indicates that the events for which process tracers go looking are those posited by theory (128); and Gerring (2006) describes theory as a source of predictions that the case-study analyst tests (116). Theory, in these accounts, is supposed to help us figure out where to look for discriminating evidence. What is not clear, however, is a how researchers can derive within-case empirical predictions from theory and how exactly doing so provides leverage on a causal question. From what elements of a theory can scholars derive informative within-case observations? Of the many possible observations suggested by a theory, how can we determine which would add probative value to the evidence already at hand? How do the evidentiary requisites for drawing a causal inference, given a theory, depend on the particular causal question of interest---on whether, for instance, we are interested in identifying the cause of an outcome in a case, estimating an average causal effect, or identifying the pathway through which an effect is generated? Perhaps most confusingly, if the theory tells us what to look for to draw an inference, can the inferences be about the theory itself or are we constrained to make theory dependent inferences? In short, how exactly can we ground causal inferences from within-case evidence in background knowledge about how the world works? Much quantitative work in political science features a similarly weak integration between theory and research design. The modal inferential approach in quantitative work, both observational and experimental, involves looking for correlations between causes and outcomes, with less regard for intervening or surrounding causal relationships.6 If a theory suggests a set of relations, it is common to examine these separately---does \\(A\\) cause \\(B\\) does \\(B\\) cause \\(C\\)? are relations stronger or weaker here or there?---without standard procedures for bringing the disparate pieces of evidence together to form theoretical conclusions. More attention has been paid to empirical implications of theoretical models than to theoretical implications of empirical models. In this book, we seek to show how scholars can simultaneously make fuller and more explicit use of theoretical knowledge in designing their research projects and analyzing data and make use of data to update on theoretical models. Like Clarke and Primo, we treat models not as veridical accounts of the world but as maps: maps, based on prior theoretical knowledge, about causal relations in a domain of interest. Also, as in Clarke and Primo's approach, we do not write down a model in order to test its veracity (though, in later chapters, we do discuss ways of justifying and evaluating models). Rather, our focus is on how we can systematically use causal models --- in the sense of mobilizing background knowledge of the world --- to guide our empirical strategies and inform our inferences. Grounding our empirical strategy in a model allows us, in turn, to learn about features of the model itself as we encounter the data. 1.2 Key contributions This book draws on methods developed in the study of Bayesian networks, a field pioneered by scholars in computer science, statistics, and philosophy (see especially Pearl (2009)). Bayesian networks, a form of causal model, have had limited traction to date in political science. Yet the literature on Bayesian networks and their graphical counterparts, directed acyclic graphs (DAGs), is a body of work that addresses very directly the kinds of problems with which qualitative and quantitative scholars routinely grapple.7 Drawing on this work, we show in the chapters that follow how a theory can be formalized as a causal model represented by a causal graph and a set of structural equations. Engaging in this modest degree of formalization yields enormous benefits. It allows us, for a wide range of causal questions, to specify causal questions clearly and assess what inferences to make about queries from new data. For students engaging in process tracing, the benefits of this approach are multiple. In particular, the framework that we develop in this book provides: A grounding for assessing the &quot;probative value&quot; of evidence drawn from different parts of any causal network. The approach yields a principled and transparent approach to answering the question: how should the observation of a given piece of data affect my causal beliefs about a case? A transparent, replicable method of aggregating inferences from observations drawn from different locations in a causal network. Having collected multiple pieces of evidence from different parts of a causal process or case context, what should I end up believing about the causal question of interest? A common approach for assessing a wide variety of queries (estimands). We can use the same apparatus to learn simultaneously about different case-level causal questions, such as &quot;What caused the outcome in this case?&quot; and &quot;Through what pathway did this cause exert its effect?&quot; Guidance for research design. Given finite resources, researchers must make choices about where to look for evidence. A causal model framework can help researchers assess, a priori, the relative expected informativenss of different evidentiary and case-selection strategies, conditional on how they think the world works and the question they want to answer. The approach also offers a range of distinctive benefits to researchers seeking to engage in mixed-method inference and to learn about general causal relations, as well as about individual cases. The framework's central payoff for multi-method research is the systematic integration of qualitative and quantitative information to answer any given causal query. We note that the form of integration that we pursue here differs from that offered in other accounts of multi-method research. In Seawright (2016)'s approach, for instance, one form of data --- quantitative or qualitative --- is always used to draw causal inferences, while the other form of data is used to test assumptions or improve measures employed in that primary inferential strategy. In the approach that we develop in this book, in contrast, we are always using all information available to update on causal quantities of interest. In fact, within the causal models framework, there is no fundamental difference between quantitative and qualitative data, as both enter as values of nodes in a causal graph. This formalization --- this reductive move --- may well discomfit some readers. And we acknowledge that our approach undeniably involves a loss of some of what makes qualitative research distinct and valuable. Yet, this translation of qualitative and quantitative observations into a common, causal model framework offers major advantages. Beyond the integration of different forms of information, these advantages include: Transparency. The framework makes manifest precisely how each form of evidence enters into the analysis and shapes conclusions. Learning across levels of analysis. In a causal model approach, we use case-level information to learn about populations and general theory. At the same time, we use what we have learned about populations to sharpen our inferences about causal relations within individual cases. Cumulation of knowledge. A causal model framework provides a straightforward, principled mechanism for building on what we have already learned. As we see data, we update our model; and then our updated model can inform the inferences we draw from the next set of observations. Models can, likewise, provide an explicit framework for positing and learning about the generalizability and portability of findings across research contexts. Guidance for research design. With a causal model in hand, we can formally assess key multi-method design choices, including the balance we should strike between breadth (the number of cases) and depth (intensiveness of analysis in individual cases) and the choice of cases for intensive analysis. Using causal models also has substantial implications for common methodological intuitions, advice, and practice. To touch on just a few of these implications: Our elaboration and application of model-based process tracing shows that, given plausible causal models, process tracing's common focus on intervening causal chains may be much less productive than other empirical strategies, such as examining moderating conditions. Our examination of model-based case-selection indicates that for many common purposes there is nothing particularly especially about &quot;on the regression line&quot; cases or those in which the outcome occurred, and there is nothing necessarily damning about selecting on the dependent variables. Rather optimal case selection depends on factors that have to date received little attention, such as the population distribution of cases and the probative value of the available evidence. Our analysis of clue-selection as a decision problem shows that the probative value of a piece evidence cannot be assessed in isolation, but hinges critically on what we have already observed. The basic analytical apparatus that we employ here is not new. Rather, we see the book's goals as being of three kinds. First, we aim to import insights: to introduce political scientists to an approach that has received little attention in the discipline but that can be useful for addressing the sorts of causal questions with which political scientists are commonly preoccupied. As a model-based approach, it is a framework especially well suited to a field of inquiry in which exogeneity frequently cannot be assumed by design---that is, in which we often have no choice but to be engineers. Second, we draw connections between the Bayesian networks approach and key concerns and challenges with which students in social sciences routinely grapple. Working with causal models and DAGs most naturally connects to concerns about confounding and identification that have been central to much quantitative methodological development. Yet we also show how causal models can address issues central to process tracing, such as how to select cases for examination, how to think about the probative value of causal process observations, and how to structure our search for evidence, given finite resources. Third, we provide a set of usable tools for implementing the approach. We provide intuition and software, the CausalQueries package, that researchers can use to make research design choices and draw inferences from the data. There are also many important questions around this approach that we do not spend much time on, and that are better addressed elsewhere. First, while we employ a Bayesian approach we do not devote time to defending Bayesianism. Second, although we engage with qualitative inference we do not dwell on other important aspects of qualitative methodology, such as data-collection and measurement strategies. We do examine measurement error and provide guidance on what data to collect but do not engage with measurement questions more broadly. There are also important limits to this book’s contributions and aims. First, while we make use of Bayesian inference throughout, we do not engage here with fundamental debates over or critiques of Bayesianism itself. (For excellent treatments of some of the deeper issues and debates, see, for instance, Earman (1992) and Fairfield and Charman (2017).) Second, this book does not address matters of data-collection (e.g., conducting interviews, searching for archival documents) or the construction of measures. For the most part, we assume that data are either at hand or can be gathered, and we bracket the measurement process itself. That said, a core concern of the book is using causal models to identify the kinds of evidence that qualitative researchers will want to collect. In Chapter 7, we show how causal models can tell us whether observing an element of a causal process is potentially informative about a causal question; and in Chapter ?? we demonstrate how we can use models to assess the likely learning that will arise from different clue-selection strategies. We also address the problem of measurement error in Chapter 9, showing an approach to using causal models to learn about error from the data. Finally, while we will often refer to the use of causal models for “qualitative” analysis, we do not seek to assimilate all forms of qualitative inquiry into a causal models framework. Our focus is on work that is squarely addressed to matters of causation; in particular, the logic that we elaborate is most closely connected to the method of process tracing. More generally, the formalization the we introduce here---the graphical representation of beliefs and the application of mathematical operations to numerically coded observations---will surely strike some readers as reductive and not particularly &quot;qualitative.&quot; It is almost certainly the case that, as we formalize, we leave behind some of what makes qualitative research distinctive and valuable. Our aim in this book is not to discount the importance of those aspects of qualitative inquiry that resist formalization, but to show some of things we can do if we are willing to formalize. 1.3 The Road Ahead The book is divided into four main parts. In the first part of the book, we set out the basics. In Chapter 2, following a review of the common potential-outcomes approach to causality, we introduce the concept and key components of a causal model. Chapter 3 illustrates how we can represent of causal beliefs in the form causal models by translating the arguments of a several prominent works of political science into causal models. In Chapter 4, we set out a range of causal questions that researchers might want to address --- including questions about case-level causal effects, population-level effects, and mechanisms --- and define these queries within a causal model framework. Chapter 5 offers a primer on the key ideas in Bayesian inference that we will mobilize in later sections of the book. In Chapter 6, we map between causal models and theories, showing how we can think of any causal model as situated within a hierarchy of complexity: within this hierarchy, any causal model can be justified by references to a &quot;lower level&quot;, more detailed model that offers a theory of why things work the way do that the higher level. This conceptualization is crucial insofar as we use more detailed (lower-level) models to generate empirical leverage on relationships represented in simpler, higher-level models. The second part of the book shows how we can use causal models to undertake process-tracing and mixed method inference. Chapter 7 lays out the logic of case-level inference from causal models: the central idea here is that what we learn from evidence is always conditional on the prior beliefs embedded in our model. In Chapter 8, we illustrate model-based process-tracing with an application to the substantive issue of economic inequality's effects on democratization. Chapter 9 moves to mixed data problems: situations in which a researcher wants to use &quot;quantitative&quot; (broadly, \\(X,Y\\)) data on a large set of cases and more detailed (&quot;qualitative&quot;) data on some subset of these cases. We show how we can use any arbitrary mix of observations across a sample of any size (greater than 1) to update on all causal parameters in a model, and then use the updated model to address the full range of general and case-level queries of interest. In Chapter 10, we illustrate this integrative approach by revisiting the problem of inequality and democracy introduced in Chapter 8. Finally, in Chapter 11, we take the project of integration a step further by showing how we can use models to integrate findings across studies and across settings. We show, for instance, how we can learn jointly from the data generated by an observational study and an experimental study of the same causal domain and how models can help us reason in principled ways about the transportability of findings across contexts. The third part of the book unpacks what causal models can contribute to research design. Across Chapters ??, 12, and 13, we demonstrate how researchers can mobilize their models, as well as prior observations, to determine what kind of new evidence is likely to be most informative about the query of interest, how to strike the balance between extensiveness and intensiveness of analysis, and which cases to select for in-depth process tracing. The fourth and final part of the book steps back to put the model-based approach into question. Until this point, we have been advocating an embrace of models to aid inference. But the dangers of doing this are demonstrably large. The key problem is that with model-based inference, the inferences are only as good as the model. In the end, while we advocate a focus on models, we know that skeptics are right to distrust them. This final part approaches this problem from two perspectives. In Chapter 15, we demonstrate the possibility of justifying models from external evidence, though we do not pretend that the conditions for doing so will arise commonly. In Chapter 16, drawing on common practice in Bayesian statistics, we present a set of strategies that researchers can use to evaluate and compare the validity of models, and to investigate the degree to which findings hinge on model assumptions. In the concluding chapter we summarize what we see as the main advantages of a causal-model-based approach to inference, draw out a set of key concerns and limitations of the framework, and identify what we see as the key avenues for progress in model-based inference. Here we go. References "]]
