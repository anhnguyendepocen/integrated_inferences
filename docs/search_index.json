[
["elements-of-design.html", "Chapter 10 Elements of Design 10.1 Model, inquiry, data strategy, answer strategy 10.2 Evaluating a design 10.3 Illustration of Design Decaration in code", " Chapter 10 Elements of Design A fully specified causal model includes the information needed to assess the properties of a research design that seeks to learn from or learn about the model. We talk through how to go from defining causal models to “declaring” research designs and use this framework in later chapters to inform decisions about details of design choices. So far we have described a way to think about causal models, a way to specify causal estimands, and a Bayesian approach to inference, given models and estimands. Together with a strategy for data gathering these elements are enough to fully characterize a research design. If in addition we provide criteria for evaluating a design we have enough to be able to simulate the behavior of a research design and assess whether a design is up to the task fo answering the questions we want to answer. Once we have a method to assess the performance of a given design we can can start asking what kind of design is optimal, given some beliefs about the world (see Blair et al. (2019) for more on this general approach to design declaration and diagnosis). In the next chapters we use this approach to assess a set of design choices including choices regarding the clues about which data is sought, the types of cases for which data is sought, and the number of cases for which different types of data is sought. In the remainder of this chapter we discuss a simple evaluative criterion for a design and give examples for design declaration for a simple single case process tracing design and a mixed methods design. 10.1 Model, inquiry, data strategy, answer strategy We use the MIDA approach (model, inquiry, data strategy, answer strategy) approach to declare a simple process tracing design with an arbitrary model. Model. We will define a model as introduced in Chapter 2. Inquiry As discussed in Chapter 4, an inquiry is a question asked of a model. This is typically a question about the distribution of a variable in some controlled or natural condition, or some summary of such distributions. We refer to the quantity being targeted by a query as the estimand. Data strategy. The data strategy describes how data will be gathered. In typical DeclareDesign applications this includes both randomization and data gathering (sampling) strategies. We focus on data gathering (though unrestricted randomization schemes are easily accommodated in the workhorse model). A sampling strategy might indicate a sequence of conditional data gathering schemes, for instance: gather data on \\(X\\) and \\(Y\\) for 100 cases, then gather data on \\(M\\) for all cases in which \\(X=Y\\). Answer strategy. The answer strategy combines observed data with a causal model to generate an updated model from which inferences can be drawn. Importantly, the model used in the answer strategy does not need to be the same model as assumed at the model step since we could imagine analysts coming to the data with quite different models in mind. Of course any model used in the answer strategy should generally involve the same variables as in the model itself. A design is a concatenation of these four components or “steps.” The concatenation of steps lets us examine instances of runs of a design. A single instance would involve: a single draw of a true parameter vector from the distribution given in the model definition a calculation of the value of an estimand given this true parameter draw the generation of a dataset given the model implied by step 1 and the data strategy the generation of an answer to the inquiry generated from the realized data from 3. and the answer strategy With the observation of multiple instances we get to assess the distribution of our answers — and our uncertainty around these – over repeated draws, and each time we get to see how well the answer we get maps onto the assumed truth in that draw. 10.1.1 Defining a model 10.1.1.1 Five questions To define a model we declare the set of variables we are interested in and the relations of independence between them. In particular we answer the following five questions: 1. What are the nodes? As discussed in Chapter 2 we have some liberty in selecting which nodes we care about to explain a phenomenon. In defining the nodes (variables) we generally also define the ranges of the variables—indicating, for example whether they are binary, categorical, or continuous. In defining the edges we identify the set of parents of any node. 2. Where are the arrows? Can you justify conditional independence claims? Where are there plausible threats of unobserved confounding? What are the functional relations? What are the parameters? In the general case a model might specify functional relations linking relating parents to children. In the binary world a finite set of parameters can be used to characterize all possible functional relations. What restrictions? Of course restrictions are never required: rather than imposing a qualitative restriction it is possible to allow effects in all directions and let the data determine what patterns appear likely or not. It is important to note however that in general monotonicity claims cannot be inferred from data with full confidence.1 What are your priors? When defined as part of the model we think of these priors as being the priors from the vantage point of someone assessing a design and they need not be the same as the priors used in the analysis. There are different approaches to generating priors. One might be to try to generate priors that reflect the state of the literature. A second might be to more formally develop priors on the basis of prior data — that is, start with a model with uninformative priors and update the model using past evidence. A third approach is to gather data from target readers—for example to gather data from policy makers or disciplinary experts. 10.1.1.2 Example To illustrsate this process of model construction, refer back to our discussion of the inequality and democracy model in section ??. There we discussed a theory that unpacked a higher level model but that did not place any restrictions on functional forms — that is, on causal types. We might imagine such restrictions being justified by theory, however. Drawing on Boix (2003), for instance, we might theorize that inequality can have a negative effect on democratization by giving the elite more to lose from majority rule, making autocrats less willing to hand over power. Inequality’s positive effect, we might further posit, derives from the fact that it gives the poor more to gain from the redistribution that democratization would enable (Acemoglu and Robinson (2005)). However, this positive effect can only unfold to the extent that the masses are able to mobilize, and the capacity to mobilize will hinge on ethnic homogeneity. Ethnic homogeneity thus defines the causal possibilities in regard to \\(I\\)’s effect on \\(D\\). First, homogeneity is necessary for a positive effect of inequality. Second, by enabling mobilization around distributional demands, ethnic homogeneity rules out a net negative effect of inequality (as inequality’s mobilizing effects will balance out elite fears of expropriation). Third, by making mass mobilization easier in general, ethnic homogeneity makes possible mobilization and democratization without inequality. Under ethnic heterogeneity, on the other hand, inequality can have a negative effect, or it can no effect at all with autocracy entrenched. Put differently, under ethnic homogeneity, inequality’s effect can only correspond to a \\(b\\) type or a \\(d\\) type, while under heterogeneity the effect can only be of type \\(a\\) or type \\(c\\). \\(E\\) thus allows us to partition the range of causal possibilities that model (a) had lumped together under \\(\\theta^D\\). Now we can capture this logic with a functional equation in which \\(\\theta^D\\) now takes on just two possible values (0 or 1), rather than four: \\[\\begin{equation} D=IE(1-\\theta^D)+\\theta^DE+(1-E)(1-I)\\theta^D \\end{equation}\\] We can work through the arithmetic to observe \\(E\\)’s causal-partitioning effect. Whether \\(E\\) is 0 or 1 determines whether we are in a world of posiitve effects (\\(b\\) types) and democratization regardless (\\(d\\) types), or a world of negative effects (\\(a\\) types) and autocracy regardless (\\(c\\) types). Note that the righthand side is a sum of three expressions. We can think of \\(E\\) as a “switch” that turns these expressions “on” or “off.” When \\(E=1\\), the third expression goes to 0, leaving only the first two in play “on.” Now, \\(\\theta^D\\) determines whether \\(I\\) has a positive effect (when \\(\\theta^D=0\\)) or no effect with \\(D\\) fixed at \\(1\\) (when \\(\\theta^D=1\\)). Conversely, when \\(E=0\\), the first two expressions both go to \\(0\\), and \\(\\theta^D\\) determines whether \\(I\\) will have a negative effect (when \\(\\theta^D=1\\)) or no effect with \\(D\\) stuck at 0 (when \\(\\theta^D=0\\)). 10.2 Evaluating a design How do you know if you have a good design? One metric is to assess the expected (squared) error resulting from posterior beliefs. In other words: how wrong are you likely to be if you base your best guess on your posterior mean? Formally, define the loss function for query \\(Q\\) given data \\(K\\) as: \\[E_{k, q} \\left(\\left( \\int q&#39; P(q&#39; | k, w)dq&#39; - q\\right)^2\\right)\\] where the expectation is taken over the joint distribution of \\(K\\) and \\(Q\\), given \\(W\\). This is measure of expected loss is sometimes called the Bayes risk. The inner term \\(P(q&#39;|k, w)\\) is the posterior distribution on \\(q&#39;\\) given observation of \\(k\\) and \\(w\\). Another way to think of the gains is as the expected posterior variance: how certain do you expect you will be after you make use of this new information? In fact these two quantities are equivalent (see for example Scharf (1991)). To see this imagine a situation in which there is an unknown parameter \\(q\\) and we have a data strategy that produces a distribution over data \\(k\\), given \\(q\\). Let \\(p(q,k)\\) denote the joint prior distribution over \\(q\\) and \\(k\\) with marginal distributions \\(p(k)\\) and \\(p(q)\\). For any \\(k\\) there is posterior estimate \\(q_k\\) and a posterior variance \\(v_k\\), both estimated using Bayes rule. The expected squared error is then: \\[ESE := \\int_q\\int_k \\left({q}_k-q\\right)^2p(k, q)dkdq \\] This takes the error one might get with repsect to any true value of the parameter (\\(q\\)), given the data one might see given \\(q\\) and the inferences one might draw. The expected posterior variance can be written: \\[EV := \\int_k v_k p(k)dk\\] This takes the posterior variance, given some data, over all the possible data one might see given marginal distribution \\(p(k)\\). We want to show that these are equivalent. We take advantage of the fact that \\(p(q,k) = p(k)p(q|k) = p(q)p(k|q)\\) and that \\(p(q|k)\\) gives the posterior distribution of \\(q\\) given \\(k\\). We then have: \\[ \\begin{eqnarray} ESE &amp;=&amp; \\int_q\\int_k \\left({q}_k-q\\right)^2p(q,k)dkdq \\\\ &amp;=&amp; \\int_k\\int_q \\left({q}_k-q\\right)^2p(q,k)dq dk \\\\ &amp;=&amp; \\int_k\\int_q \\left({q}_k-q\\right)^2p(k)p(q|k)dq dk \\\\ &amp;=&amp; \\int_k\\int_q \\left({q}_k-q\\right)^2p(q|k)dq p(k)dk \\\\ &amp;=&amp; \\int_k\\left[\\int_q \\left({q}_k-q\\right)^2p(q|k)dq\\right]p(k)dk \\\\ &amp;=&amp; \\int_k v_k p(k)dk \\\\ &amp; = &amp; EV \\end{eqnarray} \\] Note that the key move is in recognizing that \\(p(q |k)\\) is in fact the posterior distribution on \\(q\\) given \\(k\\). In using this we assume that the same distribution is used for assessing error and for conducing analysis—that is we take the researcher’s prior to be the relevant one for assessing error. 10.2.1 Expected variance (almost) always goes down Moreover, it is easy to see that whenever inferences are sensitive to \\(K\\), the expected variance of the posterior will be lower than the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of \\(K\\), given what is already known from observation of \\(W\\).2 \\[Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))\\] However although expected posterior variance goes down, it is still always possible that posterior variance rises. The increase in uncertainty does not however mean you haven’t been learning. Rather you have learned that things aren’t as simple as you thought. 10.2.2 Illustration For illustration say that it is known that \\(X=1, Y=1\\) and that, given this information (playing the role of \\(W\\)), the posterior probability that a unit is of type \\(b\\)—for whom \\(Y\\) would be 0 were \\(X=0\\) (and not type \\(d\\), for which \\(Y\\) would be 1 regardless) is \\(p\\). Say then that that under some theory we have \\(\\phi_b := \\Pr(K=1 | Y(0)=0, Y(1)=1, X=1)\\), \\(\\phi_d := \\Pr(K=1 | Y(0)=1, Y(1)=1, X=1)\\). Then what is the value added of this theory? Define \\(Q\\) here as the query regarding whether the unit is a \\(b\\) type. Then the prior variance, \\(Var(Q|W)\\), is simply \\(p(1-p)^2 +(1-p)p^2 = p(1-p)\\). To calculate \\(E_{K|W}(Var(Q|K,W))\\), note that the posterior if \\(K\\) is observed is \\(\\frac{\\phi_bp}{\\phi_bp+\\phi_d(1-p)}\\). Let us call this \\(\\hat{q}_K\\), and the belief when \\(K\\) is not observed \\(\\hat{q}_{\\overline{K}}\\). In that case the expected error is: \\[\\text{Expected Error} = p\\phi_b\\left(1-\\hat{q}_K\\right)^2+(1-p)\\phi_d\\hat{q}_K^2+p(1-\\phi_b)\\left(1-\\hat{q}_{\\overline{K}}\\right)^2+(1-p)(1-\\phi_d)\\hat{q}_{\\overline{K}}^2\\] where the four terms are the errors when \\(K\\) is seen for a \\(b\\) type, when \\(K\\) is seen for a \\(d\\) type, when \\(K\\) is not seen for a \\(b\\) type, and when \\(K\\) is not see for a \\(d\\) type. Defining \\(\\rho_K = (p\\phi_b+(1-p)\\phi_d)\\) as the probability of observing \\(K\\) given the prior, we can write the posterior variance as: \\[\\text{Expected Posterior Variance} = \\rho_K\\hat{q}_K(1-\\hat{q}_K)+(1-\\rho_K)\\hat{q}_{\\overline{K}}(1-\\hat{q}_{\\overline{K}})\\] With a little manipulation, both of these expressions simplify to: \\[\\text{Expected Posterior Variance} =p(1-p)\\left(\\frac{\\phi_b\\phi_d}{\\phi_bp+\\phi_d(1-p)} + \\frac{(1-\\phi_b)(1-\\phi_d)}{(1-\\phi_b)p+(1-\\phi_d)(1-p)}\\right)\\] The gains are then: \\[\\text{Gains} =1- \\frac{\\phi_b\\phi_d}{\\phi_bp+\\phi_d(1-p)} - \\frac{(1-\\phi_b)(1-\\phi_d)}{(1-\\phi_b)p+(1-\\phi_d)(1-p)}\\] Note that we still learn even if our posterior variance increases. For example say \\(p = 1/5\\), \\(\\phi_d = 1/3\\), \\(\\phi_b = 2/3\\) and we observe \\(K=1\\). Then our prior variace is \\(p(1-p) = 4/15\\). Our posterior is \\(1/3\\) and our posterior variance is 2/9, an increase. Even still although we are more uncertain we are wiser since we attribute a squared error to the guesses made by our former selves now of \\((1/3)(1-1/5)^2 + (2/3)(0 - 1/5)^2 = 6/25\\). 10.2.3 Other loss functions Other loss functions could be used, including functions that take account of the costs of collecting additional data,3 or to the risks associated with false diagnoses.4 10.3 Illustration of Design Decaration in code In the gbiqq package there is a single function that lets you declare a full design in one go by letting you supply arguments to declare a model, an inquiry, a data strategy, and an answer strategy library(DeclareDesign) n &lt;- 10 analysis_model &lt;- make_model(&quot;X -&gt; Y&quot;) %&gt;% set_priors(distribution = &quot;jeffreys&quot;) reference_model &lt;- analysis_model %&gt;% set_priors(c(.5, .5, 1, 1,7, 1)) my_design &lt;- gbiqq_designer( reference_model = reference_model, analysis_model = analysis_model, n = n, query = list(PC = &quot;Y[X=1] &gt; Y[X=0]&quot;), given = &quot;X==1 &amp; Y==1&quot; ) With this model in hand you can use it to draw likely data, run analyses, and run diagnostics. Sample data can be generated with the DeclareDesign function draw_data: Table 10.1: Sample Data (shown in compact form) event strategy count X0Y0 XY 5 X1Y0 XY 0 X0Y1 XY 4 X1Y1 XY 1 We can look at sample estimands using draw_estimands: Table 10.2: Sample Estimands estimand_label estimand PC 0.515 We can look at sample estimates using draw_estimates: Table 10.3: Sample Estimates estimand_label Given Using estimate sd estimator_label PC - posteriors 0.247 0.194 est_PC Diagnosis is then implemented using diagnose_design. The function diagnose_design simulates the design many times and in each simulation gathers the estimand as well as the estimate and other statistics, and uses these to generate diagnosands – such as mean squared error or expected posterior variance. Moreoever once a design is declared it is easy to sets of neighboring designs. Here’s an example in which for the diagnsis we compare the performance of our declared design to those of designs of different sizes. multiple_designs &lt;- redesign(my_design, n = c(10, 40, 200)) diagnose_design(multiple_designs, sims = 20) n MSE Posterior Var Mean Estimate SD Estimate Mean Estimand Bias 10 0.08 0.05 0.44 0.16 0.68 -0.24 (0.01) (0.00) (0.02) (0.01) (0.01) (0.01) 40 0.05 0.04 0.54 0.18 0.70 -0.16 (0.01) (0.00) (0.02) (0.01) (0.01) (0.02) 200 0.02 0.02 0.63 0.18 0.69 -0.06 (0.00) (0.00) (0.02) (0.01) (0.01) (0.01) In this case the reference model is different from the analysis model. This might be the case for a resaercher with beliefs about causal effects contemplating how their analysis performed if they commited to undertaking an analysis using flat priors. Or it might reflect the evaluation of one researcher regarding the analyses of another. In any event the fact that these are out of line means that the MSE and expected posterior variance do not converge. Both however decline with larger designs. In the chapters that follow we use essentially this procedure, though, for efficiency, rather than repeatedly iterating the full design we use a procedure in which we store the inferences that we would make given different possible data, figure out the probability that we would observe data of a given type, and then calculate the expected value of different diagnosands given a refernece model and an analysis model. References "]
]
