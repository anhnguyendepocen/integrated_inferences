[
["evaluation.html", "Chapter 15 Evaluating models 15.1 Can we spot a bad model? 15.2 LOO 15.3 Sensitivity approaches 15.4 Evaluating the Democracy-Inequality model", " Chapter 15 Evaluating models Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good. Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these may be wrong. 15.1 Can we spot a bad model? Sometimes a model is just not able to fit the observed data well. Imagine a situation in which in fact \\(X\\) has an effect on \\(Y\\) but researchers wrongly assume that the effect runs entirely through \\(M\\). They assume a model was of the form \\(X \\rightarrow M \\rightarrow Y\\). In fact none of the effect runs through \\(M\\) and in the data we see that although \\(X\\) correlates with \\(Y\\), \\(X\\) does not correlate with \\(M\\) and \\(M\\) does not correlate with \\(Y\\). This data is inconsistent with the model since under this model if \\(X\\) doesn’t cause \\(M\\) and \\(M\\) doesn’t cause \\(Y\\) there is no other way for \\(X\\) to cause \\(Y\\). Will we notice? We are able to update using this model and the data but the updated model can produce wildly inaccurate inferences. In Figure ?? we imagine a true model of the form \\(X \\rightarrow M \\rightarrow Y \\leftarrow X\\) with an average effect of \\(X\\) on \\(Y\\) of \\(1/3\\) but no effect of \\(X\\) on \\(M\\) or \\(M\\) on \\(Y\\). The figure show the inferences on the treatment effect for an updated \\(X \\rightarrow M \\rightarrow Y\\) model and an updated \\(X \\rightarrow M \\rightarrow Y \\leftarrow X\\) model, both starting with flat priors. Figure 15.1: A restricted model yields a credibility interval that does not contain the actual average effect. 15.1.1 Check conditional independence First, even before engaging in updating we can look to see whether data is consistent with our causal model. In particular we can check whether there are inconsistencies with the Markov condition that we introduced in Chapter 2: nodes are conditionally independent of their nondescendants, conditional on their parents. In this case, given \\(M\\), \\(Y\\) should be independent of \\(X\\). Is it? One way to check is to look at the covariance of \\(X\\) and \\(Y\\) given \\(M=1\\) and again given \\(M=0\\). term estimate std.error statistic p.value conf.low conf.high df outcome (Intercept) 0.45 0.16 2.89 0.01 0.14 0.77 46 Y X 0.05 0.25 0.18 0.85 -0.45 0.54 46 Y M -0.12 0.20 -0.60 0.55 -0.53 0.28 46 Y X:M 0.18 0.30 0.60 0.55 -0.43 0.80 46 Y We see right way we have a problem. Here, note, we ran a classical test. WE hypothesized that our model was correct and looked to see whether the data was unusual given the model.1 15.1.2 Computational clues Second we may be lucky and run into computation issues. In this example there is a good chance that when you run the restricted model stan will throw an error (Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.) 15.1.3 Check fit Approaches using simulated data from the posterior predictive distribution are described in Gabry et al. (2019). The graphs below compare (using tools in the bayesplot) package, show how typical the data we observe is for the model that correctly assumes a direct effect and the model that incorrectly excludes it. First we look just at the distribution of the outcome \\(Y\\) to see how the actual distribution compares to the predicted distribution. We see here not too much difference. x &lt;- data.frame( unrestricted = read_rds(&quot;saved/ch15_replicates_unrestricted_b.rds&quot;), restricted = read_rds(&quot;saved/ch15_replicates_restricted_b.rds&quot;) ) reshape2::melt(x) %&gt;% ggplot(aes(x=value, fill=variable)) + geom_density(alpha=0.25) ## No id variables; using all as measure variables Figure 15.2: Distribution of How does the researcher’s model do? replicates_restricted &lt;- read_rds(&quot;saved/ch15_replicates_restricted.rds&quot;) bayesplot::ppc_stat(data$Y, replicates_restricted) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 15.3: Posterior distribution of test statistics under researcher’s model How would we evaluate the less constrained (“true”) model? ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 15.4: Distribution of \\(Y\\) under the (data updated) true model 15.1.4 Bayes factors: COmparing model likelihoods under different models NEED TO DO THIS WITH HOLDOUT OR COPLETELY NEW DATA Table 15.1: Posterior odds: the relative likelihood of one model over another Bayes factors Unrestricted / Prior 74.24 Restricted / Prior 42.54 Unrestricted / Restricted 1.74 Compare likelihoods of the data under different models Check look package for rstan 15.2 LOO DO THE LONG WAY POINT TO TECHNIQUES (Maybe look at WAIC) 15.3 Sensitivity approaches 15.3.1 Qualitative inference You need to provide priors in order to get Bayesian updating off the ground. But seeing your priors reported as hard cold numbers may make them feel like forced confessions. What if you just don’t believe them? Even in this case you might still engage in a form of “qualitative inference.” There is a literature on probabilistic causal models that assesses the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see Parsons (2001). Under this kind of approach a researcher might willing to say that they think some probability \\(p\\) is not plausibly greater than .5, but unwilling to make a statement about their beliefs about where in the \\(0\\) to \\(0.5\\) range it lies. Such incomplete statements can be enough to rule our classes of conclusion. Consider first process tracing models for which you are unsure Say for instance in our running example Figure 15.5: Possible inferences on X caused Y given observation of M=0 and M=1 given different possible models. The highest inference when M=0 is lower than the lowest inference when M=1 A graph showing how some conclusions changes as we relax one of the restrictions. How much do our conclusions depend on qual restrictions? How do conclusions differ if we drop all restrictions 15.3.2 Check confounding assumptions approach 2 – say actual confound is q~=0; but model assumes q = 0. Draw data from priors, draw data; given data type (001, 100 etc) plot (a) the posterior distribution under no confounding nad (b) the distribution of estimands that gave rise to the data. (Verma and Pearl, 1990) identify conditions under which we can check some independence assumptions. Say we have model model &lt;- make_model(&quot;X -&gt; M1 -&gt; M2 -&gt; Y &quot;) %&gt;% set_confound(list(M1 = &quot;Y[M2=1]&gt;Y[M2=0]&quot;)) Can we check that there is no direct path from \\(X\\) to \\(Y\\)? Pearl (1995) gives conditions for assessing for discrete data whether \\(Z\\) has a direct effect on \\(Y\\). (Involves inequalities) Evans (Graphical methods for inequality constraints in marginalized DAGs) generalizes the instrumental inequality. 15.4 Evaluating the Democracy-Inequality model 15.4.1 Prior check In a second iteration of the analysis, we show what happens if we loosen the monotonicity restriction on \\(I\\)’s effect on \\(M\\). Here we consider negative effects of \\(I\\) on \\(M\\) unlikely, rather than impossible, and we consider null and positive effects somewhat likely. We refer to these priors as “quantitative priors” in the sense that they place a numerical value on beliefs rather than a logical restriction. Here, we set our prior on \\(\\theta^M\\) as: \\(p(\\theta^M=\\theta^M_{10})=0.1\\), \\(p(\\theta^M=\\theta^M_{00})=0.25\\), \\(p(\\theta^M=\\theta^M_{11})=0.25\\), and \\(p(\\theta^M=\\theta^M_{01})=0.4\\). We show the results for the inferences given different findings in tables and . The mapping into expected posterior variance associated with each strategy is shown by the numbers in parentheses in Table . The results differ in various modest ways. However, the biggest difference we observe is in the degree to which the mobilization clue matters when we are looking for negative effects of inequality. As discussed, if we assumed monotonic positive effects of inequality on mobilization and monotonic positive effects of mobilization on inequality, then the mediator clue is uninformative about the indirect pathway since that pathway can only generate a positive effect. However, if we allow for the possibility of a negative effect of inequality on mobilization, we now make \\(M\\) informative as a mediator even when the effect of inequality that we are interested in is negative: it is now possible that inequality has a negative effect on democratization via a negative effect on mobilization, followed by a positive effect of mobilization on democratization. So now, observing whether mobilization occurred adds information about whether a negative effect could have occurred via the mobilization pathway. Moreover, it is possible for the two effects of observing \\(M\\) on our beliefs to work in opposite ways. What we learn from observing \\(M\\) about the \\(I \\rightarrow M \\rightarrow D\\) pathway may push in a different direction from what we learn from observing \\(M\\) about the direct \\(I \\rightarrow D\\) pathway. We see this dynamic at work in a case with low inequality and democratization. Where we are only learning about \\(M\\) as a moderator of \\(I\\)’s direct effect (monotonicity assumption in place), observing \\(M=0\\) shifts our beliefs in favor of \\(I\\)’s negative effect. But where we are learning about \\(M\\) as both mediator and moderator, observing \\(M=0\\) shifts our beliefs against \\(I\\)’s negative effect. The reason for this latter result is straightforward: if \\(I=0\\) and we then see \\(M=0\\), then we have just learned that inequality’s possible indirect negative effect, running via the mobilization pathway, has not in fact occurred; and this has a considerable downward effect on our beliefs in an overall negative effect of inequality. This learning outweighs the small positive impact of observing \\(M=0\\) on our confidence that \\(I\\) had a direct negative effect on \\(D\\). We see these differences most clearly in the cases of Albania (as compared to Mexico) and Nicaragua (as compared to Taiwan). Under priors fully constrained to monotonic causal effects, we saw that the mediator clue, \\(M\\), made only a small difference to our inferences. However, if we allow for a negative effect of \\(I\\) on \\(M\\), even while believing it to be unlikely, observing mobilization in Albania and Nicaragua makes us substantially more confident that inequality mattered, and differentiates our conclusions about these cases more sharply from our conclusions about Mexico and Taiwan, respectively. ** GENERATE A TABLE SHOWING HOW PIMD DOES ON 6 CRITEREA ** References "]
]
