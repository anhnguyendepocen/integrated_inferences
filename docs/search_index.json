[
["clue-selection-as-a-decision-problem.html", "Chapter 11 Clue Selection as a Decision Problem 11.1 A strategic approach 11.2 Clue selection for the running example 11.3 Clue selection for the democratization model 11.4 Conclusion", " Chapter 11 Clue Selection as a Decision Problem We draw out the implications of the causal model approach for clue selection strategies. We introduce a tool for generating an optimal decision tree for clue selection given. Consider now the problem of determining what qualitative data to gather on a case. Evidently it makes sense to gather information on clues that have large probative value, but whether or not clues have probative value can depend on what clues have already been collected: Finding out that the Butler had no motive may be informative for the claim that he is innocent, but it may not be useful if you already know he had no opportunity. To motivate our thinking about clue-selection, consider again our running example with the free press and government removal. We can use this toy example to see, intuitively, how researchers may have a choice among observations that could be informative, and how the informativeness of an observation can depend on what is already known. In Figure , we showed how one can use the structural equations to provide a set of conditional causal graphs that let one see easily what caused what at different values of the exogenous nodes \\(S\\) and \\(X\\). Each of these plots graphs a particular context. We can thus readily see which collection of exogenous nodes constitutes—gives the answer to—a given query, or estimand. Turning things around, we can also see, given a query, which nodes are informative about the probability that the query is true.1 FLAG: Where is the running example figure currently? Need to reference it. FLAG: I’m having trouble getting the logical progression here. Seems somewhat broken in the sense that it’s incomplete. Conceptually, we might think of informativeness situations as taking four possible forms of interest: a clue is always informative; never informative; informative only conditional on something else (obviously, what the something else is can vary); conditional only in the absence of something else. We seem to be covering always informative and conditionally uninformative. Seems odd not to also show always uninformative and informative only conditional on something else. But not sure if we can do this with this example. For example, suppose that one can see that \\(Y=0\\) but does not know the causal effect of \\(X\\) on \\(Y\\). This is equivalent to saying that we know that we are in panel \\(A\\), \\(B\\), or \\(C\\) but do not know which of these we are in. Would it be helpful to collect the clue \\(S\\) if one has no other information? Defining the query in terms of root nodes, the question becomes \\(S \\stackrel{?}{=} 1\\), or \\(P(S=1|X=0,Y=0)\\); the difference between the contexts in the two panels is that \\(S=0\\) when, and only when, \\(X=0\\) causes \\(Y=0\\). Given the structural equation for \\(S\\), \\(P(S|X=0,Y=0) = P(S|X=0)\\), and given independence of \\(X\\) and \\(S\\), \\(P(S=1|X=0)= \\pi^S\\) (the simple assignment propensity). Figuring out \\(S\\) fully answers the query.2 We can also see instances in this example of how existing data can make clues uninformative. Say one wanted to know if \\(X\\) causes \\(C\\) in a case. As we can see from inspection of the panels, this query is equivalent to asking whether \\(S=1\\) (as \\(X\\) causes \\(C\\) only in those two panels (\\(B\\) and \\(D\\)) where \\(S=1\\)). Data on \\(R\\) is unconditionally informative about this query as \\(R\\) is not \\(d-\\)separated from \\(S\\). For example, \\(R=1\\) implies \\(S=0\\). However, if \\(C\\) and \\(X\\) are already known, then \\(R\\) is no longer informative because \\(C\\) and \\(X\\) together d-separate \\(R\\) from \\(S\\).3 The running example also lets us demonstrate how informative clues can be found in many different places in a graph. Informative spouses Spouses—parents of the same child—can inform on one another. As we have seen in other examples, when an outcome has multiple causes, knowing the value of one of those causes helps assess the effect(s) of the other(s). For example, here, \\(S\\) and \\(X\\) are both parents of \\(C\\); \\(S\\) is thus informative for assessing whether \\(X\\) causes \\(C\\). Indeed this query, written in terms of roots, is simply \\(P(S)\\): \\(X\\) causes \\(C\\) if and only if \\(S=1\\). Likewise, \\(S\\) causes \\(C\\) (negatively) if and only if \\(X=1\\). Pre-treatment clues. Did the absence of media reports on corruption (\\(R=0\\)) cause government survival (\\(Y=0\\))? Look to the pre-treatment clue, \\(X\\): \\(X=0\\) is a smoking gun establishing that the absence of a report produced government survival. Or, substantively, if there were a free press, then a missing report would never be a cause of survival since it would occur only in the absence of corruption, which would itself be sufficient for survival. More broadly, this example illustrates how knowledge of selection into treatment can be informative about treatment effects. Post-outcome clues. Suppose we observe the presence of a free press (\\(X=1\\)) and want to know if it caused a lack of corruption (\\(C=0\\)), but cannot observe the level of corruption directly. Observing \\(Y\\)—which occurs after the outcome—is informative here: if \\(X=1\\), then \\(X\\) causes \\(C\\) (negatively) if and only if \\(Y=0\\). When an outcome is not observed, a consequence of that outcome can be informative about its value and, thus, about the effect of an observed suspected cause. Mediators as clues: We see a politically sensitive government (\\(S=1\\)) and its survival (\\(S=0\\)). Did the government survive because of its sensitivity to public opinion? Here, the mediation clue \\(C\\) is helpful: a lack of corruption, \\(C=0\\), is evidence of \\(S\\)’s negative effect on \\(Y\\). And, of course, different clues can be informative in different ways for different types of estimand. Needed then is a systematic way for identifying what clues to look for for answering a given type of causal quesiton, given what we already know—and perhaps, in what order to look for them. 11.1 A strategic approach The representation of inference problems as one of querying a Bayesian model points to a relatively simple method for answering this question, at least for small problems. Consider, first, a situation in which one has access to data \\(W\\) and wants to know the expected probative value of all possible collections of data one could gather. This can be done as follows: Define the model. Define a query on the model. Define a data strategy: a set of clues for which one might search (e.g., observe the value of \\(C\\)). Given prior data, figure out the probability of different realizations of the new data, and for each possible realization calculate the posterior variance. Then calculate the expected posterior variance for the data strategy by taking an average of these variances, with weights given by the probability of observing the clue realization in question. Repeat steps 3-4 for different data strategies. This procedure returns the expected posterior variances associated with different data strategies. A still more sophisticated strategy would, for multiple clues, take sequence into account: it would tell us which clues to search for later in the process given the realization of clues sought earlier. The path-dependence of clue selection arises from the possibility that the informativeness of a clue may depend on the value of other nodes in the model. A given clue \\(K_2\\), for instance, may be informative if another clue \\(K_1\\) has the value of 1 but not if it has the value 0. We provide tools for both of these approaches and illustrate them below for both the running example and the democracy application. 11.2 Clue selection for the running example Let’s return to the running example and assess the informativeness of different clue strategies for different estimands. Whereas we have in previous chapters specified fully deterministic functional equations for this model, we amend the model here by allowing for uncertainty over the nodal types for \\(C\\) and \\(R\\). At \\(C\\), we allow for the possibilities that corruption is always present and that corruption is always present except when there is both a free press (\\(X=1\\)) and sensitivity to public opinion (\\(S=1\\)). Thus, we permit both \\(\\theta^C_{1111}\\) and \\(\\theta^C_{1110}\\). At \\(R\\), we allow for both \\(\\theta^R_{0001}\\) and \\(\\theta^R_{0000}\\): the possibility that there is reporting on corruption if and only if there is corruption and a free press, and the possibility that there is never reporting on corruption. To summarize the intuition, then, governments will fall only if there is both corruption and reporting on corruption. We are uncertain whether corruption is ever-present or not, but if it is ever absent, it can only be because there exists both a free press and a government that cares about public opinion. We are also uncertain whether or not media reporting on corruption is always absent; but if it is ever present, it is only because there is both corruption and a free press. One implication is that governments that are sensitive to public opinion will never fall because they will always eschew corruption when a free press — the only mechanism that can generate reporting on corruption — is present. In turn, the presence of a free press can only matter for government survival if governments are not sensitive and thus do not strategically adjust their behavior in response to the risk of reporting. This model is formally defined in gbiqq as follows: model &lt;- make_model(&quot;S -&gt; C -&gt; Y &lt;- R &lt;- X; X -&gt; C -&gt; R&quot;) %&gt;% set_restrictions(labels = list(C = c(&quot;C1110&quot;, &quot;C1111&quot;), R = c(&quot;R0001&quot;, &quot;R0000&quot;), Y = c(&quot;Y0001&quot;)), keep = TRUE) %&gt;% set_priors() Suppose that our query is whether \\(X\\) has a positive effect on \\(Y\\). Using this model we can ask how likely different data realizations are and what we would infer about our query from each possible data realization, given existing data. We illustrate for a situation in which we already know that \\(X=0\\) and \\(Y=0\\). Application of the function conditional_inferences produces a matrix with the results. We reproduce these as Table 11.1. The first five columns of Table 11.1 define the data realizations. We have included all 12 combinations of possible realized values for all available clue strategies given that we have already observed \\(X=0\\) and \\(Y=0\\). A “0” or “1” represents the observed value for a node that we have chosen to observe while \\(NA\\) under a node indicates that that node is not observed under the given strategy. Thus, for instance, in the first row, we are collecting no clues beyond our prior data on \\(X\\) and \\(Y\\). In the second row, we have additionally sought clue \\(S\\) and observed a value of \\(0\\), while in the third row we have chosen the same strategy but observed \\(S=1\\). In the sixth column, we see the inference we would make from each data-realization, the posterior probability that \\(X\\) has a positive effect on \\(Y\\). The final column indicates the probability of each data-realization, given the chosen strategy. FLAG: NEEDS COMPLETING. FLAG: Let’s add posterior variance to Table 11.1 Table 11.1: Inferences given different data patterns. S X C R Y posterior prob NA NA NA NA 0 0.231 0.8125 0 NA NA NA 0 0.333 0.3750 1 NA NA NA 0 0.143 0.4375 NA 0 NA NA 0 0.375 0.5000 0 0 NA NA 0 0.500 0.2500 1 0 NA NA 0 0.250 0.2500 NA 1 NA NA 0 0.000 0.3125 0 1 NA NA 0 0.000 0.1250 1 1 NA NA 0 0.000 0.1875 NA NA 0 NA 0 0.000 0.1250 1 NA 0 NA 0 0.000 0.1250 NA 1 0 NA 0 0.000 0.1250 1 1 0 NA 0 0.000 0.1250 NA NA 1 NA 0 0.273 0.6875 0 NA 1 NA 0 0.333 0.3750 1 NA 1 NA 0 0.200 0.3125 NA 0 1 NA 0 0.375 0.5000 0 0 1 NA 0 0.500 0.2500 1 0 1 NA 0 0.250 0.2500 NA 1 1 NA 0 0.000 0.1875 0 1 1 NA 0 0.000 0.1250 1 1 1 NA 0 0.000 0.0625 NA NA NA 0 0 0.231 0.8125 0 NA NA 0 0 0.333 0.3750 1 NA NA 0 0 0.143 0.4375 NA 0 NA 0 0 0.375 0.5000 0 0 NA 0 0 0.500 0.2500 1 0 NA 0 0 0.250 0.2500 NA 1 NA 0 0 0.000 0.3125 0 1 NA 0 0 0.000 0.1250 1 1 NA 0 0 0.000 0.1875 NA NA 0 0 0 0.000 0.1250 1 NA 0 0 0 0.000 0.1250 NA 1 0 0 0 0.000 0.1250 1 1 0 0 0 0.000 0.1250 NA NA 1 0 0 0.273 0.6875 0 NA 1 0 0 0.333 0.3750 1 NA 1 0 0 0.200 0.3125 NA 0 1 0 0 0.375 0.5000 0 0 1 0 0 0.500 0.2500 1 0 1 0 0 0.250 0.2500 NA 1 1 0 0 0.000 0.1875 0 1 1 0 0 0.000 0.1250 1 1 1 0 0 0.000 0.0625 Each inference, under each data-realization, also has an associated posterior variance, or level of uncertainty. Given the probability of each data-realization, conditional on the clue strategy, it is easy to assess the expected reduction in variance from a given clue strategy. We present these expected posterior variances in Table ??. Strategy Given Prior belief Prior Uncertainty Expected Posterior Uncertainty X Y==0 0.230769230769231 0.177514792899408 0.144230769230769 S Y==0 0.230692307692308 0.177473366863905 0.168501769230769 C Y==0 0.231 0.177639 0.167937 R Y==0 0.231 0.177639 0.177639 X, R Y==0 0.230769230769231 0.177514792899408 0.144230769230769 X, S Y==0 0.230769230769231 0.177514792899408 0.134615384615385 X, C Y==0 0.230769230769231 0.177514792899408 0.144230769230769 C, R Y==0 0.231 0.177639 0.167937 C, S Y==0 0.230615384615385 0.177431928994083 0.164051230769231 S, R Y==0 0.230692307692308 0.177473366863905 0.168501769230769 X, C, S Y==0 0.230769230769231 0.177514792899408 0.134615384615385 X, C, R Y==0 0.230769230769231 0.177514792899408 0.144230769230769 X, S, R Y==0 0.230769230769231 0.177514792899408 0.134615384615385 C, S, R Y==0 0.230615384615385 0.177431928994083 0.164051230769231 X, C, S, R Y==0 0.230769230769231 0.177514792899408 0.134615384615385 We operationalize higher levels of expected learning from a strategy as a greater expected reduction in variance upon observing the data. We can see a couple of patterns here. By far the biggest gains in learning come from observing \\(X\\). We can see this most readily by comparing the 1-clue strategies to one another. But in general, any strategy that includes observing \\(X\\) always does substantially better than the comparable strategy that excludes \\(X\\). The intuition here is fairly straightforward: if we want to know whether \\(Y=0\\) was caused by \\(X=0\\), and start out very uncertain about \\(X\\)’s value, we should expect to learn a good deal from figuring out whether \\(X\\) is in fact equal to \\(0\\). Once we decide to observe \\(X\\), then the next-most informative clue to add to our research design is \\(S\\): \\(X, S\\) has the lowest expected posterior variance of any of the 2-clue strategies; and the 3-clue strategies that include both \\(X\\) and \\(S\\) perform better in expectation than does the one that excludes \\(S\\). This result is consistent with a broader point that we wish to emphasize in this chapter: while process tracing often focuses on examining steps along causal pathways, it will often be the case that we learn more from moderators, like \\(S\\) in this model, than from mediators. We return to this point below. In this model, the \\(S\\) is potentially highly informative because if a government is sensitive, it will never fall (\\(Y\\) will be 0) regardless of the presence or absence of a free press. We learn less from \\(S\\) if we observe \\(S\\) and find out if is \\(0\\); but if we look for \\(S\\), we have a 0.5 chance of observing a value that completely settles the matter. We cannot improve on an \\(X, S\\) strategy by gathering more data. Once we have decided to observe \\(X\\) and \\(S\\), there are no expected gains from also observing either \\(C\\) or \\(R\\) or both. They cannot tell us anything about whether \\(X=0\\) could have caused \\(Y=0\\) that we don’t already know from \\(X\\) and \\(S\\).^[Here is the logic: \\(Y=0\\) can occur either because \\(C=0\\) or because \\(R=0\\). The only way for \\(C=0\\) to arise is if the government is sensitive (\\(S=1\\)) and there is a free press (\\(X=1\\)). And while we are uncertain whether \\(C\\)’s nodal type is such that \\(C\\) will in fact be \\(0\\) when \\(S=1\\) and \\(C=1\\), this would represent a negative effect of \\(X\\) on \\(Y\\), not a positive one. There are three ways to get no reporting (\\(R=0\\)): (1) there is never any reporting, in which case \\(X\\) does not mattereither \\(C=0\\) Thus, if the search for information is costly, the dominant strategy is the first, looking only for \\(X\\) and \\(S\\) dominates all 3- and 4-clue strategies. Clues can be more informative jointly than separately: the expected gains to observing one clue can depend on which other clues we plan to observe. To see this, notice that among the 1-clue strategies, observing \\(C\\) is slightly more informative than observing \\(S\\). OTHER ANALYSES: Try asking about a negative effect. Try for different priors on X and S being 1 – makes them less informative. 11.3 Clue selection for the democratization model We now apply this approach to the model of democratization that we worked with in Chapters (ptapp) and (mixingapp). ## Parameters in set I-1 do not sum to 1. Using normalized parameters ## Parameters in set I do not sum to 1. Using normalized parameters ## Warning in draw_event_prob(model, parameters = parameters): Event ## probabilities not summing to 1 Table 11.2: Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization. I P M D posterior prob 1 NA NA 1 0.128 0.098 1 0 NA 1 0.231 0.027 1 1 NA 1 0.088 0.071 1 NA 0 1 0.000 0.015 1 0 0 1 0.000 0.002 1 1 0 1 0.000 0.013 1 NA 1 1 0.150 0.083 1 0 1 1 0.250 0.025 1 1 1 1 0.107 0.058 We show in Table how uncertainty is likely to be reduced with different research designs. In this table, we show these reductions for the two kinds of cases in which democratization does occur. The first row displays the variance on our posterior belief about the effect of \\(I\\) on \\(D\\) before we observe anything at all. The second row shows what happens to that uncertainty when we observe just cause and outcome, \\(I\\) and \\(D\\). The next four rows show the results for four possible choices in regard to process tracing: looking for neither \\(M\\) nor \\(P\\) (which is identical to doing no process tracing at all); looking for \\(P\\); looking for \\(M\\); and looking for both. The clearest message here is that, if we had to choose between clues, we should observe \\(P\\): given our model (including our priors on the types), we reduce our uncertainty more by learning about an alternative cause than by learning about a mediator. We also see that the mediator is much more informative when the causal effect we are looking for is one that could have operated via the mediator, as compared to when the mediator is informative only as a moderator of the cause’s direct effects. FLAG: Table needs a column defining the strategies. Table 11.3: Variances and expected variances given different clue seeking stratgies for cases in which we have observed high inequality and democratization. given prior_estimand prior_var E_post_var I==1 &amp; D==1 0.128 0.112 0.112 I==1 &amp; D==1 0.128 0.111 0.107 I==1 &amp; D==1 0.128 0.111 0.109 I==1 &amp; D==1 0.128 0.111 0.105 FLAG: Are we merging these two tables? Have same label. Table 11.4: Variances and expected variances given different clue seeking stratgies for cases in which we have observed low inequality and democratization. strategy given prior_estimand prior_var E_post_var None I==0 &amp; D==1 0.438 0.246 0.246 P I==0 &amp; D==1 0.438 0.246 0.229 M I==0 &amp; D==1 0.438 0.246 0.245 P and M I==0 &amp; D==1 0.438 0.246 0.225 FLAG: applied case-level analyses involving causal pathways, actual causes, and notable causes. For population-level estimands or is this redundant? See issue 53. 11.3.1 Dynamic Strategies The clue-collection strategies described above assume that researchers identify the full set of clues to be gathered in advance and do not alter their strategies as they go along. However, the expected informativeness of a clue may depend on the values of other clues that we see first, implying that an optimal strategy will be dynamic, taking into account earlier observations in selecting later ones. Given \\(n\\) nodes, a dynamic data collection strategy will be of the form: \\[\\sigma = \\{K_1, (K_2|K_1 = 1), (K_2|K_1 = 0), (K_3|K_1=1, K_2 =0)\\dots\\}\\] where each \\(K_j\\) is en element of the nodes on the graph, or is the empty set. Thus, we start with observing \\(K_1\\); then, whether we choose to observe \\(K_2\\) depends on the value of \\(K_1\\); whether we choose to observe \\(K_3\\) depends on the value of \\(K_1\\) and (if we observed it) \\(K_2\\); and so on. A strategy vector specifies a series of conditional clue-search actions: it identifies the first clue sought and then which clues are sought conditional on the realization of all prior clues sought. Each possible strategy has an associated expected reduction in variance as well as an associated expected cost. To illustrate with the running example we imagine a situation in which it is known that \\(Y=1\\) and we are interested in whether \\(Y=0\\) because of \\(S\\) (though we don’t know at the outset what the value of \\(S\\) is). We consider strategies in which we first seek information on one node and then, conditional on what we find, we look for data on one other node (or not). With five nodes, one already known, there are \\(4 \\times 4^2\\) strategies of this form. Suppose that we observe that \\(Y=0\\): the government was not replaced. We then want to know whether this is because the government was sophisticated (\\(S=1\\)). If we learn that the government was not sophisticated, then this answers the question in the negative. If we learn that the government was sophisticated then we can infer that this was the cause if we learn that there was a free press (or if we learn that there was no corruption). For each strategy we can then assess the expected variance reduction; in addition, if collecting different clues comes at different costs—but collection depends on past findings—then we can also calculate the expected costs of each strategy. Illustration of three (of many) possible two step strategies. Strategy Step 1 Step 2 if 0 Step 2 if 1 Expected variance Expected Cost 1 S None None 0.167 1 2 S X X 0 2.5 3 S None X 0 2 Figure ?? plots a collection of strategies based on two criteria—the variance reduction and the expected number of clues sought, which could be an indicator for cost. One can see a frontier of optimal strategies, depending on how these two desiderata trade-off against each other. For the figure, we imagined that \\(X\\) is the most costly to collect, followed by \\(C\\), then \\(S\\), then \\(Y\\), then \\(R\\). The cheapest strategy among those that minimize variance involves gathering \\(C\\) only. The lowest variance strategy that minimizes costs involves gathering \\(Y\\) only. FLAG: Think we need to label the strategies in graph somehow. Here we implement the same exercise for the democracy model. We illustrate with a case where we know there is inequality and democratization and we want to know if inequality caused democratization. We will assume for the illustratation that pressure is easy to observe but moviliazation is difficult. one two_0 two_1 expected_n expected_variance expected_costs P NA NA 1.0000 0.1072 1.5000 P M NA 1.2766 0.1059 1.8319 P NA M 1.7234 0.1061 2.3681 P M M 2.0000 0.1048 2.7000 M NA NA 1.0000 0.1085 1.2000 M P NA 1.1489 0.1085 1.4234 M NA P 1.8511 0.1048 2.4766 M P P 2.0000 0.1048 2.7000 11.4 Conclusion Explicit statement of a causal model—including prior beliefs over roots—allows one to assess what will be inferred from all possible observations. This opens the way for simple strategies for assessing what data is most valuable, and in what order it should be gathered. We are conscious that here we are pushing the basic logic to the limits. In practice researchers will often find it difficult to describe a model in advance and to place beliefs on nodes. Moreover the collection of new data could easily give rise to possibilities and logics that were not previously contemplated. Nothing here seeks to deny these facts; the claim here is a simpler one: insofar as one can specify a model before engaging in data gathering, the model provides a powerful tool to assess what data is most useful to gather. With larger graphs, continuous variables, and more stochastic components, it may not be feasible to graph every possible context; but the strategy for inference remains the same.↩ Graphically what is important is that \\(S\\) is informative not because it is \\(d-\\)connected with \\(Y\\), but because it is \\(d-\\)connected to the query variable—here, simply, to itself.↩ We can come to the same conclusion by reasoning with the graphs: if \\(X=0\\) and \\(C=1\\), we know we are in subfigure \\(A\\) or \\(B\\), and \\(X\\) causes \\(C\\) only in panel \\(B\\). However, \\(R\\) is of no help to us in distinguishing between the two contexts as it takes the same value in both graphs.↩ "]
]
