[
["mm.html", "Chapter 10 Mixing models 10.1 A jigsaw puzzle: Integrating across a model 10.2 Multilevel models, meta-analysis 10.3 Combining observational and experimental data 10.4 Transportation of findings across contexts", " Chapter 10 Mixing models We provide three examples of situations in which, by combining models, researchers learn more than they could from any single model. In the previous two chapters, we described one form of integration that structural causal models can enable: the systematic combination of (what we typically think of as) qualitative and quantitative evidence for the purposes of drawing population- and case-level causal inferences. One feature of the analyses we have been considering so far is that the integration is essentially “nested.” We are, for instance, integrating quantitative evidence for a large set of cases with qualitative evidence for a subset of those cases. We are, moreover, drawing inferences from the set of cases we observe to a population within which that sample of cases is situated. In this chapter, we examine how we can use structural causal models to integrate across studies or settings that are, in a sense, more disjointed from one another: across studies that examine different causal relationships altogether; study designs that require different assumptions about exogeneity; and contexts across which the causal quantities of interest may vary. Integrating across a model Often, individual studies in a substantive domain examine distinct segments of a broader web of causal relationships. For instance, while one study might examine the effect of \\(X\\) on \\(Y\\), another might examine the effect of \\(Z\\) on \\(Y\\), and yet another might examine the effect of \\(Z\\) on \\(K\\). We show in this chapter how we can integrate across such studies in ways that yield learning that we could not achieve by taking each study on its own terms. Integrating between experimental and observational studies One form of multi-method research that has become increasingly common is the use of both observational and experimental methods to study the same basic causal relationships. While an experiment can offer causal identification in a usually local or highly controlled setting, an observational analysis can often shed light on how the same relationships operate “in the wild,” if with greater risk of confounding. Usually, observational and experimental results are presented in parallel, as separate sources of support for a causal claim. We show how, in a causal model setup, we can use experimental and observational data jointly to address questions that cannot be answered when the designs are considered separately. Transporting knowledge across contexts Researchers are sometimes in a situation in which they can identify causal quantities in a particular setting — say, from a randomized controlled trial implemented in a specific local context — but want to know how those inferences travel to other settings. Would the intervention work differently in other countries or regions? As we will explain, with an appropriately specified causal model and the right data from the original context, we can draw inferences about causal relationships in other contexts. Before delving into the details of these strategies, we make one key qualification explicit: each of these approaches requires us to believe that setting-, or study-, specific causal model can be nested within a lower level, “encompssing,” model that operates across the multiple settings that we are learning from and want to draw inferences about. Encompassing models, of course, can specifically take heterogeneity across settings into account, for instance by including in the model moderators that condition the effects of interest. But we have to believe that we have indeed captured in the model any ways in which relationships vary across the set of contexts across which we are integrating evidence or transporting inferences. Put differently, and perhaps more positively, we see social scientists commonly seeking to transport knowledge or combine information informally across studies and settings. Often such efforts are motivated, sometimes implicitly, by an interest in or reliance on general theoretical propositions. The approaches that we describe below force the researcher to be explicit about the underlying causal beliefs that warrant that integration while also ensuring that the integration proceeds in a way that is logically consistent with stated beliefs. 10.1 A jigsaw puzzle: Integrating across a model Generating knowledge about a causal domain often involves cumulating learning across studies that each focus in on some specific part of the domain. For instance, scholars interested in the political economy and democratization might undertake studies focused on the relationship between inequality and mass protests; studies on the role of mass mobilization in generating regime change; pathways other than mass mobilization through which inequality might affect democratization; studies of the role of international sanctions on the likelihood that autocracies will democratize; and studies of the effects of democratization on other things, such as growth or the distribution of resources. We can think of these studies as each analyzing data on a particular part of a broader, more encompassing causal model. In an informal way, if findings “hold together” in a reasonably intuitive way, we might be able to piece together an impression of the overall relations among variables in this domain. Yet an informal approach becomes more difficult for complex models or data patterns and, more importantly, will leave opportunities for learning unexaploited. Consider this simple DAG, in which both \\(X\\) and \\(Z\\) are causes of \\(Y\\), and \\(Z\\) also causes \\(K\\). Now imagine three studies, all conducted in contexts in which we believe this model to hold: Study 1 is an RCT in which \\(X\\) is randomized, with data collected on both \\(Y\\) and \\(K\\). \\(K\\) is collected. \\(Z\\) is not observed. Study 2 is a factorial experiment, in which \\(X\\) and \\(Z\\) are independently randomized, allowing an examination of the joint effects of \\(X\\) and \\(Z\\) on \\(Y\\). \\(K\\) is not observed. Study 3 is an experiment randomizing \\(Z\\), with only \\(K\\) observed as an outcome. \\(X\\) and \\(Y\\) are not observed. Now, let’s say that our primary interest is in the relationship between \\(X\\) and \\(Y\\). Obviously, Study 1 will, with a sufficiently large sample, perform just fine in estimaing the average treatment effect of \\(X\\) on \\(Y\\). However, what if we are interested in a case-oriented query, such as the probability of causation: the probability, say, \\(X=1\\) caused \\(Y=1\\) in a given \\(X=1, Y=1\\) case? We know that within-case, process-tracing clues can sometimes provide probative value on case-level estimands like the probability of causation, and we have observed \\(K\\) in the Study 3 cases. So what if we combine the \\(X\\), \\(Y\\), and \\(K\\) data? A simple analysis of the graph tells us that \\(K\\) cannot help us learn about \\(Y\\)’s potential outcomes since \\(K\\) and \\(Y\\) are \\(d\\)-separated by \\(Z\\), and we have not observed \\(Z\\) in Study 3. We see this confirmed in Table 10.1. In the first pair of rows, we show the results of analyses in which we have simulated data from the whole model, then updated using the Study 1 observations. We give here the posterior mean on the probability of causation for an \\(X=Y=1\\) case, conditional on each possible value that \\(K\\) might take on. As we can see, our beliefs about the estimand remain unaffected by \\(K\\)’s value, meaning that it contains no information about \\(X\\)’s effect in the case. We see that the same thing is true for each of the other studies. In study 2, we have not used \\(K\\) to update the model, and so have not learned anything form the data about \\(K\\)’s relationship to the other variables. Thus, we have no foundation on which to ground probative value fo \\(K\\). In study 3, we understand the \\(Z,K\\) relationship well, but know nothing quantitatively about how \\(Z\\) and \\(X\\) relate to \\(Y\\). Thus, we have learned nothing from Study 3 about what observing \\(K\\) might tell us about the effect of \\(X\\) on \\(Y\\). Table 10.1: The clue \\(K\\) uninformative in all three studies Study Given mean sd 1 X == 1 &amp; Y == 1 &amp; K == 1 0.652 0.107 X == 1 &amp; Y == 1 &amp; K == 0 0.640 0.108 2 X == 1 &amp; Y == 1 &amp; K == 1 0.645 0.138 X == 1 &amp; Y == 1 &amp; K == 0 0.645 0.137 3 X == 1 &amp; Y == 1 &amp; K == 1 0.499 0.150 X == 1 &amp; Y == 1 &amp; K == 0 0.499 0.134 We can do much better, however, if we combine the data and update jointly across all model paramaters. The results are shown in Table 10.2. Updating simultaneously across the studies allows us, in a sense, to bridge across inferences. In particular, inferences from Study 2 make \\(Z\\) informative about \\(Y\\)’s potential outcomes under different values of \\(X\\). Meanwhile, inferences from the data in Study 3 allow us to use information on \\(K\\) to update on values for \\(Z\\). As we now see in rows 1 and 2, having updated the model in an integrated fashion, \\(K\\) now is informative about the probability of causation, with our posterior mean on this query changing substantially depending on the value of \\(K\\) that we observe in a case. Rows 3-4 highlight that the updating works through inferences on \\(Z\\): we see that if \\(Z\\) is already known (we show this for \\(Z=1\\), but it holds for \\(Z=0\\) as well), then there are no additional gains from knowledge of \\(K\\). Table 10.2: Clue is informative after combining studies linking \\(K\\) to \\(Z\\) and \\(Z\\) to \\(Y\\) Given mean sd X == 1 &amp; Y == 1 &amp; K == 1 0.79 0.08 X == 1 &amp; Y == 1 &amp; K == 0 0.62 0.12 X == 1 &amp; Y == 1 &amp; K == 1 &amp; Z == 1 0.84 0.08 X == 1 &amp; Y == 1 &amp; K == 0 &amp; Z == 1 0.84 0.08 We devote Chapter 15 to a discussion of how we justify a model. However, we note already that in this example we have an instance in which a researcher (examining a case in study 3) might wish to draw inferences using \\(K\\), but she does not have anything in study 1 that justifies using \\(K\\) for inference. However with access to studies 2 and 3, and conditional on the overall model, she has a justification for process tracing strategy. The general principle is that weaker commitments to lower level theories —here the causal structure—can justify more fully inferences from more fully specified higher-level theories. 10.2 Multilevel models, meta-analysis A key idea in Bayesian meta-analysis is that when you analyze multiple studies together you learn not only about common processes that give rise to the different results seen in different sites, but you also learn more about each study from seeing the other studies. A classic setup is provided in GELMAN, which we have access to estimates of effects and uncertainty in eight sites (schools), \\((b_j, se_j)_{j \\in \\{1,2,\\dots,8\\}}\\). We assume that each \\(b_j\\) is a draw from distribution \\(N(\\beta_j, se_j)\\) and that each \\(\\beta_j\\) is a draw from distribution \\(N(\\beta, \\sigma)\\). In that setup we want to learn not just about the superpopulation parameters \\(\\beta, \\sigma\\), but also about the study level effects \\((\\beta_j)_{j \\in \\{1,2,\\dots,8\\}}\\). In a similar way here we define a model in which “setting” is a node. We observe an experiment that takes pace in setting 0 and in setting 1. We then conside the different conclusions we draw for the effect of \\(X\\) on Table 10.3: Inferences from separate analyses and from integrated analysis (meta analysis) given (a) flat priors and (b) expectation of similar effects across studies Query Given Using mean sd Setting 0 - posteriors 0.43 0.09 Setting 1 - posteriors 0.20 0.10 Integrated (flat priors) - posteriors 0.26 0.06 Integrated (flat priors) Setting==0 posteriors 0.36 0.08 Integrated (flat priors) Setting==1 posteriors 0.17 0.09 Integrated (weak heterogeneity) - posteriors 0.29 0.06 Integrated (weak heterogeneity) Setting==0 posteriors 0.32 0.07 Integrated (weak heterogeneity) Setting==1 posteriors 0.26 0.08 We see in both cases a drop in our estimates for effects in Setting 1 in both cases, relative to the single study case. Where weak heterogeneity is assumed we also see a rise in estimates for Setting 2. We can also update on the amount of heterogeneity. Here the estimand is the share of units that would respond differently to treatment if they were in a different setting. We see a dropin expectations of heterogeneitz in the analysis with flat priors, but a rise relative if we start thinking heterogeneity is unlikley. Table 10.4: Interaction | Flat priors Query Given Using mean sd Q 1 - priors 0.628 0.119 Q 1 - posteriors 0.588 0.122 Table 10.4: Interaction | Expected homogeneity Query Given Using mean sd Q 1 - priors 0.077 0.072 Q 1 - posteriors 0.087 0.074 10.3 Combining observational and experimental data Experimental studies are sometimes referred to as gold standard. But an interesting weakness of experimental studies is that, by dealing so effectively with self selection into treatment, they limit our ability to learn about self selection. Often however we want to know what causal effects would be specifically for people that would take up a treatment in non experimental settings. This kind of problem is studied for example by Knox et al. (2019). A causal model can encompass both experimental and observational data and let you answer this kind of question. To illustrate, imagine that node \\(R\\) indicates whether a unit was assigned to be randomly assigned to treatment (\\(X=Z\\) if \\(R=1\\)) or took on its observational value (\\(X=O\\) if \\(R=0\\)). We assume the exclusion restriction that entering the experimental sample is not related to \\(Y\\) other than through assignment of \\(X\\). We plot the model in Figure 10.1. Figure 10.1: A model that nests an observational and an experimental study. The treatment \\(X\\) either takes on the observational value \\(O\\), or the assigned values \\(Z\\), depending on whether or not the case has been randomized, \\(R\\). In this model, \\(X\\) has only one causal type since its job is to operate as a kind of switch, inheriting the value of \\(Z\\) or \\(O\\) depending on \\(R\\). Parameters allow for complete confounding between \\(O\\) and \\(Y\\) but \\(Z\\) and \\(Y\\) are unconfounded. We imagine parameter values in which there is a true .2 effect of \\(X\\) on \\(Y\\). However the effect is positive (.6) for cases in which \\(X=1\\) under observational assignment but negative (-.2) for cases in which \\(X=0\\) under observational assignment. (See appendix for complete specification.) The implied estimands and priors are as in Table 10.5. Table 10.5: Estimands in different sites Query Given Using mean sd ATE - parameters 0.2 ATE - priors 0.0 0.26 ATE R==0 parameters 0.2 ATE R==0 priors 0.0 0.26 ATE R==1 parameters 0.2 ATE R==1 priors 0.0 0.26 The true effect is .2 but naive analysis on the observational data would yield a strongly upwardly biased estimate. Table 10.6 shows differences-in-means estimates using data on observational units only drawn from this model. Table 10.6: Inferences on the ATE from differences in means Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF X 0.816 0.027 30.105 0 0.762 0.869 205 The estimates from updating on the full model, shown in Table 10.7, are much better. Table 10.7: Estimates on the ATE for observational (\\(R=0\\)) and experimental (\\(R=1\\)) set. Query Given Using mean sd ATE - posteriors 0.203 0.031 ATE R==0 posteriors 0.203 0.031 ATE R==1 posteriors 0.203 0.031 Since the model used both the experimental and the observational data, it is interesting to ask whether the observational data improved the estimates of the treatment effect or does inference draw only from the experimental data? We answer the question in the appendix by updating using experimental data only. We find there that we do indeed get a tightening of posterior variance and a more accurate result when we use the observational data but that the gains are relatively small: the experimental data alone is quite powerful. The gains would be smaller still if we had more data, in which case inferences from the experimental data would be more accurate still. However we do learn something of particular interest from this model. A key feature here is that there is heterogeneity between those that are in treatment and those that are in control in the observational sample. We learn nothing about this heterogeneity from the experimental data alone but we learn a lot from the mixed model, picking up the strong self selection into treatment in the observational group: Table 10.8: Effects of \\(X\\) conditional on \\(X\\) for units that were randomly assigned or not. Effects of \\(X\\) do not depend on \\(X\\) in the experimental group, but they do in the observational group becuase of seld selection. Query Given Using mean sd ATE R==1 &amp; X==0 posteriors 0.203 0.031 ATE R==1 &amp; X==1 posteriors 0.203 0.031 ATE R==0 &amp; X==0 posteriors -0.183 0.027 ATE R==0 &amp; X==1 posteriors 0.593 0.048 In essence by mixing the experimental and observational data we can learn what the effects for those that self select into treatment and what they would be for those that self select into control. The results here relate to the LATE theorem (Angrist and Imbens 1995) in the following way. If we imagine using data only on (a) the experimental group in control and (b) the observational group, some of whom are in treatment, we can conceptualize our design as one in which the observational group are “encouraged” to take up treatment and we figure out the effect for the compliers in this group (those that self select into treatment). At the same time if we imagine using data only on (a) the experimental group in treatment and (b) the observational group, some of whom are in control, we can conceptualize our design as one in which the observational group are “encouraged” to take teh control condition and we figure out the effect for the compliers in this group (those that self select into control). 10.4 Transportation of findings across contexts Say we study the effect of \\(X\\) on \\(Y\\) in case 0 (a country, for instance) and want to make inferences to case 1 (another country). Our problem however is that effects are heterogeneous and features that differ across units may be related both to treatment assignment, outcomes, and selection into the sample. This is the problem studied by Pearl and Bareinboim (2014). In particular Pearl and Bareinboim (2014) show for which nodes data is needed in order to “licence” external claims, given a model. We illustrate with a simple model in which a confounder has a different distribution in a study site and a target site. Figure 10.2: Extrapolation when confounders have different distributions across cases. Table 10.9: Priors and true values (parameters) for three estimand: the frequency of \\(W\\), the effect of \\(X\\) on \\(Y\\), and the effect conditional on \\(W=1\\) Query Given Using mean sd Incidence Case==0 priors 0.334 0.237 Incidence Case==0 parameters 0.333 Incidence Case==1 priors 0.666 0.238 Incidence Case==1 parameters 0.667 ATE Case==0 priors 0.002 0.140 ATE Case==0 parameters 0.333 ATE Case==1 priors 0.003 0.142 ATE Case==1 parameters 0.573 CATE Case==0 priors 0.002 0.174 CATE Case==0 parameters 0.812 CATE Case==1 priors 0.002 0.174 CATE Case==1 parameters 0.812 Priors and estimands (parameters) are show in Table 10.9. We see that the incidence of \\(W\\) as well as the ATE of \\(X\\) on \\(Y\\) is larger in case 1 than in case 0 (in parameters, though not in priors). However the effect of \\(X\\) on \\(Y\\) conditional on \\(W\\) is the same in both places. We now update the model using data on \\(X\\) and \\(Y\\) only from one case (case 1) and data on W from both and check inferences on the other. Table 10.10: Extrapolation when two sites differ on \\(W\\) and \\(W\\) is observable in both sites Query Given Using mean sd Incidence Case==0 posteriors 0.336 0.007 Incidence Case==0 parameters 0.333 Incidence Case==1 posteriors 0.661 0.007 Incidence Case==1 parameters 0.667 ATE Case==0 posteriors 0.340 0.011 ATE Case==0 parameters 0.333 ATE Case==1 posteriors 0.570 0.009 ATE Case==1 parameters 0.573 CATE Case==0 posteriors 0.810 0.009 CATE Case==0 parameters 0.812 CATE Case==1 posteriors 0.810 0.009 CATE Case==1 parameters 0.812 We do well in recovering the (different) effects both in the location we study and the one in which we do not. In essence querying the model for the out of sample case requests a type of post stratification. We get the right answer, though as always this depends on the model being correct. Had we attempted to make the extrapolation without data on \\(W\\) in country 1 we would get it wrong. In that case however we would also report greater posterior variance. The posterior variance here captures the fact that we know things could be different in country 1, but we don’t know in what way they are different. Note that we get the CATE right since in the model this is assumed to be the same across cases. Table 10.11: Extrapolation when two sites differ on \\(W\\) and \\(W\\) is not observable in target country. Query Given Using mean sd Incidence Case==0 posteriors 0.329 0.007 Incidence Case==0 parameters 0.333 Incidence Case==1 posteriors 0.675 0.007 Incidence Case==1 parameters 0.667 ATE Case==0 posteriors 0.319 0.011 ATE Case==0 parameters 0.333 ATE Case==1 posteriors 0.572 0.009 ATE Case==1 parameters 0.573 CATE Case==0 posteriors 0.811 0.009 CATE Case==0 parameters 0.812 CATE Case==1 posteriors 0.811 0.009 CATE Case==1 parameters 0.812 References "]
]
