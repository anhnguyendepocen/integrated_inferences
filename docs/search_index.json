[
["models.html", "Chapter 2 Causal Models 2.1 The counterfactual model 2.2 Causal Models and Directed Acyclic Graphs 2.3 Graphing models and using graphs 2.4 Chapter Appendix", " Chapter 2 Causal Models :::: {.headerbox data-latex=“”} ::: {.center data-latex=“”} ::: We provide a lay language primer on the logic of causal models. :::: Causal claims are everywhere. Causal knowledge is often the end goal of empirical social science. It is also a key input into causal inference. Rarely do we arrive at causal inquiry fully agnostic about causal relations in the domain of interest.1 Causal assumptions are also hidden in seemingly descriptive statements: claims that someone is guilty, or exploited, or powerful, or weak, involve beliefs about how things would be were conditions different. Even when scholars carefully try to avoid causal claim-making, causal verbs–depends, drives, produces, influences—tend to surface. But while causal claims are commonplace, it is not always clear (1) what exactly is meant by a causal relation and (2) how causal knowledge about one thing can be marshaled to justify causal claims about another. For our purposes, the counterfactual view of causality addresses the first question. Causal models address the second. 2.1 The counterfactual model We begin with what we might think of as a meta-model, the counterfactual model of causation. At its core, a counterfactual understanding of causation captures a simple notion of causation as “difference-making.”2 In the counterfactual view, to say that \\(X\\) caused \\(Y\\) is to say: had \\(X\\) been different, \\(Y\\) would have been different. Implicitly, the antecedent, “had \\(X\\) been different,” imagines a controlled change in \\(X\\)—an intervention that altered \\(X\\)’s value—rather than a naturally arising difference in \\(X\\). The counterfactual claim, then, is not that \\(Y\\) is different in those cases in which \\(X\\) is different; it is, rather, that if one could somehow have made \\(X\\) different, \\(Y\\) would have been different. Consider a toy example. Students in class A perform well without studying. Students in class B perform well if they study, and do not perform well if they do not study. Moreover, only class B students in fact study, and all perform well. When we say that one of the class B students did well because they studied, we are comparing the outcome that they experienced to the outcome they would have experienced if they had been in class B (as they were) but (counterfactually) had not studied. Notably, we are not comparing their realized outcome to the outcome they would have experienced if they had been among the people that in fact didn’t study (i.e., if they had been class A students). A second example. Consider the claim that India democratized (\\(Y\\)) because it had a relatively high level of economic equality (\\(X\\)) (drawing on the logic of Boix (2003)). In the counterfactual view, this is equivalent to saying that, had India not had a high level of equality, the country would not have democratized. High economic equality made a difference. The comparison for the causal statement is with the outcome India would have experienced under an intervention that boosted its historical level of economic inequality — not with how India would have performed if it had been one of the countries that in fact had higher levels of inequality, cases that likely differ from India in other causally relevant ways. Along with this notion of causation as difference-making, we also want to allow for variability in how \\(X\\) acts on the world. \\(X\\) might sometimes make a difference, for some units of interest, and sometimes not. High levels of equality might generate democratization in some countries or historical settings but not in others. Moreover, while equality might make democratization happen in some times in places, it might prevent that same outcome in others. We need a language to describe these different types of relations. 2.1.1 Potential outcomes The “potential outcomes” framework is useful for describing the different kinds of counterfactual causal relations that might prevail between variables . In this framework we characterize how a given unit responds to a causal variable by positing the outcomes that it would take on at different values of the causal variable. A setting in which it is quite natural to think about potential outcomes is medical treatment. Imagine some individuals in a diseased population are observed to have received a drug (\\(X=1\\)) while others have not (\\(X=0\\)). Assume that, subsequently, a researcher observes which individuals become healthy (\\(Y=1\\)) and which do not (\\(Y=0\\)). Given the assignments of all other individuals,3 we can treat each individual as belonging to one of four unobserved response “types,” defined by the outcomes that the individual would have if they received or did not receive treatment:4 adverse: Those individuals who would get better if and only if they do not receive the treatment beneficial: Those who would get better if and only if they do receive the treatment chronic: Those who will remain sick whether or not they receive treatment destined: Those who will get better whether or not they receive treatment Table 2.1 maps the four types (\\(a, b, c, d\\)) onto their respective potential outcomes. In each column, we have simply written down the outcome that a patient of a given type would experience if they are not treated, and the outcome they would experience if they are treated. Table 2.1: . Potential outcomes: What would happen to each of four possible types of case if they were or were not treated. Type a Type b Type c Type d adverse effects beneficial Effects chronic cases destined cases Outcome if not treated Healthy Sick Sick Healthy Outcome if treated Sick Healthy Sick Healthy We highlight that, in this framework, case-level causal relations are treated as deterministic. A given case has a set of potential outcomes. Any uncertainty about outcomes enters as incomplete knowledge of a case’s “type,” not from underlying randomness in causal relations. This understanding of causality—as ontologically deterministic, but empirically imperfectly understood—is compatible with views of causation commonly employed by qualitative researchers (see, e.g., Mahoney (2008)), and with understandings of causal determinism going back at least to Laplace (1901). As we will also see, we can readily express this kind of incompleteness of knowledge within a causal model framework: indeed, the way in which causal models manage uncertainty is central to how they allow us to pose questions of interest and to learn from evidence. There are certainly situations we could imagine in which one might want to conceptualize potential outcomes themselves as random (for instance, if individuals in different conditions play different lotteries). But for the vast majority of the settings we condsider, not much of importance is lost if we treat potential outcomes as deterministic but possibly unknown: at the end of the day something will occur or it will not occur, we just do not know which it is. 2.1.2 Generalization Throughout the book, we generalize from this simple setup. Whenever we have one causal variable and one outcome, and both variables are binary (i.e., each can take on two possible values, 0 or 1), there are only four sets of possible potential outcomes, or “types.” More generally, for any pair of causal and outcome variables, we will use \\(\\theta^Y\\) to denote a case’s type in regard to outcome variable \\(Y\\).5 We, further, add subscripts to denote particular types. Where there are four possible types, for instance, we use the notation \\(\\theta^Y_{ij}\\), where \\(i\\) represents the case’s potential outcome when \\(X=0\\) and \\(j\\) is the case’s potential outcome when \\(X=1\\). Adopting this notation, for a causal structure with one binary causal variable and a binary outcome, the four types can be represented as \\(\\{\\theta^Y_{10}, \\theta^Y_{01}, \\theta^Y_{00}, \\theta^Y_{11}\\}\\), as shown in Table 2.2: Table 2.2: . Generalizing from Table 2.1, the table gives for each causal type the values that \\(Y\\) would take on if \\(X\\) is set at \\(0\\) and if \\(X\\) is set at 1. Type a Type b Type c Type d \\(\\theta^Y=\\theta^Y_{10}\\) \\(\\theta^Y=\\theta^Y_{01}\\) \\(\\theta^Y=\\theta^Y_{00}\\) \\(\\theta^Y=\\theta^Y_{11}\\) Set \\(X=0\\) \\(Y(0)=1\\) \\(Y(0)=0\\) \\(Y(0)=0\\) \\(Y(0)=1\\) Set \\(X=1\\) \\(Y(1)=0\\) \\(Y(1)=1\\) \\(Y(1)=0\\) \\(Y(1)=1\\) Returning to the matter of inequality and democratization to illustrate, let \\(I=1\\) represent a high level of economic equality and \\(I=0\\) its absence; let \\(D=1\\) represent democratization and \\(D=0\\) its absence. A \\(\\theta^D_{10}\\) (or \\(a\\)) type is a case in which a high level of equality, if it occurs, prevents democratization in a country that would otherwise have democratized. The causal effect of high equality in a case, \\(i\\), of \\(\\theta^D_{10}\\) type is \\(\\tau_i= -1\\). A \\(\\theta^D_{01}\\) type (or \\(b\\) type) is a case in which high equality, if it occurs, generates democratization in a country that would otherwise have remained non-democratic (effect \\(\\tau_i= 1\\)). A \\(\\theta^D_{00}\\) type (\\(c\\) type) is a case that will not democratize regardless of the level of equality (effect \\(\\tau_i = 0\\)); and a \\(\\theta^D_{11}\\) type (\\(d\\) type) is one that will democratize regardless of the level of equality (again, effect \\(\\tau_i= 0\\)). In this setting, a causal explanation of a given case outcome amounts to a statement about its type. The claim that India’s high level of equality was a cause of its democratization is equivalent to saying that India democratized and is \\(\\theta^D_{01}\\) type. To claim that Sierra Leone democratized because of low inequality is equivalent to saying that Sierra Leone democratized and is a \\(\\theta^D_{10}\\) type. To claim, on the other hand, that Malawi democratized for reasons having nothing to do with its level of economic equality is to characterize Malawi as a \\(\\theta^D_{11}\\) type (which already specifies its outcome). Now, let us consider more complex causal relations. Suppose now that there are two binary causal variables \\(X_1\\) and \\(X_2\\). We can specify any given case’s potential outcomes for each of the different possible combinations of causal conditions—there now being four such conditions, as each causal variable may take on \\(0\\) or \\(1\\) when the other is at \\(0\\) or \\(1\\). As for notation, we now need to expand \\(\\theta\\)’s subscript since we need to represent the value that \\(Y\\) takes on under each of the four possible combinations of \\(X_1\\) and \\(X_2\\) values. We construct the four-digit subscript with the ordering (and, in general, mapping any two parent nodes alphabetically into \\(X_1\\) and \\(X_2\\)): \\[Y_{hijk} \\left\\{\\begin{array}{ccc} h&amp; =&amp; Y|(X_1=0, X_2=0)\\\\ i &amp;=&amp; Y|(X_1=1, X_2=0)\\\\ j &amp;=&amp; Y|(X_1=0, X_2=1)\\\\ k &amp;=&amp; Y|(X_1=1, X_2=1) \\end{array} \\right.\\] Thus, for instance, \\(\\theta^Y_{0100}\\) means that \\(Y\\) is 1 if \\(X_1=1\\) and \\(X_2=0\\) and is 0 otherwise. We now have 16 causal types: 16 different patterns that \\(Y\\) might display in response to changes in \\(X_1\\) and \\(X_2\\). The full set is represented in Table 2.3, which also makes clear how we read types off of four-digit subscripts. For instance, the table shows us that for nodal type \\(\\theta^Y_{0101}\\), \\(X_1\\) has a positive causal effect on \\(Y\\) but \\(X_2\\) has no effect. On the other hand, for type \\(\\theta^Y_{0011}\\), \\(X_2\\) has a positive effect while \\(X_1\\) has none. We also capture interactions here. For instance, in a \\(\\theta^Y_{0001}\\) type, \\(X_2\\) has a positive causal effect if and only if \\(X_1\\) is 1. In that type, \\(X_1\\) and \\(X_2\\) serve as “complements.” For \\(\\theta^Y_{0111}\\), \\(X_2\\) has a positive causal effect if and only if \\(X_1\\) is 0. In that setup, \\(X_1\\) and \\(X_2\\) are “substitutes.” Table 2.3: With two binary causal variables, there are 16 causal types: 16 ways in which \\(Y\\) might respond to changes in the other two variables. \\(\\theta^Y\\) if \\(X_1=0, X_2=0\\) if \\(X_1=1,X_2=0\\) if \\(X_1=0,X_2=1\\) if \\(X_1=1, X_2=1\\) \\(\\theta^Y_{0000}\\) 0 0 0 0 \\(\\theta^Y_{1000}\\) 1 0 0 0 \\(\\theta^Y_{0100}\\) 0 1 0 0 \\(\\theta^Y_{1100}\\) 1 1 0 0 \\(\\theta^Y_{0010}\\) 0 0 1 0 \\(\\theta^Y_{1010}\\) 1 0 1 0 \\(\\theta^Y_{0110}\\) 0 1 1 0 \\(\\theta^Y_{1110}\\) 1 1 1 0 \\(\\theta^Y_{0001}\\) 0 0 0 1 \\(\\theta^Y_{1001}\\) 1 0 0 1 \\(\\theta^Y_{0101}\\) 0 1 0 1 \\(\\theta^Y_{1101}\\) 1 1 0 1 \\(\\theta^Y_{0011}\\) 0 0 1 1 \\(\\theta^Y_{1011}\\) 1 0 1 1 \\(\\theta^Y_{0111}\\) 0 1 1 1 \\(\\theta^Y_{1111}\\) 1 1 1 1 This is a rich framework in that it allows for all possible ways in which a set of multiple causes can interact with each other. Often, when seeking to explain the outcome in a case, researchers proceed as though causes are necessarily rival, where \\(X_1\\) being a cause of \\(Y\\) implies that \\(X_2\\) was not. Did Malawi democratize because it was a relatively economically equal society or because of international pressure to do so? In the counterfactual model, however, causal relations can be non-rival. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if either supporter’s vote were different. This typological, potential-outcomes conceptualization provides a straightforward way of representing this kind of complex causation. Because of this complexity, when we say that \\(X\\) caused \\(Y\\) in a given case, we will generally mean that \\(X\\) was a cause, not the (only) cause. Malawi might not have democratized if either a relatively high level of economic equality or international pressure had been absent. For most social phenomena that we study, there will be multiple, and sometimes a great many, difference-makers for any given case outcome. We will mostly use \\(\\theta^Y_{ij}\\)-style notation in this book to refer to types. We will, however, occasionally revert to the simpler \\(a, b, c, d\\) designations when that helps ease exposition. As types play a central role in the causal-model framework, we recommend getting comfortable with both forms of notation before going further. Using the same framework, we can generalize to structures in which a unit has any number of causes and also to cases in which causes and outcomes are non-binary. As one might imagine, the number of types increases rapidly (very rapidly) as the number of considered causal variables increases, as it also does as we allow \\(X\\) or \\(Y\\) to take on more than 2 possible values. For example, if there are \\(n\\) binary causes of an outcome, then there can be \\(2^{\\left(2^n\\right)}\\) types of this form: that is, \\(k=2^n\\) combinations of values of causes to consider, and \\(2^k\\) distinct ways to react to each combination. If causes and outcomes are ternary instead of binary, we have \\(3^{\\left(3^n\\right)}\\) causal types of this form. Yet, the basic principle of representing possible causal relations as patterns of potential outcomes remains unchanged, at least as long as variables are discrete. 2.1.3 Summaries of potential outcomes So far, we have focused on causal relations at the level of an individual case. Causal relations at the level of a population is, however, simply a summary of causal relations for cases, and the same basic ideas can be used. We could, for instance, summarize our beliefs about the relationship between economic equality and democratization by saying that we think that the world is comprised of a mixture of \\(a\\), \\(b\\), \\(c\\), and \\(d\\) types, as defined above. We could get more specific and express a belief about what proportions of cases in the world are of each of the four types. For instance, we might believe that \\(a\\) types and \\(d\\) types are quite rare while \\(b\\) and \\(c\\) types are quite common. Moreover, our belief about the proportions of \\(b\\) (positive effect) and \\(a\\) (negative effect) cases imply a belief about equality’s average effect on democratization as, in a binary setup, this quantity is simply the proportion of \\(b\\) types minus the proportion of \\(a\\) types. Such summaries allow us to move from discussion of the cause of a single outcome to discussions of average effects, a distinction that we take up again in Chapter 5. 2.2 Causal Models and Directed Acyclic Graphs So far we have discussed how a single outcome is affected by one or more possible causes. However, these same ideas can be used to describe more complex relations between collections of variables — for example, with one variable affecting another directly as well as indirectly via its impact on some mediating variable. Potential outcomes tables can be used to describe such complex relations. However, as causal structures become more complex—especially, as the number of variables in a domain increases—a causal model can be a powerful organizing tool. In this section, we show how causal models and their visual counterparts, directed acyclic graphs (DAGs), can represent substantive beliefs about counterfactual causal relationships in the world. The key ideas in this section can be found in many texts (see, e.g., Halpern and Pearl (2005) and Galles and Pearl (1998)), and we introduce here a set of basic principles that readers will need to keep in mind in order to follow the argumentation in this book. As we shift to talking about networks of causal relations between variables we will also shift our language. When talking about causal networks, or causal graphs, we will generally refer to variables as “nodes.” And we will sometimes use familial terms to describe relations between nodes. For instance, two nodes directly connected by an arrow are known as “parent” and “child,” while two nodes with a child in common (both directly affecting the same variable) are “spouses.” We can also say that \\(I\\) is an “ancestor” of \\(D\\) (a node upstream from \\(D\\)’s parent) and conversely that \\(D\\) is a descendant of \\(I\\) (a node downstream from \\(I\\)’s child). Return to our running democratization example, but suppose now that we have more fully specified beliefs about how the level of economic inequality can have an effect on whether a country democratizes. We might believe that inequality affects the likelihood of democratization by generating demands for redistribution, which in turn can cause the mobilization of lower-income citizens, which in turn can cause democratization. We might also believe that mobilization itself is not just a function of redistributive preferences but also of the degree of ethnic homogeneity, which shapes the capacities of lower-income citizens for collective action. In this model \\(R\\) is a parent of \\(M\\), \\(I\\) is an ancestor of \\(M\\) but not a parent. \\(R\\) and \\(E\\) are spouses and \\(M\\) is their child. We can visualize this model as a Directed Acyclic Diagram (DAG) in Figure 2.1. Figure 2.1: A simple causal model in which high inequality (\\(I\\)) affects the democratization (\\(D\\)) via redistributive demands and mass mobilization (\\(M\\)), which is also a function of ethnic homogeneity (\\(E\\)). The arrows show relations of causal dependence between variables. The graph does not capture the ranges of the variables and the functional relations between them. Fundamentally, we treat causal models in this book as formal representations of beliefs about how the world works—or, more specifically, about causal relations within a given domain. We might use a causal model to capture our own beliefs, a working simplifcation of our beliefs, or a set of potential beliefs that one might hold. The formalization of prior beliefs in the form of a causal model is the starting point for research design and inference in this book’s analytic framework. Using the democratization example, we will now walk through the three components of a causal model in which our beliefs get embedded: nodes, functions, and distributions. 2.2.1 Components of a Causal Model The three components of a causal model are (i) the nodes—that is, the set of variables we are focused on and how are they defined (ii) the functional relations—which nodes are “explained” by which other nodes and how, and (iii) probability distributions over unexplained elements of a model. 2.2.1.1 The nodes The first component of a causal model is the set of variables (nodes) across which the model characterizes causal relations. On the graph in Figure 2.1, the five included variables are represented by the five lettered nodes. (In addition we mark \\(\\theta^D\\) on the graph though, as will be made clear, we will not think of this as a variable.) In identifying the nodes, we also need to specify the over which they can vary. We might specify, for instance, that all nodes in the model are binary, taking on the values 0 or 1. We could, alternatively, define a set of categories across which a node ranges or allow a node to take on any real number value or any value between a set of bounds.6 Notice that some of these nodes have arrows pointing into them: \\(R, M\\), and \\(D\\) are endogenous nodes, meaning that their values are determined entirely by other nodes in the model. Other nodes have arrows pointing out of them but no arrows pointing into them: \\(I\\) and \\(E\\). \\(I\\) and \\(E\\) are “exogenous” nodes; they influence other nodes in the model but themselves have no causes specified in the model. The \\(\\theta\\) terms require a little more explanation since they do not describe substantive nodes. In our discussion above, we introduce \\(\\theta\\) notation for representing types, or causal effects. In a more general sense, however, we can think of \\(\\theta\\) terms on a causal graph as unobservable and unspecified features of a causal domain that affect outcomes. These might include random processes (noise) or contextual features that we are unable to identify or do not understand. We note that our notation for representing these unobservable, unspecified influences differs from that commonly found in the literature on causal models. In many treatments, these components are themselves referred to as “exogenous” variables, and often labelled as sets \\(\\mathcal U\\), to be distinguished from the endogenous—named–variables often labelled as \\(\\mathcal V\\). We will generally use \\(\\theta\\) to denote these unobserved, unspecified influences to emphasize their particular role, as direct objects of interest in causal inquiry. As we will show, we can think of \\(\\theta\\) nodes as capturing the functional relations between endogeneous variables and as being quantities of direct interest for causal inquiry. We more fully develop this point — returning to the notion of \\(\\theta\\) terms as receptacles for causal effects — below. 2.2.1.2 The functions Next, we need to specify our beliefs about the causal relations among the nodes in our model. How is the value of one node affected by, and how does it affect, the values of others? For each endogenous node—each node influenced by others in the model—we need to express beliefs about how its value is affected by its parents, its immediate causes. The DAG already represents a critical part of these beliefs: the arrows, or directed edges, tell us which nodes we believe to be direct causal inputs into other nodes. So, for instance, we believe that democratization (\\(D\\)) is determined jointly by mobilization (\\(M\\)) and some exogenous, unspecified factor (or set of factors), \\(\\theta^D\\). As we have said, we can think of \\(\\theta^D\\) as all of the other influences on democratization, besides mobilization, that we either do not know of or have decided not to explicitly include in the model. We believe, likewise, that \\(M\\) is determined by \\(I\\) and an unspecified exogenous factor (or set of factors), \\(\\theta^M\\). And we are conceptualizing inequality (\\(I\\)) and ethnic heterogeneity (\\(E\\)) as shaped solely by a factors exogenous to the model, captured by \\(\\theta^I\\) and \\(\\theta^E\\), respectively. Beyond the beliefs captured by the arrows in a DAG, we can express more specific beliefs about causal relations in the form of a causal function.7 Specifying a function means writing down whatever general or theoretical knowledge we have about the direct causal relations between nodes. A function specifies how the value that one node takes on is determined by the values that other nodes—its parents—take on. We can specify this relationship in a vast variety of ways. It is useful however to distinguish broadly between parametric and non-parametric approaches. We take a non-parametric approach in this book, but it is helpful to juxtapose that approach to a parametric one. Parametric approaches. A parametric approach specifies a functional form that relates parents to children. For instance, we might model one node as a linear function of another and write \\(D=\\beta M\\), where \\(\\beta\\) is a parameter that we do not know the value of at the outset of a study but about which we wish to learn. If we believe \\(D\\) to be linearly affected by \\(M\\) but also subject to forces that we do not yet understand and have not yet specified in our theory, then we can write: \\(D=\\alpha + \\beta M+\\theta^D\\). In this function, \\(\\alpha, \\beta\\) might be the parameters of interest, with \\(\\theta^D\\) treated merely as a random disturbance. We can be still more agnostic by, for example, including parameters that govern how other parameters operate. Consider, for instance the function, \\(D=\\beta M^{\\theta^D}\\). Here, \\(D\\) and \\(M\\) are linearly related if \\(\\theta^D=1\\), but not otherwise. Note that functions can be written to be quite specific or extremely general, depending on the state of prior knowledge about the phenomenon under investigation. The use of a structural model does not require precise knowledge of specific causal relations, even of the functional forms through which two nodes are related. The non-parametric approach. With discrete data, causal functions can take fully non-parametric form, allowing for any possible relation between parents and children. We use this framework for most of this book and thus spend some time developing the approach here. We begin by returning to the concept of types. Drawing on our original four types from earlier in this chapter, we know that we can fully specify causal relations between a binary \\(M\\) and a binary \\(D\\) by allowing the node \\(\\theta_D\\) to range across four possible values \\(\\{\\theta^D_{10}, \\theta^D_{01}, \\theta^D_{00}, \\theta^D_{11}\\}\\). For instance, {^D_{10} represents a negative causal effect of \\(M\\) on \\(D\\) while \\(\\theta^D_{00}\\) represents \\(D\\) remaining at 0 regardless of \\(M\\). Put differently, \\(\\theta^D\\) represents the non-parametric function that relates \\(M\\) to \\(D\\). We can formally specify \\(D\\)’s behavior as a function of \\(M\\) and \\(\\theta^D\\) in the following way: \\[D(M, \\theta^D_{ij}) = \\left\\{\\begin{array}{ccx} i &amp; \\text{if} &amp; M=0 \\\\ j &amp; \\text{if} &amp; M=1 \\end{array}\\right.\\] Note that \\(\\theta^D\\) ranges over all possible functional forms between these two binary variables. How should we think about what kind of thing \\(\\theta^D\\) is, in a more substantive sense? It is probably most helpful to think of \\(\\theta^D\\) as an unknown and possibly random factor that conditions the effect of mobilization on democratization, determining whether \\(M\\) has a negative effect, a positive effect, no effect with democratization never occurring, or no effect with democratization bound to occur regardless of mobilization. Importantly, however, while we might think of \\(\\theta^D\\) as an unknown or random quantity, in this formulation \\(\\theta^D\\) should not be thought of as a nuisance — as “noise” that we would like to get rid of — but as the quantity that we want to learn about: we want to know whether \\(M\\) likely had a positive, negative, or no effect on \\(D\\). We elaborate on this point at much greater length in Chapter 5. We can similarly use \\(\\theta\\) terms to capture causal relations involving any number of parent nodes. Every substantively defined node, \\(J\\), in a graph can be thought of as having a \\(\\theta^J\\) term pointing into it, and the (unobservable) value of \\(\\theta^J\\) represents the mapping from \\(J\\)’s parents (if it has any) to the value of \\(J\\). Applied to the binary nodes in Figure 2.1, \\(\\theta^J\\) ranges as follows: Nodes with no parents: For an exogenous node, like \\(I\\) or \\(E\\), \\(\\theta^J\\) represents an external “assignment” process can take on one of two values, \\(\\theta^J_{0}\\), meaning that \\(J\\) is “assigned” to \\(0\\) or \\(\\theta^J_{1}\\), meaning that \\(J\\) is assigned to 1. For instance, \\(\\theta^I_{0}\\) typifies a case in which exogenous forces have generated low inequality. Binary nodes with 1 binary parent: For endogenous node \\(R\\), with only one parent (\\(I\\)), \\(\\theta^R\\) takes on one of four values of the form \\(\\theta^R_{ij}\\) (our four original types, \\(\\theta^R_{10}\\), \\(\\theta^R_{01}\\), etc.). Binary nodes with 2 binary parents: \\(M\\) has two parent nodes. Thus, \\(\\theta^M\\) will take on a possible 16 values of the form \\(\\theta^M_{hijk}\\) (\\(\\theta^M_{0000}\\), \\(\\theta^M_{0001}\\), etc.), using the syntax detailed earlier in this chapter. Nodal types and causal types. For analytic applications later in the book, we will want to be able to think both about the type operating at a particular node and about collections of types operating across a model. We thus refer to \\(\\theta^J\\) as a unit’s nodal causal type, or simply nodal type, for \\(J\\). We refer to the collection of nodal types across all nodes for a given unit (i.e., a case) as the case’s unit causal type, or simply causal type, denoted by the vector \\(\\Theta\\). If we hypothetically knew a unit’s causal type—all nodal types for all nodes—then we would know everything there is to know about that unit. Since the nodal types of exogenous nodes include values of all exogenous nodes, and the nodal types of all endogenous nodes specify how those nodes respond to all of their parents, a unit’s causal type fully specifies all nodal values as well as all counterfactual nodal values for a unit. We will sometimes refer to the values of \\(\\Theta\\) as a unit’s context. This is because \\(\\Theta\\) captures all exogenous forces acting on a unit. This includes the assignment process driving the model’s exogenous nodes (in our example, \\(\\theta^I\\) and \\(\\theta^E\\)) as well as all contextual factors that shape causal relations between nodes (\\(\\theta^R\\), \\(\\theta^M\\), and \\(\\theta^D\\)). Put differently, \\(\\Theta\\) captures both how a unit reacts to situations and which situations it is reacting to. One implication is that there is no formal distinction between a unit’s type and a unit’s situation—between, say, a hungry person, and a person who has had no food. ::: {.headerbox data-latex=“”} ::: {.center data-latex=“”} ::: Box: Nodal types, causal types term symbol meaning nodal type \\(\\theta^X\\) The way that a node responds to the values of its parents. Example: \\(\\theta^Y_{10}\\): \\(Y\\) takes the value 1 if \\(X=0\\) and 0 if \\(X=1\\). causal type \\(\\Theta\\) A causal type is a concatenation of nodal types, one for each node. Example: \\((\\theta^X_0, \\theta^Y_{00})\\), is a causal type that has \\(X=0\\) and that has \\(Y=0\\) no matter what the value of \\(X\\). :::: A few important aspects of causal functions are worth highlighting. These functions express causal beliefs. When we write \\(D=\\beta M\\) as a function, we do not just mean that we believe the values of \\(M\\) and \\(D\\) in the world to be linearly related. We mean that we believe that the value of \\(M\\) determines the value of \\(D\\) through this linear function. Functions are, in this sense, directional statements, with causes on the righthand side and an outcome on the left. The collection of simple functions that map from the values of parents of a given node to the values of that node are sufficient to represent potentially complex webs of causal relations. For each node, we do not need to think through entire sequences of causation that might precede it. We need only specify how we believe it to be affected by its parents—that is to say, those nodes pointing directly into it. Our outcome of interest, \\(D\\), may be shaped by multiple, long chains of causality. To theorize how \\(D\\) is generated, however, we write down how we believe \\(D\\) is shaped by its parent—its direct cause, \\(M\\). We then, separately, express a belief about how \\(M\\) is shaped by its parents, \\(R\\) and \\(E\\). A node’s function must include as inputs all, and only, those nodes that point directly into that node.8 As in the general potential-outcomes framework, all relations in a causal model are conceptualized as deterministic at the case level. Yet there is not as much at stake here though as one might think at first; by this we simply mean that a node’s value is determined by the values of its parents along with any stochastic or unknown components. We express uncertainty about causal relations, however, as unknown parameters, such as the causal types \\(\\theta\\). 2.2.1.3 The distributions Putting causal structure and causal functions together gives us a structural causal model. In a structural causal model, all endogenous nodes are, either directly or by implication, functions of a case’s context (the values of the set of exogenous nodes).9 What we have not yet inscribed into the model, however, is beliefs about how likely or common different kinds of contexts might be. Thus, for instance, a structural causal model consistent with Figure 2.1 stipulates which nodes may have effects on which other nodes. But it says nothing in itself about the distribution of values of either the exogenous nodes or of the causal relations between nodes.10 We have not said anything, for instance, about how common high inequality is across the relevant domain of cases or how common ethnic homogeneity is. Put differently, we have said nothing about the distribution of \\(\\theta^I\\) or of \\(\\theta^E\\). Similarly, we have said nothing yet about how commonly mobilization has positive, negative, or null effects of democratization—that is, the distribution of \\(\\theta^D\\)—or about how commonly \\(I\\) and \\(E\\) have different possible joint causal effects on \\(M\\) (the distribution of \\(\\theta^M\\)). In many research situations, we will have or want to posit a set of prior beliefs about how the world works under different conditions and about what kinds of conditions are more likely than others. We can express these beliefs about context as probability distributions over the model’s \\(\\theta^J\\) terms. For instance, our structural causal model might tell us that \\(E\\) and \\(R\\) can jointly affect \\(M\\). We might, then, add to this a belief about \\(\\theta^M\\) such that, in the population of interest, redistribution rarely has a positive effect on mobilization when ethnic homogeneity is low. We would thus putting a low probability on the nodal types for \\(M\\) in which \\(R\\) has a positive effect on \\(M\\) when \\(E=0\\), relative to \\(M\\)’ss other nodal types.11 We might add to this the belief that \\(E=1\\) in only 10% of cases in the population of interest, thus setting a 0.1 probability on \\(\\theta^E_1\\). Note that these two beliefs jointly imply that \\(R\\) will rarely have a positive effect on \\(M\\). As with functions, we can also (and typically would) build uncertainty into our beliefs about the shares of different nodal types in the population. We do this by specifying a distribution over possible share allocations. For instance, we can specify a distribution over the shares of cases with ethnic homogeneity (\\(\\theta^E_1\\)), and a distribution over the shares of \\(\\theta^M\\) types, with our degrees of uncertainty captured by each distribution’s variance. (More on these distributions in Chapter ??.) In the default setup, we assume that each \\(\\theta\\) term (\\(\\theta^I, \\theta^E, \\theta^R\\), etc.) is generated independently of the others—that each node’s type for a given case is drawn from an independent distribution. While this is not without loss of generality, it is not as constraining as it might at first appear: any graph in which two \\(\\theta\\) terms are not independent can be replaced by a graph in which these two terms are themselves generated by a common, third \\(\\theta\\) term.12 Note also that one could envision “incomplete probabilistic causal models” in which researchers claim knowledge regarding distributions over subsets of \\(\\theta\\). :::: {.headerbox data-latex=“”} ::: {.center data-latex=“”} ::: Technical Note on the Markov Property The assumptions that no node is its own descendant and that the \\(\\theta\\) terms are generated independently make the model Markovian, and the parents of a given node are Markovian parents. Knowing the set of Markovian parents allows one to write relatively simple factorizations of a joint probability distribution, exploiting the fact (“the Markov condition”) that all nodes are conditionally independent of their nondescendants, conditional on their parents. nodes \\(A\\) and \\(B\\) are “conditionally independent” given \\(C\\) if \\(P(a|b,c) = P(a|c)\\) for all values of \\(a, b\\) and \\(c\\). To see how this Markovian property allows for simple factorization of \\(P\\) for Figure 2.1, note that \\(P(X, R, Y)\\) can always be written as: \\[P(X, R, Y) = P(X)P(R|X)P(Y|R, X)\\] If we believe, as in the figure, that \\(X\\) causes \\(Y\\) only through \\(R\\) then we have the slightly simpler factorization: \\[P(X, R, Y) = P(X)P(R|X)P(Y|R)\\] Or, more generally: \\[\\begin{equation} P(v_1,v_2,\\dots v_n) = \\prod P(v_i|pa_i) \\tag{2.1} \\end{equation}\\] The distribution \\(P\\) on \\(\\theta\\) induces a joint probability distribution on \\(\\mathcal{V}\\) that captures not just information about how likely different states are to arise but also the relations of conditional independence between nodes that are implied by the underlying causal process. For example, if we thought that \\(X\\) caused \\(Y\\) via \\(R\\) (and only via \\(R\\)), we would then hold that \\(P(Y | R) = P(Y | X, R)\\): in other words if \\(X\\) matters for \\(Y\\) only via \\(R\\) then, conditional on \\(R\\), \\(X\\) should not be informative about \\(Y\\). In this way, a probability distribution \\(P\\) over a set of nodes can be consistent with some causal models but not others. This does not, however, mean that a specific causal model can be extracted from \\(P\\). To demonstrate with a simple example for two nodes, any probability distribution on \\((X,Y)\\) with \\(P(x)\\neq P(x|y)\\) is consistent both with a model in which \\(X\\) is a parent of \\(Y\\) and with a model in which \\(Y\\) is a parent of \\(X\\). :::: Once we introduce beliefs about the distribution of values of the exogenous terms (in our setup, the \\(\\theta\\) terms) in a model, we have specified a probabilistic causal model. We need not say much more, for the moment, about the probabilistic components of causal models. But to foreshadow the argument to come, our prior beliefs about the likelihoods of different contexts play a central role in the framework that we develop here. We will see how the encoding of contextual knowledge—beliefs that some kinds of conditions and causal effects are more common than others—forms a key foundation for causal inference. At the same time, our expressions of uncertainty about context represent scope for learning: it is the very things that we are, at a study’s outset, uncertain about that we can update our beliefs about as we encounter evidence. 2.3 Graphing models and using graphs While we have been speaking to causal graphs throughout this chapter, we want to take some time to unpack their core features and uses. A key benefit of causal models is that they lend themselves to graphical representations; in turn, graphs constructed according to particular rules can aid causal analysis. In the next subsection we discuss a set of rules for representing a model in graphical form. THe following subsection then demonstrates how access to a graph facilitates causal inference. 2.3.1 Rules for graphing causal models The diagram in Figure 2.1 is a causal DAG (Hernán and Robins 2006). We endow it with the interpretation that an arrow from a parent to a child that a change in the parent can, under some circumstances, induce a change in the child. Though we have already been making use of this causal graph to help us visualize elements of a causal model, we now explicitly point out a number of general features of causal graphs as we will be using them throughout this book. Causal graphs have their own distinctive “grammar,” a set of rules that give them important analytic features. Directed, acyclic. A causal graph represents elements of a causal model as a set of nodes (or vertices), representing nodes, connected by a collection of single-headed arrows (or directed edges). We draw an arrow from node \\(A\\) to node \\(B\\) if and only if we believe that \\(A\\) can have a direct effect on \\(B\\). The resulting diagram is a directed acyclic graph (DAG) if there are no paths along directed edges that lead from any node back to itself—i.e., if the graph contains no causal cycles. The absence of cycles (or “feedback loops”) is less constraining than it might appear at first. In particular if one thinks that \\(A\\) today causes \\(B\\) tomorrow which in turn causes \\(A\\) today, we can represent this as \\(A_1 \\rightarrow B \\rightarrow A_2\\) rather than \\(A \\leftrightarrow B\\). That is, we timestamp the nodes, turning what might informally appear as feedback into a non-cyclical chain. Meaning of missing arrows. The absence of an arrow between \\(A\\) and \\(B\\) means that \\(A\\) is not a direct cause of \\(B\\).13 Here lies an important asymmetry: drawing an \\(A \\rightarrow B\\) arrow does not mean that we know that \\(A\\) does directly cause \\(B\\); but omitting such an arrow implies that we know that \\(A\\) does not directly cause \\(B\\). We say more, in other words, with the arrows we omit than with the arrows that we include. Returning to Figure 2.1, we have here expressed the belief that redistributive preferences exert no direct effect on democratization; we have done so by not drawing an arrow directly from \\(R\\) to \\(D\\). In the context of this model, saying that redistributive preferences have no direct effect on democratization is to say that any effect of redistributive preferences on democratization must run through mobilization; there is no other pathway through which such an effect can operate. This might be a way of encoding the knowledge that mass preferences for redistribution cannot induce autocratic elites to liberalize the regime absent collective action in pursuit of those preferences. The same goes for the effects of \\(I\\) on \\(M\\), \\(I\\) on \\(D\\), and \\(E\\) on \\(D\\): the graph in Figure 2.1 implies that we believe that these effects also do not operate directly, but only along the indicated, mediated paths. Sometimes-causes. The existence of an arrow from \\(A\\) to \\(B\\) does not imply that \\(A\\) always has a direct effect on \\(B\\). Consider, for instance, the arrow running from \\(R\\) to \\(M\\). The existence of this arrow requires that \\(R\\) appears somewhere in \\(M\\)’s functional equation, as a node’s functional equation must include all nodes pointing directly into it. Imagine, though, that \\(M\\)’s causal function were specified as: \\(M = RE\\). This function would allow for the possibility that \\(R\\) affects \\(M\\), as it will whenever \\(E=1\\). However, it would also allow that \\(R\\) will have no effect, as it will when \\(E=0\\). AJ: Have commented out above because I think we’ve sufficiently dealt with the \\(U\\)’s and it’s going to be very confusing to say U’s are implicit when we’ve said we’re using theta’s instead of U;s. No excluded common causes Any cause common to multiple nodes on a graph must itself be represented on the graph. If \\(A\\) and \\(B\\) on a graph are both affected by some third node, \\(C\\), then we must represent this common cause. Put differently, any two nodes without common causes on the graph are taken to be independent of one another. Thus, for instance, the graph in Figure 2.1 implies that the values of \\(I\\) and \\(E\\) are determined independently of one another. If in fact we believed that a country’s level of inequality and its ethnic composition were both shaped by, say, its colonial heritage, then this DAG would not be an accurate representation of our beliefs about the world. To make it accurate, we would need to add to the graph a node capturing that colonial heritage and include arrows running from colonial heritage to both \\(I\\) and \\(E\\). This rule ensures that the graph captures all potential correlations among nodes that are implied by our beliefs. If \\(I\\) and \\(E\\) are in fact driven by some common cause, then this means not just that these two nodes will be correlated but also that each will be correlated with any consequences of the other. For instance, a common cause of \\(I\\) and \\(E\\) would also imply a correlation between \\(R\\) and \\(E\\). \\(R\\) and \\(E\\) are implied to be independent in the current graph but would be implied to be correlated if a common node pointed into both \\(I\\) and \\(E\\). Of particular interest in Figure 2.1 is the implied independence of \\(\\theta\\)’s from one another. Imagine, for instance, an additional node pointing into both \\(\\theta^I\\) and \\(\\theta^D\\). This would represent a classic form of confounding: the assignment of cases to values on the explanatory node would be correlated with the case’s potential outcomes on \\(D\\). The omission of any such pathway is precisely equivalent to expressing the belief that \\(I\\) is exogenous, i.e., (as if) randomly assigned. Representing unobserved confounding. It may be however that there are common causes for nodes that we simply do not understand. We might believe that some unknown factor (partially) determines both \\(I\\) and \\(D\\), which is the same as saying that \\(\\theta^I\\) and \\(\\theta^D\\) are not independently distributed. Unobserved confounding is often represented by adding a dotted line, or a two headed arrow, connecting nodes whose random components are not independent. Figure 2.2 illustrates. We address this kind of unobserved confounding later in the book and show how we can seek to learn about the joint distribution of nodal types in such situations. Figure 2.2: A DAG with unobserved confounding Licence to exclude nodes. The flip side of the “no excluded common causes” rule is that a causal graph, to do the work it must do, does not need to include everything we know about a substantive domain of interest. We may know quite a lot about the causes of economic inequality, for example. But we can safely omit any other factor from the graph as long as it does not affect multiple nodes in the model. Indeed, \\(\\theta^I\\) in Figure 2.1 already implicitly captures all factors that affect \\(I\\), just as \\(\\theta^D\\) captures all factors other than mobilization that affect democratization. We may be aware of a vast range of forces shaping whether countries democratize, but choose to bracket them for the purposes of an examination of the role of economic inequality. This bracketing is permissible as long as none of these unspecified factors also act on other nodes included in the model. We can’t read functional equations from a graph. As should be clear, a DAG does not represent all features of a causal model. What it does record is which nodes enter into the structural equation for every other node: what can directly cause what. But the DAG contains no other information about the form of those causal relations. Thus, for instance, the DAG in Figure 2.1 tells us that \\(M\\) is function of both \\(R\\) and \\(E\\), but it does not tell us whether that joint effect is additive (\\(R\\) and \\(E\\) separately increase mobilization) or interactive (the effect of each depends on the value of the other), or whether either effect is linear, curvilinear, or something else. This lack of information about functional forms often puzzles those encountering causal graphs for the first time; surely it would be convenient to visually differentiate, say, additive from conditioning effects. As one thinks about the variety of possible causal functions, however, it quickly becomes clear that there would be no simple visual way of capturing all possible functional relations. Moreover, causal graphs do not require functional statements to perform their original analytic purpose—a purpose to which we now turn. 2.3.2 Conditional independence from DAGs If we encode our prior knowledge using the grammar of a causal graph, we can put that knowledge to work for us in powerful ways. In particular, the rules of DAG-construction allow for an easy reading of the conditional independencies implied by our beliefs. (For another, somewhat more extended treatment of the ideas in this section, see Rohrer (2018).) To begin thinking about conditional independence, it can be helpful to conceptualize dependencies between nodes as generating flows of information. Let us first consider a simple relationship of dependence. Returning to Figure 2.1, the arrow running from \\(I\\) to \\(R\\), implying a direct causal dependency, means that if we expect \\(I\\) and \\(R\\) to be correlated. Put differently, observing the value of one of these nodes also gives us information about the value of the other. If we measured redistributive preferences, the graph implies that we would also be in a better position to infer the level of inequality, and vice versa. Likewise, \\(I\\) and \\(M\\) are also linked in a relationship of dependence: since inequality can affect mobilization (through \\(R\\)), knowing the the level of inequality would allow us to improve our estimate of the level of mobilization and vice versa. In contrast, consider \\(I\\) and \\(E\\), which are in this graph indicated as being independent of one another. Learning the level of inequality, according to this graph, would give us no information whatsoever about the degree of ethnic homogeneity, and vice-versa. Moreover, sometimes what we learn depends on what we already know. Suppose that we already knew the level of redistributive preferences. Would we then be in a position to learn about the level of inequality by observing the level of mobilization? According to this graph we would not: since the causal link—and, hence, flow of information between \\(I\\) and \\(M\\)—runs through \\(R\\), and we already know \\(R\\), there is nothing left to be learned about \\(I\\) by also observing \\(M\\). Anything we could have learned about inequality by observing mobilization is already captured by the level of redistributive preferences, which we have already seen. In other words, if we were not to include \\(R\\) in the causal model, then \\(I\\) and \\(M\\) would be dependent and informative about each other. When we do include \\(R\\) in the causal graph, \\(I\\) and \\(M\\) are independent of one another and, hence, uninformative about each other. We can express this idea by saying that \\(I\\) and \\(M\\) are conditionally independent given \\(R\\). We say that two nodes, \\(A\\) and \\(C\\), are “conditionally independent” given a set of nodes \\(\\mathcal B\\) if, once we have knowledge of the values in \\(\\mathcal B\\), knowledge of \\(A\\) provides no information about \\(C\\) and vice-versa. Taking \\(\\mathcal B\\) into account thus “breaks” any relationship that might exist unconditionally between \\(A\\) and \\(C\\). To take up another example, suppose that war is a cause of both military casualties and price inflation, as depicted in Figure 2.3. Casualties and inflation will then be (unconditionally) correlated with one another because of their shared cause. If we learn that there have been military casualties, this information will lead us to think it more likely that there is also war and, in turn, price inflation (and vice versa). However, assuming that war is their only common cause, we would say that military casualties and price inflation are conditionally independent given war. If we already know that there is war, then we can learn nothing further about the level of casualties (price inflation) by learning about price inflation (casualties). We can think of war, when observed, as blocking the flow of information between its two consequences; everything we would learn about inflation from casualties is already contained in the observation that there is war. Put differently, if we were just to look at cases where war is present (i.e., if we hold war constant), we should find no correlation between military casualties and price inflation; likewise, for cases in which war is absent. Figure 2.3: This graph represents a simple causal model in which war (\\(W\\)) affects both military casualties (\\(C\\)) and price inflation (\\(P\\)). Relations of conditional independence are central to the strategy of statistical control, or covariate adjustment, in correlation-based forms of causal inference, such as regression. In a regression framework, identifying the causal effect of an explanatory node, \\(X\\), on a dependent node, \\(Y\\), requires the assumption that \\(X\\)’s value is conditionally independent of \\(Y\\)’s potential outcomes (over values of \\(X\\)) given the model’s covariates. To draw a causal inference from a regression coefficient, in other words, we have to believe that including the covariates in the model “breaks” any biasing correlation between the value of the causal node and its unit-level effect. As we will explore, however, relations of conditional independence are also of more general interest in that they tell us, given a model, when information about one feature of the world may be informative about another feature of the world, given what we already know. By identifying the possibilities for learning, relations of conditional independence can thus guide research design. We discuss these research-design implications in Chapter 7, but focus here on showing how relations of conditional independence operate on a DAG. To see more systematically how a DAG can reveal conditional independencies, it is useful spell out three elemental structures according to which which information can flow across a causal graph: Figure 2.4: Three elemental relations of conditional independence. (1a) Information can flow unconditionally along a path of arrows pointing in the same direction. In Panel 1 of Figure , information flows across all three nodes. Learning about any one will tell us something about the other two. (1b) Learning the value of a node along a path of arrows pointing in the same direction blocks flows of information across that node. Knowing the value of \\(B\\) in Panel 1 renders \\(A\\) no longer informative about \\(C\\), and vice versa: anything that \\(A\\) might tell us about \\(C\\) is already captured by the information about \\(B\\). (2a) Information can flow unconditionally across the branches of any forked path. In Panel 2 learning only \\(A\\) can provide information about \\(C\\) and vice-versa. (2b) Learning the value of the node at the forking point blocks flows of information across the branches of a forked path. In Panel 2, learning \\(A\\) provides no information about \\(C\\) if we already know the value of \\(B\\).14 (3a) When two or more arrowheads collide, generating an inverted fork, there is no unconditional flow of information between the incoming sequences of arrows. In Panel 3, learning only \\(A\\) provides no information about \\(C\\), and vice-versa. (3b) Collisions can be sites of conditional flows of information. In the jargon of causal graphs, \\(B\\) in Panel 2 is a “collider” for \\(A\\) and \\(C\\).15 Although information does not flow unconditionally across colliding sequences, it does flow across them conditional on knowing the value of the collider node or any of its downstream consequences. In Panel 2, learning \\(A\\) does provide new information about \\(C\\), and vice-versa, if we also know the value of \\(B\\) (or, in principle, the value of anything that \\(B\\) causes). The last point is somewhat counter-intuitive and warrants further discussion. It is easy enough to see that, for two nodes that are correlated unconditionally, that correlation can be “broken” by controlling for a third node. In the case of collision, two nodes that are not correlated when taken by themselves become correlated when we condition on (i.e., learn the value of) a third node, the collider. The reason is in fact quite straightforward once one sees it: if an outcome is a joint function of two inputs, then if we know the outcome, information about one of the inputs can provide information about the other input. For example, if I know that you have brown eyes, then learning that your mother has blue eyes makes me more confident that your father has brown eyes. Looking back at our democratization DAG in Figure 2.1, \\(M\\) is a collider for \\(R\\) and \\(E\\), its two inputs. Suppose that we again have the functional equation \\(M=RE\\). Knowing about redistributive preferences alone provides no information whatsoever about ethnic homogeneity since the two are determined independently of one another. On the other hand, imagine that we already know that there was no mobilization. Now, if we observe that there were redistributive preferences, we can figure out the level of ethnic homogeneity: it must be 0. (And likewise in going from homogeneity to preferences.) Using these basic principles, conditional independencies can be read off any DAG. We do so by checking every path connecting two nodes of interest and ask whether, along those paths, the flow of information is open or blocked, given any other nodes whose values are already observed. Conditional independence is established when all paths are blocked given what we already know; otherwise, conditional independence is absent. 2.4 Chapter Appendix 2.4.1 Steps for constructing causal models Box: Steps for constructing causal models Identify a set of variables in a domain of interest. These become the nodes of the model. You should specify the range of each node: is it continuous or discrete? May include \\(U\\) terms representing unspecified, random influences Draw a causal graph (DAG) representing beliefs about causal dependencies among these nodes Capture direct effects only Arrows indicate possible, not constant or certain, causal effects The absence of an arrow between two nodes indicates a belief of no direct causal relationship between them Ensure that the graph captures all correlations among nodes. This means that either (a) any common cause of two or more nodes is included on the graph (with implications for Step 1) or (b) correlated nodes are connected with a dashed, undirected edge. Write down one causal function for each endogenous node Each node’s function must include all nodes directly pointing into it on the graph Functions may take any form, as long as each set of possible causal values maps onto a single outcome value Functions may express arbitrary amounts of uncertainty about causal relations State probabilistic beliefs about the distributions of the exogenous nodes How common or likely to do we think different values of the exogenous nodes are? Are they independently distributed? If in step 2 you drew an undirected edge between nodes then you believe that the connected nodes are not independently distributed. 2.4.2 Model construction in code Our gbiqq package provides a set of functions to implement all of these steps concisely for binary models – models in which all nodes are dichotomous. # Steps 1 and 2 # We define a model with three binary nodes and specified edges between them: model &lt;- make_model(&quot;X -&gt; M -&gt; Y&quot;) # Step 3 # Unrestricted functional forms are allowed by default, though these can # also be reduced. Here we impose monotonicity at each step # by removing one type for M and one for Y model &lt;- set_restrictions(model, labels = list(M = &quot;10&quot;, Y=&quot;10&quot;)) # Step 4 # We set priors over the distribution of (remaining) causal types. # Here we set &quot;jeffreys priors&quot; model &lt;- set_priors(model, distribution = &quot;jeffreys&quot;) # We now have a model defined as an R object. # Later we will ask questions of this model and update it using data. These steps are enough to fully describe a binary causal model. Later in this book we will see how we can ask questions of a model like this but also how to use data to train it. 2.4.3 Test yourself! Can you read conditional independence from a graph? As an exercise, see whether you can identify the relations of conditional independence between \\(A\\) and \\(D\\) in Figure . Figure 2.5: An exercise: \\(A\\) and \\(D\\) are conditionally independent, given which other node(s)? Are A and D independent: unconditionally? Yes. \\(B\\) is a collider, and information does not flow across a collider if the value of the collider node or its consequences is not known. Since no information can flow between \\(A\\) and \\(C\\), no information can flow between \\(A\\) and \\(D\\) simply because any such flow would have to run through \\(C\\). if you condition on \\(B\\)? No. Conditioning on a collider opens the flow of information across the incoming paths. Now, information flows between \\(A\\) and \\(C\\). And since information flows between \\(C\\) and \\(D\\), \\(A\\) and \\(D\\) are now also connected by an unbroken path. While \\(A\\) and \\(D\\) were independent when we conditioned on nothing, they cease to be independent when we condition on \\(B\\). if you condition on \\(C\\)? Yes. Conditioning on \\(C\\), in fact, has no effect on the situation. Doing so cuts off \\(B\\) from \\(D\\), but this is irrelevant to the \\(A\\)-\\(D\\) relationship since the flow between \\(A\\) and \\(D\\) was already blocked at \\(B\\), an unobserved collider. if you condition on \\(B\\) and \\(C\\)? Yes. Now we are doing two, countervailing things at once. While conditioning on \\(B\\) opens the path connecting \\(A\\) and \\(D\\), conditioning on \\(C\\) closes it again, leaving \\(A\\) and \\(D\\) conditionally independent. Analyzing a causal graph for relations of independence represents one payoff to formally encoding our beliefs about the world in a causal model. We are, in essence, drawing out implications of those beliefs: given what we believe about a set of direct causal relations (the arrows on the graph), what must this logically imply about other dependencies and independencies on the graph, conditional on having observed some particular set of nodes? We show in a later chapter how these implications can be deployed to guide research design, by indicating which parts of a causal system are potentially informative about other parts that may be of interest. References "]
]
