[
["evaluation.html", "Chapter 15 Evaluating models 15.1 Five Strategies 15.2 Evaluating the Democracy-Inequality model", " Chapter 15 Evaluating models Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good. Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these are almost certainly wrong. The question then is not how much you believe your model (or whether you really believe what you say you believe) but whether your model is useful is some sense. How can we evaluate the usefulness of our models? 15.1 Five Strategies Sometimes a model is just a poor representation of the underlying causal processes. Imagine a situation in which researchers believe that the effect of \\(X\\) on \\(Y\\) runs entirely through \\(M\\), positing a model of the form \\(X \\rightarrow M \\rightarrow Y\\). Imagine, however, that the true causal process is one in which \\(X\\) affects \\(Y\\) directly; \\(X\\) has no effect on \\(M\\), so that there is no indirect effect of \\(X\\) on \\(Y\\); and \\(M\\) never has a negative effect on \\(Y\\). The problem with the posited model, then, is that it represents overly strong beliefs about independence relations: it does not allow for a direct effect that is in fact operating. We are perfectly able to update using this too-strong \\(X \\rightarrow M \\rightarrow Y\\) model and the data — but the updated model can produce wildly inaccurate causal inferences. We show this using set of 200 observations simulated from a “true” model with direct effects only and an average effect of \\(X\\) on \\(Y\\) of \\(1/3\\).1 In the left panel of Figure ??, we show the estimated average treatment effect of \\(X\\) on \\(Y\\) when using these data to update the \\(X \\rightarrow M \\rightarrow Y\\) model. In the righthand panel of the figure, we show the inferences using the same data but a model that makes weaker assumptions by allowing for direct effects: a \\(X \\rightarrow M \\rightarrow Y \\leftarrow X\\) model. With both models, we start with flat priors over nodal types.2 We represent the true average effect with the vertical line in each graph. As we can see, the weaker (i.e., more permissive) model performs OK: the true effect falls well within the posterior predictive distribution on the ATE. However, the stronger model, which excludes direct effects, generates a posterior predictive distribution that essentially excludes the right answer. So, if we go into the analysis with the stronger model, we have a problem. But will we notice? In the remainder of this section, we explore a range of diagnostics that researchers can undertake to evaluate the usefulness of their models or to compare models with one another: checking assumptions of conditional independence built into a model; attending to computational clues; checking the model’s fit; using “leave-one-out” cross-validation; and assessing model sensitivity. Figure 15.1: A restricted model yields a credibility interval that does not contain the actual average effect. 15.1.1 Check conditional independence First, even before engaging in updating, we can look to see whether the data pattern is consistent with our causal model. In particular, we can check whether there are inconsistencies with the Markov condition that we introduced in Chapter 2: that every node is conditionally independent of its non-descendants, given its parents. In this case, if the stronger model is right, then given \\(M\\), \\(Y\\) should be independent of \\(X\\). Is it? One way to check is to assess the covariance of \\(X\\) and \\(Y\\) given \\(M\\) in the data. Specifically, we regress \\(Y\\) on \\(X\\) for each value of \\(M\\), once for \\(M=1\\) and again for \\(M=0\\); a correlation between \\(X\\) and \\(Y\\) at either value of \\(M\\) would be problematic for the conditional independence assumption embedded in the stronger model. Note that this form of diagnostic test is a classical one in the frequentist sense: we start by hypothesizing that our model is correct and then ask whether the data were unlikely given the model. Table 15.1: Regression coefficient on \\(X\\) given \\(M=0\\) and \\(M=1\\) M estimate std.error p.value 0 0.369 0.099 0 1 0.573 0.080 0 We report the regression coefficients on \\(X\\) in the table below. It is immediately apparent that we have a problem. At both values of \\(M\\), there is a strong correlation between \\(X\\) and \\(Y\\), evidence of a violation of the Markov condition implied by the stronger model.3 Identifying the full set of conditional independence assumptions in a causal model can be difficult. There are however well developed algorithms for identifying what sets, if any, you need to conditional on to ensure conditional Independence between two nodes given a DAG. R users can quickly access such results using the impliedConditionalIndependencies function in dagitty package. 15.1.2 Computational clues Second, we may be lucky and run into computation issues. In this example there is a good chance that when you fit the stronger model, stan will throw an error: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. 15.1.3 Bayesian \\(p\\) value: Are the data unusual given your model? A third approach asks whether features of the data you observe are in some sense unusual given your model, or more unusual given your model than another model. For instance, if one model assumed no adverse effects of \\(X\\) on \\(Y\\) and no confounding, then a strong negative correlation between \\(X\\) and \\(Y\\) would be unusual, even for the model updated with this data; and this negative correlation would more unusual for this model than for a model that allowed for adverse effects. In fact, this approach is quite classical: we are looking to see whether we should “reject” our model in light of inconistent data. An approach for doing this using simulated data from the posterior predictive distribution is described in Gabry et al. (2019).4 We consider two test statistics, comparing our stronger to our weaker model. First, we look just at the distribution of the outcome \\(Y\\) to see how the actual distribution in the data compares to the predicted distribution from the updated model. Second, we look at the actual correlation between \\(X\\) and \\(Y\\) and see how this compares to the predicted distribution. For the first test, we see that the predicted distribution of the outcome \\(Y\\) is similar for both updated models; and the actual mean outcome is within the distribution of predicted mean outcomes. The two models are thus equally good at “learning” from the data about the observed distribution of the outcome. When it comes to the correlation between \\(X\\) and \\(Y\\), however, the two models perform very differently. The posterior predictive distribution from the stronger model is centered around a \\(0\\) correlation. Thus, from the perspective of the stronger model the strong \\(X,Y\\) correlation in the data is unexpected. The updated weaker model, in contrast, predicts a strong correlation, and the observed correlation is comfortably within the posterior predictive distribution. A frequentist looking at the observed correlation between \\(X\\) and \\(Y\\) should feel comfortable rejecting the stronger model. At first blush, the abysmal performance of the stronger model may seem paradoxical. Even after this model has seen the \\(X,Y\\) correlations in the data, the model still finds those correlations highly surprising. What keeps the \\(X \\rightarrow M \\rightarrow Y\\) model from learning, however, is the strength of the assumptions it contains. The problem is that \\(M\\) is uncorrelated with \\(X\\) in the true data-generating process, so the stronger model learns that there is no indirect effect. But, at the same time, this model does not allow for a direct effect. Despite what would seem to be overwhelming evidence of a systematic \\(X,Y\\) correlation, a causal relationship connecting \\(X\\) to \\(Y\\) remains extremely unlikely given the \\(X,M\\) data pattern and the impossibility of direct effects. The stronger model just can’t handle the truth. The weaker model, on the other hand, readily learns about the direct \\(X \\rightarrow Y\\) effect. 15.1.4 Leave-one-out cross-validation A further class of model-validation methods involves cross-validation. Rather than asking how well the updated model predicts the data used to update it, cross-validation uses the data at hand to estimate how well the model is likely to predict new data that have not yet been seen. One way to do this is to split the available data, using one subsample to update and then assessing predictions using the other subsample. We focus here, however, on approaches that uses the all of the available data to estimate out-of-sample predictive performance. One such approach is the “leave-one-out” (LOO) algorithm. In a LOO approach, we update the model using all data points except for one and then ask how well the model performs in predicting the left-out observation. We repeat this for every data point in the dataset to assess how well we can predict the entire dataset. Often, the LOO approach is used to predict a particular outcome variable. In a framework, however, we are interested in predictions over the joint realization of all nodes, not just a single “outcome.” Thus, we calculate the posterior probability of each data point, using the model updated with all of the other observations. The LOO estimate of out-of-sample predictive fit, for a dataset with \\(n\\) observations, is then: \\[\\prod_1^np(y_i|y_{-i}, \\text{model})\\] where \\(y_{-i}\\) is the data pattern with observation \\(y_i\\) left out, and \\(y_i\\) represents the values of all nodes of interest for observation \\(i\\). We implement LOO cross-validation of the stronger and weaker models using 200 observations generated from the same data-generating model employed above. We find that the LOO likelihood of the data under the stronger model is 1.64e-182 while the likelihood is 4.33e-175 under the weaker model. Thus, the weaker model represents an estimated improvement in out-of-sample prediction on the order of 2.64e+07. We can visualize the pattern in Figure XXXX, where we plot the likelihood of each possible data type under the stronger model against the likelihood of that data type under the weaker model. Looking at the scales of the two axes—which is much more compressed on the horizontal than on the vertical—one can see that the stronger model is not able to differentiate as much across the data types as the weaker. Notably, the stronger model is not able to “learn” from the data about the (in fact, operative) relationship between \\(X\\) and \\(Y\\). We can see that, for any given \\(X, M\\) combination, the two possible values of \\(Y\\) are predicted with essentially the same likelihood. The stronger model also seems to have “learned” from chance correlations in the data that different values \\(X,M\\) combinations are differentially likely—even though they are uncorrelated under the true model. The weaker model, on the other hand, basically divides the data types into two groups: those with a positive \\(X,Y\\) correlation and those with a negative \\(X,Y\\) correlation and has correctly (given the true model) learned that the former is more likely than the latter. In Figure XXXX, we then see how the likelihoods of each data type line up with the actual count of each data type. As we can see, the weaker model updates to likelihoods that fit the actual data pattern well while the stronger model does not. We can also turn the tables and imagine that the stronger model represents the true data-generating process. We implement LOO cross-validation of the two models using 200 data points generated from the stronger model. In Figure XXXX, we see a comparison of the likelihoods of the data types under the two updated models and note that they are extremely similar. This represents an important asymmetry: the model that makes weaker assumptions performs far better in handling data generated by a “stronger” true model than does the stronger model in learning about a process that violates one of its assumptions. Since the weaker model allows for both direct and indirect effects, the weaker can learn about the parameters of the true process in the first situation; but the strong model cannot do so in the second situation because it has by assumption ruled out a key feature of that process (the direct effect). BETTER: PLOT CASES ON BOTTOM AND TWO LIKELIHOODS AS POINTS; HAVE TYPES AS LABELS X AI IF POSSIBLE While it is difficult to see this in Figure XXXX, the stronger model still performs better here than the weaker model. The likelihood of the data under the stronger model is now 3.57e-119, compared to the likelihood of 1.04e-124 under the weaker model. Thus, the weaker model represents an estimated loss to out-of-sample prediction on the order of 2.91e-06. This is not surprising insofar as the stronger model precisely models the data-generating process while the extra parameters in the weaker model allow for “learning” from chance features of the data. These examples display features of estimation of out-of-sample prediction accuracy familiar from a regression context. In a regression framework, adding parameters to a model may improve fit to sample—generating gains to out-of-sample prediction accuracy when the new parameters pick up systematic features of the data-generating process—but run a risk of over-fitting to chance pattenrs in the data. Similarly, in a structural-causal-model framework, for a model with weaker assumptions and more parameters. We saw that the weaker model performed much better when the true process involved direct effects since the extra parameters, allowing for direct effects, captured something “real” going on. But the same model performed slightly worse than the stronger model when there were no direct effects to pick up, such that the extra parameter could only model noise. 15.1.5 Sensitivity The last approach we consider brackets the question of which model is better and asks, instead: how much do your conclusions depend on the model? You can worry less about your assumptions if the conclusions are not strongly dependent on them. To illustrate using a process tracing example, consider a situation in which we are unsure about posited parameter values: that is, about the probability of particular effects at particular nodes. It is likely to be the case in many research situations that we are considerably uncertain about how to quantify intuitive or theoretically informed beliefs about the relative likelihood of different effects. Suppose, for instance, that we begin with an \\(X \\rightarrow M \\rightarrow Y\\) model. And suppose, further, that we believe that it is unlikely that \\(M\\) has an adverse effect on \\(Y\\). But we are not sure how unlikely that adverse effect is. (We assume all other modal types are equally likely.) Finally, say that we want to use the observation of \\(M\\) to draw an inference about whether \\(X=1\\) caused \\(Y=1\\) in an \\(X=Y=1\\) case. How much does our inference on \\(X\\)’s effect on \\(Y\\)—when we see \\(M=0\\) or \\(M=1\\)—depend on this second stage assumption about the probability of a negative \\(M \\rightarrow Y\\) effect? We answer the question by looking at posterior beliefs for a range of possible values for the relevant parameter, \\(\\lambda^Y_{10}\\). In Table REF, we examine a range of values for \\(\\lambda^Y_{10}\\), from 0 to 0.25 (full parity with other types). For each parameter value, we first show the resulting prior belief about the probability that \\(X=1\\) caused \\(Y=1\\). We can see that, before we observe \\(M\\), we think that a positive \\(X \\rightarrow Y\\) effect is more likely as a negative \\(M \\rightarrow Y\\) effect is more likely. This stands to reason since a negative second-stage effect is one possible process in which a positive \\(X \\rightarrow Y\\) effect might occur. And higher values for \\(\\lambda^Y_{10}\\) come disproportionately at the expense of types under which \\(X\\) cannot affect \\(Y\\).5 In the next two columns, we show the posterior belief we arrive at when we observe \\(M=0\\) and then \\(M=1\\), for each \\(\\lambda^Y_{10}\\) assumption. Looking at the last column first, we see that our inference from \\(M=1\\) does not depend at all on our beliefs about adverse \\(M \\rightarrow Y\\) effects. The reason is that, if we see \\(M=1\\), we already know that \\(M\\) did not have a negative effect on \\(Y\\), given that we also know \\(Y=1\\). Our beliefs are purely a function of the probability that there are positive effects at both stages as compared to the probability of other causal types that could yield \\(X=M=Y=1\\), a comparison unaffected by the probability of a negative \\(M \\rightarrow Y\\) effect. Our inferences when \\(M=0\\), on the other hand, do depend on \\(\\lambda^Y_{10}\\): when we see \\(M=0\\), our belief about a positive \\(X \\rightarrow Y\\) effect depends on the likelihood of negative effects at both stages. We see, then, that the likelier we think negative effects are at the second stage, the higher our posterior confidence in a positive \\(X \\rightarrow Y\\) effect when we see \\(M=0\\). Table 15.2: Inferences on the probability that \\(X\\) caused \\(Y\\) upon seeing \\(M=0\\) or \\(M=1\\) for a range of possible values of \\(\\lambda^Y_{10}\\) \\(\\lambda^Y_{10}\\) Prior \\(M=0\\) \\(M=1\\) 0.00 0.167 0.000 0.25 0.05 0.183 0.068 0.25 0.10 0.200 0.125 0.25 0.15 0.217 0.173 0.25 0.20 0.233 0.214 0.25 0.25 0.250 0.250 0.25 Even though our inferences given \\(M=1\\) do not depend on \\(\\lambda^Y_{10}\\), the amount that we update if we see \\(M=1\\) does depend on \\(\\lambda^Y_{10}\\). This is because \\(\\lambda^Y_{10}\\) affects our belief, prior to seeing \\(M\\), that \\(X=1\\) caused \\(Y=1\\). Working with a low \\(\\lambda^Y_{10}\\) value, we start out less confident that \\(X=1\\) caused \\(Y=1\\), and thus our beliefs make a bigger jump if we do see \\(M=1\\) than if we had worked with a \\(\\lambda^Y_{10}\\) higher value. However, to the extent that we want to know how our assumptions affect our conclusions, the interesting feature of this illustration is that sensitivity depends on what we find. The answer to our query is sensitive to the \\(\\lambda^Y_{10}\\) assumption if we find \\(M=0\\), but not if we find \\(M=1\\). It is also worth noting that, even if we observe \\(M=0\\), the sensitivity is limited across the range of parameter values tested. In particular, for all \\(\\lambda^Y_{10}\\) values below parity (0.25), seeing \\(M=0\\) moves our beliefs in the same direction. We can use the same basic approach to examine how our conclusions change if we relax assumptions about nodal-type restrictions, about confounds, or about causal structure. We also note that in cases in which you cannot quantify uncertainty about parameters you might still be able to engage in a form of “qualitative inference.” There is a literature on probabilistic causal models that assesses the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see Parsons (2001). Under this kind of approach, a researcher might willing to say that they think some probability \\(p\\) is not plausibly greater than .5, but unwilling to make a statement about their beliefs about where in the \\(0\\) to \\(0.5\\) range it lies. Such incomplete statements can be enough to rule our classes of conclusion. 15.2 Evaluating the Democracy-Inequality model ** GENERATE A TABLE SHOWING HOW PIMD DOES ON 5 CRITERIA ** ###Estimating the model Figure 15.2: ATE of I on D. 15.2.1 Check assumptions of conditional independence Our model presupposes that \\(P\\) and \\(I\\) are independent and that \\(P\\) and \\(M\\) are independent. Note that the model is consistent with the possibility that, conditional on \\(D\\), there is a correlation betwen \\(M\\) and \\(P\\) or between \\(I\\) and \\(P\\), as in these cases \\(D\\) acts as a collider. The relations in the first two rows of Table @(tab:ch15cipimd) are consistent with this assumption. Table 15.3: Regression coefficients to assess conditional independence Given estimate std.error p.value - 0.151 0.155 0.337 - -0.175 0.164 0.292 M = 0 -0.071 0.277 0.800 M = 1 0.329 0.170 0.066 I = 0 -0.400 0.214 0.080 I = 1 0.000 0.244 1.000 We can dig a little deeper however. The model also implies that \\(P\\) should be independent of \\(I\\) given \\(M\\), and of \\(M\\) given \\(I\\) since in each case \\(D\\) blocks all paths between \\(P\\) and \\(I\\) or \\(M\\). Here the evidence is more worrying. We see for example a relative strong relation between \\(P\\) and \\(I\\) when \\(M=1\\) and between \\(P\\) and \\(M\\) when \\(I=0\\). Substantively, at least in states with less inequality, mobilization is common in cases in which there is no international pressure (\\(P=0\\)), arising in 0 of 13 cases, but rare in cases in which there is pressure arising in just 1 of 5 cases. This puts pressure on our assumption that mobilization is not directly affected by (the absence of) international pressure. 15.2.2 Bayesian \\(p\\)-value Turning to leave one out model assessment we now consider comparing the base model to a model in in which we drop the assumption of monotonicity of \\(M\\) in \\(I\\). This model has more parameters but is also more. 15.2.3 5 : Sensitivity Prior check In our model we assumed that \\(M\\) is monotonic in \\(I\\). How much do conclusions depend on this? Here we consider negative effects of \\(I\\) on \\(M\\) unlikely, rather than impossible, and we consider null and positive effects somewhat likely. We refer to these priors as “quantitative priors” in the sense that they place a numerical value on beliefs rather than a logical restriction. Here, we set our prior on \\(\\theta^M\\) as: \\(p(\\theta^M=\\theta^M_{10})=0.1\\), \\(p(\\theta^M=\\theta^M_{00})=0.25\\), \\(p(\\theta^M=\\theta^M_{11})=0.25\\), and \\(p(\\theta^M=\\theta^M_{01})=0.4\\). We show the results for the inferences given different findings in tables and . The mapping into expected posterior variance associated with each strategy is shown by the numbers in parentheses in Table . The results differ in various modest ways. However, the biggest difference we observe is in the degree to which the mobilization clue matters when we are looking for negative effects of inequality. As discussed, if we assumed monotonic positive effects of inequality on mobilization and monotonic positive effects of mobilization on inequality, then the mediator clue is uninformative about the indirect pathway since that pathway can only generate a positive effect. However, if we allow for the possibility of a negative effect of inequality on mobilization, we now make \\(M\\) informative as a mediator even when the effect of inequality that we are interested in is negative: it is now possible that inequality has a negative effect on democratization via a negative effect on mobilization, followed by a positive effect of mobilization on democratization. So now, observing whether mobilization occurred adds information about whether a negative effect could have occurred via the mobilization pathway. Moreover, it is possible for the two effects of observing \\(M\\) on our beliefs to work in opposite ways. What we learn from observing \\(M\\) about the \\(I \\rightarrow M \\rightarrow D\\) pathway may push in a different direction from what we learn from observing \\(M\\) about the direct \\(I \\rightarrow D\\) pathway. We see this dynamic at work in a case with low inequality and democratization. Where we are only learning about \\(M\\) as a moderator of \\(I\\)’s direct effect (monotonicity assumption in place), observing \\(M=0\\) shifts our beliefs in favor of \\(I\\)’s negative effect. But where we are learning about \\(M\\) as both mediator and moderator, observing \\(M=0\\) shifts our beliefs against \\(I\\)’s negative effect. The reason for this latter result is straightforward: if \\(I=0\\) and we then see \\(M=0\\), then we have just learned that inequality’s possible indirect negative effect, running via the mobilization pathway, has not in fact occurred; and this has a considerable downward effect on our beliefs in an overall negative effect of inequality. This learning outweighs the small positive impact of observing \\(M=0\\) on our confidence that \\(I\\) had a direct negative effect on \\(D\\). We see these differences most clearly in the cases of Albania (as compared to Mexico) and Nicaragua (as compared to Taiwan). Under priors fully constrained to monotonic causal effects, we saw that the mediator clue, \\(M\\), made only a small difference to our inferences. However, if we allow for a negative effect of \\(I\\) on \\(M\\), even while believing it to be unlikely, observing mobilization in Albania and Nicaragua makes us substantially more confident that inequality mattered, and differentiates our conclusions about these cases more sharply from our conclusions about Mexico and Taiwan, respectively. References "]
]
