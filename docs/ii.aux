\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{apalike}
\newlabel{preface}{{}{7}{Preface}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{7}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Integrands}{9}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{9}{Integrands}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Centrality of Causal Models}{10}{section.1.1}}
\newlabel{the-centrality-of-causal-models}{{1.1}{10}{The Centrality of Causal Models}{section.1.1}{}}
\citation{humphreys2015mixing}
\citation{humphreys2015mixing}
\citation{GlynnQuinn2011}
\citation{george1985case}
\citation{bennett2014process}
\citation{george2005case}
\citation{brady2010rethinking}
\citation{Hall2003aligning}
\citation{mahoney2010after}
\citation{king1994designing}
\citation{collier2011understanding}
\citation{Mahony:Logic:2012}
\citation{rohlfing2013comparative}
\citation{fairfield2017explicit}
\citation{BennettAppendix}
\citation{humphreys2015mixing}
\citation{humphreys2015mixing}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Qualitative and mixed-method inference}{11}{subsection.1.1.1}}
\newlabel{qualitative-and-mixed-method-inference}{{1.1.1}{11}{Qualitative and mixed-method inference}{subsection.1.1.1}{}}
\citation{lieberman2003race}
\citation{swank2002global}
\citation{stokes2001mandates}
\citation{collier2010sources}
\citation{Lieberman2005nested}
\citation{SeawrightGerring2008}
\citation{gordon2004quantitative}
\citation{WesternJackman1994}
\citation{GlynnQuinn2011}
\citation{seawrightbook}
\citation{dunning2012natural}
\citation{GerGreKap04}
\citation{druckman2011experimentation}
\citation{palfrey2009laboratory}
\citation{Rubin1974}
\citation{splawa1990application}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}The limits to design-based inference}{13}{subsection.1.1.2}}
\newlabel{the-limits-to-design-based-inference}{{1.1.2}{13}{The limits to design-based inference}{subsection.1.1.2}{}}
\citation{thelen2015comparative}
\citation{clarke2012model}
\citation{george2005case}
\citation{Hall2003aligning}
\citation{mahoney2010after}
\citation{gerring2006case}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Connecting theory and empirics}{14}{subsection.1.1.3}}
\newlabel{connecting-theory-and-empirics}{{1.1.3}{14}{Connecting theory and empirics}{subsection.1.1.3}{}}
\citation{glynn2007non}
\citation{garcia2015graphical}
\citation{Waldner2015completeness}
\citation{weller2014finding}
\citation{Waldner2015completeness}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The Road Ahead}{17}{section.1.2}}
\newlabel{the-road-ahead}{{1.2}{17}{The Road Ahead}{section.1.2}{}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Foundations}{19}{part.1}}
\citation{hume2000enquiry}
\citation{splawa1990application}
\citation{lewis1973counterfactuals}
\citation{lewis1986causation}
\newlabel{part-foundations}{{I}{21}{Foundations}{part.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Causal Models}{21}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{models}{{2}{21}{Causal Models}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The counterfactual model}{21}{section.2.1}}
\newlabel{the-counterfactual-model}{{2.1}{21}{The counterfactual model}{section.2.1}{}}
\citation{boix2003democracy}
\citation{Rubin1974}
\citation{HerronQuinn}
\gdef \LT@i {\LT@entry 
    {3}{65.72389pt}\LT@entry 
    {3}{86.51698pt}\LT@entry 
    {3}{99.038pt}\LT@entry 
    {3}{79.49118pt}\LT@entry 
    {3}{78.97285pt}}
\gdef \LT@ii {\LT@entry 
    {3}{70.03131pt}\LT@entry 
    {1}{48.22903pt}\LT@entry 
    {1}{48.88203pt}\LT@entry 
    {1}{47.57603pt}\LT@entry 
    {1}{42.88203pt}}
\newlabel{tab:PO}{{2.1}{23}{The counterfactual model}{table.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{. Potential outcomes: What would happen to each of four possible types of case if they were or were not treated.}}{23}{table.2.1}}
\newlabel{tab:POGEN}{{2.2}{23}{The counterfactual model}{table.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{. Generalizing from Table \ref  {tab:PO}, the table gives for each causal type the values that \(Y\) would take on if \(X\) is set at \(0\) and if \(X\) is set at 1.}}{23}{table.2.2}}
\gdef \LT@iii {\LT@entry 
    {1}{76.46037pt}\LT@entry 
    {1}{91.8575pt}\LT@entry 
    {1}{96.55247pt}\LT@entry 
    {1}{101.25461pt}\LT@entry 
    {1}{81.16254pt}}
\newlabel{tab:PO16}{{2.3}{24}{The counterfactual model}{table.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{ With two binary causal variables, there are 16 causal types: 16 ways in which \(Y\) might respond to changes in the two variables.}}{24}{table.2.3}}
\citation{mahoney2008toward}
\citation{laplace1901philosophical}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Causal Models and Directed Acyclic Graphs}{25}{section.2.2}}
\newlabel{causal-models-and-directed-acyclic-graphs}{{2.2}{25}{Causal Models and Directed Acyclic Graphs}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A simple causal model in which high inequality ($I$) affects the democratization ($D$) via redistributive demands and mass mobilization ($M$), which is also a function of ethnic homogeneity ($E$). The arrows show relations of causal dependence between variables. The graph does not capture the ranges of the variables and the functional relations between them.}}{26}{figure.2.1}}
\newlabel{fig:simpleDAG}{{2.1}{26}{A simple causal model in which high inequality ($I$) affects the democratization ($D$) via redistributive demands and mass mobilization ($M$), which is also a function of ethnic homogeneity ($E$). The arrows show relations of causal dependence between variables. The graph does not capture the ranges of the variables and the functional relations between them}{figure.2.1}{}}
\citation{humphreys2015mixing}
\newlabel{eq:markov}{{2.1}{31}{Causal Models and Directed Acyclic Graphs}{equation.2.2.1}{}}
\citation{hernan2006instruments}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Features of causal graphs}{32}{section.2.3}}
\newlabel{features-of-causal-graphs}{{2.3}{32}{Features of causal graphs}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conditional independence from DAGs}{34}{section.2.4}}
\newlabel{conditional-independence-from-dags}{{2.4}{34}{Conditional independence from DAGs}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces This graph represents a simple causal model in which war ($W$) affects both military casualties ($C$) and price inflation ($P$).}}{35}{figure.2.2}}
\newlabel{fig:warDAG}{{2.2}{35}{This graph represents a simple causal model in which war ($W$) affects both military casualties ($C$) and price inflation ($P$)}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  Three elementary relations of conditional independence.}}{36}{figure.2.3}}
\newlabel{fig:CI}{{2.3}{36}{Three elementary relations of conditional independence}{figure.2.3}{}}
\newlabel{fig:unnamed-chunk-6}{{2.3}{36}{Three elementary relations of conditional independence}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  An exercise: $A$ and $D$ are conditionally independent, given which other variable(s)?}}{37}{figure.2.4}}
\newlabel{fig:CItest}{{2.4}{37}{An exercise: $A$ and $D$ are conditionally independent, given which other variable(s)?}{figure.2.4}{}}
\newlabel{fig:unnamed-chunk-7}{{2.4}{37}{An exercise: $A$ and $D$ are conditionally independent, given which other variable(s)?}{figure.2.4}{}}
\citation{pierson1994dismantling}
\citation{saunders2011leaders}
\citation{przeworski1997modernization}
\citation{boix2003democracy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  A graphical representation of Pierson (1994).}}{38}{figure.2.5}}
\newlabel{fig:DAGPierson}{{2.5}{38}{A graphical representation of Pierson (1994)}{figure.2.5}{}}
\newlabel{fig:unnamed-chunk-8}{{2.5}{38}{A graphical representation of Pierson (1994)}{figure.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Causal models from the literature}{38}{section.2.5}}
\newlabel{causal-models-from-the-literature}{{2.5}{38}{Causal models from the literature}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Pierson on dismantling the welfare state}{38}{subsection.2.5.1}}
\newlabel{pierson-on-dismantling-the-welfare-state}{{2.5.1}{38}{Pierson on dismantling the welfare state}{subsection.2.5.1}{}}
\citation{saunders2011leaders}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Saunders (2011)}{40}{subsection.2.5.2}}
\newlabel{saunders-2011}{{2.5.2}{40}{Saunders (2011)}{subsection.2.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  A graphical representation of Saunders' (2011) argument.}}{41}{figure.2.6}}
\newlabel{fig:DAGSaunders}{{2.6}{41}{A graphical representation of Saunders' (2011) argument}{figure.2.6}{}}
\newlabel{fig:unnamed-chunk-9}{{2.6}{41}{A graphical representation of Saunders' (2011) argument}{figure.2.6}{}}
\citation{przeworski1997modernization}
\citation{boix2003democracy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A graphical representation of Przeworski and Limongi's argument, where $D_{t-1}$=democracy in the previous period; $GDP_t$=per capita GDP in the current period; $D_t$=democracy in the current period.}}{42}{figure.2.7}}
\newlabel{fig:DAGPL}{{2.7}{42}{A graphical representation of Przeworski and Limongi's argument, where $D_{t-1}$=democracy in the previous period; $GDP_t$=per capita GDP in the current period; $D_t$=democracy in the current period}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Przeworski and Limongi (1997)}{42}{subsection.2.5.3}}
\newlabel{przeworski-and-limongi-1997}{{2.5.3}{42}{Przeworski and Limongi (1997)}{subsection.2.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Steps for constructing causal models}{43}{section.2.6}}
\newlabel{steps-for-constructing-causal-models}{{2.6}{43}{Steps for constructing causal models}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Abstract procedure}{44}{subsection.2.6.1}}
\newlabel{abstract-procedure}{{2.6.1}{44}{Abstract procedure}{subsection.2.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Model construction in code}{44}{subsection.2.6.2}}
\newlabel{model-construction-in-code}{{2.6.2}{44}{Model construction in code}{subsection.2.6.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theories as causal models}{47}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{theory}{{3}{47}{Theories as causal models}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a).}}{48}{figure.3.1}}
\newlabel{fig:demtheory5}{{3.1}{48}{DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a)}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Two theories of inequality's effects on democratization}{48}{section.3.1}}
\newlabel{inequalitytheory}{{3.1}{48}{Two theories of inequality's effects on democratization}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Theory as causal functions}{48}{subsection.3.1.1}}
\newlabel{theory-as-causal-functions}{{3.1.1}{48}{Theory as causal functions}{subsection.3.1.1}{}}
\citation{boix2003democracy}
\citation{acemoglu2005economic}
\citation{pearl2009causality}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Theory as a ``lower-level'' model}{50}{section.3.2}}
\newlabel{theory-as-a-lower-level-model}{{3.2}{50}{Theory as a ``lower-level'' model}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Disggregating nodes}{50}{subsection.3.2.1}}
\newlabel{disggregating-nodes}{{3.2.1}{50}{Disggregating nodes}{subsection.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes.}}{51}{figure.3.2}}
\newlabel{fig:Highlow}{{3.2}{51}{Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes}{figure.3.2}{}}
\newlabel{fig:unnamed-chunk-12}{{3.2}{51}{Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Generalizing a model}{52}{subsection.3.2.2}}
\newlabel{generalizing-a-model}{{3.2.2}{52}{Generalizing a model}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Moving between higher- and lower-level models}{52}{section.3.3}}
\newlabel{moving-between-higher--and-lower-level-models}{{3.3}{52}{Moving between higher- and lower-level models}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Mappings are not one-to-one}{53}{subsection.3.3.1}}
\newlabel{mappings-are-not-one-to-one}{{3.3.1}{53}{Mappings are not one-to-one}{subsection.3.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A higher-level model and a lower-level model that is impermissible.}}{54}{figure.3.3}}
\newlabel{fig:incompat}{{3.3}{54}{A higher-level model and a lower-level model that is impermissible}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Permissible moves across levels}{54}{subsection.3.3.2}}
\newlabel{permissible-moves-across-levels}{{3.3.2}{54}{Permissible moves across levels}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.1}Moving down levels}{54}{subsubsection.3.3.2.1}}
\newlabel{moving-down-levels}{{3.3.2.1}{54}{Moving down levels}{subsubsection.3.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.2}Moving up levels}{54}{subsubsection.3.3.2.2}}
\newlabel{moving-up-levels}{{3.3.2.2}{54}{Moving up levels}{subsubsection.3.3.2.2}{}}
\citation{koller2009probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Here we represent the basic principles for eliminating exogenous nodes.}}{55}{figure.3.4}}
\newlabel{fig:elimrules}{{3.4}{55}{Here we represent the basic principles for eliminating exogenous nodes}{figure.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.3.2.2.1}Eliminating endogenous nodes}{55}{paragraph.3.3.2.2.1}}
\newlabel{eliminating-endogenous-nodes}{{3.3.2.2.1}{55}{Eliminating endogenous nodes}{paragraph.3.3.2.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.3.2.2.2}Eliminating exogenous nodes}{55}{paragraph.3.3.2.2.2}}
\newlabel{eliminating-exogenous-nodes}{{3.3.2.2.2}{55}{Eliminating exogenous nodes}{paragraph.3.3.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A lower-level model from which multiple higher level models can be derived.}}{57}{figure.3.5}}
\newlabel{fig:lowercomplexdem}{{3.5}{57}{A lower-level model from which multiple higher level models can be derived}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Higher level models derived from the lower level model of Figure X. Nodes that are eliminated are marked in grey; circles denote exogenous nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two other nodes could equivalently be indicated as an undirected edge connecting the two.) Note that \(M\), \(R\), and \(D\) are deterministic functions of \(I\) and \(E\) in this example, with no random inputs pointing into them.}}{57}{figure.3.6}}
\newlabel{fig:runningsubs}{{3.6}{57}{Higher level models derived from the lower level model of Figure X. Nodes that are eliminated are marked in grey; circles denote exogenous nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two other nodes could equivalently be indicated as an undirected edge connecting the two.) Note that \(M\), \(R\), and \(D\) are deterministic functions of \(I\) and \(E\) in this example, with no random inputs pointing into them}{figure.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.3.2.2.3}Conditioning on nodes}{57}{paragraph.3.3.2.2.3}}
\newlabel{conditioning-on-nodes}{{3.3.2.2.3}{57}{Conditioning on nodes}{paragraph.3.3.2.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Beneath the Graph: Causal Types in Lower-Level Models}{59}{section.3.4}}
\newlabel{beneath-the-graph-causal-types-in-lower-level-models}{{3.4}{59}{Beneath the Graph: Causal Types in Lower-Level Models}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Mediation as Theory}{60}{subsection.3.4.1}}
\newlabel{medtheory}{{3.4.1}{60}{Mediation as Theory}{subsection.3.4.1}{}}
\gdef \LT@iv {\LT@entry 
    {1}{67.07045pt}\LT@entry 
    {1}{96.55247pt}\LT@entry 
    {1}{96.55247pt}\LT@entry 
    {1}{96.55247pt}\LT@entry 
    {1}{90.55247pt}}
\newlabel{tab:highlowmapping}{{3.1}{61}{Mediation as Theory}{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{ Mapping from lower level nodal types on \(M\) and \(Y\) to higher level causal types on \(Y\).}}{61}{table.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Moderation as Theory}{61}{subsection.3.4.2}}
\newlabel{modtheory}{{3.4.2}{61}{Moderation as Theory}{subsection.3.4.2}{}}
\citation{clarke2012model}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusion}{62}{section.3.5}}
\newlabel{conclusion}{{3.5}{62}{Conclusion}{section.3.5}{}}
\citation{clarke2012model}
\citation{giere2010explaining}
\citation{popper2014conjectures}
\citation{clarke2012model}
\citation{Van-Evera:1997}
\citation{przeworski1970logic}
\citation{Van-Evera:1997}
\citation{przeworski1970logic}
\citation{koller2003multi}
\citation{white2009settable}
\citation{dawid2002influence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces  A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which \(b=0\).}}{65}{figure.3.7}}
\newlabel{fig:game1}{{3.7}{65}{A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which \(b=0\)}{figure.3.7}{}}
\newlabel{fig:tree}{{3.7}{65}{A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which \(b=0\)}{figure.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Appendix: Illustration of a Mapping from a Game to a DAG}{65}{section.3.6}}
\newlabel{appendix-illustration-of-a-mapping-from-a-game-to-a-dag}{{3.6}{65}{Appendix: Illustration of a Mapping from a Game to a DAG}{section.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces  The upper panel shows a causal graph that describes relations between nodes suggested by analysis of the game in Figure \ref  {fig:tree} and which can imply the causal graph of Figure \ref  {fig:running}. The game itself (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors. The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''}}{66}{figure.3.8}}
\newlabel{fig:gamedag}{{3.8}{66}{The upper panel shows a causal graph that describes relations between nodes suggested by analysis of the game in Figure \ref {fig:tree} and which can imply the causal graph of Figure \ref {fig:running}. The game itself (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors. The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''}{figure.3.8}{}}
\newlabel{fig:unnamed-chunk-13}{{3.8}{66}{The upper panel shows a causal graph that describes relations between nodes suggested by analysis of the game in Figure \ref {fig:tree} and which can imply the causal graph of Figure \ref {fig:running}. The game itself (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors. The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''}{figure.3.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Causal Questions}{69}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{questions}{{4}{69}{Causal Questions}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Causal queries}{70}{section.4.1}}
\newlabel{causal-queries}{{4.1}{70}{Causal queries}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Case-level causal effects}{70}{subsection.4.1.1}}
\newlabel{case-level-causal-effects}{{4.1.1}{70}{Case-level causal effects}{subsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta ^Y$. With a single binary causal variable of interest, we let $\theta _Y$ take on values $\theta ^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta ^Y$ ranges over the four values: $\theta ^Y_{00}$, $\theta ^Y_{10}$, $\theta ^Y_{01}$ and $\theta ^Y_{11}$.}}{71}{figure.4.1}}
\newlabel{fig:casequery}{{4.1}{71}{This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta ^Y$. With a single binary causal variable of interest, we let $\theta _Y$ take on values $\theta ^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta ^Y$ ranges over the four values: $\theta ^Y_{00}$, $\theta ^Y_{10}$, $\theta ^Y_{01}$ and $\theta ^Y_{11}$}{figure.4.1}{}}
\newlabel{fig:unnamed-chunk-15}{{4.1}{71}{This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta ^Y$. With a single binary causal variable of interest, we let $\theta _Y$ take on values $\theta ^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta ^Y$ ranges over the four values: $\theta ^Y_{00}$, $\theta ^Y_{10}$, $\theta ^Y_{01}$ and $\theta ^Y_{11}$}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Case-level causal attribution}{71}{subsection.4.1.2}}
\newlabel{case-level-causal-attribution}{{4.1.2}{71}{Case-level causal attribution}{subsection.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces This DAG is a graphical representation of the simple causal setup in which $Y$ depends on two variables $X1$ and $X2$. How $Y$ responds to X1 and X2 depnds on $\theta ^Y$, the DAG itself does not provide information on whether or how X1 and X2 interact with each other.}}{72}{figure.4.2}}
\newlabel{fig:attribquery}{{4.2}{72}{This DAG is a graphical representation of the simple causal setup in which $Y$ depends on two variables $X1$ and $X2$. How $Y$ responds to X1 and X2 depnds on $\theta ^Y$, the DAG itself does not provide information on whether or how X1 and X2 interact with each other}{figure.4.2}{}}
\citation{halpern2015modification}
\citation{halpern2005causesa}
\citation{hall2004two}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Case-level explanation}{73}{subsection.4.1.3}}
\newlabel{case-level-explanation}{{4.1.3}{73}{Case-level explanation}{subsection.4.1.3}{}}
\citation{menzies1989probabilistic}
\citation{halpern2016actual}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta ^Y$. With a single binary causal variable of interest, we let $\theta _Y$ take on values $\theta ^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta ^Y$ ranges over the four values: $\theta ^Y_{00}$, $\theta ^Y_{10}$, $\theta ^Y_{01}$ and $\theta ^Y_{11}$.}}{76}{figure.4.3}}
\newlabel{fig:actualquery}{{4.3}{76}{This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta ^Y$. With a single binary causal variable of interest, we let $\theta _Y$ take on values $\theta ^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta ^Y$ ranges over the four values: $\theta ^Y_{00}$, $\theta ^Y_{10}$, $\theta ^Y_{01}$ and $\theta ^Y_{11}$}{figure.4.3}{}}
\newlabel{fig:unnamed-chunk-16}{{4.3}{76}{This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta ^Y$. With a single binary causal variable of interest, we let $\theta _Y$ take on values $\theta ^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta ^Y$ ranges over the four values: $\theta ^Y_{00}$, $\theta ^Y_{10}$, $\theta ^Y_{01}$ and $\theta ^Y_{11}$}{figure.4.3}{}}
\citation{humphreys2015mixing}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Average causal effects}{77}{subsection.4.1.4}}
\newlabel{average-causal-effects}{{4.1.4}{77}{Average causal effects}{subsection.4.1.4}{}}
\citation{pearl2009causality}
\citation{imai2010general}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces  This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different causal types. As before, $X$'s effect on $Y$ is a function of a causal-type variable, $\theta ^Y$. Yet here we explicitly model the process through which the case's type is drawn from a distribution of types in a population. The variable $\lambda $ is a vector representing the multinomial distribution of causal types in the population while $U_\theta $ is a random variable representing the draw of each case from the distribution defined by $\lambda $. A case's causal type, $\theta ^Y$, is thus a joint function of $\lambda ^Y$ and $U^{\theta _Y}$.}}{78}{figure.4.4}}
\newlabel{fig:DAGace}{{4.4}{78}{This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different causal types. As before, $X$'s effect on $Y$ is a function of a causal-type variable, $\theta ^Y$. Yet here we explicitly model the process through which the case's type is drawn from a distribution of types in a population. The variable $\lambda $ is a vector representing the multinomial distribution of causal types in the population while $U_\theta $ is a random variable representing the draw of each case from the distribution defined by $\lambda $. A case's causal type, $\theta ^Y$, is thus a joint function of $\lambda ^Y$ and $U^{\theta _Y}$}{figure.4.4}{}}
\newlabel{fig:unnamed-chunk-17}{{4.4}{78}{This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different causal types. As before, $X$'s effect on $Y$ is a function of a causal-type variable, $\theta ^Y$. Yet here we explicitly model the process through which the case's type is drawn from a distribution of types in a population. The variable $\lambda $ is a vector representing the multinomial distribution of causal types in the population while $U_\theta $ is a random variable representing the draw of each case from the distribution defined by $\lambda $. A case's causal type, $\theta ^Y$, is thus a joint function of $\lambda ^Y$ and $U^{\theta _Y}$}{figure.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Causal Paths}{78}{subsection.4.1.5}}
\newlabel{causal-paths}{{4.1.5}{78}{Causal Paths}{subsection.4.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  Here $X$ has effects on $Y$ both indirectly through $M$ and directly.}}{79}{figure.4.5}}
\newlabel{fig:DAGpaths}{{4.5}{79}{Here $X$ has effects on $Y$ both indirectly through $M$ and directly}{figure.4.5}{}}
\newlabel{fig:unnamed-chunk-18}{{4.5}{79}{Here $X$ has effects on $Y$ both indirectly through $M$ and directly}{figure.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The table defines the 16 values (causal types) that $\theta _Y$ can take on, given a binary $X$ and $M$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies.}}{80}{table.4.1}}
\newlabel{tab:typespaths}{{4.1}{80}{The table defines the 16 values (causal types) that $\theta _Y$ can take on, given a binary $X$ and $M$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies}{table.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}A Running Example}{82}{section.4.2}}
\newlabel{a-running-example}{{4.2}{82}{A Running Example}{section.4.2}{}}
\citation{pearl2009causality}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces  The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\pi $ value.}}{83}{figure.4.6}}
\newlabel{fig:running}{{4.6}{83}{The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\pi $ value}{figure.4.6}{}}
\newlabel{fig:unnamed-chunk-20}{{4.6}{83}{The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\pi $ value}{figure.4.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Bayesian Answers}{89}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{bayeschapter}{{5}{89}{Bayesian Answers}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Bayes Basics}{89}{section.5.1}}
\newlabel{bayes-basics}{{5.1}{89}{Bayes Basics}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Simple instances}{89}{subsection.5.1.1}}
\newlabel{simple-instances}{{5.1.1}{89}{Simple instances}{subsection.5.1.1}{}}
\citation{gardner1961second}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Bayes' Rule for Discrete Hypotheses}{90}{subsection.5.1.2}}
\newlabel{bayes-rule-for-discrete-hypotheses}{{5.1.2}{90}{Bayes' Rule for Discrete Hypotheses}{subsection.5.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Bayes' Rule for Continuous Parameters}{91}{subsection.5.1.3}}
\newlabel{bayes-rule-for-continuous-parameters}{{5.1.3}{91}{Bayes' Rule for Continuous Parameters}{subsection.5.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.1}The Dirichlet distributions}{91}{subsubsection.5.1.3.1}}
\newlabel{the-dirichlet-distributions}{{5.1.3.1}{91}{The Dirichlet distributions}{subsubsection.5.1.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Beta distributions}}{92}{figure.5.1}}
\newlabel{fig:Betas}{{5.1}{92}{Beta distributions}{figure.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.2}Moments}{93}{subsubsection.5.1.3.2}}
\newlabel{moments}{{5.1.3.2}{93}{Moments}{subsubsection.5.1.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.3}Bayes estimation in practice}{93}{subsubsection.5.1.3.3}}
\newlabel{bayes-estimation-in-practice}{{5.1.3.3}{93}{Bayes estimation in practice}{subsubsection.5.1.3.3}{}}
\citation{pearl2009causality}
\citation{pearl2012causal}
\citation{Rubin1974}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Bayes applied}{94}{section.5.2}}
\newlabel{bayes-applied}{{5.2}{94}{Bayes applied}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Bayesian Inference on Queries}{94}{subsection.5.2.1}}
\newlabel{bayesian-inference-on-queries}{{5.2.1}{94}{Bayesian Inference on Queries}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Bayesian correlational inference}{94}{subsection.5.2.2}}
\newlabel{bayesian-correlational-inference}{{5.2.2}{94}{Bayesian correlational inference}{subsection.5.2.2}{}}
\citation{heckman2014treatment}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A graph depicting a situation in which it is possible that $X$ causes $Y$; the unit level causal type is $\theta ^Y$ and the distribution of causal types is $\lambda ^Y$.}}{95}{figure.5.2}}
\newlabel{fig:simpleXYDAG}{{5.2}{95}{A graph depicting a situation in which it is possible that $X$ causes $Y$; the unit level causal type is $\theta ^Y$ and the distribution of causal types is $\lambda ^Y$}{figure.5.2}{}}
\citation{collier2004sources}
\citation{BennettBayes}
\citation{beachpedersen2013process}
\citation{rohlfing2012case}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Simple Bayesian Process Tracing}{97}{subsection.5.2.3}}
\newlabel{simple-bayesian-process-tracing}{{5.2.3}{97}{Simple Bayesian Process Tracing}{subsection.5.2.3}{}}
\newlabel{fnPV}{{6}{99}{}{Hfootnote.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces  A mapping from the probability of observing a clue if the proposition that a case is a \(b\) type is true (\(\phi _b\)) or false (\(\phi _d\)) to a generalization of the tests described in Van-Evera (1997).}}{100}{figure.5.3}}
\newlabel{fig:unnamed-chunk-24}{{5.3}{100}{A mapping from the probability of observing a clue if the proposition that a case is a \(b\) type is true (\(\phi _b\)) or false (\(\phi _d\)) to a generalization of the tests described in Van-Evera (1997)}{figure.5.3}{}}
\newlabel{CluesInferences1}{{5.3}{100}{A mapping from the probability of observing a clue if the proposition that a case is a \(b\) type is true (\(\phi _b\)) or false (\(\phi _d\)) to a generalization of the tests described in Van-Evera (1997)}{figure.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Three principles of Bayesian updating}{101}{section.5.3}}
\newlabel{three-principles-of-bayesian-updating}{{5.3}{101}{Three principles of Bayesian updating}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Priors matter}{101}{subsection.5.3.1}}
\newlabel{AppPriors}{{5.3.1}{101}{Priors matter}{subsection.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Figure shows how the learning from different types of tests depends on priors regarding the proposition. A smoking gun test has the greatest impact on beliefs when priors are middling low and the clue is observed; a `'hoop test'' has the greatest effect when priors are middling high and the clue is not observed.}}{102}{figure.5.4}}
\newlabel{fig:CluesInferences2}{{5.4}{102}{Figure shows how the learning from different types of tests depends on priors regarding the proposition. A smoking gun test has the greatest impact on beliefs when priors are middling low and the clue is observed; a `'hoop test'' has the greatest effect when priors are middling high and the clue is not observed}{figure.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Simultaneoues, joint updating}{103}{subsection.5.3.2}}
\newlabel{simultaneoues-joint-updating}{{5.3.2}{103}{Simultaneoues, joint updating}{subsection.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces  Joint posteriors distribution on whether a case is a \(b\) or \(d\) and on the probability of seeing a clue for a \(b\) type (\(\phi _b\)).}}{104}{figure.5.5}}
\newlabel{fig:unnamed-chunk-25}{{5.5}{104}{Joint posteriors distribution on whether a case is a \(b\) or \(d\) and on the probability of seeing a clue for a \(b\) type (\(\phi _b\))}{figure.5.5}{}}
\newlabel{fig:correlation}{{5.5}{104}{Joint posteriors distribution on whether a case is a \(b\) or \(d\) and on the probability of seeing a clue for a \(b\) type (\(\phi _b\))}{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Posteriors are independent of the ordering of data}{104}{subsection.5.3.3}}
\newlabel{posteriors-are-independent-of-the-ordering-of-data}{{5.3.3}{104}{Posteriors are independent of the ordering of data}{subsection.5.3.3}{}}
\citation{haggard2012inequality}
\citation{boix2003democracy}
\citation{acemoglu2005economic}
\citation{haggard2012inequality}
\citation{dahl1973polyarchy}
\citation{bollen1985political}
\citation{acemoglu2005economic}
\citation{boix2003democracy}
\citation{ansell2014inequality}
\citation{boix2003democracy}
\citation{acemoglu2005economic}
\citation{ansell2014inequality}
\citation{haggard2012inequality}
\citation{boix2003democracy}
\citation{acemoglu2005economic}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Application: Process Tracing with a Causal Model}{107}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{application-process-tracing-with-a-causal-model}{{6}{107}{Application: Process Tracing with a Causal Model}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Inequality and Democratization: The Debate}{107}{section.6.1}}
\newlabel{inequality-and-democratization-the-debate}{{6.1}{107}{Inequality and Democratization: The Debate}{section.6.1}{}}
\citation{meltzer1981rational}
\citation{huntington1993third}
\citation{linz1996problems}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}A Structural Causal Model}{109}{section.6.2}}
\newlabel{a-structural-causal-model}{{6.2}{109}{A Structural Causal Model}{section.6.2}{}}
\citation{humphreys2015mixing}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Simple democracy, inequality model}}{110}{figure.6.1}}
\newlabel{fig:dagdemochigh}{{6.1}{110}{Simple democracy, inequality model}{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces  A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type.}}{111}{figure.6.2}}
\newlabel{fig:lowdem}{{6.2}{111}{A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type}{figure.6.2}{}}
\newlabel{fig:unnamed-chunk-27}{{6.2}{111}{A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type}{figure.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forming Priors}{113}{subsection.6.2.1}}
\newlabel{forming-priors}{{6.2.1}{113}{Forming Priors}{subsection.6.2.1}{}}
\newlabel{tab:apptypes}{{6.1}{114}{Forming Priors}{table.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{ Possible (families of) types defining the possible effects of inequality (\(I\)), mobilization, (\(M\)), and international pressure (\(P\)) on democratization. We exclude families based on a set of monotonicity assumptions, which reduces the allowable types from 256 to 20.}}{114}{table.6.1}}
\gdef \LT@v {\LT@entry 
    {1}{118.74377pt}\LT@entry 
    {1}{54.27623pt}\LT@entry 
    {1}{105.94957pt}\LT@entry 
    {1}{96.55246pt}\LT@entry 
    {1}{76.46037pt}}
\newlabel{tab:apptypes2}{{6.2}{115}{Forming Priors}{table.6.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{ Table of allowable types. The table characterizes the nature of the interaction, using the notation explained in the main text. We impose flat priors across all non-excluded types.}}{115}{table.6.2}}
\gdef \LT@vi {\LT@entry 
    {1}{118.74377pt}\LT@entry 
    {1}{54.27623pt}\LT@entry 
    {1}{105.94957pt}\LT@entry 
    {1}{96.55246pt}\LT@entry 
    {1}{76.46037pt}}
\citation{haggard2012inequality}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{117}{section.6.3}}
\newlabel{results}{{6.3}{117}{Results}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Cases with incomplete data}{117}{subsection.6.3.1}}
\newlabel{cases-with-incomplete-data}{{6.3.1}{117}{Cases with incomplete data}{subsection.6.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1.1}\(I=0, D=0\): Non democracy with low inequality}{117}{subsubsection.6.3.1.1}}
\newlabel{i0-d0-non-democracy-with-low-inequality}{{6.3.1.1}{117}{\texorpdfstring {\(I=0, D=0\): Non democracy with low inequality}{I=0, D=0: Non democracy with low inequality}}{subsubsection.6.3.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Mapping from admissible types to responses to ancestor variables. Cell entry shows value for D, given row, for each combination of $P$,$M$,$I$, as indicated in column header.}}{118}{table.6.3}}
\newlabel{tab:typemat}{{6.3}{118}{Mapping from admissible types to responses to ancestor variables. Cell entry shows value for D, given row, for each combination of $P$,$M$,$I$, as indicated in column header}{table.6.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.}}{119}{table.6.4}}
\newlabel{tab:Tapp1}{{6.4}{119}{No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects}{table.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.}}{119}{table.6.5}}
\newlabel{tab:Tapp2}{{6.5}{119}{Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects}{table.6.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1.2}\(I=1, D=0\): Non democracy with high inequality}{119}{subsubsection.6.3.1.2}}
\newlabel{i1-d0-non-democracy-with-high-inequality}{{6.3.1.2}{119}{\texorpdfstring {\(I=1, D=0\): Non democracy with high inequality}{I=1, D=0: Non democracy with high inequality}}{subsubsection.6.3.1.2}{}}
\citation{haggard2012inequality}
\citation{cheibub2010democracy}
\citation{haggard2012distributive}
\citation{haggard2012inequality}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Inferences for cases with observed democratization}{120}{subsection.6.3.2}}
\newlabel{inferences-for-cases-with-observed-democratization}{{6.3.2}{120}{Inferences for cases with observed democratization}{subsection.6.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Four cases with low inequality and democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.}}{121}{table.6.6}}
\newlabel{tab:HK8cases1}{{6.6}{121}{Four cases with low inequality and democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects}{table.6.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2.1}\(I=0, D=1\): Low inequality democracies}{121}{subsubsection.6.3.2.1}}
\newlabel{i0-d1-low-inequality-democracies}{{6.3.2.1}{121}{\texorpdfstring {\(I=0, D=1\): Low inequality democracies}{I=0, D=1: Low inequality democracies}}{subsubsection.6.3.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Four cases with high inequality and democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.}}{122}{table.6.7}}
\newlabel{tab:HK8cases2}{{6.7}{122}{Four cases with high inequality and democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects}{table.6.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2.2}\(I=1, D=1\): High inequality democracies}{122}{subsubsection.6.3.2.2}}
\newlabel{i1-d1-high-inequality-democracies}{{6.3.2.2}{122}{\texorpdfstring {\(I=1, D=1\): High inequality democracies}{I=1, D=1: High inequality democracies}}{subsubsection.6.3.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Model definition and inference in code}{123}{section.6.4}}
\newlabel{model-definition-and-inference-in-code}{{6.4}{123}{Model definition and inference in code}{section.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Concluding thoughts}{123}{section.6.5}}
\newlabel{concluding-thoughts}{{6.5}{123}{Concluding thoughts}{section.6.5}{}}
\citation{humphreys2015mixing}
\citation{king1994designing}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Integrated inferences}{125}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{integrated-inferences}{{7}{125}{Integrated inferences}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}There's only ever one case}{125}{section.7.1}}
\newlabel{theres-only-ever-one-case}{{7.1}{125}{There's only ever one case}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}General procedure}{126}{section.7.2}}
\newlabel{general-procedure}{{7.2}{126}{General procedure}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}The parameter matrix}{127}{subsection.7.2.1}}
\newlabel{the-parameter-matrix}{{7.2.1}{127}{The parameter matrix}{subsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}The ambiguity matrix}{127}{subsection.7.2.2}}
\newlabel{the-ambiguity-matrix}{{7.2.2}{127}{The ambiguity matrix}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Likelihood}{127}{subsection.7.2.3}}
\newlabel{likelihood}{{7.2.3}{127}{Likelihood}{subsection.7.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Estimation}{127}{subsection.7.2.4}}
\newlabel{estimation}{{7.2.4}{127}{Estimation}{subsection.7.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.5}Mixed data}{127}{subsection.7.2.5}}
\newlabel{mixed-data}{{7.2.5}{127}{Mixed data}{subsection.7.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Illustration}{127}{section.7.3}}
\newlabel{illustration}{{7.3}{127}{Illustration}{section.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces  Cell entries denote transformed type. Row labels indicate $Y$ type, $t^Y$ (which arises with prior probability $\lambda ^Y$) for each combination of $X$ and $K$; columns show $K$ type, $t^K$, (arising with prior probability $\lambda ^K$) defined as outcomes for each value of $X$.}}{128}{table.7.1}}
\newlabel{tab:types}{{7.1}{128}{Cell entries denote transformed type. Row labels indicate $Y$ type, $t^Y$ (which arises with prior probability $\lambda ^Y$) for each combination of $X$ and $K$; columns show $K$ type, $t^K$, (arising with prior probability $\lambda ^K$) defined as outcomes for each value of $X$}{table.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}The identification problem}{129}{section.7.4}}
\newlabel{the-identification-problem}{{7.4}{129}{The identification problem}{section.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Extensions}{129}{section.7.5}}
\newlabel{extensions}{{7.5}{129}{Extensions}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Continuous data}{129}{subsection.7.5.1}}
\newlabel{continuous-data}{{7.5.1}{129}{Continuous data}{subsection.7.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Measurement error}{129}{subsection.7.5.2}}
\newlabel{measurement-error}{{7.5.2}{129}{Measurement error}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Spillovers}{130}{subsection.7.5.3}}
\newlabel{spillovers}{{7.5.3}{130}{Spillovers}{subsection.7.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Parameteric models}{130}{subsection.7.5.4}}
\newlabel{parameteric-models}{{7.5.4}{130}{Parameteric models}{subsection.7.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Conclusion}{130}{section.7.6}}
\newlabel{conclusion-1}{{7.6}{130}{Conclusion}{section.7.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Mixed-Method Application: Inequality and Democracy Revisited}{131}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{mixed-method-application-inequality-and-democracy-revisited}{{8}{131}{Mixed-Method Application: Inequality and Democracy Revisited}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}A trained model}{131}{section.8.1}}
\newlabel{a-trained-model}{{8.1}{131}{A trained model}{section.8.1}{}}
\citation{haggard2012inequality}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Data}{132}{section.8.2}}
\newlabel{data}{{8.2}{132}{Data}{section.8.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Inference}{133}{section.8.3}}
\newlabel{inference}{{8.3}{133}{Inference}{section.8.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Prior}}{134}{table.8.1}}
\newlabel{tab:unnamed-chunk-48}{{8.1}{134}{Prior}{table.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Did inequality \emph  {cause} democracy?}{134}{subsection.8.3.1}}
\newlabel{did-inequality-cause-democracy}{{8.3.1}{134}{\texorpdfstring {Did inequality \emph {cause} democracy?}{Did inequality cause democracy?}}{subsection.8.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Did inequality \emph  {prevent} democracy?}{134}{subsection.8.3.2}}
\newlabel{did-inequality-prevent-democracy}{{8.3.2}{134}{\texorpdfstring {Did inequality \emph {prevent} democracy?}{Did inequality prevent democracy?}}{subsection.8.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Prior / posterior comparison for multiple estimands}{134}{section.8.4}}
\newlabel{prior-posterior-comparison-for-multiple-estimands}{{8.4}{134}{Prior / posterior comparison for multiple estimands}{section.8.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Discussion}{134}{section.8.5}}
\newlabel{discussion}{{8.5}{134}{Discussion}{section.8.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces Posterior}}{135}{table.8.2}}
\newlabel{tab:unnamed-chunk-48}{{8.2}{135}{Posterior}{table.8.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.3}{\ignorespaces Prior}}{135}{table.8.3}}
\newlabel{tab:unnamed-chunk-49}{{8.3}{135}{Prior}{table.8.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.4}{\ignorespaces Posterior}}{135}{table.8.4}}
\newlabel{tab:unnamed-chunk-49}{{8.4}{135}{Posterior}{table.8.4}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Design Choices}{137}{part.2}}
\citation{blair2016declaring}
\newlabel{part-design-choices}{{II}{139}{Design Choices}{part.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Elements of Design}{139}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{elements-of-design}{{9}{139}{Elements of Design}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Declaring a process tracing design}{140}{section.9.1}}
\newlabel{declaring-a-process-tracing-design}{{9.1}{140}{Declaring a process tracing design}{section.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Steps}{140}{subsection.9.1.1}}
\newlabel{steps}{{9.1.1}{140}{Steps}{subsection.9.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Illustration in code}{141}{subsection.9.1.2}}
\newlabel{illustration-in-code}{{9.1.2}{141}{Illustration in code}{subsection.9.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Diagnosands: Evaluating a model}{141}{subsection.9.1.3}}
\newlabel{diagnosands-evaluating-a-model}{{9.1.3}{141}{Diagnosands: Evaluating a model}{subsection.9.1.3}{}}
\citation{scharf1991statistical}
\citation{geweke2014analysis}
\citation{gelman2006bayesian}
\citation{heckerman1991toward}
\citation{heckerman1991toward}
\citation{zhang2003properties}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces  The solid line shows gains in precision (reduced posterior variance) for different values of $\phi _b$ given $\phi _d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\phi _b - \phi _d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$.}}{144}{figure.9.1}}
\newlabel{fig:probative_value}{{9.1}{144}{The solid line shows gains in precision (reduced posterior variance) for different values of $\phi _b$ given $\phi _d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\phi _b - \phi _d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$}{figure.9.1}{}}
\newlabel{fig:unnamed-chunk-57}{{9.1}{144}{The solid line shows gains in precision (reduced posterior variance) for different values of $\phi _b$ given $\phi _d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\phi _b - \phi _d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$}{figure.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Other measures of a gain from a theory}{144}{subsection.9.1.4}}
\newlabel{other-measures-of-a-gain-from-a-theory}{{9.1.4}{144}{Other measures of a gain from a theory}{subsection.9.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Declaring a mixed methods design}{145}{section.9.2}}
\newlabel{declaring-a-mixed-methods-design}{{9.2}{145}{Declaring a mixed methods design}{section.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Model}{145}{subsection.9.2.1}}
\newlabel{model}{{9.2.1}{145}{Model}{subsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Data strategies}{145}{subsection.9.2.2}}
\newlabel{data-strategies}{{9.2.2}{145}{Data strategies}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Estimands}{145}{subsection.9.2.3}}
\newlabel{estimands}{{9.2.3}{145}{Estimands}{subsection.9.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Answer Strategies}{145}{subsection.9.2.4}}
\newlabel{answer-strategies}{{9.2.4}{145}{Answer Strategies}{subsection.9.2.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Clue Selection as a Decision Problem}{147}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{clue-selection-as-a-decision-problem}{{10}{147}{Clue Selection as a Decision Problem}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}A strategic approach}{149}{section.10.1}}
\newlabel{a-strategic-approach}{{10.1}{149}{A strategic approach}{section.10.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Clue selection for the running example}{149}{section.10.2}}
\newlabel{clue-selection-for-the-running-example}{{10.2}{149}{Clue selection for the running example}{section.10.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces A fragment of the table of expected posterior variance for all two clue strategies given observations on two nodes. Firs column gives prior variance, second gives variance condition on data pattern $W$, as indicated by row labels; subsequent columns give expected variance when different clues are sought. }}{150}{table.10.1}}
\newlabel{tab:showstrats5xx}{{10.1}{150}{A fragment of the table of expected posterior variance for all two clue strategies given observations on two nodes. Firs column gives prior variance, second gives variance condition on data pattern $W$, as indicated by row labels; subsequent columns give expected variance when different clues are sought}{table.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Dynamic Strategies}{151}{subsection.10.2.1}}
\newlabel{dynamic-strategies}{{10.2.1}{151}{Dynamic Strategies}{subsection.10.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.2}{\ignorespaces  Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization.}}{152}{table.10.2}}
\newlabel{tab:unnamed-chunk-66}{{10.2}{152}{Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization}{table.10.2}{}}
\newlabel{possible_outcomes}{{10.2}{152}{Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization}{table.10.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Clue selection for the Democracy model}{152}{section.10.3}}
\newlabel{clue-selection-for-the-democracy-model}{{10.3}{152}{Clue selection for the Democracy model}{section.10.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.3}{\ignorespaces Variances and expected variances given different clue seeking stratgies for cases in which we have observed inequality and democratization.}}{153}{table.10.3}}
\newlabel{tab:unnamed-chunk-67}{{10.3}{153}{Variances and expected variances given different clue seeking stratgies for cases in which we have observed inequality and democratization}{table.10.3}{}}
\newlabel{CaseLearn}{{10.3}{153}{Variances and expected variances given different clue seeking stratgies for cases in which we have observed inequality and democratization}{table.10.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.4}{\ignorespaces Variances and expected variances given different clue seeking stratgies for cases in which we have observed low inequality and democratization.}}{153}{table.10.4}}
\newlabel{tab:unnamed-chunk-68}{{10.4}{153}{Variances and expected variances given different clue seeking stratgies for cases in which we have observed low inequality and democratization}{table.10.4}{}}
\newlabel{CaseLearn}{{10.4}{153}{Variances and expected variances given different clue seeking stratgies for cases in which we have observed low inequality and democratization}{table.10.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Conclusion}{153}{section.10.4}}
\newlabel{conclusion-2}{{10.4}{153}{Conclusion}{section.10.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Going wide and going deep}{155}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{wide}{{11}{155}{Going wide and going deep}{chapter.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Intuitions: Does a sufficiently large \(N\) always trump \(K\)?}{155}{section.11.1}}
\newlabel{intuitions-does-a-sufficiently-large-n-always-trump-k}{{11.1}{155}{\texorpdfstring {Intuitions: Does a sufficiently large \(N\) always trump \(K\)?}{Intuitions: Does a sufficiently large N always trump K?}}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces {Figure shows posterior estimates of the average treatment effect and posterior variances given a situation with 5, 10, 50, or 5000 observations in each cell of an $X,Y$ table and given collection of data on $0 - 5$ cases in the $X=Y=1$ cell, all of which provide evidence from a doubly decisive clue in favor of causal effects.}}}{157}{figure.11.1}}
\newlabel{morn}{{11.1}{157}{Figure shows posterior estimates of the average treatment effect and posterior variances given a situation with 5, 10, 50, or 5000 observations in each cell of an $X,Y$ table and given collection of data on $0 - 5$ cases in the $X=Y=1$ cell, all of which provide evidence from a doubly decisive clue in favor of causal effects}{figure.11.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Evaluating strategies}{157}{section.11.2}}
\newlabel{evaluating-strategies}{{11.2}{157}{Evaluating strategies}{section.11.2}{}}
\newlabel{Loss}{{11.1}{157}{Evaluating strategies}{equation.11.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Varieties of mixing}{158}{section.11.3}}
\newlabel{varieties}{{11.3}{158}{Varieties of mixing}{section.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces {Expected errors in the estimation of average treatment effects for designs in which $X, Y$, data is sought in $n$ studies (horizontal axis) and clue data is sought within $m$ of these. The shading of dots indicates the proportion of cases for which within-case data is sought (white = none; black = all). For small sample sizes ($n \in \{1,2,3,4\}$) we show results for all designs ($m \in \{1,2,\dots  , n\})$. For larger sample sizes, we show only designs with clues sought in 0, half, and all cases.}}}{159}{figure.11.2}}
\newlabel{morn}{{11.2}{159}{Expected errors in the estimation of average treatment effects for designs in which $X, Y$, data is sought in $n$ studies (horizontal axis) and clue data is sought within $m$ of these. The shading of dots indicates the proportion of cases for which within-case data is sought (white = none; black = all). For small sample sizes ($n \in \{1,2,3,4\}$) we show results for all designs ($m \in \{1,2,\dots , n\})$. For larger sample sizes, we show only designs with clues sought in 0, half, and all cases}{figure.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}Designs in Context}{159}{subsection.11.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1.1}Probative value of clues}{159}{subsubsection.11.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Figure shows the expected error in mean posterior estimates of average treatment effects for different designs. In these graphs the horizontal axis denotes some feature of the research setting (captured in priors); the white and black circles represent errors from designs in which within case information is sought for no and all cases, respectively; the numbers marked in the circles indicate the number of data points in the study design.}}{160}{figure.11.3}}
\newlabel{experiments}{{11.3}{160}{Figure shows the expected error in mean posterior estimates of average treatment effects for different designs. In these graphs the horizontal axis denotes some feature of the research setting (captured in priors); the white and black circles represent errors from designs in which within case information is sought for no and all cases, respectively; the numbers marked in the circles indicate the number of data points in the study design}{figure.11.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1.2}Effect Heterogeneity}{160}{subsubsection.11.3.1.2}}
\citation{GerGreKap04}
\citation{Lieberman2005nested}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1.3}Uncertainty Regarding Assignment Processes}{161}{subsubsection.11.3.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1.4}Uncertainty regarding the probative value of clues}{161}{subsubsection.11.3.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Notes on Simulations}{162}{section.11.4}}
\newlabel{AppSimNotes}{{11.4}{162}{Notes on Simulations}{section.11.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Probative values}{162}{subsection.11.4.1}}
\newlabel{AppE1}{{11.4.1}{162}{Probative values}{subsection.11.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}Effect heterogeneity}{162}{subsection.11.4.2}}
\newlabel{AppE2}{{11.4.2}{162}{Effect heterogeneity}{subsection.11.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.3}Uncertainty about assignment processes}{163}{subsection.11.4.3}}
\newlabel{AppE3}{{11.4.3}{163}{Uncertainty about assignment processes}{subsection.11.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.4}Uncertainty regarding the probative value of clues}{163}{subsection.11.4.4}}
\newlabel{AppE4}{{11.4.4}{163}{Uncertainty regarding the probative value of clues}{subsection.11.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.5}Details on simulation experiments}{163}{subsection.11.4.5}}
\newlabel{details-on-simulation-experiments}{{11.4.5}{163}{Details on simulation experiments}{subsection.11.4.5}{}}
\gdef \LT@vii {\LT@entry 
    {1}{20.09207pt}\LT@entry 
    {1}{26.09207pt}\LT@entry 
    {1}{26.09207pt}\LT@entry 
    {1}{26.09209pt}\LT@entry 
    {1}{35.48918pt}\LT@entry 
    {1}{26.09207pt}\LT@entry 
    {1}{26.09207pt}\LT@entry 
    {1}{40.18414pt}\LT@entry 
    {1}{26.09209pt}\LT@entry 
    {1}{26.09207pt}\LT@entry 
    {1}{44.88629pt}\LT@entry 
    {1}{26.09209pt}\LT@entry 
    {1}{30.78703pt}\LT@entry 
    {1}{35.48918pt}\LT@entry 
    {1}{26.09209pt}\LT@entry 
    {1}{20.09207pt}}
\newlabel{tab:sims}{{11.1}{164}{Details on simulation experiments}{table.11.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{Simulation parameters. Each column details parameters used to generate prior distributions for one of the simulations below. The prior distribution for the full parameter vector is formed from independent draws from Beta distributions for all probabilities and the Dirichlet distribution for shares. Note that the mean and standard deviation parameterization we provide for Beta distributions can be mapped directly to the more standard \(\alpha , \beta \) parameterization}}{164}{table.11.1}}
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip *Note:* Each experiment takes $j$ steps through different values of $\theta $. At each $\theta _j$, the data is simulated $k$ times. For each simulation, a call is made to the Stan model and HMC (Hamiltonian Monte Carlo) sampling is used to approximate the posterior distribution. In each such call to Stan, we run 4 chains with 6000 iterations, and 1000 warmup draws.}}{166}{table.11.2}}
\newlabel{simdetails}{{11.2}{166}{\footnotesize *Note:* Each experiment takes $j$ steps through different values of $\theta $. At each $\theta _j$, the data is simulated $k$ times. For each simulation, a call is made to the Stan model and HMC (Hamiltonian Monte Carlo) sampling is used to approximate the posterior distribution. In each such call to Stan, we run 4 chains with 6000 iterations, and 1000 warmup draws}{table.11.2}{}}
\citation{Lieberman2005nested}
\citation{king1994designing}
\citation{FL2008}
\citation{HerronQuinn}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Case selection as a Decision Problem}{167}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{caseselection}{{12}{167}{Case selection as a Decision Problem}{chapter.12}{}}
\citation{Lieberman2005nested}
\@writefile{lot}{\contentsline {table}{\numberline {12.1}{\ignorespaces The ambiguity about types in each $X, Y$ cell.}}{168}{table.12.1}}
\newlabel{FP}{{12.1}{168}{The ambiguity about types in each $X, Y$ cell}{table.12.1}{}}
\citation{HerronQuinn}
\citation{SeawrightGerring2008}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{HerronQuinn}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Explorations}{169}{section.12.1}}
\newlabel{explorations}{{12.1}{169}{Explorations}{section.12.1}{}}
\citation{HerronQuinn}
\citation{HerronQuinn}
\citation{SeawrightGerring2008}
\citation{HerronQuinn}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Procedure}{170}{subsection.12.1.1}}
\newlabel{procedure}{{12.1.1}{170}{Procedure}{subsection.12.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12.2}{\ignorespaces Inferences from strategy A: one cell from one on-the-regression-line cell; two from the other. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case.}}{172}{table.12.2}}
\newlabel{tab:unnamed-chunk-72}{{12.2}{172}{Inferences from strategy A: one cell from one on-the-regression-line cell; two from the other. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case}{table.12.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12.3}{\ignorespaces Inferences from strategy B: one case from each of the diagonal cells and one from the off-diagonal $X=1, Y=0$ cell. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case.}}{172}{table.12.3}}
\newlabel{tab:unnamed-chunk-72}{{12.3}{172}{Inferences from strategy B: one case from each of the diagonal cells and one from the off-diagonal $X=1, Y=0$ cell. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case}{table.12.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Experiments}{172}{section.12.2}}
\newlabel{experiments}{{12.2}{172}{Experiments}{section.12.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Gains from different strategies involving process tracing in 1 case. We treat process tracing in a case as the search for one clue in that case, though this ``clue'' could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 ``quantitative'' cases. Moving down the rows of graphs, we vary the distribution of these cases over an \(XY\) table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing.}}{174}{figure.12.1}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Chapter Appendix: Accounting for case selection}{175}{section.12.3}}
\newlabel{chapter-appendix-accounting-for-case-selection}{{12.3}{175}{Chapter Appendix: Accounting for case selection}{section.12.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Independent case selection strategy}{175}{subsection.12.3.1}}
\newlabel{independent-case-selection-strategy}{{12.3.1}{175}{Independent case selection strategy}{subsection.12.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces Gains from different strategies involving process tracing in 2 cases. We treat process tracing in a case as the search for one clue in that case, though this ``clue'' could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 ``quantitative'' cases. Moving down the rows of graphs, we vary the distribution of these cases over an \(XY\) table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. Families of strategies are grouped and color-coded as follows: red=maximally dispersing across cells; yellow=}}{176}{figure.12.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces Gains from different strategies involving process tracing in 3 cases. We treat process tracing in a case as the search for one clue in that case, though this ``clue'' could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 ``quantitative'' cases. Moving down the rows of graphs, we vary the distribution of these cases over an \(XY\) table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing.}}{177}{figure.12.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces Gains from different strategies involving process tracing in 4 cases. We treat process tracing in a case as the search for one clue in that case, though this ``clue'' could be conceived of as a collection of clues that jointly have the probative value indicated. Simulations invole 16 units distributed over an \(XY\) table in three patterns (rows) and variation over the probative value of different clues (columns).}}{178}{figure.12.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.1.1}Non-random $XY$ Sample Selection}{179}{subsubsection.12.3.1.1}}
\newlabel{nonrandomcase}{{12.3.1.1}{179}{Non-random $XY$ Sample Selection}{subsubsection.12.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.2}Conditional random case selection}{180}{subsection.12.3.2}}
\newlabel{conditional-random-case-selection}{{12.3.2}{180}{Conditional random case selection}{subsection.12.3.2}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Models in Question}{181}{part.3}}
\newlabel{part-models-in-question}{{III}{183}{Models in Question}{part.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Where does probative value come from?}{183}{chapter.13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{where-does-probative-value-come-from}{{13}{183}{Where does probative value come from?}{chapter.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Causal discovery}{184}{section.13.1}}
\newlabel{causal-discovery}{{13.1}{184}{Causal discovery}{section.13.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}The possibility of identification of probative value from experimental data}{184}{section.13.2}}
\newlabel{the-possibility-of-identification-of-probative-value-from-experimental-data}{{13.2}{184}{The possibility of identification of probative value from experimental data}{section.13.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Moderator}{185}{subsection.13.2.1}}
\newlabel{moderator}{{13.2.1}{185}{Moderator}{subsection.13.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Mediator}{185}{subsection.13.2.2}}
\newlabel{mediator}{{13.2.2}{185}{Mediator}{subsection.13.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Generally not so easy}{186}{subsection.13.2.3}}
\newlabel{generally-not-so-easy}{{13.2.3}{186}{Generally not so easy}{subsection.13.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Bounds on causes of effects}{186}{section.13.3}}
\newlabel{bounds-on-causes-of-effects}{{13.3}{186}{Bounds on causes of effects}{section.13.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Qualitative beliefs and Sensitivity Analyses}{187}{section.13.4}}
\newlabel{qualitative-beliefs-and-sensitivity-analyses}{{13.4}{187}{Qualitative beliefs and Sensitivity Analyses}{section.13.4}{}}
\citation{parsons2001qualitative}
\citation{clarke2012model}
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Conditional claims}{188}{section.13.5}}
\newlabel{conditional-claims}{{13.5}{188}{Conditional claims}{section.13.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.6}Learning about parameters within a model}{188}{section.13.6}}
\newlabel{learning-about-parameters-within-a-model}{{13.6}{188}{Learning about parameters within a model}{section.13.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.7}Learning from observational and experimental mixtures}{189}{section.13.7}}
\newlabel{learning-from-observational-and-experimental-mixtures}{{13.7}{189}{Learning from observational and experimental mixtures}{section.13.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.8}Learning across populations}{189}{section.13.8}}
\newlabel{learning-across-populations}{{13.8}{189}{Learning across populations}{section.13.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13.1}{\ignorespaces Population 1}}{190}{table.13.1}}
\newlabel{S1}{{13.1}{190}{Population 1}{table.13.1}{}}
\newlabel{C1}{{13.1}{190}{Learning across populations}{equation.13.8.1}{}}
\newlabel{CC1}{{13.8}{190}{Learning across populations}{equation.13.8.1}{}}
\newlabel{CC2}{{13.8}{190}{Learning across populations}{equation.13.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces  Combinations of $\phi _{b1}$, $\phi _{d1}$ and $b$ values consistent with data from three populations. Populations are assumed to differ in the sizes of groups $A,B,C,D$ but not in the $\phi $ values. Furthermore it is assumed in this illustration that observed data is identical across populations with respect to $X$ and $Y$ but differs with respect to $K$.}}{191}{figure.13.1}}
\newlabel{fig:somethingfig}{{13.1}{191}{Combinations of $\phi _{b1}$, $\phi _{d1}$ and $b$ values consistent with data from three populations. Populations are assumed to differ in the sizes of groups $A,B,C,D$ but not in the $\phi $ values. Furthermore it is assumed in this illustration that observed data is identical across populations with respect to $X$ and $Y$ but differs with respect to $K$}{figure.13.1}{}}
\newlabel{fig:unnamed-chunk-77}{{13.1}{191}{Combinations of $\phi _{b1}$, $\phi _{d1}$ and $b$ values consistent with data from three populations. Populations are assumed to differ in the sizes of groups $A,B,C,D$ but not in the $\phi $ values. Furthermore it is assumed in this illustration that observed data is identical across populations with respect to $X$ and $Y$ but differs with respect to $K$}{figure.13.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13.2}{\ignorespaces Population 2}}{191}{table.13.2}}
\newlabel{S2}{{13.2}{191}{Population 2}{table.13.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Post-estimation: Robustness and Model Evaluation}{193}{chapter.14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{evaluation}{{14}{193}{Post-estimation: Robustness and Model Evaluation}{chapter.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Tools for evaluating models}{193}{section.14.1}}
\newlabel{tools-for-evaluating-models}{{14.1}{193}{Tools for evaluating models}{section.14.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Evaluating the Democracy-Inequality model}{194}{section.14.2}}
\newlabel{evaluating-the-democracy-inequality-model}{{14.2}{194}{Evaluating the Democracy-Inequality model}{section.14.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Prior check}{194}{section.14.3}}
\newlabel{prior-check}{{14.3}{194}{Prior check}{section.14.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Monotonic restrictions}{195}{section.14.4}}
\newlabel{monotonic-restrictions}{{14.4}{195}{Monotonic restrictions}{section.14.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Final Words}{197}{chapter.15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{final-words}{{15}{197}{Final Words}{chapter.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Words of warning}{197}{section.15.1}}
\newlabel{words-of-warning}{{15.1}{197}{Words of warning}{section.15.1}{}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Appendices}{199}{part.4}}
\newlabel{part-appendices}{{IV}{201}{Appendices}{part.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Analysis of canonical models with \texttt  {gbiqq}}{201}{chapter.16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{examplesappendix}{{16}{201}{\texorpdfstring {Analysis of canonical models with \texttt {gbiqq}}{Analysis of canonical models with gbiqq}}{chapter.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}\(X\) causes \(Y\), no confounding}{201}{section.16.1}}
\newlabel{x-causes-y-no-confounding}{{16.1}{201}{\texorpdfstring {\(X\) causes \(Y\), no confounding}{X causes Y, no confounding}}{section.16.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {16.1}{\ignorespaces Parameter matrix for X causes Y model without confounding}}{202}{table.16.1}}
\newlabel{tab:unnamed-chunk-83}{{16.1}{202}{Parameter matrix for X causes Y model without confounding}{table.16.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {16.2}{\ignorespaces Parameter matrix for X causes Y model with arbitrary confounding}}{203}{table.16.2}}
\newlabel{tab:unnamed-chunk-92}{{16.2}{203}{Parameter matrix for X causes Y model with arbitrary confounding}{table.16.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}\(X\) causes \(Y\), with confounding}{203}{section.16.2}}
\newlabel{x-causes-y-with-confounding}{{16.2}{203}{\texorpdfstring {\(X\) causes \(Y\), with confounding}{X causes Y, with confounding}}{section.16.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Simple mediation model}{204}{section.16.3}}
\newlabel{simple-mediation-model}{{16.3}{204}{Simple mediation model}{section.16.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Simple moderator model}{205}{section.16.4}}
\newlabel{simple-moderator-model}{{16.4}{205}{Simple moderator model}{section.16.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}An IV model}{206}{section.16.5}}
\newlabel{an-iv-model}{{16.5}{206}{An IV model}{section.16.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.6}A model that allows application of the frontdoor criterion}{206}{section.16.6}}
\newlabel{a-model-that-allows-application-of-the-frontdoor-criterion}{{16.6}{206}{A model that allows application of the frontdoor criterion}{section.16.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.7}A model with a violation of sequential ignorability}{206}{section.16.7}}
\newlabel{a-model-with-a-violation-of-sequential-ignorability}{{16.7}{206}{A model with a violation of sequential ignorability}{section.16.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.8}Learning from a collider}{206}{section.16.8}}
\newlabel{learning-from-a-collider}{{16.8}{206}{Learning from a collider}{section.16.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.9}A model mixing observational and experimental data}{208}{section.16.9}}
\newlabel{a-model-mixing-observational-and-experimental-data}{{16.9}{208}{A model mixing observational and experimental data}{section.16.9}{}}
\bibdata{bib.bib,packages.bib}
\@writefile{toc}{\contentsline {section}{\numberline {16.10}Transportation of findings across contexts}{210}{section.16.10}}
\newlabel{transportation-of-findings-across-contexts}{{16.10}{210}{Transportation of findings across contexts}{section.16.10}{}}
