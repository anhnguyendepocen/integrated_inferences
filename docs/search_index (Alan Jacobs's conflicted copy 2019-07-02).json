[
["clues.html", "Chapter 6 Process Tracing with Causal Models 6.1 Process tracing and causal models 6.2 Four principles 6.3 Running example", " Chapter 6 Process Tracing with Causal Models We connect the literature on causal models to qualitative inference strategies used in process tracing. We provide a procedure for general inference on case level queries from causal models and walk through applications with simple models. In addition we extract a set of general principles for model based process tracing. We show how a key result from the causal models literature provides a condition for when clues may be (or certainly will not be) informative. We have described the general problem of process tracing as using observable data to make inferences about unobserved, or unobservable parts of a causal structure. We describe a procedure for doing this and then turn to ask whether we can say when, in general, there are data supports updating on estimands of interest. 6.1 Process tracing and causal models We focus in this chapter on the particular case of drawing inferences about a single case from a causal model and case level data. The causal model may itself be derived from data from other case (as described in Chapter 8) or it might reflect a theoretical position. 6.1.1 Intuition 6.1.2 General approach The general approach to inference draws on the components we outlined in chapters 2 to 4: graphical causal models (DAGs), types and collections of types, and priors. We now show how these elements interact with data to generate inferences. We continue to focus on a situation with binary variables, though suggest later in the chapter how this can be extended. Though we walk through the procedure for simple models, the approach outlined here can be applies to any causal model with binary variables and to any estimands defined over the model. The process tracing procedure involves several connected “moving parts”: A DAG We begin with a DAG, or graphical causal model. As we know, a DAG identifies a set of variables and describes the parent-child relations between them, indicating for each variable which other variables are its direct (possible) causes. These relationship, in turn, tell us which (non-descendant) variables a given variable is not independent of given the other variables in the model. Nodal types. Once we have specified a DAG, we have defined the full set of possible nodal types: the types defining the value that a variable will take on given the values of its parents, which we have denoted with \\(\\theta\\) values. At each node, the range and number of nodal types is defined by the number of parents that that node has and the number of values the variables can take on. For instance, assuming all variables to be binary, if \\(Y\\) has parents \\(X\\) and \\(W\\) (so \\(k=2\\)), then there are \\(2^{\\left(2^2\\right)}=16\\)) possible causal types for the \\(Y\\) node. There are \\(2^2\\) possible combinations of values that two binary causal variables can take on—-\\((X=0,W=0), (X=0,W=1), (X=1,W=0), (X=1,W=1)\\)—which implies four possible causal conditions over which \\(Y\\)’s possible responses must be defined. For instance, as we have seen, with two causal variables, we can have \\(\\theta^Y_{0000}\\), where \\(Y\\) is always 0; \\(\\theta^Y_{0001}\\), where \\(Y\\) is 0 unless both \\(X\\) and \\(W\\) are 1; and so on. To get the total number of nodal types, we simply raise \\(2\\) (since \\(Y\\) is binary) to the number of causal conditions (4), giving the number of possible patterns of \\(Y\\) values that could be generated across these four conditions (16). (The full set of nodal types for two causal variables in a binary setup is given in ??.)1 All variables in a model have nodal types defining the value they take on given the value of their parents, including those variables without substantive parents. Suppose that \\(X\\) and \\(W\\), in this model, have no substantively defined parents. We nonetheless define a nodel type for each of them, which simply captures their exogenous assignment to some value. With \\(X\\) binary, for instance, there are two nodal types, \\(\\theta^X_{0}\\), where \\(X\\) is set to \\(0\\), and \\(\\theta^X_{1}\\), where \\(X\\) is set to \\(1\\). Causal types. We will want to be able to conceive not just of types for individual nodes but of the full collection of nodal types across all nodes in a model. We refer to a unit’s full set of nodal types as its causal type, which we represent as \\(\\theta\\). A causal type is simply a listing that contains one nodal type for each node in the model. For instance, with a model with variable \\(X\\), \\(W\\), and \\(Y\\), each unit has a causal type composed of its nodal types on each of the three nodes.2 Thus, one causal type in this model could be \\(\\theta = (\\theta^X = \\theta^X_1, \\theta^W = \\theta^W_1, \\theta^Y = \\theta^Y_{1101})\\); another could be \\(\\theta = (\\theta^X = \\theta^X_0, \\theta^W = \\theta^W_1, \\theta^Y = \\theta^Y_{0001})\\); and so on. We can express the relationship between causal types, nodal types, and the joint probability of nodal types in what we call a parameter matrix. We show a parameter matrix for a simple \\(X \\rightarrow Y\\) model with confounding in Table 6.1. Table 6.1: . A parameter matrix for a simple \\(X \\rightarrow Y\\) model with confounding. Causal Types \\(\\rightarrow\\) \\(\\theta^X_0\\).\\(\\theta^Y_{00}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{00}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{10}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{10}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{01}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{01}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{11}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{11}\\) Parameters \\(\\downarrow\\) $^X_0 ^Y= ^Y_{01}$ 0 0 0 0 1 0 0 $theta^X_1, thetaY_{01}$$X_1 ^Y= ^Y_{01}$ 0 0 0 0 0 1 $^X_0 ^Y ^Y_{01}$ 1 0 1 0 0 0 1 $^X_1 ^Y ^Y_{01}$ 0 1 0 1 0 0 0 \\(\\theta^Y_{00}\\) 1 1 0 0 0 0 0 0 \\(\\theta^Y_{10}\\) 0 0 1 1 0 0 0 0 \\(\\theta^Y_{01}\\) 0 0 0 0 1 1 0 0 \\(\\theta^Y_{11}\\) 0 0 0 0 0 0 1 1 \\(\\theta^X_0\\).\\(\\theta^Y_{00}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{00}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{10}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{10}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{01}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{01}\\) \\(\\theta^X_0\\).\\(\\theta^Y_{11}\\) \\(\\theta^X_1\\).\\(\\theta^Y_{11}\\) $^X_0 ^Y= ^Y_{01}$ 0 0 0 0 1 0 0 $^X_1 ^Y= ^Y_{01}$ 0 0 0 0 0 1 0 $^X_0 ^Y ^Y_{01}$ 1 0 1 0 0 0 1 $^X_1 ^Y ^Y_{01}$ 0 1 0 1 0 0 0 \\(\\theta^Y_{00}\\) 1 1 0 0 0 0 0 0 \\(\\theta^Y_{10}\\) 0 0 1 1 0 0 0 0 \\(\\theta^Y_{01}\\) 0 0 0 0 1 1 0 0 \\(\\theta^Y_{11}\\) 0 0 0 0 0 0 1 1 Priors: Our background beliefs about a causal domain usually will consist of more than just beliefs about which variables have causal connections; they will also usually contain beliefs about what kinds of effects operate between variables. That is, they will contain beliefs about which types are more or less likely or common. We express these beliefs over causal effects as probability distributions over the nodal (or causal) types. As these are distributions that we express before we have seen the data, they are prior distributions. We let \\(\\lambda^j\\) denote a probability distribution over the possible nodal types at some node, \\(j\\). A \\(\\lambda^j\\) is simply a vector probabilities, one for each possible nodal type, with the probabilities adding up to \\(1\\). So, for instance, \\(\\lambda^Y\\) for our current example would be a vector with four values, each of which places a probability on one of the four nodal types at \\(Y\\). So we might have \\(\\lambda^Y_{0001}=0.1\\), \\(\\lambda^Y_{0011}=0.05\\), and so on – with the \\(\\lambda^Y\\) values summing to \\(1\\) because these probabilities are defined over the full set of possible nodal types for \\(Y\\). Now, to create a prior over a causal type we need to join together \\(\\lambda\\)’s across the nodes in a model. How we do so depends on whether the nodal types are independent of one another or correlated with one another. We can think of a situation in which all nodal types are indepdendent of one another as one in which there is no unobserved confounding (as a matter of our prior beliefs): there is no variable, missing from the model, that is a common ancestor of multiple nodes in the model. Where there is no unobserved confounding, so the nodal types are all independent, our beliefs over causal types are simply the product of our beliefs over the component nodal types (since the joint probability of independent events is simply the product of their individual probabilities. For instance, a causal type might be “a unit in which \\(X=1\\) and in which \\(Y=1\\) no matter what value \\(X\\) takes.” In this case the probability that a unit has a causal type might be written \\(\\Pr(\\theta^X = \\theta^X_1)\\Pr(\\theta^Y = \\theta^Y_{11}) = \\lambda^X_1\\lambda^Y_{11}\\). In a situation of unobserved confounding, our beliefs over causal types are still well defined, but they are no longer the simple product of beliefs over nodal types. Let us imagine for instance, in our \\(X, W, Y\\) model, that we believe that cases that in which some unobserved factor both makes cases more likely to have \\(X = 1\\) and makes it more likely that \\(X\\) has a positive effect on \\(Y\\) (independent of \\(W\\)). This is the same as saying that the probability that \\(\\theta^X = \\theta^X_1\\) is positively correlated with the probability that \\(\\theta^Y = \\theta^Y_{0101}\\). Now, our probability that both \\(X=1\\) and \\(X\\) has a positive effect on \\(Y\\) are not simply defined by the separate probabilities of these events. Rather, we must now use the joint probability formula, \\(\\Pr(A, B) = \\Pr(A)\\Pr(B|A)\\).3 Thus, \\(\\Pr(\\theta^Y = \\theta^Y_{0101}, \\theta^X = \\theta^X_1) = \\Pr(\\theta^Y = \\theta^Y_{0101})\\Pr(\\theta^X = \\theta^X_1 | \\theta^Y = \\theta^Y_{0101})\\). To form priors over causal types, we thus have to form a belief about the probability that \\(X\\) will be “assigned” to \\(1\\), and about the probability that \\(X\\) has a positive effect on \\(Y\\) if \\(X=1\\).4 Possible data types. A data type is set of node values such that each node in a model takes on a single value. For instance, in our \\(X, W, Y\\) setup, \\(X=1, W=0, Y=0\\) would be one data type. Crucially, each possible causal type maps into a single data type. One intuitive way to think about why this is the case is that a causal type tells us (a) the values to which all exogenous variables in a model are assigned and (b) how all endogenous variables respond to their parents. For example, causal type \\(\\theta = (\\theta^X = \\theta^X_1, \\theta^W = \\theta^W_0, \\theta^Y = \\theta^Y_{0100})\\) imples data \\(X=1, W=0, Y=1\\). There is no other set of data that can be generated by this causal type. The mapping is not however one-to-one. For instance, the causal type \\(\\theta = (\\theta^X = \\theta^X_1, \\theta^W = \\theta^W_0, \\theta^Y = \\theta^Y_{1101})\\) will generate the same data type. The mapping can be summarized by an “ambiguity matrix” that says which causal types (rows) map into which data types (columns). Using the gbiqq package we provide two examples of such ambiguity matrices. For an \\(X \\rightarrow Y\\) model: ## using not provided, priors assumed ## Parameter matrix missing from model. Generated on the fly. Table 6.2: Ambiguity matrix for X -&gt; Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows. X0Y0 X1Y0 X0Y1 X1Y1 prior X0Y00 1 0 0 0 0.1034344 X1Y00 0 1 0 0 0.0034424 X0Y10 0 0 1 0 0.5362522 X1Y10 0 1 0 0 0.0178468 X0Y01 1 0 0 0 0.2342163 X1Y01 0 0 0 1 0.0077949 X0Y11 0 0 1 0 0.0938884 X1Y11 0 0 0 1 0.0031247 Here the column names have the obvious interpretation—\\(X0Y0\\) means a unit with \\(X=0, Y=0\\) observed. The row names are a concatenation of the nodal types that yield a causal type.5 For an \\(X \\rightarrow M \\rightarrow Y\\) model: ## using not provided, priors assumed ## Parameter matrix missing from model. Generated on the fly. Table 6.3: Ambiguity matrix for X -&gt; M -&gt; Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows. X0M0Y0 X1M0Y0 X0M1Y0 X1M1Y0 X0M0Y1 X1M0Y1 X0M1Y1 X1M1Y1 prior X0M00Y00 1 0 0 0 0 0 0 0 0.0286446 X1M00Y00 0 1 0 0 0 0 0 0 0.0210846 X0M10Y00 0 0 1 0 0 0 0 0 0.0049965 X1M10Y00 0 1 0 0 0 0 0 0 0.0036778 X0M01Y00 1 0 0 0 0 0 0 0 0.0547570 X1M01Y00 0 0 0 1 0 0 0 0 0.0403053 X0M11Y00 0 0 1 0 0 0 0 0 0.0141359 X1M11Y00 0 0 0 1 0 0 0 0 0.0104051 X0M00Y10 0 0 0 0 1 0 0 0 0.0628939 X1M00Y10 0 0 0 0 0 1 0 0 0.0462947 X0M10Y10 0 0 1 0 0 0 0 0 0.0109705 X1M10Y10 0 0 0 0 0 1 0 0 0.0080752 X0M01Y10 0 0 0 0 1 0 0 0 0.1202277 X1M01Y10 0 0 0 1 0 0 0 0 0.0884968 X0M11Y10 0 0 1 0 0 0 0 0 0.0310377 X1M11Y10 0 0 0 1 0 0 0 0 0.0228461 X0M00Y01 1 0 0 0 0 0 0 0 0.0111610 X1M00Y01 0 1 0 0 0 0 0 0 0.0082153 X0M10Y01 0 0 0 0 0 0 1 0 0.0019468 X1M10Y01 0 1 0 0 0 0 0 0 0.0014330 X0M01Y01 1 0 0 0 0 0 0 0 0.0213353 X1M01Y01 0 0 0 0 0 0 0 1 0.0157044 X0M11Y01 0 0 0 0 0 0 1 0 0.0055079 X1M11Y01 0 0 0 0 0 0 0 1 0.0040542 X0M00Y11 0 0 0 0 1 0 0 0 0.0582192 X1M00Y11 0 0 0 0 0 1 0 0 0.0428538 X0M10Y11 0 0 0 0 0 0 1 0 0.0101551 X1M10Y11 0 0 0 0 0 1 0 0 0.0074750 X0M01Y11 0 0 0 0 1 0 0 0 0.1112916 X1M01Y11 0 0 0 0 0 0 0 1 0.0819191 X0M11Y11 0 0 0 0 0 0 1 0 0.0287308 X1M11Y11 0 0 0 0 0 0 0 1 0.0211481 Updating on types. Updating on the distribution of causal types given the complete data is then straightforward. Still assuming a single case, when we observe data, we place 0 weight on all causal types that could not have produced this data and then scale up weights on all causal types that could have. In terms of the ambiguity matrices, we start out with a set of probability weights on all rows of the ambiguity matrix. If we observe data corresponding to a given column of the ambiguity matrix, we put 0 weight on all rows that do not contain a 1 in this column and scale up weights on all rows that do contain a 1. For instance in the case with \\(X=Y=1\\) our updated matrix would be: ## using not provided, priors assumed ## Parameter matrix missing from model. Generated on the fly. ## using not provided, priors assumed ## Parameter matrix missing from model. Generated on the fly. X1Y1 prior posterior 1 0.0127291 0.4572984 1 0.0151064 0.5427016 In the case with model \\(X \\rightarrow M \\rightarrow Y\\) and data \\(X=M=Y=1\\) this exercise would yield: ## using not provided, priors assumed ## Parameter matrix missing from model. Generated on the fly. type X1M1Y1 prior posterior X1M01Y01 1 0.00 0.02 X1M11Y01 1 0.02 0.32 X1M01Y11 1 0.00 0.04 X1M11Y11 1 0.04 0.62 The same logic applies if partial data is observed, except now rather than reducing to one column we entertain the possibility of any column consistent with the observed data. For instance if \\(M\\) is not observed we have: ## using not provided, priors assumed ## Parameter matrix missing from model. Generated on the fly. type X1M0Y1 X1M1Y1 prior posterior X1M00Y10 1 0 0.11 0.67 X1M10Y10 1 0 0.01 0.09 X1M01Y01 0 1 0.01 0.06 X1M11Y01 0 1 0.01 0.07 X1M00Y11 1 0 0.01 0.05 X1M10Y11 1 0 0.00 0.01 X1M01Y11 0 1 0.00 0.03 X1M11Y11 0 1 0.00 0.03 Updating on estimands. We now have a posterior over the case level model itself and posteriors over any particular estimand can be expressed as a combination of these. For instance the estimand “\\(X=1\\) caused \\(Y=1\\)” in the \\(X \\rightarrow M \\rightarrow Y\\) model is equivalent to the statement “The type is either”\\(X=1\\) and \\(X=1\\) causes \\(M=1\\) and \\(M=1\\) causes \\(Y=1\\)&quot; or “\\(X=1\\) and \\(X=1\\) causes \\(M=0\\) and \\(M=0\\) causes \\(Y=1\\)”. Relating causal types to estimands can be tricky when the number of types gets large. In practive we use a function to do this for us. For example the estimand “\\(X\\) matters for \\(Y\\)” is a relatively large though still easily calculated collection of types: get_types(XMY, &quot;Y[X=1] != Y[X=0]&quot;) X0.M10.Y10, X1.M10.Y10, X0.M01.Y10, X1.M01.Y10, X0.M10.Y01, X1.M10.Y01, X0.M01.Y01, X1.M01.Y01 The posterior on the estimand is then just given by summing up the posterior probability on each of the causal types that correspond to the estimand. That completes the abstract representation of the procedure. We now build up the intuition by walking through the procedure for simple mediation and moderation models. 6.1.3 Walk through using a simple mediation model Suppose that, in a binary setting, we have observed \\(X=1\\) and \\(Y=1\\) in a case, but we do not know and want to learn if \\(X\\) caused \\(Y\\) here. So, for instance, we see that a country has a free press, and that its government was removed; but we want to find out whether the free press caused government removal. In other words, we want to know the value of \\(\\theta_Y^{higher}\\). Suppose, further, that we have a theory—a lower-level model—of the effect of \\(X\\) on \\(Y\\) represented by the graph in Figure . Say, a free press causes government removal through media reports of corruption. What can be learned, then, from observing the moderator, \\(K\\), given this lower-level model? Figure 6.1: A model with one explanatory variable and one mediator. Observing \\(X=1, Y=1\\) already tells us that \\(\\theta_Y^{higher}\\), the higher-level type variable for \\(X\\)’s effect on \\(Y\\), takes on either \\(\\theta_{01}\\) or \\(\\theta_{11}\\): either \\(X\\) has had a positive effect on \\(Y\\) or it has no effect (\\(Y\\) would be \\(1\\) regardless of \\(X\\)’s value). We now want the mediator clue, \\(K\\), to help us distinguish between these possibilities. What could we learn if we went looking for information on \\(K\\) and observed \\(K=1\\)? When we combine this observation for \\(K\\) with the observations we already have for \\(X\\) and for \\(Y\\), we can begin to eliminate values for the type nodes in the lower-level model. First, seeing \\(K=1\\) with \\(X=1\\) eliminates two possible values of \\(K\\)’s response type, \\(\\theta_K\\): \\(\\theta_{00}^{K}\\) and \\(\\theta_{10}^{K}\\). That is, by observing \\(X=1\\) and \\(K=1\\), we know that \\(K\\) is not fixed at \\(0\\), and we know that \\(X\\) does not have a negative effect on \\(K\\). So, for instance, if we observe a high number of media reports in corruption (along with a free press and government removal), we know (1) that reports of corruption are not fixed at a low level in this case and (2) that a free press does not cause a low level of media reports in this case. Second, observing \\(K=1\\) with \\(Y=1\\) allows us to rule out the values \\(\\theta_{00}^{Y}\\) and \\(\\theta_{10}^{Y}\\) for \\(Y\\)’s response type node, \\(\\theta_Y^{lower}\\): we have learned that \\(Y\\) is not “stuck” at \\(0\\), regardless of \\(K\\)’s value; and we have learned that \\(K\\) does not exert a negative effect on \\(Y\\). Observing together many corruption reports and government removal tells us (1) that government removal can occur in this case and (2) that media reports of corruption do not prevent government removal. So what have we learned? By eliminating these two pairs of lower-level types we have excluded three possible causal sequences that could be consistent with our initial observation of \\(X=Y=1\\)—that is, with the two higher-level types that are in contention (\\(\\theta_{01}\\), or \\(b\\)-type, and \\(\\theta_{11}\\)), or \\(d\\)-type). The first excluded sequence—which would be theoretically consistent with a \\(d\\) response-type at the higher level—is that \\(K\\) is fixed at \\(0\\), followed by \\(Y\\) being \\(1\\) for some other reason. Second, we have ruled out the possibility that—also consistent with a higher-level \\(d\\) type—that \\(X\\) has a negative effect on \\(K\\), followed by no effect of \\(K\\) on \\(Y\\) because \\(Y\\) is fixed at \\(0\\). Third, we have excluded the possibility—consistent with a positive effect of \\(X\\) on \\(Y\\)—that \\(X\\) has a negative effect on \\(K\\) and that \\(K\\) in turn has a negative effect on \\(Y\\). At this point, however, the learning we have achieved is purely formal, stuck in the realm of lower-level types. While we have eliminated values of lower-level type variables and their associated sequences, doing so does not yet speak to the question of interest: it does not eliminate either of the original two higher-level types, \\(\\theta_{01}\\) and \\(\\theta_{11}\\), that could define \\(Y\\)’s response to \\(X\\). The observed data (\\(X=K=Y=1\\)) are consistent with \\(X\\) having a positive effect on \\(Y\\) via linked positive effects (\\(\\theta_K=\\theta_{01}^K\\) and \\(\\theta_Y^{lower}=\\theta_{01}^Y\\)); and they are consistent with \\(X\\) having no effect on \\(Y\\), whether because \\(K\\) is always \\(1\\) (\\(\\theta_K=\\theta_{11}^K\\)), because \\(Y\\) is always \\(1\\) (\\(\\theta_Y^{lower}=\\theta_{11}^Y\\)), or both. Tracing out a series of events between possible cause and outcome cannot by itself deliver a causal inference. We might observe a free press, corruption reports, and government removal, picking up clues along a possible causal path; but that cannot tell us whether the free press caused government removal. Corruption reports may have happened without a free press, and government removal might have occurred without corruption reports. To break the inferential logjam we need to introduce a “thicker” base of substantive, prior knowledge about the phenomenon under scrutiny. The DAG alone—the map of possible causal links—is not enough to permit causal inference. We need to impose structure on the problem in a way that allows us to weight some causal possibilities consistent with the data more heavily than other causal possibilities consistent with the data. We can do this by saying something about the probability distributions governing our query nodes: that is, by drawing on prior beliefs about the probabilities of the different lower-level types captured by \\(\\theta^K\\) and \\(\\theta^{Y_{lower}}\\). To see how this might work, let us posit some rough probabilities we might have placed, prior to observing \\(K\\), on each of the possible causal effects at the two stages in the causal chain. We can think of these probabilities as the degree of confidence that we might have had in the existence of these effects in this case, given everything we know about the case before we search for the clue. This includes its \\(X\\) and \\(Y\\) values (that it has a free press and its government was removed) as well anything else that might affect the causal process and outcome of interest (say, regime type or economic conditions). We might alternatively think about these probabilities as our beliefs about how common each of these effects are in the population of cases from which this case has been “drawn”—i.e., in cases that are, in relevant ways, like this case (again, using only information about this case that we have prior to observing \\(K\\), including its \\(X=Y=1\\) status). Thinking about things in this second way, we can thus think of the probabilities as shares of the population containing each type. Using the \\(\\lambda\\) notation introduced in Chapter 2 to denote population-level type shares, we can thus write our belief about the probability of a case’s being of type \\(\\theta_{ij}^K\\) as \\(\\lambda_{ij}^K\\), and of type \\(\\theta_{ij}^Y\\) as \\(\\lambda_{ij}^Y\\). Of course, the beliefs that we posit here are intended to be purely illustrative. \\(\\theta^K\\): \\(X\\)’s effect on \\(K\\)6 \\(\\theta_{10}^{K}\\): A free press reduces the number of media reports of corruption in this case: Very unlikely/uncommon (say, \\(\\lambda^K_{10}=0.1\\))7 \\(\\theta_{00}^{K}\\): The number of media reports of corruption will be low in this case regardless of whether the press is free: Somewhat likely/common (\\(\\lambda^K_{00}=0.4\\))8 \\(\\theta_{01}^{K}\\): A free press increases the number of media reports of corruption in this case: Somewhat likely/common (\\(\\lambda^K_{01}=0.4\\))9 \\(\\theta_{11}^{K}\\): The number of media reports of corruption will be high in this case regardless of whether the press is free: Very unlikely/uncommon (\\(\\lambda^K_{11}=0.1\\))10 \\(\\theta^{Y_{lower}}\\): \\(K\\)’s effect on \\(Y\\)11 \\(\\theta_{10}^{Y}\\): A high number of media reports of corruption prevents government removal in this case: Rather unlikely/uncommon (\\(\\lambda^Y_{10}=0.2\\))12 \\(\\theta_{00}^{Y}\\): Government removal will not happen in this case regardless of the number of media reports of corruption. Impossible/excluded from relevant population since we have already observed \\(Y=1\\) before we do the process tracing: (\\(\\lambda^Y_{00}=0\\)). \\(\\theta_{01}^{Y}\\): A high number of media reports of corruption causes government removal in this case: More likely/common than not (\\(\\lambda^Y_{01}=0.6\\))13 \\(\\theta_{11}^{Y}\\): Government removal will happen in this case regardless of the number of media reports of corruption: Rather unlikely/uncommon (\\(\\lambda^Y_{11}=0.2\\))14 These prior beliefs about lower-level causal effects imply a prior belief about the higher-level effect—that is, about whether \\(X\\) has a positive effect on \\(Y\\) in this case. We know that we can only get a positive causal effect at the higher level if we have either linked positive effects (\\(\\theta^K=\\theta_{01}^{K}\\) and \\(\\theta^{Y^{lower}}=\\theta_{01}^{Y}\\)) or linked negative effects (\\(\\theta^K=\\theta_{10}^{K}\\) and \\(\\theta^{Y^{lower}}=\\theta_{10}^{Y}\\)). So, to calculate the prior probability of a positive higher-level effect in our case, we simply need to calculate the share of cases in the population that have either of these two combinations of values as a proportion of the share of cases in the population containing all combinations of types that are consistent with our prior observation of \\(X=1, Y=1\\).15 This calculation yields a prior probability of a positive causal effect of \\(0.433\\). Now, conducting the process tracing and observing \\(K\\) shifts our beliefs. As noted, observing \\(K=1\\) allows us to eliminate from contention—i.e., assign a 0 posterior probability to—the three lower-level types \\(\\theta_{00}^{K}\\), \\(\\theta_{10}^{K}\\), and \\(\\theta_{10}^{Y}\\) inconsistent with this observation (with \\(\\theta_{00}^{Y}\\) eliminated already by seeing \\(Y=1\\)). In calculating our posterior probability of a positive higher-level effect, we again take the share of cases containing combinations of types consistent with a positive higher-level effect as a proportion of all combinations—but this time excluding any combination containing an eliminated lower-level type.16 This calculation gives a posterior probability that \\(X\\) had a positive on \\(Y\\) in this case of \\(0.48\\)–a small upward shift in our belief in this effect. We thus still think it is slightly more likely that the government would have been replaced anyway than that its removal was caused by the free press. However, we are now more confident that the free press mattered than we were before we observed the media reports of corruption. If we were to look for the clue and instead find \\(K=0\\)—few media reports of corruption—then our beliefs would shift in the opposite direction. We would now remove from contention all those types in which \\(K\\) is \\(1\\) when \\(X=1\\) or in which \\(Y=0\\) when \\(K=0\\): \\(\\theta_{01}^{K}\\), \\(\\theta_{11}^{K}\\), and \\(\\theta_{11}^{Y}\\) (again, with \\(\\theta_{00}^{Y}\\) previously excluded). The resulting calculation—the share represented by the remaining combinations of types consistent with a positive higher-level effect divided by all remaining combinations of types—now generates a dramaticaly lower posterior probability of \\(0.067\\) that \\(X\\) had a positive effect on \\(Y\\). Having observed few media reports of corruption, we now infer it to be highly unlikely that the free press caused government removal in this case. The reason we are able to learn from the clue in this setup is that we have brought prior knowledge to the problem in a way that allows us to weight some possibilities consistent with the data more heavily than others. Based on what we already knew about the phenomenon of interest, we consider some lower-level response types more likely than others before we search for the clue. Then if we see the clue (\\(K=1\\)), we eliminate both some lower-level causal types that are consistent with a positive \\(X \\rightarrow Y\\) causal effect (\\(\\theta_{10}^{K}\\) and \\(\\theta_{10}^{K}\\)) and a causal type that would yield zero effect (\\(\\theta_{00}^{K}\\)). However, the zero-effect type that we eliminate was one that we had initially (before the process tracing) considered somewhat likely; while the positive-effect types that the clue eliminates were ones that prior knowledge had already told us were quite unlikely. The net result is to leave us with a set of beliefs a little less favorable to a zero-effect conclusion than where we had started. If instead we see \\(K=0\\), in contrast, we must eliminate from contention that causal sequence—linked positive effects of \\(X\\) on \\(K\\) and of \\(K\\) on \\(Y\\)—which we thought the much more likely route to a positive causal effect. The only sequence consistent with the \\(X, Y, K\\) data that could generate a positive effect—the free press reduces corruption reports, which causes government removal—is one that prior knowledge already told us was very unlikely. Meanwhile, the no-effect types that we have eliminated were among those that we thought least likely to begin with. We thus end up with a belief set heavily tilted in favor of no effect. The most general point here is that it is our prior knowledge, embedded in a probabilistic causal model, that tells us the probative value of possible observations. As readers familiar with Van Evera’s test types will have noticed, the clue in the above example, given the prior beliefs we specified, functions as a “hoop test” for the claim that \\(X=1\\) caused \\(Y=1\\) (Van Evera (1997)): finding the clue only slightly strengthens our confidence in the claim while failing to find the clue greatly weakens that confidence. Yet the particular way in which we can learn from a clue—and whether we can learn from it at all—will depend on the prior probabilities that we place on possible values of the query nodes. To take an extreme example, \\(K\\) would be ‘’doubly decisive’’ if our prior beliefs are: \\(\\lambda^K_{01}, \\lambda^Y_{01}&gt;0\\): It is possible that \\(X=1\\) causes \\(K=1\\), which in turn causes \\(Y=1\\), \\(\\lambda^K_{10}=0\\) or \\(\\lambda^Y_{10}=0\\): \\(X=1\\) can only cause \\(Y=1\\) by first causing \\(K=1\\), and so seeing \\(K=0\\) would be sure evidence that \\(X\\) did not cause \\(Y\\); and \\(\\lambda^Y_{11}=0\\) and \\(\\lambda^K_{11}=0\\): It is impossible that \\(K\\) would be 1 no matter what the value of \\(X\\), or that \\(Y\\) would be 1 no matter what the value of \\(K\\) Under these starting beliefs, observing the clue would lead us believe with certainly that \\(X\\) had a positive effect on \\(Y\\); and not finding the clue when we look for it would lead us to believe with certainty that \\(X\\) had no effect. At the other extreme, the clue would be completely uninformaive about \\(X\\)’s causal effect if we began with these beliefs: \\(\\lambda_{10}^{K}\\lambda_{10}^{Y} = \\lambda_{01}^{K}\\lambda_{01}^{Y}\\): a positive \\(X \\rightarrow Y\\) effect is equally likely to operate via linked positive effects as it is via linked negative effects, and \\(\\lambda_{00}^{K}\\lambda_{10}^{Y} = \\lambda_{11}^{K}\\lambda_{01}^{Y}\\): In those situations in which \\(X\\) does not affect \\(K\\), but \\(K\\) produces \\(Y=1\\), \\(K\\) is just as likely to be fixed at 0 as it is to be fixed at 1. Under these starting beliefs, our beliefs about \\(X\\)’s causal effect will not shift, whether we find the clue or not. We are providing here an account of process tracing focused on studying links in the causal chain between cause and effect. It is sometimes argued that studying a causal effect by breaking it down into component steps merely shifts the challenge of causal inference to a lower level of aggregation, and in fact multiplies it, by forcing us to identify causal effects at each link in the posited causal chain. The understanding of mediation-based process tracing that we are developing here makes clear why this view is misguided, at least insofar as we are willing to make our inferences conditional on background knowledge. When we invoke a more detailed lower-level model to investigate a higher-level effect, we are putting our prior beliefs to work to allow for greater learning than might otherwise occur. As we discussed in Chapter X, a lower-level model of mediation can be thought of as partitioning an unobservable quantity—the higher-level response node, \\(\\theta_Y^{higher}\\)—into a potentially observable quantity \\(K\\) (the mediator clue) and an unobservable quantity, \\(\\theta_Y^{lower}\\) (\\(Y\\)’s lower-level response type). Rather than addressing a query about \\(Y\\)’s response to \\(X\\), our task is now to address two queries: one about \\(K\\)’s response to \\(X\\) and another about \\(Y\\)’s response to \\(K\\). Importantly, this move will not always be helpful. But it will be fruitful is when we have stronger prior beliefs about smaller, intermediate links in a causal chain than about the \\(X \\rightarrow Y\\) relationship taken as a whole. We are arguably often in this situation. To return to our running example involving government replacement, we may not know much about the frequency with which a free press makes government replacement more likely. However, we may have some prior knowledge indicating that a free press increases reports of government corruption more often than it has no effect; and that greater reports of corruption are more likely to reduce governments’ survival in office than to leave their survival prospects untouched. It is precisely those differential weights that we are able to put on causal effects at the lower-level—and not for the higher-level claim of interest—that allow the theory of mediation, and the empirical strategy that it suggests, to be informative. More formally, we can use the concept of gains to theory, discussed in Chapter X, to asks: what is the value-added of the lower-level model of mediation, relative to the higher model? How much do we expect to learn from the research design—go and observe (\\(K\\))—that the lower-level model suggests to us? Supposing that we have a theory of mediation with a single mediator, we can calculate the expected error without information on the clue, \\(P(t_{01}^{higher})(1-P(t_{01}^{higher}))\\), as: \\[\\text{Prior variance} = \\frac{\\left(\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}\\right)\\left(\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}\\right)} {\\left(\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y} +\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}\\right)^2}\\] We can then calculate the expected error after with information on \\(K\\), over the possible values of \\(K\\) that we might see, as: \\[\\text{Expected Posterior Var} = \\frac{\\left(\\lambda_{01}^{K}+\\lambda_{11}^{K}\\right) \\lambda_{01}^{K}\\lambda_{01}^{Y}\\left(\\lambda_{11}^{Y}+\\lambda_{11}^{K}\\lambda_{01}^{Y} \\right) } {\\left(\\lambda_{01}^{K}\\lambda_{01}^{Y} +\\lambda_{11}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}\\right)^2}+ \\frac{\\left(1-\\lambda_{01}^{K}-\\lambda_{11}^{K}\\right)\\lambda_{10}^{K}\\lambda_{10}^{Y}\\left(\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y}\\right) } {\\left(\\lambda_{10}^{K}\\lambda_{10}^{Y} +\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y}\\right)^2}\\] To assess the gains provided by the lower-level theory, we can simply compare the first of these to the second: the expected error assuming no search for \\(K\\) with the expected error assuming a search for \\(K\\). The greater the reduction in expected error from searching for \\(K\\), the more useful the lower-level theory has been. As can be seen from the above expressions, how much our expected error is reduced by \\(K\\)—that is, how much we expect to learn from this empirical strategy—will depend upon the \\(\\lambda\\) values: that is, on the prior probabilities that we place on the lower-level response nodes, (\\(\\theta_K\\) and \\(\\theta_Y^{lower}\\). 6.1.3.1 Connection to qualitative tests The approach we have described here updates on the model given data on all variables, and from the model makes inferences to estimands. This procedure appears different to the approach described for example in Collier, Brady, and Seawright (2004) and outlines in Chapter 5 in which one seeks specific evidence that is directly informative about causal propositions: “clues” that are arise with different probabilities if one proposition or another is true. In fact however the approaches are deeeply connected. This “probative value of clues” approach can indeed be justified by reference to more fully elaborated models of the world. To see this we can write down the probability of observing \\(K=1\\) conditional on causal type and \\(X\\), using the \\(\\phi\\) notation from Humphreys and Jacobs (2015) and introduced in Chapter 5. Here \\(\\phi_{jx}\\) refers to the probability of observing a clue in a case of type \\(j\\) when \\(X=x\\). Starting with our prior distribution over the lower-level causal types (the \\(\\lambda\\)’s), we can derive, for an \\(X=1\\) case, the probability of seeing the clue if the case is of type \\(b\\) (positive effect) or of type \\(d\\) (no effect, \\(Y\\) always \\(1\\)): \\[\\begin{equation} \\begin{split} \\phi_{b1} &amp; = \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y}}{\\lambda_{01}^{K}\\lambda_{01}^{Y}+\\lambda_{10}^{K}\\lambda_{10}^{Y}}\\\\ \\phi_{d1} &amp; = \\frac{\\lambda_{11}^{Y}(\\lambda_{01}^{K}+\\lambda_{11}^{K})+\\lambda_{11}^{K}\\lambda_{01}^{Y}}{\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}} \\end{split} \\tag{6.1} \\end{equation}\\] These quantities allow for easy mapping between our prior beliefs about our causal query—as expressed in the lower level model—and the classic process-tracing tests in Van Evera (1997). Figure 6.2 illustrates. In each panel, we manipulate a prior for one or more of the lower-level causal effects, keeping all other priors flat, and we see how probative value changes. As the curves for \\(\\phi_b\\) and \\(\\phi_d\\) diverge, probative value is increasing since there is an increasing difference between the probability of seeing the clue if \\(X\\) has a positive effect on \\(Y\\) and the probability of seeing the lcue if \\(X\\) has no effect. In the left panel, we see that as we place a lower prior probability on \\(K\\)’s being negatively affected by \\(X\\),17 seeking \\(K=1\\) increasingly takes on the quality of a hoop test for \\(X\\)’s having a positive effect on \\(Y\\). The clue, that is, increasingly becomes something we must see if \\(X\\) positively affects \\(Y\\), with the clue remaining moderately probable if there is no effect. Why? The less likely we believe it is that \\(K=0\\) was caused by \\(X=1\\), the less consistent the observation of \\(K=0\\) is with \\(X\\) having a positive causal effect on \\(Y\\) via \\(K\\) (since, to have such an effect, if \\(X=1\\) and \\(K=0\\), would precisely have to mean that \\(X=1\\) caused \\(K=0\\)). In the second graph, we simultaneously change the prior probabilities of zero effects at both stages in the sequence: of \\(K\\) and \\(Y\\) being \\(1\\) regardless of the values of \\(X\\) and \\(K\\), respectively.18 We see here that, as the probabilities of zero effects jointly diminish, seeking \\(K=1\\) increasingly becomes a smoking-gun test for a positive effect of \\(X\\) on \\(Y\\): the probability of seeing the clue if the case is a \\(d\\) type diminishes. The reason is that, as zero effects at the lower level become less likely, it becomes increasingly unlikely that \\(K=1\\) could have occurred without a positive effect of \\(X\\) on \\(K\\), and that \\(Y=1\\) could have occurred (given that we have seen \\(K=1\\)) without a posiitve effect of \\(K\\) on \\(Y\\). Figure 6.2: The probability of observing \\(K\\) given causal type for different beliefs on lower-level causal effects. In the left figure, priors on all lower-level causal effects are flat except for the probability that \\(X\\) has a negative effect on \\(K\\). If we believe that it is unlikely that \\(X\\) has a negative effect on \\(K\\), \\(K\\) becomes a `hoop’ test for the proposition that a case is of type \\(b\\). The righthand figure considers simultaneous changes in \\(\\lambda_{11}^K\\) and \\(\\lambda_{11}^Y\\)—the probabilities that \\(K=1\\) regardless of \\(X\\), and that \\(Y=1\\) regardless of \\(K\\), with flat distributions on all other lower-level effects. With \\(\\lambda_{11}^K\\), \\(\\lambda_{11}^Y\\) both close to 0, \\(K\\) becomes a ‘smoking gun’ test for the proposition that \\(X\\) has a positive effect on \\(Y\\) (\\(b\\) type). 6.1.3.2 Gains from uncertainty One further implication of this analysis is that sometimes uncertainty about causal paths is better than certainty if we seek to generate empirical strategies. If we know for certain that \\(X\\) always has a positive effect on \\(K\\) (a mediator), then we learn nothing from observing \\(K\\). We have, in effect, already observed \\(K\\) once we have observed \\(X\\). For a mediator clue to convey new information about \\(X\\)’s effect on \\(Y\\), we must to some degree start out uncertain about \\(X\\)’s case-level effect on intervening steps in a causal chain in order for process tracing along the causal chain to be informative about \\(X\\)’s effect on \\(Y\\). For learning to occur, there must be uncertainty for \\(K\\) to resolve. When it comes to using mediators as clues to causal effects, then, strong theories can make for weaker process tracing. This is a general feature of these strategies. 6.1.4 Walk through using a simple moderation model Return now to the moderation example from Chapter 4, graphed here in Figure . Here \\(Y\\) is directly affected by both \\(X\\) and a second variable, \\(K\\), which could be a potential clue. As we discuss in Chapter 4, when we theorize moderation of an \\(X \\rightarrow Y\\) effect, we are again partitioning the higher-level response node, \\(\\theta_Y^{higher}\\), into an unobservable \\(\\theta_Y^{lower}\\) and a potentially observable (\\(K\\)) component. Whereas \\(Y\\)’s response to \\(X\\) in the higher-level model is represented by a non-substantive response-node, we have now framed \\(Y\\)’s response in part in terms of a substantive concept that we believe conditions \\(X\\)’s effect. Another important difference between a higher-level \\(X \\rightarrow Y\\) model and a lower-level model of moderation is that \\(\\theta_Y^{lower}\\), in the latter model, determines \\(Y\\)’s response to both \\(X\\) and \\(K\\) (whereas \\(\\theta_Y^{higher}\\) only determines \\(Y\\)’s response to \\(X\\)). We can think of \\(\\theta_Y^{lower}\\), thus, as fully defining the interaction between \\(X\\) and \\(K\\): how, if at all, does \\(X\\)’s effect depend on \\(K\\); and how, if at all, does \\(K\\)’s effect depend on \\(X\\)? Using the notation from Chapter 4, we write a value for \\(\\theta_Y^{lower}\\) in a moderation model in the form \\(\\theta_{ij}^{gh}\\), where the four sub- and superscripts provide \\(Y\\)’s potential outcomes for the four possible combinations of \\(X\\) and \\(K\\) values. We arrange the sub- and superscripts to give them an easy visual interpretation, with \\(X=0, K=0\\) being in the lower lefthand corner (\\(i\\)); \\(X\\) going to \\(1\\) as we move to the right; and \\(K\\) going to \\(1\\) as we move up. In other words, a unit has outcome \\(i\\) when \\(X=0, K=0\\), \\(j\\) when \\(X=1, K=0\\), \\(g\\) when \\(X=0, K=1\\), \\(h\\) when \\(X=1, K=1\\). \\(\\theta_Y^{lower}\\) in this graph denotes a multinomial distribution over the sixteen possible values of \\(\\theta_{ij}^{gh}\\). We represent all 16 types for \\(\\theta_Y^{lower}\\) in Table . Parallel to our notation for mediation, we further represent the population-level shares of the response types as \\(\\lambda_{ij}^{gh}\\). Suppose, again, that we have observed \\(X=Y=1\\) in a case and want to know whether \\(X=1\\) caused \\(Y=1\\). How can observing \\(K\\), the moderator, help? In this setup, we can think of learning about \\(K\\) as shaping inferences through two channels. When we observe a moderator clue, we learn (1) about the laws governing the case and (2) about the case being governed by those laws. Or, mapped onto Figure , we learn from a moderator clue both about (1) which row we are in and (2) which pair of columns we are in. First, observing the moderator clue together with \\(X\\) provides information about the value of \\(\\theta_Y^{lower}\\)—that is, which row in Figure the case belongs to. It tells us something, in other words, about how the case would respond in different contingencies. In fact, before we even observe \\(K\\), we can eliminate rows—values of \\(\\theta_Y^{lower}\\)—based on having observed \\(X\\), given that we know \\(Y\\). (This stands to reason, given that \\(X\\) is \\(d\\)-connected to \\(\\theta_Y^{lower}\\) by \\(Y\\).) Specifically, observing \\(X=1\\) with \\(Y=1\\) allows us to eliminate any types in which \\(Y\\) is always \\(0\\) when \\(X=1\\): specifically, the types in rows 1, 5, 9, and 13. Then, observing \\(K\\) allows us to go further. If we observe, \\(K=1\\) (having already observed \\(X=Y=1\\)), then we are now able to rule out additional types: specifically, those in which \\(Y=0\\) if \\(X=K=1\\). These are the types in rows 3, 7, 11, and 15. (Alternatively, we if observe \\(K=0\\), we eliminate any type for which \\(Y=0\\) when \\(X=1\\) and \\(K=0\\), namely the types in rows 2, 6, 10, and 14.) Thus, the case’s possible response types upon observing \\(X=Y=K=1\\) are those in rows 2, 4, 6, 8, 10, 12, 14, and 16. These types describe the possible ‘’laws’’ governing the case: each indicates \\(X\\)’s effect in different contexts, as defined by \\(K\\). The next step is to use \\(K\\) once more, this time to tell us which context we are in. Since \\(K=1\\), we know we are in the context represented by the second and fourth columns of potential outcomes. We can thus ask for which of the eight possible types does \\(X\\) have a positive causal effect when \\(K=1\\) (i.e., restricting to the second and fourth columns of potential outcomes). These are the types in rows 2, 4, 10, and 12: \\(\\theta_{00}^{01}\\), \\(\\theta_{01}^{01}\\), \\(\\theta_{10}^{01}\\), and \\(\\theta_{11}^{01}\\). In rows 6 (\\(\\theta_{00}^{11}\\)), 8 (\\(\\theta_{01}^{11}\\)), 14 (\\(\\theta_{10}^{11}\\)), and 16 (\\(\\theta_{11}^{11}\\)), \\(X\\) has zero effect. Thus, for half of the types consistent with the data, \\(X\\) has a positive effect on \\(Y\\), and for half \\(Y\\) would have been \\(1\\) regardless of \\(X\\)’s value. As for a mediator clue, we need to draw on prior beliefs over the causal possibilities in order to learn from the moderator clue. To help us think through how such beliefs might operate in this setting, let us return to our running example. Suppose that \\(X\\) is the presence of a free press, \\(K\\) is a measure of government sensitivity to public opinion, and \\(Y\\) is government removal. Let us suppose that we do not know the structural equations connecting the variables (i.e., we bracket those presented in Chapter 2). However, based on previous research, we have beliefs over causal possibilities. In the final column of Table , we provide a set of priors representing a possible set of such beliefs. Broadly, the priors we use for this illustration represent a few intuitive beliefs about the causal relationships in play: that the free press will represent a threat to governments that are insensitive to public opinion; that a free press could plausibly reinforce the legitimacy and thus prevent the removal of a government sensitive to public opinion (i.e., could have the opposite effect for a sensitive and an insensitive government); that a free press rarely prevents government removal when the government is insensitive to public opinion; that sensitivity of the government to public opinion rarely causes government removal; that it is not unlikely that the free press has no effect, regardless of sensitivity; and that it is somewhat likely that neither a free press nor sensitivity has no effect, regardless of the other. Types excluded by the \\(X, Y\\) data alone are given a prior weight of 0 since we know that they are impossible before we observe the clue. Given these priors, we can readily calculate our prior confidence—based just on the \\(X=Y=1\\) observation—that the free press caused government removal (i.e., that the higher-level type is \\(\\theta_{01}\\). This is the population share represented by those types for which \\(X\\) has a positive effect on \\(Y\\) at some value of \\(K\\), with each type weighted in the calculation by the probability that \\(K\\) in fact takes on that value under which that type displays a positive effect of \\(X\\) on \\(Y\\).19 Thus, to calculate our prior on the causal effect of \\(X\\), we need to also set a prior on how commonly we think governments (in cases with a free press and government removal) are sensitive to public opinion (\\(\\pi^K\\) from our original causal model). Here we set the prior probability of \\(K=1\\) to 0.5. Using our specified prior beliefs, our prior confidence that the free press caused government removal is 0.61. Now, what happens if we go and collect our clue, measuring government sensitivity? If we look for the clue and find it (\\(K=1\\)), then our posterior probability becomes the population share of those types consistent with \\(X=Y=K=1\\) for which \\(X\\) has a positive effect on \\(Y\\) when \\(K\\) in fact is \\(1\\).20 (We can see in this statement the learning both about which the laws governing the case’s responses to context and about the context that the case is in.) This share works out to 0.325. Thus, if we observe a sensitive government, we become much less confident that \\(X\\) caused \\(Y\\) in this case. On the other hand, if we measure the government’s sensitivity and find that it is insensitive (\\(K=0\\)), our confidence that \\(X\\) caused \\(Y\\) goes up somewhat to 0.73.21 Thus, the search for governmental insensitivity functions as a relatively easy hoop test for the proposition that the free press caused the government’s removal. Figure 6.3: A model with one explanatory variable and one moderator. 6.1.4.1 Connection to qualitative tests Again we can undertake this analysis by calculating \\(\\phi\\) values as used in Humphreys and Jacobs (2015): that is, the probability of observing a clue, \\(K\\), if a given proposition is true. Since we have already observed \\(X=Y=1\\) in the case, the \\(phi\\) values of interest here are \\(\\phi_{t_{01}1}\\) and \\(\\phi_{t_{11}1}\\): the probability of observing the clue for an \\(X=1\\) case of type \\(\\theta_{01}\\) (a \\(b\\) type, or positive effect) and the probability of observing the clue for an \\(X=1\\) case of type \\(\\theta_{11}\\) (a \\(d\\) type, or zero effect). By assumption in the lower-level model, \\(K\\) is as-if randomly assigned: in particular (without conditioning on \\(Y\\)), \\(K\\) is orthogonal to \\(\\theta_Y^{lower}\\)—that is, to \\(Y\\)’s responsiveness to \\(K\\). While the probability of observing \\(K\\) is thus the same for all lower-level \\(Y\\) types, that probability is—critically—not the same for all the types in the higher-level theory. The higher-level types—those of interest for determining \\(X\\)’s effect on \\(Y\\)—are themselves related to the probability of \\(K\\). Moreover, \\(K\\) is also correlated with the lower-level type variable, \\(\\theta_Y^{lower}\\), conditional on \\(Y\\). In particular, given \\(Y=1\\), we can calculate the probability of observing the clue if \\(X=1\\) and the case is of type \\(\\theta_{j1}\\) as: \\[\\phi_{t_{j1}1} = \\frac{\\pi^K\\left(\\lambda_{00}^{j1}+\\lambda_{10}^{j1}+\\lambda_{01}^{j1}+\\lambda_{11}^{j1}\\right)}{\\pi^K\\left(\\lambda_{00}^{j1}+\\lambda_{10}^{j1}+\\lambda_{01}^{j1}+\\lambda_{11}^{j1}\\right) + (1-\\pi^K)\\left(\\lambda_{j1}^{00}+\\lambda_{j1}^{10}+\\lambda_{j1}^{01}+\\lambda_{j1}^{11}\\right)}\\] 6.1.5 Illustration with code model &lt;- make_model(&quot;X -&gt; Y &lt;- K&quot;) %&gt;% set_parameters(c(.5, .5, .5, .5, 0,1/3,0,0,0,0,0,0, 1/3,0,0,0,0,0,0,1/3)) get_estimands( model, queries = list(COE = &quot;Y[X=1] &gt; Y[X=0]&quot;), subsets = list(&quot;X==1 &amp; Y==1&quot;, &quot;X==1 &amp; Y==1 &amp; K==0&quot;, &quot;X==1 &amp; Y==1 &amp; K==1&quot;), using = &quot;parameters&quot;) ## Query Subset Using mean sd ## 1 COE X==1 &amp; Y==1 parameters 0.333 NA ## 2 COE X==1 &amp; Y==1 &amp; K==0 parameters 0.000 NA ## 3 COE X==1 &amp; Y==1 &amp; K==1 parameters 0.500 NA 6.2 Four principles 6.2.1 Conditional independence alone does not provide probative value 6.2.2 Uncertainty does not alter inference for single case causal inference In the procedure described for process tracing in this chapter (and different to what we introduce in Chapter 8) we assume that \\(\\lambda\\) is known and we do not place uncertainty around it. This might appear somewhat heroic, but in fact for single case inference it is without loss of generality. The expected inferences we would make for any estimand accounting for priors is the same as the inferences we if we use the expectation only. To see this, let \\(\\pi_j\\) denote the probability of observing causal type \\(j\\) and \\(p(D)\\) te probability of observing data realization \\(D\\). Say that \\(j \\in D\\) if type \\(j\\) produces data type \\(D\\) and say \\(j \\in E\\) if casual type \\(j\\) is an element of the estimand set of interest. For instance in an \\(X \\rightarrow Y\\) model, if we observe \\(X=Y=1\\) then \\(D\\) consists of causal types \\(D={(\\theta^X_1, \\theta^Y_{01}), (\\theta^X_1, \\theta^Y_{11})})\\) and the estimand set for “\\(X\\) has a positive effect on \\(Y\\)” consists of \\(E={(\\theta^X_1, \\theta^Y_{01}), (\\theta^X_0, \\theta^Y_{01})})\\). The posterior on an estimand \\(E\\) given data \\(D\\) given prior over \\(\\pi\\), \\(p(\\pi)\\) is: \\[\\Pr(E | D) = \\int_\\pi \\frac{\\sum_{j \\in E \\cap D}\\pi_j}{\\sum_{j \\in D}\\pi_j} f(\\pi)d\\pi\\] However, since for any \\(\\pi\\), \\(\\sum_{j \\in D}\\pi_j = p(D)\\) we have: \\[\\Pr(E | D) = \\int_\\pi \\sum_{j \\in E \\cap D}\\pi_j f(\\pi)d\\pi/p(D) = \\sum_{j \\in E \\cap D} \\overline{\\pi}_j/p(D)\\] 6.2.3 Probative value requires \\(d-\\)connection As we have argued, causal estimands can be expressed as the values of exogenous nodes in a causal graph. Case-level causal effects and causal paths can be defined in terms of response-type nodes; average effects and notable causes in terms of population-level parameter nodes (e.g., \\(\\pi\\) or \\(\\lambda\\) terms); and questions about actual causes in terms of exogenous conditions that yield particular endogenous values (conditioning on which makes some variable a counterfactual cause). We thus define causal inference more generally as the assessment of the value of one or more unobserved (possibly unobservable) exogenous nodes on a causal graph, given observable data. To think through the steps in this process, it is useful to distinguish among three different features of the world, as represented in our causal model: there are the things we want to learn about; the things we have already observed; and the things we could observe. As notation going forward, let: \\(\\mathcal Q\\) denote the exogenous variables that define our query; we generally assume that \\(\\mathcal Q\\) cannot be directly observed so that its values must be inferred \\(\\mathcal W\\) denote a set of previously observed nodes in the causal model, and \\(\\mathcal K\\) denote a set of additional variables—clues—that we have not yet observed but could observe. Now suppose that we seek to design a research project to investigate a causal question. How should the study be designed? Given that there are some features of the world that we have already observed, which additional clues should we seek to collect to shed new light on our question? In terms of the above notation, what we need to figure out is whether a given \\(\\mathcal K\\) might be informative about—might provide additional leverage on—\\(\\mathcal Q\\) given the prior observation of \\(\\mathcal W\\). To ask whether one variable (or set of variables) is informative about another is to ask whether the two (sets of) variables are, on average, correlated with one another, given whatever we already know. Likewise, if two variables’ distributions are fully independent of one another (conditional on what else we have observed), then knowing the value of one variable can provide no new information about the value of the other. Thus, asking whether a set of clues, \\(\\mathcal K\\), is informative about \\(\\mathcal Q\\) given the prior observation of \\(\\mathcal W\\), is equivalent to asking whether \\(\\mathcal K\\) and \\(\\mathcal Q\\) are conditionally independent given \\(\\mathcal W\\). That is, \\(\\mathcal K\\) can be informative about \\(\\mathcal Q\\) given \\(\\mathcal W\\) only if \\(\\mathcal K\\) and \\(\\mathcal Q\\) are not conditionally independent of one another given \\(\\mathcal W\\). As we have shown, as long as we have built \\(\\mathcal Q\\), \\(\\mathcal K\\), and \\(\\mathcal W\\) into our causal model of the phenomenon of interest, we can answer this kind of question by inspecting the structure of the model’s DAG. In particular, what we need to go looking for are relationships of \\(d\\)-separation. The following proposition, with only the names of the variable sets altered, is from Pearl (2009) (Proposition 1.2.4): Proposition 1: If sets \\(\\mathcal Q\\) and \\(\\mathcal K\\) are \\(d\\)-separated by \\(\\mathcal W\\) in a DAG, \\(\\mathcal G\\), then \\(\\mathcal Q\\) is independent of \\(\\mathcal K\\) conditional on \\(\\mathcal W\\) in every distribution compatible with \\(\\mathcal G\\). Conversely, if \\(\\mathcal Q\\) and \\(\\mathcal K\\) are not \\(d\\)-separated by \\(\\mathcal W\\) in DAG \\(\\mathcal W\\), then \\(\\mathcal Q\\) and \\(\\mathcal K\\) are dependent conditional on \\(\\mathcal W\\) in at least one distribution compatible with \\(\\mathcal G\\). We begin with a causal graph and a set of nodes on the graph (\\(W\\)) that we have already observed. Given what we have already observed, a collection of clue nodes, \\(\\mathcal K\\), will be uninformative about the query nodes, \\(\\mathcal Q\\), if \\(\\mathcal K\\) is \\(d\\)-separated from \\(\\mathcal Q\\) by \\(\\mathcal W\\) on the graph. When \\(\\mathcal W\\) \\(d\\)-separates \\(\\mathcal K\\) from \\(\\mathcal Q\\), this means that what we have already observed already captures all information that the clues might yield about our query. On the other hand, if \\(\\mathcal K\\) and \\(\\mathcal Q\\) are \\(d\\)-connected (i.e., not \\(d\\)-separated) by \\(W\\), then \\(K\\) is possibly informative about \\(Q\\).\\(K\\) is not \\(d\\)-separated from \\(\\mathcal Q\\) by \\(\\mathcal W\\).22 Note, moreover, that under quite general conditions (referred to in the literature as the faithfulness of a probability distribution) then there are at least some values of \\(\\mathcal W\\) for which \\(\\mathcal K\\) will be informative about \\(\\mathcal Q\\).23 Let us examine Proposition 1 in practice. We begin with the simplest case possible, and then move on to more complex models. The very simplest probabilistic causal graph has \\(X\\) influencing \\(Y\\), with \\(X\\) determined by a coin flip. Assuming that there is some causal heterogeneity—that is, it is unknown in any particular case whether \\(X\\) causes \\(Y\\)—we also include a response-type variable, \\(Q\\), pointing into \\(Y\\), as shown in Figure . Here, \\(Q^Y\\) determines the value of \\(Y\\) that will be generated by \\(X\\). Asking about the causal effect of \\(X\\) in a case thus means learning the value of \\(Q^Y\\) in that case. As will be recalled, in a binary setup with one causal variable, a response-type variable can take on one of four values, \\(q^Y_{00}\\), \\(q^Y_{10}\\), \\(q^Y_{01}\\) and \\(q^Y_{11}\\),24 corresponding to the four possible causal types in this setting. Figure 6.4: A simple causal setup in which the effect of \\(X\\) on \\(Y\\) in a given case depends on the case’s response type for \\(Y\\). Let us assume that we have observed nothing yet in this case and then ask what clue(s) might be informative about \\(Q^Y\\), the node of interest. The other two nodes in the graph are \\(X\\) and \\(Y\\): these are thus the possible clues that we might go looking for in our effort to learn about \\(Q^Y\\) (i.e., they are the possible members of \\(\\mathcal K\\)). First, can we learn about \\(Q^Y\\) by observing \\(X\\)? We can answer this question by asking whether \\(X\\) is \\(d\\)-connected to \\(Q^Y\\) on the graph given what we have already observed (which is nothing). We can see visually that there is no active path from \\(X\\) to \\(Q^Y\\): the only path between \\(X\\) and \\(Q\\) is blocked by colliding arrow heads. Thus, \\(X\\) and \\(Q^Y\\) are \\(d\\)-separated, meaning that \\(X\\) will not be informative about \\(Q^Y\\): observing the value that a causal variable takes on in a case—having seen nothing else in the case—tells us nothing whatsoever about that variable’s effect on the outcome. If we want to know whether a case is of a type in which the presence of natural resources would cause civil war, observing only that the case has natural resources does not help answer the question. –&gt; What, then, if we instead were to observe only \\(Y\\)? Is \\(Y\\) \\(d\\)-connected to \\(Q\\) given what we have already observed (which, again, is nothing)? It is: the arrow from \\(Q^Y\\) to \\(Y\\) is an active path. Observing only the outcome in a case does tell us something about causal effects. Returning to the natural resources and civil war example, observing only that a country has had a civil is informative about the case’s causal type (the value of \\(Q^Y\\)). In particular, it rules out the possibility that this is a case in which nothing could cause a civil war: that is, it excludes \\(q^Y_{00}\\) (i.e., \\(c\\)-type) as a possible value of \\(Q^Y\\). Suppose now, having observed \\(Y\\), that we were to consider also observing \\(X\\). Would we learn anything further about \\(Q^Y\\) from doing so? We have already seen that observing \\(X\\) alone yields no information about \\(Q^Y\\) because the two nodes are unconditionally \\(d\\)-separated, the path between them blocked by the colliding arrowheads at \\(Y\\). However, as we have seen, observing a collider variable (or one of its descendants) unblocks the flow of information, generating relations of conditional dependence across the colliding arrowheads. Here, \\(X\\) and \\(Q^Y\\) are \\(d\\)-connected by \\(Y\\): thus, if we have already observed \\(Y\\), then observing \\(X\\) does confer additional information about \\(Q^Y\\). Knowing only that a country has natural resources tells us nothing about those resources’ effect on civil war in that country. But if we already know that the country has a civil war, then learning that the country has natural resources helps narrow down the case’s possible response types. Having already used the observation of \\(Y=1\\) to rule out the possibility that \\(Q^Y=q^Y_{00}\\), observing \\(X=1\\) together with \\(Y=1\\) allows us to additionally rule out the possibility that natural resources prevent civil war, i.e., that \\(Q^Y=q^Y_{01}\\).25 Finally, what if we observe \\(X\\) first and are considering whether to seek information about \\(Y\\)? Would doing so be informative? \\(X\\) does not \\(d-\\)separate \\(Q^Y\\) from \\(Y\\); thus, observing \\(Y\\) will be informative about \\(Q^Y\\). In fact, observing \\(Y\\) if we have already seen \\(X\\) is more informative than observing \\(Y\\) alone. The reasoning follows the logic of collision discussed just above. If we observe \\(Y\\) having already seen \\(X\\), not only do we reap the information about \\(Q^Y\\) provided by \\(Y\\)’s correlation with \\(Q^Y\\); we simultaneously open up the path between \\(X\\) and \\(Q^Y\\), learning additionally from the conditional dependence between \\(X\\) and \\(Q^Y\\) given \\(Y\\). We put Proposition 1 to work in a slightly more complex set of models in Figure . Here we investigate the informativeness of a clue that is neither \\(X\\) nor \\(Y\\). Each graph in Figure has four variables: \\(X\\); \\(Y\\); a possible clue, \\(K\\); and a response-type variable, \\(Q\\). We draw all 34 possible graphs with variables \\(X\\), \\(Y\\), \\(K\\), and \\(Q\\) for causal models in which (a) all variables are connected to at least one other variable, (b) \\(X\\) causes \\(Y\\) either directly or indirectly, and (c) \\(Q\\) is a direct cause of \\(Y\\) but is not caused by any other variable in the model and is thus exogenous. The title of each panel reports \\(K\\)’s conditional informativeness using principles of \\(d\\)-separation: it tells us when \\(K\\) is possibly informative about \\(Q\\) depending on whether \\(X\\), \\(Y\\), both or none are observed.26 Figure 6.5: All connected directed acyclic graphs over \\(X,Y,K,Q\\), in which \\(Q\\) is an exogenous variable that directly causes \\(Y\\), and \\(X\\) is a direct or indirect cause of \\(Y\\). The title of each graph indicates the conditions under which \\(K\\) can be informative about (i.e., is not \\(d\\)-separated from) \\(Q\\), given the prior observation of \\(X\\), \\(Y\\), both, or neither (…). 6.2.4 Probative value The results show us not just what kinds of variables can be informative about a case’s response-type but also what combinations of observations yield leverage on case-level causal effects. A number of features the graphs are worth highlighting: Clues at many stages. Process tracing has focused a great deal on observations that lie “along the path” between suspected causes and outcomes. What we see in Figure , however, is that observations at many different locations in a causal model can be informative about causal effects. We see here that \\(K\\) can be informative when it is pre-treatment (causally prior to \\(X\\)—e.g. panel (3)), post-treatment but pre-outcome (that is, “between” \\(X\\) and \\(Y\\) as, e.g., in panel (20)), an auxiliary effect of \\(X\\) that itself has no effect on \\(Y\\) (e.g., in panel (19)), post-outcome (after \\(Y\\)—e.g., in panel (15)), or a joint effect of both the suspected cause and the outcome (e.g., panel (31)). Mediator Clues. While clues that lie in between \\(X\\) and \\(Y\\) may be informative, they can only be informative under certain conditions. For instance, when a clue serves only as a mediator in our model (i.e., its only linkages are being caused by \\(X\\) and being affected by \\(Y\\)) and \\(Q\\) only affects \\(Y\\), as in panels (20) and (21), the clue is only informative about \\(Q\\) if we have also observed the outcome, \\(Y\\). Of course, this condition may commonly be met—qualitative researchers usually engage in retrospective research and learn the outcome of the cases they are studying early on—but it is nonetheless worth noting why it matters: in this setup, \\(K\\) is unconditionally \\(d\\)-separated from \\(Q\\) by the collision at \\(Y\\); it is only by observing \\(Y\\) (the collider) that the path between \\(K\\) and \\(Q\\) becomes unblocked. (As we saw above, the very same is true for observing \\(X\\); it is only when we know \\(Y\\) that \\(X\\) is informative about \\(Q\\).) In short, observations along causal paths are more helpful in identifying causal effects to the extent that we have measured the outcome. Importantly, this is not the same as saying that mediator clues are only informative about causal effects where we have observed the outcome. Observing \\(Y\\) is necessary for the mediator to be informative about a \\(Q\\) term that is connected only to \\(Y\\). Observing a mediator without the outcome, however, could still be informative about the overall effect of \\(X\\) on \\(Y\\) by providing leverage on how the mediator responds to \\(X\\), which is itself informative about \\(X\\)’s effect on \\(Y\\) via the mediator.27 Moreover, observing the mediator could be informative without the observation of \\(Y\\) if, for instance, \\(Q\\) also points into \\(K\\) itself or into a cause of \\(K\\). As we discuss below, the clue then is informative as a “symptom” of the case’s response type, generating learning that does not hinge on observing the outcome. Symptoms as clues. Some clues may themselves be affected by \\(Q\\): that is to say, they may be symptoms of the same conditions that determine causal effects in a case. For instance, in our illustrative model involving government survival, government sensitivity functions as a response-type variable for the effect of a free press (\\(X\\)) on government removal (\\(Y\\)): a free press only generates government removal when the government is non-sensitive to public opinion. Sensitivity to public opinion thus represents our query variable, \\(Q\\), if we seek to learn whether a free press causes government removal in a case. While it may not be possible to observe or otherwise measure the government’s sensitivity, there may be consequences of government sensitivity that are observable: for instance, whether government officials regularly consult with civil-society actors on policy issues. While consultations would not be part of the causal chain generating the free press’s effect, observing consultations (or the lack of them) would be informative about that effect because consultations are a symptom of the same conditions that enable the effect. We see that \\(K\\) is a child or descendant of \\(Q\\) in several of the graphs in Figure : \\(Q\\) directly causes \\(K\\) in panels (7) through (14), (17), (18), (25)-(30), (33), and (34); \\(Q\\) causes (K) only indirectly through \\(X\\) in panels (22) through (24); \\(Q\\) causes (K) only indirectly through \\(Y\\) in panels (15), (16), and (31); and \\(Q\\) causes \\(K\\) only indirectly through \\(X\\) and through \\(Y\\) in panel (32). We can then use the principle of \\(d\\)-separation to figure out when the symptom clue is potentially informative, given what we have already observed. It is easy to see that \\(K\\) is potentially informative, no matter what we have already observed, if \\(K\\) is directly affected by \\(Q\\); there is nothing we could observe that would block the \\(Q \\rightarrow K\\) path. Thus, \\(Q\\)’s “symptom” can, in this setup, contain information about type above and beyond that contained in the \\(X\\) and \\(Y\\) values. However, where \\(Q\\) affects \\(K\\) only through some other variable, observing that other variable renders \\(K\\) uninformative by blocking the \\(Q\\)-to-\\(K\\) path. For instance, where \\(Q\\) affects \\(K\\) indirectly through \\(X\\), once we observe \\(X\\), we already have all the information about \\(Q\\) that would be contained in \\(K\\). Surrogates as clues. Clues may be consequences of the outcome, as in graphs (15) and (16). If \\(K\\) is a consequence only of \\(Y\\), then it will contain no new information about \\(Q\\) where \\(Y\\) is already known. However, in situations where the outcome has not been observed, \\(K\\) can act as a “surrogate” for the outcome and thus yield leverage on \\(Q\\) (Frangakis and Rubin (2002)). A researcher might, for instance, seek to understand causal effects on an outcome that is difficult to directly observe: consider, for instance, studies that seek to explain ideational change. Ideas themselves, the \\(Y\\) in such studies, are not directly observable. However, their consequences—such as statements by actors or policy decisions—will be observable and can thus serve as informative surrogates for the outcome of interest. Clues may similarly serve as surrogates of a cause, as in graphs (19) and (22). Here \\(X\\) causes \\(K\\), but \\(K\\) plays no role in the causal process generating \\(Y\\). \\(K\\) is of no help if we can directly measure \\(X\\) since the latter \\(d\\)-separates \\(K\\) from \\(Q\\). But if an explanatory variable cannot be directly measured—consider, e.g., ideas or preferences as causes—then its consequences, including those that have no relationship to the outcome of interest, can provide leverage on the case-level causal effect. Clues can also be a consequence of both our suspected cause and the outcome of interest, thus serving as what we might call “double surrogates,” as in panels (31) and (32). Here \\(X\\) is a direct cause of \\(Y\\), and \\(K\\) is a joint product of \\(X\\) and \\(Y\\). A double surrogate can be informative as long as we have not already observed both \\(X\\) and \\(Y\\). Where data on either \\(X\\) or \\(Y\\) are missing, there is an open path between \\(K\\) and \\(Q\\). If we have already observed both, however, then there is nothing left to be learned from \\(K\\). Instruments as clues. Clues that are causally prior to an explanatory variable, and have no other effect on the outcome, can sometimes be informative. Consider, for instance, graph (3). Here \\(K\\) is the only cause of \\(X\\). It can thus serve as a proxy. If we have seen \\(X\\), then \\(X\\) blocks the path between \\(K\\) and \\(Q\\), and so \\(K\\) is unhelpful. \\(K\\) can be informative, though, if we have not observed \\(X\\). Note that informativeness here still requires that we observe \\(Y\\). Since \\(Y\\) is a collider for \\(Q\\) and the \\(K \\rightarrow X \\rightarrow\\) chain, we need to observe \\(Y\\) in order to \\(d\\)-connect \\(K\\) to \\(Q\\). A rather different setup appears in graph (5), where both \\(K\\) and \\(Q\\) cause \\(X\\). Now the conditions for \\(K\\)’s informativeness are broader. Observing \\(X\\) still makes \\(K\\) uninformative as a proxy for \\(X\\) itself. However, because \\(X\\) is a collider for \\(K\\) and \\(Q\\), observing \\(X\\) opens up a path from \\(K\\) to \\(Q\\), rendering a dependency between them. Still, we have to observe at least one of \\(X\\) or \\(Y\\) for the instrument to be informative here. This is because both of \\(K\\)’s paths to \\(Q\\) run through a collision that we need to unblock by observing the collider. For one path, the collider is \\(X\\); for the other path, the collider is \\(Y\\).28 Other patterns involving instrumentation are also imaginable, though not graphed here. For example, we might have a causal structure that combines instrumentation and surrogacy. Suppose that \\(X\\) is affected by \\(Q\\) and by an unobservable variable \\(\\theta_X\\); and that \\(\\theta_X\\) has an observable consequence, \\(K\\). Then \\(K\\), though not a cause of \\(X\\), is a “surrogate instrument” (Hernán and Robins 2006) as it is a descendant of an unobserved instrument, \\(U\\), and thus allows us to extract inferences similar to those that we could draw from a true instrument. Confounders as clues. In several of the graphs, \\(K\\) is a confounder in that it is a direct cause of both \\(X\\) and \\(Y\\) (panels (4), (6), (12), and (14)). Let us focus on graph (4), which isolates \\(K\\)’s role as a confounder. Here \\(K\\) can be informative via two possible paths. First, if \\(X\\) is not observed but \\(Y\\) is, then \\(K\\) is \\(d\\)-connected to \\(Q\\) along the path \\(K \\rightarrow X \\rightarrow Y \\leftarrow Q\\). \\(K\\) is in this sense serving as a proxy for \\(X\\), with its path to \\(Q\\) opened up by the observation of the collider, \\(Y\\). Second, with \\(Y\\) observed, \\(K\\) can provide information on \\(Q\\) via the more direct collision, \\(K \\rightarrow Y \\leftarrow Q\\). If \\(X\\) is observed, then the first path is blocked, but the second still remains active. As with any pre-outcome variable, for a confounder clue to provide purchase on \\(Y\\)’s response type, \\(Y\\) itself must be observed. In a sense, then, the role of confounders as clues in case-level inference is the mirror image of the role of confounders as covariates in cross-case correlational inference. In a correlational inferential framework, controlling for a variable in \\(K\\)’s position in graph (5) renders the \\(X, Y\\) correlation (which we assume to be observed) informative about \\(X\\)’s average causal effect. When we use confounders as evidence in within-case inference, it is our observations of other variables that determine how informative the confounder itself will be about \\(X\\)’s causal effect. It is important to be precise about the kinds of claims that one can make from graphs like those in Figure {fig:34graphs}. The graphs in this figure allow us to identify informativeness about an unobserved node \\(Q\\) that is a parent of \\(Y\\). This setup does not, however, capture all ways in which clues can be informative about the causal effect of \\(X\\) on \\(Y\\) or about other causal estimands of interest. For instance, as noted above, even if a clue is uninformative about a \\(Q\\) node pointing into \\(Y\\), it may still help establish whether \\(X\\) causes \\(Y\\): the statement that \\(X\\) causes \\(Y\\) will for some graphs be a statement about a collection of nodes that form the set of query variables \\(\\mathcal Q\\). This is the case, for instance, in any graph of the form \\(X \\rightarrow M \\rightarrow Y\\), where we are interested not just in \\(Y\\)’s response to \\(M\\) (the mediator) but also in \\(M\\)’s response to \\(X\\). Of interest, thus, are not just a \\(Q^Y\\) response-type node pointing into \\(Y\\) but also a \\(Q^M\\) response-type node that is a parent of \\(M\\). Observations that provide leverage on either \\(Q\\) term will thus aid an inference about the overall causal effect. A clue \\(K\\) that is \\(d-\\)separated from \\(Q^Y\\) may nevertheless be informative about \\(X\\)’s effect on \\(Y\\) if it is not \\(d-\\)separated from \\(Q^M\\); this opens up a broader range of variables as informative clues. Additionally, as our discussion in Chapter 2 makes clear, estimands other than the case-level causal effect—such as average causal effects, actual causes, and causal paths—involve particular features of context: particular sets of exogenous nodes as members of our query set, \\(\\mathcal Q\\). Thus, even for the same causal model, informativeness will be defined differently for each causal question that we seek to address. The broader point is that we can identify what kinds of observations may address our estimand if we can place that estimand on a causal graph and then assess the graph for relationships of \\(d\\)-separation and -connection. Further, we emphasize that a DAG can only tell us when a clue may be informative (conditional some prior observation): \\(d-\\)connectedness is necessary but not sufficient for informativeness. This fact derives directly from the rules for drawing a causal graph: the absence of an arrow between two variables implies that they are not directly causally related, while the presence of an arrow does not imply that they always are. As we saw in our analysis of the government-removal example in Chapter 2, whether variables connected to one another by arrows in the original DAG were in fact linked by a causal effect depended on the context. Likewise, whether a clue \\(K\\) is in fact informative may depend on particular values of \\(\\mathcal W\\)—the variables that have already been observed. As a simple example, let \\(q = k_1w + (1-w)k_2\\), where \\(W\\) is a variable that we have already observed and \\(K_1\\) and \\(K_2\\) are clues that we might choose to observe next. Here, if \\(w=1\\) then learning \\(K_1\\) will be informative about \\(Q\\), and learning \\(K_2\\) will not; but if \\(w=0\\), then \\(K_1\\) will be uninformative (and \\(K_2\\) informative). DO WE NEED TO SAY SOMETHING ABOUT STABILITY/FAITHFULNESS HERE? In general, then, graphical analysis alone can help us exclude unhelpful research designs, given our prior observations and a fairly minimal set of prior beliefs about causal linkages. This is no small feat. But identifying those empirical strategies that will yield the greatest leverage requires engaging more deeply with our causal model, as we explore next. 6.3 Running example 6.3.1 Causal effects 6.3.2 Attribution 6.3.3 Explanation References "]
]
