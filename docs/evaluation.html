<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Evaluating models | Integrated Inferences</title>
  <meta name="description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Evaluating models | Integrated Inferences" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="dnieperriver.png" />
  <meta property="og:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="github-repo" content="rstudio/ii" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Evaluating models | Integrated Inferences" />
  
  <meta name="twitter:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="twitter:image" content="dnieperriver.png" />

<meta name="author" content="Macartan Humphreys and Alan Jacobs" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="justifying-models.html"/>
<link rel="next" href="final-words.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="headers\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Model based causal inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-case-for-causal-models"><i class="fa fa-check"></i><b>1.1</b> The Case for Causal Models</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#the-limits-to-design-based-inference"><i class="fa fa-check"></i><b>1.1.1</b> The limits to design-based inference</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#qualitative-and-mixed-method-inference"><i class="fa fa-check"></i><b>1.1.2</b> Qualitative and mixed-method inference</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#connecting-theory-and-empirics"><i class="fa fa-check"></i><b>1.1.3</b> Connecting theory and empirics</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-contributions"><i class="fa fa-check"></i><b>1.2</b> Key contributions</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-road-ahead"><i class="fa fa-check"></i><b>1.3</b> The Road Ahead</a></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Causal Models</a><ul>
<li class="chapter" data-level="2.1" data-path="models.html"><a href="models.html#the-counterfactual-model"><i class="fa fa-check"></i><b>2.1</b> The counterfactual model</a><ul>
<li class="chapter" data-level="2.1.1" data-path="models.html"><a href="models.html#generalizing-to-outcomes-with-many-causes"><i class="fa fa-check"></i><b>2.1.1</b> Generalizing to outcomes with many causes</a></li>
<li class="chapter" data-level="2.1.2" data-path="models.html"><a href="models.html#deterministic-relations"><i class="fa fa-check"></i><b>2.1.2</b> Deterministic relations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="models.html"><a href="models.html#causal-models-and-directed-acyclic-graphs"><i class="fa fa-check"></i><b>2.2</b> Causal Models and Directed Acyclic Graphs</a><ul>
<li class="chapter" data-level="2.2.1" data-path="models.html"><a href="models.html#components-of-a-causal-model"><i class="fa fa-check"></i><b>2.2.1</b> Components of a Causal Model</a></li>
<li class="chapter" data-level="2.2.2" data-path="models.html"><a href="models.html#rules-for-graphing-causal-models"><i class="fa fa-check"></i><b>2.2.2</b> Rules for graphing causal models</a></li>
<li class="chapter" data-level="2.2.3" data-path="models.html"><a href="models.html#conditional-independence-from-dags"><i class="fa fa-check"></i><b>2.2.3</b> Conditional independence from DAGs</a></li>
<li class="chapter" data-level="2.2.4" data-path="models.html"><a href="models.html#a-simple-running-example"><i class="fa fa-check"></i><b>2.2.4</b> A simple running example</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="models.html"><a href="models.html#illustrations"><i class="fa fa-check"></i><b>2.3</b> Illustrations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="models.html"><a href="models.html#welfare-state-reform-pierson-1994"><i class="fa fa-check"></i><b>2.3.1</b> Welfare state reform: Pierson (1994)</a></li>
<li class="chapter" data-level="2.3.2" data-path="models.html"><a href="models.html#military-interventions-saunders-2011"><i class="fa fa-check"></i><b>2.3.2</b> Military Interventions: Saunders (2011)</a></li>
<li class="chapter" data-level="2.3.3" data-path="models.html"><a href="models.html#development-and-democratization-przeworski-and-limongi-1997"><i class="fa fa-check"></i><b>2.3.3</b> Development and Democratization: Przeworski and Limongi (1997)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="models.html"><a href="models.html#chapter-appendix"><i class="fa fa-check"></i><b>2.4</b> Chapter Appendix</a><ul>
<li class="chapter" data-level="2.4.1" data-path="models.html"><a href="models.html#steps-for-constructing-causal-models"><i class="fa fa-check"></i><b>2.4.1</b> Steps for constructing causal models</a></li>
<li class="chapter" data-level="2.4.2" data-path="models.html"><a href="models.html#model-construction-in-code"><i class="fa fa-check"></i><b>2.4.2</b> Model construction in code</a></li>
<li class="chapter" data-level="2.4.3" data-path="models.html"><a href="models.html#test-yourself-can-you-read-conditional-independence-from-a-graph"><i class="fa fa-check"></i><b>2.4.3</b> Test yourself! Can you read conditional independence from a graph?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>3</b> Theories as causal models</a><ul>
<li class="chapter" data-level="3.1" data-path="theory.html"><a href="theory.html#theory-as-a-lower-level-model"><i class="fa fa-check"></i><b>3.1</b> Theory as a “lower-level” model</a></li>
<li class="chapter" data-level="3.2" data-path="theory.html"><a href="theory.html#illustration-of-unpacking-causal-types"><i class="fa fa-check"></i><b>3.2</b> Illustration of unpacking causal types</a><ul>
<li class="chapter" data-level="3.2.1" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-mediation-model"><i class="fa fa-check"></i><b>3.2.1</b> Type disaggregation in a mediation model</a></li>
<li class="chapter" data-level="3.2.2" data-path="theory.html"><a href="theory.html#type-disaggregation-in-a-moderation-model"><i class="fa fa-check"></i><b>3.2.2</b> Type disaggregation in a moderation model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="theory.html"><a href="theory.html#rules-for-moving-between-higher--and-lower-level-models"><i class="fa fa-check"></i><b>3.3</b> Rules for moving between higher- and lower-level models</a><ul>
<li class="chapter" data-level="3.3.1" data-path="theory.html"><a href="theory.html#moving-down-levels"><i class="fa fa-check"></i><b>3.3.1</b> Moving down levels</a></li>
<li class="chapter" data-level="3.3.2" data-path="theory.html"><a href="theory.html#moving-up-levels"><i class="fa fa-check"></i><b>3.3.2</b> Moving up levels</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="theory.html"><a href="theory.html#conclusion"><i class="fa fa-check"></i><b>3.4</b> Conclusion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="theory.html"><a href="theory.html#quantifying-the-gains-of-a-theory"><i class="fa fa-check"></i><b>3.4.1</b> Quantifying the gains of a theory</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="theory.html"><a href="theory.html#chapter-appendices"><i class="fa fa-check"></i><b>3.5</b> Chapter Appendices</a><ul>
<li class="chapter" data-level="3.5.1" data-path="theory.html"><a href="theory.html#summary-boxes"><i class="fa fa-check"></i><b>3.5.1</b> Summary Boxes</a></li>
<li class="chapter" data-level="3.5.2" data-path="theory.html"><a href="theory.html#illustration-of-a-mapping-from-a-game-to-a-dag"><i class="fa fa-check"></i><b>3.5.2</b> Illustration of a Mapping from a Game to a DAG</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="questions.html"><a href="questions.html"><i class="fa fa-check"></i><b>4</b> Causal Questions</a><ul>
<li class="chapter" data-level="4.1" data-path="questions.html"><a href="questions.html#case-level-causal-effects"><i class="fa fa-check"></i><b>4.1</b> Case-level causal effects</a></li>
<li class="chapter" data-level="4.2" data-path="questions.html"><a href="questions.html#case-level-causal-attribution"><i class="fa fa-check"></i><b>4.2</b> Case-level causal attribution</a></li>
<li class="chapter" data-level="4.3" data-path="questions.html"><a href="questions.html#case-level-explanation"><i class="fa fa-check"></i><b>4.3</b> Case-level explanation</a></li>
<li class="chapter" data-level="4.4" data-path="questions.html"><a href="questions.html#average-causal-effects"><i class="fa fa-check"></i><b>4.4</b> Average causal effects</a></li>
<li class="chapter" data-level="4.5" data-path="questions.html"><a href="questions.html#causal-paths"><i class="fa fa-check"></i><b>4.5</b> Causal Paths</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayeschapter.html"><a href="bayeschapter.html"><i class="fa fa-check"></i><b>5</b> Bayesian Answers</a><ul>
<li class="chapter" data-level="5.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-basics"><i class="fa fa-check"></i><b>5.1</b> Bayes Basics</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-instances"><i class="fa fa-check"></i><b>5.1.1</b> Simple instances</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-rule-for-discrete-hypotheses"><i class="fa fa-check"></i><b>5.1.2</b> Bayes’ Rule for Discrete Hypotheses</a></li>
<li class="chapter" data-level="5.1.3" data-path="bayeschapter.html"><a href="bayeschapter.html#the-dirichlet-family-and-bayes-rule-for-continuous-parameters"><i class="fa fa-check"></i><b>5.1.3</b> The Dirichlet family and Bayes’ Rule for Continuous Parameters</a></li>
<li class="chapter" data-level="5.1.4" data-path="bayeschapter.html"><a href="bayeschapter.html#moments"><i class="fa fa-check"></i><b>5.1.4</b> Moments</a></li>
<li class="chapter" data-level="5.1.5" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-estimation-in-practice"><i class="fa fa-check"></i><b>5.1.5</b> Bayes estimation in practice</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayeschapter.html"><a href="bayeschapter.html#bayes-applied"><i class="fa fa-check"></i><b>5.2</b> Bayes applied</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayeschapter.html"><a href="bayeschapter.html#bayesian-inference-on-queries"><i class="fa fa-check"></i><b>5.2.1</b> Bayesian Inference on Queries</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simple-bayesian-process-tracing"><i class="fa fa-check"></i><b>5.2.2</b> Simple Bayesian Process Tracing</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayeschapter.html"><a href="bayeschapter.html#three-principles-of-bayesian-updating"><i class="fa fa-check"></i><b>5.3</b> Three principles of Bayesian updating</a><ul>
<li class="chapter" data-level="5.3.1" data-path="bayeschapter.html"><a href="bayeschapter.html#AppPriors"><i class="fa fa-check"></i><b>5.3.1</b> Priors matter</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayeschapter.html"><a href="bayeschapter.html#simultaneous-joint-updating"><i class="fa fa-check"></i><b>5.3.2</b> Simultaneous, joint updating</a></li>
<li class="chapter" data-level="5.3.3" data-path="bayeschapter.html"><a href="bayeschapter.html#posteriors-are-independent-of-the-ordering-of-data"><i class="fa fa-check"></i><b>5.3.3</b> Posteriors are independent of the ordering of data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Model-Based Causal Inference</b></span></li>
<li class="chapter" data-level="6" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>6</b> Process Tracing with Causal Models</a><ul>
<li class="chapter" data-level="6.1" data-path="pt.html"><a href="pt.html#process-tracing-and-causal-models"><i class="fa fa-check"></i><b>6.1</b> Process tracing and causal models</a><ul>
<li class="chapter" data-level="6.1.1" data-path="pt.html"><a href="pt.html#the-intuition"><i class="fa fa-check"></i><b>6.1.1</b> The intuition</a></li>
<li class="chapter" data-level="6.1.2" data-path="pt.html"><a href="pt.html#a-formalization-of-the-general-approach"><i class="fa fa-check"></i><b>6.1.2</b> A formalization of the general approach</a></li>
<li class="chapter" data-level="6.1.3" data-path="pt.html"><a href="pt.html#illustration-with-code"><i class="fa fa-check"></i><b>6.1.3</b> Illustration with code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pt.html"><a href="pt.html#five-principles"><i class="fa fa-check"></i><b>6.2</b> Five principles</a><ul>
<li class="chapter" data-level="6.2.1" data-path="pt.html"><a href="pt.html#classic-qualitative-tests-are-special-cases-of-updating-on-a-model"><i class="fa fa-check"></i><b>6.2.1</b> Classic qualitative tests are special cases of updating on a model</a></li>
<li class="chapter" data-level="6.2.2" data-path="pt.html"><a href="pt.html#a-dag-alone-does-not-get-you-probative-value"><i class="fa fa-check"></i><b>6.2.2</b> A DAG alone does not get you probative value</a></li>
<li class="chapter" data-level="6.2.3" data-path="pt.html"><a href="pt.html#uncertainty-does-not-alter-inference-for-single-case-causal-inference"><i class="fa fa-check"></i><b>6.2.3</b> Uncertainty does not alter inference for single case causal inference</a></li>
<li class="chapter" data-level="6.2.4" data-path="pt.html"><a href="pt.html#probative-value-requires-d-connection"><i class="fa fa-check"></i><b>6.2.4</b> Probative value requires <span class="math inline">\(d-\)</span>connection</a></li>
<li class="chapter" data-level="6.2.5" data-path="pt.html"><a href="pt.html#probative-value"><i class="fa fa-check"></i><b>6.2.5</b> Probative value</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ptapp.html"><a href="ptapp.html"><i class="fa fa-check"></i><b>7</b> Application: Process Tracing with a Causal Model</a><ul>
<li class="chapter" data-level="7.1" data-path="ptapp.html"><a href="ptapp.html#inequality-and-democratization-the-debate"><i class="fa fa-check"></i><b>7.1</b> Inequality and Democratization: The Debate</a></li>
<li class="chapter" data-level="7.2" data-path="ptapp.html"><a href="ptapp.html#a-structural-causal-model"><i class="fa fa-check"></i><b>7.2</b> A Structural Causal Model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ptapp.html"><a href="ptapp.html#forming-priors"><i class="fa fa-check"></i><b>7.2.1</b> Forming Priors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ptapp.html"><a href="ptapp.html#results"><i class="fa fa-check"></i><b>7.3</b> Results</a></li>
<li class="chapter" data-level="7.4" data-path="ptapp.html"><a href="ptapp.html#pathways"><i class="fa fa-check"></i><b>7.4</b> Pathways</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ptapp.html"><a href="ptapp.html#cases-with-incomplete-data"><i class="fa fa-check"></i><b>7.4.1</b> Cases with incomplete data</a></li>
<li class="chapter" data-level="7.4.2" data-path="ptapp.html"><a href="ptapp.html#inferences-for-cases-with-observed-democratization"><i class="fa fa-check"></i><b>7.4.2</b> Inferences for cases with observed democratization</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ptapp.html"><a href="ptapp.html#model-definition-and-inference-in-code"><i class="fa fa-check"></i><b>7.5</b> Model definition and inference in code</a></li>
<li class="chapter" data-level="7.6" data-path="ptapp.html"><a href="ptapp.html#concluding-thoughts"><i class="fa fa-check"></i><b>7.6</b> Concluding thoughts</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixing.html"><a href="mixing.html"><i class="fa fa-check"></i><b>8</b> Integrated inferences</a><ul>
<li class="chapter" data-level="8.1" data-path="mixing.html"><a href="mixing.html#theres-only-ever-one-case"><i class="fa fa-check"></i><b>8.1</b> There’s only ever one case</a></li>
<li class="chapter" data-level="8.2" data-path="mixing.html"><a href="mixing.html#general-procedure"><i class="fa fa-check"></i><b>8.2</b> General procedure</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mixing.html"><a href="mixing.html#estimation"><i class="fa fa-check"></i><b>8.2.1</b> Estimation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mixing.html"><a href="mixing.html#illustration"><i class="fa fa-check"></i><b>8.3</b> Illustration</a></li>
<li class="chapter" data-level="8.4" data-path="mixing.html"><a href="mixing.html#illustrated-inferences"><i class="fa fa-check"></i><b>8.4</b> Illustrated inferences</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mixing.html"><a href="mixing.html#xy-model"><i class="fa fa-check"></i><b>8.4.1</b> XY model</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mixing.html"><a href="mixing.html#considerations"><i class="fa fa-check"></i><b>8.5</b> Considerations</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mixing.html"><a href="mixing.html#the-identification-problem"><i class="fa fa-check"></i><b>8.5.1</b> The identification problem</a></li>
<li class="chapter" data-level="8.5.2" data-path="mixing.html"><a href="mixing.html#continuous-data"><i class="fa fa-check"></i><b>8.5.2</b> Continuous data</a></li>
<li class="chapter" data-level="8.5.3" data-path="mixing.html"><a href="mixing.html#measurement-error"><i class="fa fa-check"></i><b>8.5.3</b> Measurement error</a></li>
<li class="chapter" data-level="8.5.4" data-path="mixing.html"><a href="mixing.html#spillovers"><i class="fa fa-check"></i><b>8.5.4</b> Spillovers</a></li>
<li class="chapter" data-level="8.5.5" data-path="mixing.html"><a href="mixing.html#clustering-and-other-violations-of-independence"><i class="fa fa-check"></i><b>8.5.5</b> Clustering and other violations of independence</a></li>
<li class="chapter" data-level="8.5.6" data-path="mixing.html"><a href="mixing.html#parameteric-models"><i class="fa fa-check"></i><b>8.5.6</b> Parameteric models</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mixing.html"><a href="mixing.html#conclusion-1"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mixingapp.html"><a href="mixingapp.html"><i class="fa fa-check"></i><b>9</b> Mixed-Method Application: Inequality and Democracy Revisited</a><ul>
<li class="chapter" data-level="9.1" data-path="mixingapp.html"><a href="mixingapp.html#a-trained-model"><i class="fa fa-check"></i><b>9.1</b> A trained model</a></li>
<li class="chapter" data-level="9.2" data-path="mixingapp.html"><a href="mixingapp.html#data"><i class="fa fa-check"></i><b>9.2</b> Data</a></li>
<li class="chapter" data-level="9.3" data-path="mixingapp.html"><a href="mixingapp.html#inference"><i class="fa fa-check"></i><b>9.3</b> Inference</a><ul>
<li class="chapter" data-level="9.3.1" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-cause-democracy"><i class="fa fa-check"></i><b>9.3.1</b> Did inequality <em>cause</em> democracy?</a></li>
<li class="chapter" data-level="9.3.2" data-path="mixingapp.html"><a href="mixingapp.html#did-inequality-prevent-democracy"><i class="fa fa-check"></i><b>9.3.2</b> Did inequality <em>prevent</em> democracy?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="mixingapp.html"><a href="mixingapp.html#prior-posterior-comparison-for-multiple-estimands"><i class="fa fa-check"></i><b>9.4</b> Prior / posterior comparison for multiple estimands</a></li>
<li class="chapter" data-level="9.5" data-path="mixingapp.html"><a href="mixingapp.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
</ul></li>
<li class="part"><span><b>III Design Choices</b></span></li>
<li class="chapter" data-level="10" data-path="elements-of-design.html"><a href="elements-of-design.html"><i class="fa fa-check"></i><b>10</b> Elements of Design</a><ul>
<li class="chapter" data-level="10.1" data-path="elements-of-design.html"><a href="elements-of-design.html#model-inquiry-data-strategy-answer-strategy"><i class="fa fa-check"></i><b>10.1</b> Model, inquiry, data strategy, answer strategy</a><ul>
<li class="chapter" data-level="10.1.1" data-path="elements-of-design.html"><a href="elements-of-design.html#defining-a-model"><i class="fa fa-check"></i><b>10.1.1</b> Defining a model</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="elements-of-design.html"><a href="elements-of-design.html#evaluating-a-design"><i class="fa fa-check"></i><b>10.2</b> Evaluating a design</a><ul>
<li class="chapter" data-level="10.2.1" data-path="elements-of-design.html"><a href="elements-of-design.html#expected-error-and-expected-posterior-variance"><i class="fa fa-check"></i><b>10.2.1</b> Expected error and expected posterior variance</a></li>
<li class="chapter" data-level="10.2.2" data-path="elements-of-design.html"><a href="elements-of-design.html#expected-variance-almost-always-goes-down"><i class="fa fa-check"></i><b>10.2.2</b> Expected variance (almost) always goes down</a></li>
<li class="chapter" data-level="10.2.3" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-1"><i class="fa fa-check"></i><b>10.2.3</b> Illustration</a></li>
<li class="chapter" data-level="10.2.4" data-path="elements-of-design.html"><a href="elements-of-design.html#other-loss-functions"><i class="fa fa-check"></i><b>10.2.4</b> Other loss functions</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="elements-of-design.html"><a href="elements-of-design.html#illustration-of-design-decaration-in-code"><i class="fa fa-check"></i><b>10.3</b> Illustration of Design Decaration in code</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="clue.html"><a href="clue.html"><i class="fa fa-check"></i><b>11</b> Clue Selection as a Decision Problem</a><ul>
<li class="chapter" data-level="11.1" data-path="clue.html"><a href="clue.html#core-logic"><i class="fa fa-check"></i><b>11.1</b> Core logic</a></li>
<li class="chapter" data-level="11.2" data-path="clue.html"><a href="clue.html#a-strategic-approach"><i class="fa fa-check"></i><b>11.2</b> A strategic approach</a><ul>
<li class="chapter" data-level="11.2.1" data-path="clue.html"><a href="clue.html#clue-selection-with-a-simple-example"><i class="fa fa-check"></i><b>11.2.1</b> Clue selection with a simple example</a></li>
<li class="chapter" data-level="11.2.2" data-path="clue.html"><a href="clue.html#dependence-on-prior-beliefs"><i class="fa fa-check"></i><b>11.2.2</b> Dependence on prior beliefs</a></li>
<li class="chapter" data-level="11.2.3" data-path="clue.html"><a href="clue.html#clue-selection-for-the-democratization-model"><i class="fa fa-check"></i><b>11.2.3</b> Clue selection for the democratization model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="clue.html"><a href="clue.html#dynamic-strategies"><i class="fa fa-check"></i><b>11.3</b> Dynamic Strategies</a></li>
<li class="chapter" data-level="11.4" data-path="clue.html"><a href="clue.html#conclusion-2"><i class="fa fa-check"></i><b>11.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="wide.html"><a href="wide.html"><i class="fa fa-check"></i><b>12</b> Going wide and going deep</a><ul>
<li class="chapter" data-level="12.1" data-path="wide.html"><a href="wide.html#motivation"><i class="fa fa-check"></i><b>12.1</b> Motivation</a></li>
<li class="chapter" data-level="12.2" data-path="wide.html"><a href="wide.html#developing-some-intuitions"><i class="fa fa-check"></i><b>12.2</b> Developing some intuitions</a></li>
<li class="chapter" data-level="12.3" data-path="wide.html"><a href="wide.html#diagnosing-mixes"><i class="fa fa-check"></i><b>12.3</b> Diagnosing mixes</a><ul>
<li class="chapter" data-level="12.3.1" data-path="wide.html"><a href="wide.html#path-model"><i class="fa fa-check"></i><b>12.3.1</b> 1-path model</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="wide.html"><a href="wide.html#evaluating-strategies"><i class="fa fa-check"></i><b>12.4</b> Evaluating strategies</a></li>
<li class="chapter" data-level="12.5" data-path="wide.html"><a href="wide.html#varieties"><i class="fa fa-check"></i><b>12.5</b> Varieties of mixing</a></li>
<li class="chapter" data-level="12.6" data-path="wide.html"><a href="wide.html#probative-value-of-clues"><i class="fa fa-check"></i><b>12.6</b> Probative value of clues</a></li>
<li class="chapter" data-level="12.7" data-path="wide.html"><a href="wide.html#effect-heterogeneity"><i class="fa fa-check"></i><b>12.7</b> Effect Heterogeneity</a></li>
<li class="chapter" data-level="12.8" data-path="wide.html"><a href="wide.html#uncertainty-regarding-assignment-processes"><i class="fa fa-check"></i><b>12.8</b> Uncertainty Regarding Assignment Processes</a></li>
<li class="chapter" data-level="12.9" data-path="wide.html"><a href="wide.html#uncertainty-regarding-the-probative-value-of-clues"><i class="fa fa-check"></i><b>12.9</b> Uncertainty regarding the probative value of clues</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="caseselection.html"><a href="caseselection.html"><i class="fa fa-check"></i><b>13</b> Case selection as a Decision Problem</a><ul>
<li class="chapter" data-level="13.1" data-path="caseselection.html"><a href="caseselection.html#logic-of-strategy-comparison"><i class="fa fa-check"></i><b>13.1</b> Logic of strategy comparison</a></li>
<li class="chapter" data-level="13.2" data-path="caseselection.html"><a href="caseselection.html#explorations"><i class="fa fa-check"></i><b>13.2</b> Explorations</a><ul>
<li class="chapter" data-level="13.2.1" data-path="caseselection.html"><a href="caseselection.html#procedure"><i class="fa fa-check"></i><b>13.2.1</b> Procedure</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="caseselection.html"><a href="caseselection.html#principles"><i class="fa fa-check"></i><b>13.3</b> Principles</a><ul>
<li class="chapter" data-level="13.3.1" data-path="caseselection.html"><a href="caseselection.html#sometimes-one-case-is-not-enough"><i class="fa fa-check"></i><b>13.3.1</b> Sometimes one case is not enough</a></li>
<li class="chapter" data-level="13.3.2" data-path="caseselection.html"><a href="caseselection.html#different-strategies-for-different-estimands"><i class="fa fa-check"></i><b>13.3.2</b> Different strategies for different estimands</a></li>
<li class="chapter" data-level="13.3.3" data-path="caseselection.html"><a href="caseselection.html#where-the-probative-value-is"><i class="fa fa-check"></i><b>13.3.3</b> Where the probative value is</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Models in Question</b></span></li>
<li class="chapter" data-level="14" data-path="justifying-models.html"><a href="justifying-models.html"><i class="fa fa-check"></i><b>14</b> Justifying models</a><ul>
<li class="chapter" data-level="14.1" data-path="justifying-models.html"><a href="justifying-models.html#bounds-on-probative-value"><i class="fa fa-check"></i><b>14.1</b> Bounds on probative value</a></li>
<li class="chapter" data-level="14.2" data-path="justifying-models.html"><a href="justifying-models.html#the-possibility-of-identification-of-probative-value-from-experimental-data"><i class="fa fa-check"></i><b>14.2</b> The possibility of identification of probative value from experimental data</a><ul>
<li class="chapter" data-level="14.2.1" data-path="justifying-models.html"><a href="justifying-models.html#mediator"><i class="fa fa-check"></i><b>14.2.1</b> Mediator</a></li>
<li class="chapter" data-level="14.2.2" data-path="justifying-models.html"><a href="justifying-models.html#moderator"><i class="fa fa-check"></i><b>14.2.2</b> Moderator</a></li>
<li class="chapter" data-level="14.2.3" data-path="justifying-models.html"><a href="justifying-models.html#case-level-bounds-from-mixed-data"><i class="fa fa-check"></i><b>14.2.3</b> Case level bounds from mixed data</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="justifying-models.html"><a href="justifying-models.html#learning-across-populations"><i class="fa fa-check"></i><b>14.3</b> Learning across populations</a></li>
<li class="chapter" data-level="14.4" data-path="justifying-models.html"><a href="justifying-models.html#different-models-for-different-sites"><i class="fa fa-check"></i><b>14.4</b> Different models for different sites</a><ul>
<li class="chapter" data-level="14.4.1" data-path="justifying-models.html"><a href="justifying-models.html#observational-and-experimental"><i class="fa fa-check"></i><b>14.4.1</b> Observational and experimental</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="justifying-models.html"><a href="justifying-models.html#causal-discovery"><i class="fa fa-check"></i><b>14.5</b> Causal discovery</a><ul>
<li class="chapter" data-level="14.5.1" data-path="justifying-models.html"><a href="justifying-models.html#a-model-of-models"><i class="fa fa-check"></i><b>14.5.1</b> A model of models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="evaluation.html"><a href="evaluation.html"><i class="fa fa-check"></i><b>15</b> Evaluating models</a><ul>
<li class="chapter" data-level="15.1" data-path="evaluation.html"><a href="evaluation.html#five-strategies"><i class="fa fa-check"></i><b>15.1</b> Five Strategies</a><ul>
<li class="chapter" data-level="15.1.1" data-path="evaluation.html"><a href="evaluation.html#check-conditional-independence"><i class="fa fa-check"></i><b>15.1.1</b> Check conditional independence</a></li>
<li class="chapter" data-level="15.1.2" data-path="evaluation.html"><a href="evaluation.html#computational-clues"><i class="fa fa-check"></i><b>15.1.2</b> Computational clues</a></li>
<li class="chapter" data-level="15.1.3" data-path="evaluation.html"><a href="evaluation.html#bayesian-p-value-is-the-data-unusual-given-your-model"><i class="fa fa-check"></i><b>15.1.3</b> Bayesian <span class="math inline">\(p\)</span> value: Is the data unusual given your model?</a></li>
<li class="chapter" data-level="15.1.4" data-path="evaluation.html"><a href="evaluation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>15.1.4</b> Leave-one-out cross-validation</a></li>
<li class="chapter" data-level="15.1.5" data-path="evaluation.html"><a href="evaluation.html#sensitivity"><i class="fa fa-check"></i><b>15.1.5</b> Sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="evaluation.html"><a href="evaluation.html#evaluating-the-democracy-inequality-model"><i class="fa fa-check"></i><b>15.2</b> Evaluating the Democracy-Inequality model</a><ul>
<li class="chapter" data-level="15.2.1" data-path="evaluation.html"><a href="evaluation.html#prior-check"><i class="fa fa-check"></i><b>15.2.1</b> Prior check</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>16</b> Final Words</a><ul>
<li class="chapter" data-level="16.1" data-path="final-words.html"><a href="final-words.html#general-lessons"><i class="fa fa-check"></i><b>16.1</b> General lessons</a></li>
<li class="chapter" data-level="16.2" data-path="final-words.html"><a href="final-words.html#worries-about-what-you-have-to-put-in"><i class="fa fa-check"></i><b>16.2</b> Worries about what you have to put in</a></li>
<li class="chapter" data-level="16.3" data-path="final-words.html"><a href="final-words.html#limits-on-what-you-can-get-out"><i class="fa fa-check"></i><b>16.3</b> Limits on what you can get out</a></li>
<li class="chapter" data-level="16.4" data-path="final-words.html"><a href="final-words.html#a-world-of-models-practical-steps-forward-for-collective-cumulation"><i class="fa fa-check"></i><b>16.4</b> A world of models: Practical steps forward for collective cumulation</a></li>
</ul></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li class="chapter" data-level="17" data-path="examplesappendix.html"><a href="examplesappendix.html"><i class="fa fa-check"></i><b>17</b> <code>gbiqq</code></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/macartan/gbiqq/" target="blank">Uses gbiqq</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Integrated Inferences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="evaluation" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> Evaluating models</h1>
<hr />
<p>Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good.</p>
<hr />
<p>Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these are almost certainly wrong. The question then is not how much you believe your model (or whether you really believe what you say you believe) but whether your model is useful is some sense. How can we evaluate the usefulness of our models?</p>
<div id="five-strategies" class="section level2">
<h2><span class="header-section-number">15.1</span> Five Strategies</h2>
<p>Sometimes a model is just not able to fit the observed data well.</p>
<p>Imagine a situation in which researchers believe that the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> runs entirely through <span class="math inline">\(M\)</span>, positing a model of the form <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span>. Imagine that, however, that the true model is <span class="math inline">\(X \rightarrow M \rightarrow Y \leftarrow X\)</span>, with <span class="math inline">\(X\)</span> <em>only</em> having a direct effect on <span class="math inline">\(Y\)</span>. The problem with the posited model, then, is that it is too restrictive: it does not allow for a direct effect that is in fact operating.</p>
<!-- AJ: "Restrictedness" seems a bit off here insofar as the "unrestricted" model is also restricted in essentially assuming no operative $X -> M$ arrow, as well as no negative direct effects.  -->
<p>We are perfectly able to update using this overly restrictive model and the data — but the updated model can produce wildly inaccurate causal inferences. In Figure <a href="#modelsch15"><strong>??</strong></a>, we show the results of an analysis in which the data are generated from a true model of the form <span class="math inline">\(X \rightarrow M \rightarrow Y \leftarrow X\)</span>, with an average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> of <span class="math inline">\(1/3\)</span> but no effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(M\)</span> or <span class="math inline">\(M\)</span> on <span class="math inline">\(Y\)</span>, and thus no indirect effect.</p>
<p>In the figure, we show the inferences on the average treatment effect for two different updated models, both starting with flat priors: the more restricted, <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model and the less restricted,
<span class="math inline">\(X \rightarrow M \rightarrow Y \leftarrow X\)</span> model. We represent the true average effect with the vertical line in each graph.</p>
<p>As we can see, the more restrictive model that excludes direct effects generates a posterior credibility interval that excludes the right answer. So, if we go into the analysis with the restricted model, we have a problem.</p>
<p>But will we notice?</p>
<p>In the remainder of this section, we explore a range of diagnostics that researchers can undertake to evaluate the usefulness of their models or to compare models with one another: checking assumptions of conditional independence built into a model; attending to computational clues; checking the model’s fit; using “leave-one-out” cross-validation; and assessing model sensitivity.</p>
<div class="figure"><span id="fig:15badmodels"></span>
<img src="ii_files/figure-html/15badmodels-1.png" alt="A restricted model yields a credibility interval that does not contain the actual average effect." width="672" />
<p class="caption">
Figure 15.1: A restricted model yields a credibility interval that does not contain the actual average effect.
</p>
</div>
<!-- * $X$ correlates with $Y$ -->
<!-- * $X$ does not correlate with $M$  -->
<!-- * $M$ does not correlate with $Y$.  -->
<!-- These data are inconsistent with the model: under this model, if $X$ doesn't cause $M$ and $M$ doesn't cause $Y$, then there is no other way for $X$ to cause $Y$.  -->
<div id="check-conditional-independence" class="section level3">
<h3><span class="header-section-number">15.1.1</span> Check conditional independence</h3>
<p>First, even before engaging in updating, we can look to see whether the data pattern is consistent with our causal model. In particular, we can check whether there are inconsistencies with the Markov condition that we introduced in Chapter 2: that every node is <em>conditionally independent</em> of its non-descendants, given its parents. In this case, if the restricted model is right, then given <span class="math inline">\(M\)</span>, <span class="math inline">\(Y\)</span> should be independent of <span class="math inline">\(X\)</span>.</p>
<p>Is it?</p>
<p>One way to check is to assess the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(M\)</span> in the data. Specifically, we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> for each value of <span class="math inline">\(M\)</span>, once for <span class="math inline">\(M=1\)</span> and again for <span class="math inline">\(M=0\)</span>; a correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> at either value of <span class="math inline">\(M\)</span> would be problematic for the conditional independence assumption embedded in the restricted model.</p>
<p>Note that this form of diagnostic test is a classical one in the frequentist sense: we start by hypothesizing that our model is correct and then ask whether the data were unlikely given the model.</p>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 15.1: </span>Regression coefficient on <span class="math inline">\(X\)</span> given <span class="math inline">\(M=0\)</span> and <span class="math inline">\(M=1\)</span></caption>
<thead>
<tr class="header">
<th align="right">M</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.177</td>
<td align="right">0.101</td>
<td align="right">0.082</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.451</td>
<td align="right">0.088</td>
<td align="right">0.000</td>
</tr>
</tbody>
</table>
<p>We report the regression coefficients on <span class="math inline">\(X\)</span> in the table below. It is immediately apparent that we have a problem. At both values of <span class="math inline">\(M\)</span>, there is a strong correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, evidence of a violation of the Markov condition implied by the restricted model.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<!-- AJ: I do not follow the sentence starting with "Intuitively" in the above. -->
<!-- ROUGH TEXT: -->
<!-- approach 2 -- say actual confound is q~=0; but model  assumes q = 0. Draw data from priors, draw data; given data type (001, 100 etc) plot (a) the posterior distribution under no confounding nad (b) the distribution of estimands that gave rise to the data.   -->
<!-- (Verma and Pearl, 1990) identify conditions under which we can check some independence assumptions. -->
<!--  Pearl (1995) gives conditions for assessing for discrete data whether $Z$ has a  direct effect on $Y$. (Involves inequalities) -->
<!-- Evans (Graphical methods for inequality constraints in marginalized DAGs) generalizes the instrumental inequality. -->
</div>
<div id="computational-clues" class="section level3">
<h3><span class="header-section-number">15.1.2</span> Computational clues</h3>
<p>Second we may be lucky and run into computation issues. In this example there is a good chance that when you run the restricted model <code>stan</code> will throw an error:</p>
<blockquote>
<p><code>Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.</code></p>
</blockquote>
<!-- AJ: Needs elaboration: why is this error a function of problems with the model? -->
<!-- MH: I agree; do feel free to look into it; I will try -->
<!-- AJ: I have done some digging and not found anything helpful on this -->
<!-- MH: Well that's just great -->
</div>
<div id="bayesian-p-value-is-the-data-unusual-given-your-model" class="section level3">
<h3><span class="header-section-number">15.1.3</span> Bayesian <span class="math inline">\(p\)</span> value: Is the data unusual given your model?</h3>
<p>A third approach asks whether features of the data you observe are in some sense unusual given your model. For instance if a model assumed no adverse effects of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> and no confounding, then a strong negative correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would be unusual even for the model updated with this data.</p>
<p>In fact this approach is quite classical: we are looking to see whether we should “reject” our model in light of inconistent data.</p>
<p>An approach for doing this using simulated data from the posterior predictive distribution is described in <span class="citation">Gabry et al. (<a href="#ref-gabry2019visualization">2019</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>We consider two test statistics. First we look just at the distribution of the outcome <span class="math inline">\(Y\)</span> to see how the actual distribution compares to the predicted distribution, second we look at the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and see how this compares to the predicted distribution.</p>
<p>We see here the the distribution of the outcome <span class="math inline">\(Y\)</span> is similar for both models, and the actual mean outcome is within the distribution of predicted mean outcomes. However the predicted correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is very different. The restricted model does not in fact predict any correlation and the strong correlation in the data is unusual. The unrestricted model does predict a strong correlation and the observed correlation is comfortably in the normal range. A frequentist looking at the observed correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> should feel comfortable rejecting the restricted model.</p>
<p><img src="ii_files/figure-html/ch15simulations-1.png" width="672" /></p>
<!-- ## Model likelihoods -->
<!-- Bayesian information criterion (BIC)^[$BIC = \ln(n)k - 2\ln(\hat{L})$ where $\hat{L}$ is the maximized likelhood, $k$ is the number of parameters, and $n$ the number of data points.] -->
<!-- BIC involves a penalty for more parameters -->
<!-- ```{r, echo = FALSE} -->
<!-- if(do_diagnosis) { -->
<!-- observed_data <- collapse_data(data, restricted_model) -->
<!-- likely <- function(model, s, posterior = TRUE) { -->
<!--   if(posterior)  pars <- model$posterior_distribution[1:s,] -->
<!--   if(!posterior) pars <- get_prior_distribution(model, s)[1:s,] -->
<!--   L <- apply(pars, 1,  function(par) make_data_probabilities(model, pars = par, observed_data, normalize = FALSE)) -->
<!--   mean(L) -->
<!--   } -->
<!-- s <- 500 -->
<!-- L_prior         <- likely(restricted_model, s, posterior = FALSE) -->
<!-- L_restricted    <- likely(restricted_model, s) -->
<!-- L_unrestricted  <- likely(unrestricted_model, s) -->
<!-- df <- data.frame(L = (c(L_unrestricted/L_prior, L_restricted/L_prior, L_unrestricted/L_restricted))) -->
<!-- rownames(df) <- c("Unrestricted / Prior", "Restricted / Prior", "Unrestricted / Restricted") -->
<!-- write_rds(df, "saved/ch15_likelihoodsdf.rds")  -->
<!-- } -->
<!-- df <- read_rds("saved/ch15_likelihoodsdf.rds")  -->
<!-- kable(df, col.names = "Bayes factors", digits = 2, caption = "Posterior odds: the relative likelihood of one model over another") -->
<!-- ``` -->
<!-- Compare likelihoods of the data under different models -->
<!-- Check look package for rstan -->
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">15.1.4</span> Leave-one-out cross-validation</h3>
<p>A further class of model-validation methods involves cross-validation. Rather than asking how well the updated model predicts the data used to update it, cross-validation uses the data at hand to estimate how well the model is likely to predict actual data that have not yet been seen.</p>
<p>One approach to cross-validation is the “leave-one-out” (LOO) algorithm. In a LOO approach, we update the model using all data points except for one and then ask how well the model performs in predicting the left-out observation. We repeat this for every data point in the dataset to assess how well we can predict the entire data set.</p>
<p>Often this approach is used to predict a particular outcome variable. In a  framework, however, we are interested in predictions over the joint realization of all nodes, not just a single “outcome.” Thus, we calculate the posterior probability of each data point, using the model updated with all of the other observations.</p>
<p><img src="ii_files/figure-html/unnamed-chunk-2-1.png" width="672" /><img src="ii_files/figure-html/unnamed-chunk-2-2.png" width="672" /><img src="ii_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<p>The LOO estimate of out-of-sample predictive fit, for a dataset with <span class="math inline">\(n\)</span> observations, is then:</p>
<p><span class="math display">\[\prod_1^np(y_i|y_{-i}, \text{model})\]</span>
where <span class="math inline">\(y_{-i}\)</span> is the data pattern with observation <span class="math inline">\(y_i\)</span> left out, and <span class="math inline">\(y_i\)</span> represents the values of all nodes of interest for observation <span class="math inline">\(i\)</span>.</p>
<p>First, we implement LOO cross-validation of the two models using 200 data points generated from the unrestricted model. The likelihood of the data under the restricted model is 1.64e-182
the likelihood if 4.33e-175 under the unrestricted models. Thus, the unrestricted model represents an estimated improvement in out-of-sample prediction on the order of 2.64e+07.</p>
<p>We can visualize the pattern in Figure XXXX, where we plot the likelihood of each possible data type under the restricted model against the likelihood of that data type under the unrestricted model. Looking at the scales of the two axes—which is much more compressed on the horizontal than on the vertical—one can see that the restricted model is not able to differentiate as much across the data types as the unrestricted.</p>
<!-- AJ: What are these likelihoods exactly? They are informed by updating, I assume? But updating using what data exactly -- what's left out? Generally unsure if my text above is right.-->
<p>Notably, the restricted model is not able to “learn” from the data about the (<em>in fact</em>, operative) relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We can see that, for any given <span class="math inline">\(X, M\)</span> combination, the two possible values of <span class="math inline">\(Y\)</span> are predicted with essentially the same likelihood. The restricted model also seems to have “learned” that different values <span class="math inline">\(X,M\)</span> combinations are differentially likely, even though they are not under the true model. The unrestricted model, on the other hand, essentially divides the data types into two groups: those with a positive <span class="math inline">\(X,Y\)</span> correlation and those with a negative <span class="math inline">\(X,Y\)</span> correlation and has correctly (given the true model) learned that the former is more likely than the latter.</p>
<p>In Figure XXXX, we then see how the likelihoods of each data type line up with the actual count of each data type. As we can see, the unrestricted model updates to likelihoods that fit the actual data pattern well while the restricted model does not.</p>
<!-- AJ: The x-axis labels confuse me. What's the meaning of a data count for a restricted or unrestricted model? The data are what they are.  -->
<p><img src="ii_files/figure-html/unnamed-chunk-3-1.png" width="672" /><img src="ii_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<!-- ALS DO WITH CONFOUNDING X<-->
<p>Y X-&gt;M-&gt;Y –&gt;</p>
<p>Second, we implement LOO cross-validation of the two models using 200 data points generated from the restricted model. The likelihood of the data under the restricted model is now 3.57e-119, compared to
the likelihood of 1.04e-124 under the unrestricted models. Thus, the unrestricted model represents an estimated improvement in out-of-sample prediction on the order of 2.91e-06.</p>
</div>
<div id="sensitivity" class="section level3">
<h3><span class="header-section-number">15.1.5</span> Sensitivity</h3>
<p>The last approach we consider asks: how much do your conclusions depend on your assumptions? You can worry less about model assumptions if the conclusions are not strongly dependent on them.</p>
<p>To illustrate using a process tracing example, consider a case in which you are unsure about posited parameter values. Say for instance we have an <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model. Say we think it is unlikely that <span class="math inline">\(M\)</span> has an adverse effect on <span class="math inline">\(Y\)</span>, but we are not sure how unlikely this is. We assume all other types are equally likely. How much does our updating on <span class="math inline">\(X\)</span> causing <span class="math inline">\(Y\)</span> when we see <span class="math inline">\(M=1\)</span> or <span class="math inline">\(M=1\)</span> depend on this second stage assumption?</p>
<p>We answer the question by looking at the inferences we would make for a range of possible values. We give an example in Table REF. We see here that our inference in the case that <span class="math inline">\(M=1\)</span> does <em>not</em> depend on these beliefs about adverse effects. The reason is that in this case we only need to distinguish between cases with positive effects at both stages and other cases that yield <span class="math inline">\(M=1\)</span>. Our inferences hen <span class="math inline">\(M=0\)</span> do depend on this however since in this case we are trying to assess specifically the share of cases with a positive effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> that operate via negative effects at each stage.</p>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 15.2: </span>Inferences on the probability that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> upon seeing <span class="math inline">\(M=0\)</span> or <span class="math inline">\(M=1\)</span> for a range of possible values of <span class="math inline">\(\lambda^Y_{10}\)</span></caption>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(\lambda^Y_{10}\)</span></th>
<th align="right">Prior</th>
<th align="right"><span class="math inline">\(M=0\)</span></th>
<th align="right"><span class="math inline">\(M=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.00</td>
<td align="right">0.167</td>
<td align="right">0.000</td>
<td align="right">0.25</td>
</tr>
<tr class="even">
<td align="right">0.05</td>
<td align="right">0.183</td>
<td align="right">0.068</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td align="right">0.10</td>
<td align="right">0.200</td>
<td align="right">0.125</td>
<td align="right">0.25</td>
</tr>
<tr class="even">
<td align="right">0.15</td>
<td align="right">0.217</td>
<td align="right">0.173</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td align="right">0.20</td>
<td align="right">0.233</td>
<td align="right">0.214</td>
<td align="right">0.25</td>
</tr>
<tr class="even">
<td align="right">0.25</td>
<td align="right">0.250</td>
<td align="right">0.250</td>
<td align="right">0.25</td>
</tr>
</tbody>
</table>
<p>The <em>amount</em> we update when <span class="math inline">\(M=1\)</span> does depend on <span class="math inline">\(\lambda^Y_{10}\)</span> because <span class="math inline">\(\lambda^Y_{10}\)</span> affects our belief, prior to seeing <span class="math inline">\(M\)</span>, that <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span>. However the conclusion we reach given <span class="math inline">\(M\)</span> is observed, does not depend on <span class="math inline">\(\lambda^Y_{10}\)</span>.</p>
<p>In the same way we can ask how our conclusions change if we relax assumptions on restrictions, on confounds, or on causal structure.</p>
<p>We note that in cases in which you cannot quantify uncertainty about parameters you might still be able to engage in a form of “qualitative inference.” There is a literature on probabilistic causal models that assesses the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see <span class="citation">Parsons (<a href="#ref-parsons2001qualitative">2001</a>)</span>. Under this kind of approach, a researcher might willing to say that they think some probability <span class="math inline">\(p\)</span> is not plausibly greater than .5, but unwilling to make a statement about their beliefs about where in the <span class="math inline">\(0\)</span> to <span class="math inline">\(0.5\)</span> range it lies. Such incomplete statements can be enough to rule our classes of conclusion.</p>
</div>
</div>
<div id="evaluating-the-democracy-inequality-model" class="section level2">
<h2><span class="header-section-number">15.2</span> Evaluating the Democracy-Inequality model</h2>
<p>** GENERATE A TABLE SHOWING HOW PIMD DOES ON 5 CRITEREA **</p>
<div id="prior-check" class="section level3">
<h3><span class="header-section-number">15.2.1</span> Prior check</h3>
<p>In a second iteration of the analysis, we show what happens if we loosen the monotonicity restriction on <span class="math inline">\(I\)</span>’s effect on <span class="math inline">\(M\)</span>. Here we consider negative effects of <span class="math inline">\(I\)</span> on <span class="math inline">\(M\)</span> <em>unlikely</em>, rather than impossible, and we consider null and positive effects somewhat likely. We refer to these priors as “quantitative priors” in the sense that they place a numerical value on beliefs rather than a logical restriction. Here, we set our prior on <span class="math inline">\(\theta^M\)</span> as: <span class="math inline">\(p(\theta^M=\theta^M_{10})=0.1\)</span>, <span class="math inline">\(p(\theta^M=\theta^M_{00})=0.25\)</span>, <span class="math inline">\(p(\theta^M=\theta^M_{11})=0.25\)</span>, and <span class="math inline">\(p(\theta^M=\theta^M_{01})=0.4\)</span>. We show the results for the inferences given different findings in tables  and . The mapping into expected posterior variance associated with each strategy is shown by the numbers in parentheses in Table .</p>
<p>The results differ in various modest ways. However, the biggest difference we observe is in the degree to which the mobilization clue matters when we are looking for negative effects of inequality. As discussed, if we assumed monotonic positive effects of inequality on mobilization and monotonic positive effects of mobilization on inequality, then the mediator clue is uninformative about the indirect pathway since that pathway can only generate a positive effect. However, if we allow for the possibility of a negative effect of inequality on mobilization, we now make <span class="math inline">\(M\)</span> informative as a mediator even when the effect of inequality that we are interested in is negative: it is now possible that inequality has a negative effect on democratization via a negative effect on mobilization, followed by a positive effect of mobilization on democratization. So now, observing whether mobilization occurred adds information about whether a negative effect could have occurred via the mobilization pathway.</p>
<p>Moreover, it is possible for the two effects of observing <span class="math inline">\(M\)</span> on our beliefs to work in opposite ways. What we learn from observing <span class="math inline">\(M\)</span> about the <span class="math inline">\(I \rightarrow M \rightarrow D\)</span> pathway may push in a different direction from what we learn from observing <span class="math inline">\(M\)</span> about the direct <span class="math inline">\(I \rightarrow D\)</span> pathway. We see this dynamic at work in a case with low inequality and democratization. Where we are only learning about <span class="math inline">\(M\)</span> as a moderator of <span class="math inline">\(I\)</span>’s direct effect (monotonicity assumption in place), observing <span class="math inline">\(M=0\)</span> shifts our beliefs in favor of <span class="math inline">\(I\)</span>’s negative effect. But where we are learning about <span class="math inline">\(M\)</span> as both mediator and moderator, observing <span class="math inline">\(M=0\)</span> shifts our beliefs <em>against</em> <span class="math inline">\(I\)</span>’s negative effect. The reason for this latter result is straightforward: if <span class="math inline">\(I=0\)</span> and we then see <span class="math inline">\(M=0\)</span>, then we have just learned that inequality’s possible indirect negative effect, running via the mobilization pathway, has <em>not</em> in fact occurred; and this has a considerable downward effect on our beliefs in an overall negative effect of inequality. This learning outweighs the small positive impact of observing <span class="math inline">\(M=0\)</span> on our confidence that <span class="math inline">\(I\)</span> had a direct negative effect on <span class="math inline">\(D\)</span>.</p>
<p>We see these differences most clearly in the cases of Albania (as compared to Mexico) and Nicaragua (as compared to Taiwan). Under priors fully constrained to monotonic causal effects, we saw that the mediator clue, <span class="math inline">\(M\)</span>, made only a small difference to our inferences. However, if we allow for a negative effect of <span class="math inline">\(I\)</span> on <span class="math inline">\(M\)</span>, even while believing it to be unlikely, observing mobilization in Albania and Nicaragua makes us substantially more confident that inequality mattered, and differentiates our conclusions about these cases more sharply from our conclusions about Mexico and Taiwan, respectively.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-gabry2019visualization">
<p>Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 182 (2): 389–402.</p>
</div>
<div id="ref-parsons2001qualitative">
<p>Parsons, Simon. 2001. <em>Qualitative Methods for Reasoning Under Uncertainty</em>. Vol. 13. Mit Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>In applying the Markov condition, we also need to take into account any unobserved confounding. For instance, suppose that there was an unobserved confounder of the relationship between <span class="math inline">\(M\)</span> and <span class="math inline">\(Y\)</span> in the <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model. Then we would <em>not</em> expect <span class="math inline">\(Y\)</span> to be independent of <span class="math inline">\(X\)</span> conditional on <span class="math inline">\(M\)</span>. Intuitively, the data pattern we described could be consistent with such a model in which on average <span class="math inline">\(X\)</span> does not cause <span class="math inline">\(M\)</span> but still <em>in those cases in which <span class="math inline">\(X=M\)</span></em> we have <span class="math inline">\(Y=X\)</span>. Another way to think about this is that <span class="math inline">\(M\)</span> now acts as a collider between <span class="math inline">\(X\)</span> and another unobserved cause of <span class="math inline">\(Y\)</span>; so conditioning on <span class="math inline">\(M\)</span> introduces a correlation between <span class="math inline">\(X\)</span> and this unobserved cause, and thus between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.<a href="evaluation.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Tools in the <code>bayesplot</code> package can be used to show how typical the data we observe is for different models<a href="evaluation.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="justifying-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="final-words.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ii.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
