# Case selection as a Decision Problem {#caseselection}

```{r packagesused13, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
With a causal model in hand, together with priors over parameters, you can assess in advance what conclusions you will draw from different observations and assess what kinds of observations are most worth seeking. We draw out the implications of this idea for case selection.
::::
<br>


A critical decision for scholars employing mixed methods is to determine which cases are most valuable for within-case analysis.  

A host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of $X$, $Y$ data. Perhaps the most common strategy is to select cases in which $X=1$ and $Y=1$ and look to see whether in fact $X$ caused $Y$ in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases "on the regression line" or, for some purposes, cases "off the regression line" (e.g., @Lieberman2005nested). Some scholars suggest ensuring variation in $X$ (most prominently, @king1994designing), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on $X$ or $Y$ (@FL2008, @HerronQuinn). 

Which cases you should choose will likely depend on the purposes to which you want to put them. 

A matching strategy for instance---selecting cases that are comparable on many features but that differ on $X$---replicates at a small scale the kind of inference done by matching estimators with large-$n$ data. The strategy emphasize the inferences to be made from $X,Y$ variation rather than inferences drawn specifically from within case information beyond what is available in the measurement of $X$ and $Y$. (Citations needed.)

Other treatments seek to use qualitative information to check assumptions made in $X, Y$ analysis: for example, is the measurement of $X$ and $Y$ reliable in critical cases? (Citations needed) For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the $X,Y$ inferences: for example influential cases that have unusually extreme values on $X$ and $Y$.^[Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.] Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims.

A third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman's "model-building" mode of "nested analysis" (@Lieberman2005nested). Here it may be cases off the regression line that are of interest.

Weller and Barnes (CITE article) on case selection focus on (a) X/Y relations and (b) whether the cases are useful for hypothesis generation. 

In what follows, we focus on a simpler goal: given existing $X, Y$ data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of $X$ on $Y$?

The basic insight of this chapter is simple enough: *the optimal strategy for case selection for a model-based analysis can be determined by the model and the query*, just as we saw for the optimal clue-selection strategy in Chapter \@ref(Clues). Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as:

* go where the probative value is, and
* sample from $X$ and $Y$ values in proportion to their occurrence in the population,
* invest in collections of cases that provide complementary learning. 

Beyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with $X, Y, K$ data.


Most closely related to our analysis in this chapter is the contribution of @HerronQuinn, who build on @SeawrightGerring2008. While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches.  As we do, @HerronQuinn focus on a situation with binary $X,Y$ data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, $Z_i$  is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to "treatment", or an $X=1$ value). @HerronQuinn assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly.  

Our setup differs from that in @HerronQuinn in a few ways.  @HerronQuinn paramaterize differently, though this difference is not important.^[@HerronQuinn have a parameter $\theta$ that governs the distribution of data over $X$ and $Y$ and then, conditional on $X,Y$ values, a set of parameters $\psi_{xy}$ that describe the probability of a case's being of a given causal type. We take both $\theta$ and $\psi_{xy}$ to derive from the fundamental distribution of causal types and assignment probabilities. Thus, for example, $\psi_{00}$ from @HerronQuinn corresponds to $\frac{(1-\pi_b)\lambda_b}{(1-\pi_b)\lambda_b + (1-\pi_c)\lambda_c}$ in our notation. The difference in  paramaterization does have implications for interpretations of the priors. For example flat priors over $\theta$ and $\psi$ implies a tighter distribution that a uniform prior over the causal types. In fact @HerronQuinn use priors with greater variance than uniform in any event.] Perhaps the most important difference between our analysis and that in @HerronQuinn  is that we connect the inference strategy to process-tracing approaches. Whereas @HerronQuinn assume that causal types can be read directly, we assume that these are inferred *imperfectly* from clues. As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of $X$. And, as in the baseline model, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases.

<!-- Are Herron and Quinn's priors Jeffrey priors? -->

Here we assume that the case selection decision is made after observing the $XY$ distribution and we explore a range of different possible contingency tables. In  @HerronQuinn the distribution from which the contingency tables are drawn is fixed, though set to  exhibit an expected  observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large $XY$ data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases.

Another important difference, is that in many of their analyses, @HerronQuinn take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has *beliefs* about the true treatment effect that correspond to their priors, and for whom there is therefore no *expected* bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher's beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)?

Finally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by @SeawrightGerring2008) and evaluating them, we define a strategy as the particular distribution over $XY$ cells to be examined and proceed to examine *every possible strategy* given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies---those strategies that perform similarly---emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies.

Despite these various differences, our results will agree in key ways with those in @HerronQuinn.


## Case selection logics depends on probative value and queries

Although it might be tempting to seek general rules of the form "examine cases in which $X=1$ and $Y=1$" or, "ignore cases in which $X=0$ and $Y=0$", it is easily demonstrated that which choices are more informative depend on how waht can be learned within a case contributes to infernecnce, and that hte contributions depend on the query in question.

Say we know in a given population that:

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = 1$
  * $\Pr(Y=1|X=1, K = 0) = .5$
  * $\Pr(Y=1|X=0, K = 1) = 0$
  * $\Pr(Y=1|X=1, K = 1) = .9$

We do not know, however how common $K$ is. Thus we do not know either unit level causal effects or population level effects.

Given the information above we can see that if $X=Y=1$, then $K$ is a "doubly decisive" clue for assessing whether, in a given case, $X$ causes $Y$. In particular we have that for an $X=Y=1$ case, seeing $K=1$ implies  $X$ caused $Y$ Since otherwise $Y$ would have been 0) and seeing $K=0$ implies   $X$ did not cause $Y$ (since othereise $Y$ would have been 1 in any event.

However, if we had a case in which $X=Y=0$ then learning $K$ would be entirely uninformative for the case. In particular, we already know that $K=1$ in this case as we know there are not cases in which  $X=Y=0$ and $K=0$. And this knowledge is not enough in this case to know whether $X=0$ caused $Y=0$.

For the same reason, if, off the diagonal, $X=0, Y=1$ we learn nothing from $K$, since we learn about $K$ from $Y$. If however we had a case in which $X=1, Y=0$, if $K$ were 1 then we would know that $X=1$ did not cause $Y=0$; if $K$ were 0 we would know that  $X=1$ caused $Y=1$.


```{r, eval= FALSE, echo = FALSE}
py_k0 = .5
py_k1 = .9

py_k1 / (py_k0 + py_k1)

(1-py_k1) / (1-py_k0 + 1-py_k1)

```

Say now we were interested in the population estimand: the average effect of $X$ on $Y$. We can see that this is equal to $\Pr(K=1)\times.9 + (1-\Pr(K-0))\times(-.5)) = 1.4\Pr(K=1)-.5$. For this estimand we are only interested in the prevalence of $K$ in the population. It might seem that this means that it is irrelevant what type of case you choose; however, as noted above, we have more information about the likely value of $K$ in some cases than in others. So for this estimand also, in this case, selecting an $X=Y=1$ case  is informative for the population effect; selecting $X=Y=0$ case is not informative. 

Say now we had a case in which $X=1$. Should we choose: a case in which $Y=1$ or a case in which $Y=0$. In both cases $K$ is doubly decisive for the case level estimand. However for a $Y=1$ case we think it likely that $K=1$ (specifically, assuming a prior of $\Pr(K=1)$ we think $\Pr(K=1 | X=1, Y=1) = \frac{.9}{.9+.5}=.64$); for the  $Y=0$ case we think  $\Pr(K=1 | X=1, Y=0) = \frac{.1}{.5+.1}=.17$. Thus we are much more uncertain about the value of $K$ in the $X=Y=1$ case. We would learn more for the avergae treatment effect then by choosing off the diagonal.

Specifically let $\kappa$ denote $\Pr(K=1)$. say we begin thinking it equally likely that $\kappa=\kappa^H = .5$ and $\kappa=\kappa^L=0$. Say $\Pr(X=1) = .5$. Say we observe one case with $X=Y=1$ and another with $X=1, Y=0$. From that informaation alone we can update over $\kappa$. Specifically, conditioning on the probability that $X=1$ for both cases and focuseing only on the probability that Y=0 in one case and Y=1 in the other:

$$p(\kappa = \kappa^L|D) =  \frac{p(D|\kappa^H)}{p(D|\kappa^H)+p(D|\kappa^L)}=\frac{.5}{.5 + .25\times(2\times.5\times.5 + 2\times.9\times.1 + 2\times(.9\times.5 +.1\times.5))}$$



In summary in this case it makes sense to select and $X=Y=1$ case if you are interested in teh population estimand or if you are interested in teh case level estimand for an $X=Y=1$ case. If you are interested in the case level estimand for an $X=Y=0$ case then there are no gains from either selection strategy.

Sometimes however you learn more in a $X=Y=0$ than in a  $X=Y=1$ case. Say instead that you knew that: 

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = .5$
  * $\Pr(Y=1|X=1, K = 0) = 0$
  * $\Pr(Y=1|X=0, K = 1) = .5$
  * $\Pr(Y=1|X=1, K = 1) = 1$

In this case you learn nothing from observing a case in which $X=Y=1$---in this case you already expect that $K=1$. In contrast if $X=Y=0$ then if you learn that $K=1$ you know that were $X=1$ the $Y$ would have been 1; but if you learn that $K=0$ you know that *were* $X=1$ then $Y$ would have (still) been 0. So in this case $K$ is doubly decisive for an $X=Y=0$ case but not for a   $X=Y=1$ case. 


This logic also holds for the off diagonals.

Say instead that you knew that: 

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = 0$
  * $\Pr(Y=1|X=1, K = 0) = .5$
  * $\Pr(Y=1|X=0, K = 1) = 1$
  * $\Pr(Y=1|X=1, K = 1) = .5$

Say you have a case in which $X=1, Y=0$, then if $K=1$ you know that if $X$ were 0, Y would have been 1; but if $K=0$ you know that if $X$ were 0, $Y$ would have been 0. In that case $K$ would be doubly decisive for $X=1$ causing $Y=0$.  

Say you have a case in which $X=0, Y=1$, then it must be that $K=0$.   
  
Say instead that you knew that: 

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = .5$
  * $\Pr(Y=1|X=1, K = 0) = 0$
  * $\Pr(Y=1|X=0, K = 1) = .5$
  * $\Pr(Y=1|X=1, K = 1) = 1$

It may be better to select a case that is not "like" the cases you want to make inferences about.


Which cases to learn $K$ from.


## Logic of strategy comparison

```{r selectioncode, echo = FALSE}

# We define a model
model <- make_model("X->M->Y")  %>%
 set_restrictions(c("(Y[M=1]<Y[M=0])", "(M[X=1]<M[X=0])")) %>%
 set_parameter_matrix() %>%
 set_parameters(type = "flat")

# We imagine some preexisting data we have observed
data  <-  data.frame(X = c(0,0,0,1,1,1), M = NA, Y = c(0,0,1,0,1,1))
observed <-  collapse_data(data, model, drop_family = TRUE)

# We can then imagine what data we might observe if we examine M inside some subset of cases
A_on_regression_line <- 
  make_possible_data(
    model, 
    observed = observed, vars = "M", 
    N = c(1,1), 
    condition = c("X==1 & Y==1", "X==0 & Y==0"), 
    prefix = "A")
names(A_on_regression_line)[-c(1:2)] <- c("A1", "A2", "A3", "A4")

B_off_regression_line <- 
  make_possible_data(model, observed = observed, vars = "M", N = c(1,1), 
                     condition = c("X==1 & Y==0", "X==0 & Y==1"), prefix = "B")
names(B_off_regression_line)[-c(1:2)] <- c("B1", "B2", "B3", "B4")

C_X1Y1_only <- 
  make_possible_data(model, observed = observed, vars = "M", N = 2, 
                     condition = c("X==1 & Y==1"), prefix = "C")
names(C_X1Y1_only)[-c(1:2)] <- c("C1", "C2", "C3")

```

Consider a situation in which one has access to $X,Y$ data on just six cases of the form:

```{r observed, echo = FALSE}
kable(observed)
```

We want to examine data on $M$ for two of these cases and are wondering about what strategy we should use to select the cases. We are considering three strategies:

* Strategy $A$ chooses two cases on the regression line (one in the $X=Y=0$ cell and one in the $X=Y=1$ cell)
* Strategy $B$ chooses off the regression line 
* Strategy $C$ selects on the dependent variable -- choosing cases with $X=1, Y = 1$.

Different strategies yield different possible types of data. Each one of these is likely to arise with a different probability and is associated with a different inference. Drawing on the prior beliefs embedded in a causal model, we can thus assess the *expected* posterior variance associated with each strategy. 


```{r dtypes, echo = FALSE}
# Combine data types
df <- A_on_regression_line[,-2] %>% 
  merge(B_off_regression_line[,-2], by = "event", all = TRUE) %>%
  merge(C_X1Y1_only[,-2], by = "event", all = TRUE) %>%
  dplyr:::mutate_if(is.numeric, ~replace(., is.na(.), 0))

# table(duplicated(t(df)))
```

For each of these possible strategies we can assess the posterior that we would obtain for each type of data we might observe.

```{r illustration, echo = FALSE}

if(do_diagnosis){
  
 if(!exists("fit")) fit  <- update_model(make_model("X->Y"), keep_fit = TRUE)
 
 strategies <- 
   list(A_on_regression_line, B_off_regression_line, C_X1Y1_only)

 	write_rds(strategies, "saved/illustration_strategies.rds")
  
 
 estimates_dbs <- lapply(strategies, function(s) 
    make_estimates_database(model, observed, possible_data = s[, -2], queries = "Y[X=1]-Y[X=0]")
    )
    
	write_rds(estimates_dbs, "saved/illustration_estimates.rds")

# The reference model is the original model updated with the XY data we've already seen.
 reference_model <- update_model(model, data, stan_model = fit)
 
	write_rds(reference_model, "saved/reference_XMY_updated_model.rds")

 probabilities <- lapply(strategies, function(s) 
    average_data_probabilities(reference_model, s, using = "posteriors", sims = 3000)
    )
   
	write_rds(probabilities, "saved/illustration_probabilities.rds")

}

  reference_model <- read_rds("saved/reference_XMY_updated_model.rds")
	probabilities   <- read_rds("saved/illustration_probabilities.rds")
  strategies      <- read_rds("saved/illustration_strategies.rds")
  estimates_db    <- read_rds("saved/illustration_estimates.rds")
  
  digits <- 3

  df <- rbind(df, 
          c("Probability", round(unlist(probabilities),digits)),
          c("Posterior mean",   (unlist(lapply(estimates_db, function(j)   unlist(unlist(lapply(j, function(k) k$mean))))))),
          c("Posterior variance",   round(unlist(lapply(estimates_db, function(j)   unlist(unlist(lapply(j, function(k) k$mean^2))))), digits)))
```

The data possibilities, probabilities of each, and inferences given each, are shown in Table \@ref(tab:chselillustration).
  
```{r chselillustration, echo = FALSE}  
kable(df, caption = "Each column shows a possible distribution of data that can be generated from a given strategy. We calculate the probability of each data possibility, given the data seen so far, and the posterior variance associated with each one.", digits = 2)
```

Each of the first two strategies generates one of four different data patterns. The third data strategy generates up to three data patterns. None of these data patterns overlap across strategies. 
From the calculated  probability of each data type, given the data seen so far, and the posterior variance given each data realization, the implied *expected* variance is  easily calculated. These are summarized below:

```{r exppostvar, echo = FALSE}
expected_posterior_var <- sapply(1:3, function(i) 
  sum(probabilities[[i]]*unlist(lapply(estimates_db[[i]], function(x) x$sd^2)))
 )
kable(data.frame(Strategy = c("Online", "Offline", "X=1, Y=1"), Variance = expected_posterior_var), digits = 3)
```


In this example, we see that we would expect to be better off---in the sense of having less posterior uncertainty---by focusing her process-tracing efforts where a greater share of the population of cases lies: on the regression line.  

Why is this?


```{r test, include = FALSE}
# source("_packages_used.R")

model <- make_model("X -> M -> Y")
  
gen_observed_data <- function(n00=1, n01=1, n10=1, n11=1,times = 1) {
  data.frame(X = c(0,0,1,1), Y =c(0,1,0,1), M = NA, ns = c(n00, n01, n10, n11)*times) %>%
    tidyr::uncount(ns) %>%
    collapse_data(model, drop_family = TRUE)}


# Data strategies

data_strats <- function(N = 4) {
  if(N%%2 != 0) stop("even Ns only please")
  list(
    all_on = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==0 & Y==0", "X==1 & Y==1")),
    all_off = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==0 & Y==1", "X==1 & Y==0")),
    all_x1 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==1 & Y==0", "X==1 & Y==1")),
    all_y1 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==0 & Y==1", "X==1 & Y==1"))
  )
}

# Queries

# if(!exists("fit")) fit <- fitted_model()

# Diagnosis function 

if(do_diagnosis){
test_corr <- 
  diagnose_strategies(
    analysis_model = model,
    data_strategies = data_strats(2),
    given = gen_observed_data(6,2,2,6),
    queries = list(ATE = "Y[X=1]-Y[X=0]", 
                   a = "Y[X=1]<Y[X=0]", 
                   b = "Y[X=1]>Y[X=0]", 
                   c = "(Y[X=1]==0) & (Y[X=0]==0)", 
                   d = "(Y[X=1]==1) & (Y[X=0]==1)",
                   a1 = "M[X=1]<M[X=0]", 
                   b1 = "M[X=1]>M[X=0]", 
                   c1 = "(M[X=1]==0) & (M[X=0]==0)", 
                   d1 = "(M[X=1]==1) & (M[X=0]==1)",
                   a2 = "Y[M=1]<Y[M=0]", 
                   b2 = "Y[M=1]>Y[M=0]", 
                   c2 = "(Y[M=1]==0) & (Y[M=0]==0)", 
                   d2 = "(Y[M=1]==1) & (Y[M=0]==1)"
                   ),
    subsets = list(TRUE, TRUE,  TRUE,  TRUE),
    sims = 8000,
    fit = fit, 
    refresh = 1000,
    iter = 8000)

write_rds(test_corr, "saved/test_corr.rds")

test_flat <- 
  diagnose_strategies(
    analysis_model = model,
    data_strategies = data_strats(2),
    given = gen_observed_data(4,4,4,4),
    queries = list(ATE = "Y[X=1]-Y[X=0]", 
                   a = "Y[X=1]<Y[X=0]", 
                   b = "Y[X=1]>Y[X=0]", 
                   c = "(Y[X=1]==0) & (Y[X=0]==0)", 
                   d = "(Y[X=1]==1) & (Y[X=0]==1)",
                   a1 = "M[X=1]<M[X=0]", 
                   b1 = "M[X=1]>M[X=0]", 
                   c1 = "(M[X=1]==0) & (M[X=0]==0)", 
                   d1 = "(M[X=1]==1) & (M[X=0]==1)",
                   a2 = "Y[M=1]<Y[M=0]", 
                   b2 = "Y[M=1]>Y[M=0]", 
                   c2 = "(Y[M=1]==0) & (Y[M=0]==0)", 
                   d2 = "(Y[M=1]==1) & (Y[M=0]==1)"
                   ),
    subsets = list(TRUE, TRUE,  TRUE,  TRUE),
    sims = 8000, fit = fit, refresh = 1000, iter = 8000)


write_rds(test_flat, "saved/test_flat.rds") 
}

test_corr <- read_rds("saved/test_corr.rds") 
test_flat <- read_rds("saved/test_flat.rds") 

# key thing to note is for $all_x1[[1]] a2 = b2
# and for $all_x1[[2]] a1 = b1 
# so in both cases shareas of a and b remains contstant


```

For intuition consider first the case with flat prior data:

* if we examine two cases in different quadrants *on the diagonal* (one $X=Y=0$ case and one $X=Y=1$ case) :
  * if  we find that $M$ is the same in the two cases then we  increase our belief that $X$ has no effect at all on $Y$ and reduce our confidence that $X$ had a positive or a negative effect on $Y$. Since we are looking on the regression line however, our confidence that $X$ had a *positive* effect on $Y$ is more strongly reduced, producing a  posterior centered on a small negative effect. 
  * Conversely if we see that $M$ is different in the two cases, then we have a correlation of the same sign between both $X$ and $M$ and between $M$ and $Y$  cases. We increase our confidence that $X$ mattered for $Y$ in general an din particular that it had a positive effect, resulting in a posterior centered on a small positive ATE.

* if we examine two cases in different quadrants *off the diagonal* (one $X=0, Y=1$ case and one $X=1, Y=0$ case) :
  * if  we find that $M$ is the same in the two cases then we  increase our belief that $X$ has no effect at all on $Y$ and reduce our confidence that $X$ had a negative effect  $Y$ (producing a  posterior centered on a small positive effect). 
  * Conversely if we see that $M$ is different in the two cases, then we have a correlation (of different signs) between both $X$ and $M$ and between $M$ and $Y$  cases. We increase our confidence that $X$ mattered for $Y$ in general and in particular that it had a negative effect, resulting in a posterior centered on a small negative ATE.

* If we examine data conditioning on the value of $X$ but with variation on $Y$ (for instance $X=1, Y=0$ and   $X=1, Y=1$) 
data patterns do not discriminate between X having a positive effect on Y an d X having a negative effect on Y. However variation in $M$ is more consistent with $M$ being responsive to $X$ and thus the chances that $X$ matters, positively or negatively, overall. In teh case with flat data this changes beliefs on positve and negative effects but not the difference between them. The ate then remains unchanged. FLAG work though intuition  more

* If we examine data conditioning on the value of $Y$ but with variation on $X$ (for instance $X=0, Y=1$ and   $X=1, Y=1$) 
data patterns do not discriminate between X having a positive effect on Y an d X having a negative effect on Y. However variation in $M$ is more consistent with $M$ being responsive to $X$ and thus the chances that $X$ matters, positively or negatively, overall. In teh case with flat data this changes beliefs on positve and negative effects but not the difference between them. The ate then remains unchanged.

For correlated data similar logics apply, but the effects are stronger for evidence on the regression line. The reason is that given correlated data we believe there are more units with positive effects than negative effects. When we find evidence against causal relations on the regression line that reduces our confidence for types with positive effects more than for types with negative effects and teh difference between teh shares with positive effects and negative eeffects smaller due to the fact that the beliefs in the shares with negative effects is not so strongly reduced. Wehen we find evidence for causal relations this magnifies teh difference between beliefs in shares positive and shares negatives; these effects are magnified however since our confidence for the positive effects change more than for the negative effects, since we are examining cases on the regression line. Conversely when examining cases of teh regression line, the two forces offset each other. 



```{r cinfer, include = FALSE}
# Make sense given process tracing approach

long_data <-  expand_data(gen_observed_data(4,2,2,4), model)

if(do_diagnosis){
  updated <- update_model(model, long_data, iter = 14000, chains = 12) %>%
  set_parameters(param_type = "posterior_mean")
  write_rds(updated, "saved/ch13_longupdated.rds") 
}
updated <- read_rds("saved/ch13_longupdated.rds") 

get_parameters(updated)

conditional_inferences(updated, query = "Y[X=1]!=Y[X=0]", given = "!is.na(X) & !is.na(Y) & !is.na(M)") %>%
  arrange(X, Y, M)
```






## Explorations

Using the same basic logic we can explore performance across a wider range of strategies that might be employed under different conditions.

The qualitative case-selection literature has identified a range of possible strategies for choosing cases for in-depth analysis. These include, for instance, selecting for variation on $X$ (KKV), selecting for variation on $Y$, selecting cases on the regression line (Seawright and Gerring, Lieberman), and selecting off the regression line (Seawright and Gerring, Seawright 2017, Lieberman).[These authors view selection off the regression line as best for arriving at inductive insight. We address this strategy primarily to show the contrast with the on-the-line strategy.] While we have not seen it advocated elsewhere, we might add to this list the strategy of selecting cases that are representative in their $X,Y$ values of the larger set of cases from which we are selecting. (FLAG: insert proper citations). While it is difficult to clearly distinguish these strategies from each other with a small initial set of $X, Y$ cases, we can do so readily if we start with a larger set of $X,Y$ cases.

In Table \@ref(caseselectlots), we show the results of diagnoses of each of these five classes of strategies, assuming that we will be process-tracing 6 cases. In each diagnosis we start with 500 $X, Y$ cases, with 100 $X=Y=0$ cases, 200 $X=Y=1$ cases, 130 $X=0, Y=1$ cases, and 70 $X=1, Y=0$ cases. The regression line here represents a positive association. We work with the same model and priors as above. Importantly, this  means that the results we show here are not *general* evaluations of these strategies, but contingent on this particular model and set of priors. And that is precisely our point: optimal case-selection will always hinge on our model, a claim that we demonstrate further below.


### Procedure

The procedure requires as inputs  (i) a causal model, (ii) any data we have already observed, and (iii) the causal query we seek to answer. 

The general intuition is that we can use the causal model and any previously observed data to estimate what observations we are more or less likely to make under a given case-selection strategy, and then figure out how far off from the (under the model) true estimand we can expect to be under the strategy, given whatever causal question we seek to answer. 

Suppose that we want to estimate the average treatment effect of $X$ on $Y$ in a population and have initially observed $X$ and $Y$ data on a set of cases. 

Suppose that we are now considering gathering process-tracing evidence for one of these cases to inform our estimate of the ATE. There are many different case-selection strategies we might pursue, and each of these can give rise to different possible data and thus to different possible conclusions. What should we do?

**DAG**. We start, as always, with a DAG representing our beliefs about which variables we believe to be direct causes of other variables. For the current illustration, suppose that we are operating with a simple mediation model, $X \rightarrow M \rightarrow Y$. 

**Given data.** If we have already observed something in a set of cases, we can use this information to condition our strategy for searching for further information. For instance, if we have observed $X$'s and $Y$'s value in a set of cases, we might select cases for process tracing based on their values of $X$ and $Y$. Further, what we have already observed in the cases may constrain what possible data we could end up with once we have collected the additional (process tracing) data. 

**Priors**. As when conducting mixed-method inference, we can set qualitative restrictions and/or differential quantitative weights on the (possibly conditional) nodal types in the model. And we can indicate our uncertainty over the latter, by setting the $\alpha$ parameters of the relevant Dirichlet distributions. For the current example, let us define restrictions at both the $M$ and $Y$ nodes, positing beliefs that $X$ never has a negative effect on $M$ and that $M$ never has a negative effect on $Y$. Let us further assume that we have flat priors over the remaining nodal types and posit similar assignment propensities for all types (no unobserved confounding).

**Query**. We define our query. This might, for instance, be the share of cases in the population in which $X$ has a positive effect on $Y$; or it might be $X$'s average effect on $Y$. We can use the general procedure to identify case-selection strategies for any causal query that can be defined on a DAG. And, importantly, the optimal case-selection strategy may depend on the query. For instance, the best case-selection strategy for estimating the average causal effect of $X$ on $Y$ may not be the same as the best strategy for figuring out for what proportion of the population $X$ has a positive effect on $Y$.

**Define one or more strategies**. A strategy is defined, generically, as the search for data on a given set of *nodes*, in a given *number* of cases randomly selected *conditional* on some information we already have about potential cases. Let us assume here that our strategy will involve uncovering $M$'s value in 1 case---but we are wondering how to choose this case. Consider four possible strategies, conditional on the $X$ and $Y$ values that we already know. We could do process tracing on a randomly selected $X=1, Y=1$ case, a randomly selected $X=0, Y=0$ case, the $X=1, Y=0$ case, or the $X=0, Y=1$ case. We itemize this set of possible strategies in the first column in Table \@ref(tab:caseselect1).

**Possible data**. For each strategy, there are multiple possible sets of data that we could end up observing. In particular, the data we could end up with will be the $X,Y$ patterns we have already observed plus *either* $M=0$ *or* $M=1$ in the case that our strategy leads us to select for process tracing. We represent the data possibilities (showing just the possible $M$ values) in the second column in Table \@ref(tab:caseselect1). Thus, for instance, for a strategy in which we choose a random $X=1, Y=1$ case, we could end up observing the initial $X,Y$ pattern plus $M=0$ in one of the $X=1, Y=1$ cases, or the initial $X,Y$ pattern plus $M=1$ in one of the $X=1, Y=1$ cases. 

**Probability of the data**. We now calculate a probability of each possible data realization, given the model and the data (the $X$'s and $Y$'s) that we have already observed. In practice, we do this in `gbiqq` via simulation. Starting with the model together with our priors, we update our beliefs about $\lambda$ based on the initial $X,Y$ data. This posterior now represents our *prior* for the purposes of the process tracing; it represents what we believe about causal-type share allocations in the population, having seen the $X,Y$ data only. We then use this posterior to draw a series of $\lambda$ values. 

Given that the ambiguity matrix gives us the mapping from causal types to data realizations, we can calculate for each $lambda$ draw the probability of each data possibility given that particular $\lambda$ and the strategy. We then average across repeated $\lambda$ draws. (Since $\lambda$'s are being drawn from our prior, we are automatically weighting more heavily those $\lambda$'s that we believe to be most likely.)  We show the data probabilities in the third column of Table \@ref(tab:caseselect1): one probability for each data-possibility given each strategy.

**Posterior variance on estimate given the data**. For each data possibility, we can then use `gibiqq` to ask what inference we would get from each data possibility, given whatever query we seek to answer. What we are in fact interested in for the purposes of case selection is the *variance* of the posterior. We indicate in the fourth column of Table \@ref(tab:caseselect1) the posterior variance for each possible data realization.

**Expected posterior variance under each strategy**. The quantity of ultimate interest is the posterior variance that we expect to end up with under each *strategy*. Calculating this expectation is now elementary as we have both the posterior variance arising from each data possibility and the probability of each data possibility (given our prior beliefs and the data already observed). The expected posterior variance is simply an average of the posterior variances under each data possibility, weighted by the probability of each data possibility. The final column provides the expected posterior variances, one for each strategy.


Complicated as the procedure might seem, it can all be done in code using a single command from the `gbiqqtools` package. 


```{r showx1y1diagnosestrategies, eval=FALSE, include = TRUE}

diagnose_strategies(
    
   analysis_model = model,
   
   observed = observed,
   
   data_strategies = list(
     A_online  = list(N=c(1,1), within = TRUE, vars = "M", 
                       conditions = c("X==1 & Y==1", "X==0 & Y==0")),
     B_offline = list(N=c(1,1), within = TRUE, vars = "M", 
                       conditions = c("X==1 & Y==0", "X==0 & Y==1")),
     C_X1Y1    = list(N=2, within = TRUE, vars = "M", 
                       conditions = c("X==1 & Y==1"))),
   
   queries = "Y[X=1] - Y[X=0]"
   
   )

```

```{r ch13diagnosis387, echo = FALSE, message = FALSE}

if(do_diagnosis){
  
diagnosis <- 
  
  diagnose_strategies(
    
   analysis_model = model,
   
   observed = observed,
   
   data_strategies = list(
     A_online =  list(N=c(1,1), withins = TRUE, vars = "M", 
                       conditions = c("X==1 & Y==1", "X==0 & Y==0")),
     B_offline = list(N=c(1,1), withins = TRUE, vars = "M", 
                       conditions = c("X==1 & Y==0", "X==0 & Y==1")),
     C_X1Y1 = list(N=2, withins = TRUE, vars = "M", 
                       conditions = c("X==1 & Y==1")),
     D_random = list(N=2, withins = TRUE, vars = "M", conditions = TRUE)),
   
   queries = "Y[X=1] - Y[X=0]",
   
   sims = 100
   
   )

write_rds(diagnosis, "saved/ch13diagnosis.rds")

}

diagnosis <- read_rds("saved/ch13diagnosis.rds")
kable(diagnosis$diagnoses_df[, c(1, 4:7)], digits = 4, caption = "The estimand and the expected estimate are the same under each data strategy. ")
```



```{r diagprep, echo = FALSE}
# Models

models <- list(
  chain_model     = make_model("X->M->Y"),

  base_model     = make_model("X->M->Y<-X"),
  
  restricted_model = make_model("X->M->Y<-X") %>%
    set_restrictions(c(decreasing("X", "Y"), 
                       decreasing("X", "M"),
                       decreasing("M", "Y"))),
  
  obs_confound   = make_model("M->X->Y<-M"),
  
  unobs_confound = make_model("X->M->Y<-X; X<->Y"))

gen_observed_data <- function(n00=1, n01=1, n10=1, n11=1,times = 1) {
  data.frame(X = c(0,0,1,1), Y =c(0,1,0,1), M = NA, ns = c(n00, n01, n10, n11)*times) %>%
    tidyr::uncount(ns) %>%
    collapse_data(models$base_model, drop_family = TRUE)}

# observed_data
observed_data <- list(
  regr   = gen_observed_data(12,4,4,12),
  flat   = gen_observed_data(8,8,8,8),
  suff   = gen_observed_data(8,8,4,12),
  necs   = gen_observed_data(12,4,8,8),
  conc   = gen_observed_data(8,4,4,16))


# Data strategies

data_strats <- function(N = 4) {
  if(N%%2 != 0) stop("even Ns only please")
  
  
  out <- list(
    off_01 = list(N=N, withins = TRUE, vars = "M", conditions = "X==0 & Y==1"),
    off_10 = list(N=N, withins = TRUE, vars = "M", conditions = "X==1 & Y==0"),
    on_00  = list(N=N, withins = TRUE, vars = "M", conditions = "X==0 & Y==0"),
    on_11  = list(N=N, withins = TRUE, vars = "M", conditions = "X==1 & Y==1"),
    
    all_on = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==0 & Y==0", "X==1 & Y==1")),
    all_off = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==0 & Y==1", "X==1 & Y==0")),
    all_y0 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==0 & Y==0", "X==1 & Y==0")),
    all_y1 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==0 & Y==1", "X==1 & Y==1")),
    all_x0 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==0 & Y==0", "X==0 & Y==1")),
    all_x1 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==1 & Y==0", "X==1 & Y==1")))
  
  if(N==2){message("N<4, fully spread strategy not possible"); return(out)}
  
  c(out, list(spread = list(N=c(N,N,N,N)/4, withins = TRUE, vars = "M",
                       conditions = list("X==0 & Y==0", "X==0 & Y==1","X==1 & Y==0", "X==1 & Y==1"))
  ))
}

# Queries

queries <- list(
  ATE = "Y[X=1]-Y[X=0]", 
  ProbPos = "Y[X=1]>Y[X=0]",
  via_M = "(Y[X=1] > Y[X=0]) & (M[X=1] > M[X=0]) & (Y[M=M[X=1]] > Y[M=M[X=0]]) & (Y[X=1, M=M[X=1]] == Y[X=0, M=M[X=1]])"
)

subsets <- list(TRUE, "X==1 & Y==1", "X==1 & Y==1")

# Diagnosis function 

# fit <- fitted_model()

case_selection_diagnose <- function(model, given,  N, sims = 4000, fit, ...)
  
  diagnose_strategies(
    analysis_model = model,
    data_strategies = data_strats(N),
    given = given,
    queries = queries,
    subsets = subsets,
    sims = sims,
    fit = fit,
    iter = 6000,
    chains = 6,
    refresh = 1000,
    ...)

```


```{r diagn, echo = FALSE}  
## Implement

if(do_diagnosis){
  
  {if(!exists("fit")) fit <- fitted_model()}
  
#  for(N in c(2, 4)) { for(m in  1:length(models)) {for(g in 1:length(observed_data)) {	
#  for(N in 4) { for(m in  1:2) {for(g in 1:length(observed_data)) {	
  for(N in 4) { for(m in  2) {for(g in 5) {	
    filename <-  paste0("saved/M_", names(models)[m], "_", N, "_", names(observed_data)[g], ".rds")
    
    print(filename)
    
    write_rds(case_selection_diagnose(models[[m]], observed_data[[g]], N = N, fit = fit), 
              filename)
    
  }}}

  #  for(N in c(2, 4)) { for(m in  1:length(models)) {for(g in 1:length(observed_data)) {	
#  for(N in 4) { for(m in  3:5) {for(g in 1:length(observed_data)) {	
  for(N in 4) { for(m in  4) {for(g in 4:5) {	
    filename <-  paste0("saved/M_", names(models)[m], "_", N, "_", names(observed_data)[g], ".rds")
    
    print(filename)
    
    write_rds(case_selection_diagnose(models[[m]], observed_data[[g]], N = N, fit = fit), 
              filename)
    
  }}}
    
}
```

 Graph full set

```{r graph, echo = FALSE}



## Read and display
## PARAMETERS
rds_files <- data.frame(name = dir("saved"), stringsAsFactors = F) %>% 
  filter(str_detect(name,"^M") & str_detect(name,"rds$")) 

# If you look at rds_files you will get the whol list of simulation results
# lets look at one of them
inspect_simulations <- read_rds("saved/M_base_model_2_flat.rds")
names(inspect_simulations)


## UPLOAD MODEL DIAGNOSIS
upload_df <- function(file){
  df <- data.frame()
  toappend  <- read_rds(paste0("saved/",file))$diagnoses_df %>% 
    mutate(
      given = str_match(file,"....(?=\\.rds)"),
      model = str_match(file, "^M_(.*?)_[:digit:]")[,2],
      N     = str_match(file,"[:digit:]")
      ) %>% 
    mutate_if(is.factor, function(x) as.character(tolower(x))) %>% 
    mutate_if(is.character, tolower)
  ifelse(
    "post_var_sd" %in% colnames(toappend),
    TRUE,
    toappend <- mutate(toappend, post_var_sd = NA))
  df <- bind_rows(df, toappend)
}

df <- do.call(rbind,lapply(rds_files$name, upload_df))

## TIDY UP DF
df$strategy2                            <- df$strategy
df$strategy2[df$strategy == "all_on"]   <- "on_all"
df$strategy2[df$strategy == "all_off"]  <- "off_all"

df <- df %>% mutate(
  strategy22 = word(strategy2,2,sep = "_"),
  strategy21 = word(strategy2,1,sep = "_"),
  strategy21 = case_when(
    strategy21 == "on"  ~ "on diagonal",
    strategy21 == "off" ~ "off diagonal",
    strategy21 == "all" ~ "on x, on y",
    TRUE                ~ strategy21),
  strategy21 = factor(strategy21, levels = c("on diagonal","off diagonal","on x, on y","spread")),
  given      = factor(given, levels = c("flat","regr","conc","necs","suff"))
)


df  <- df %>%
  left_join(filter(df,strategy=="prior") %>%  select(given,Query,model,N,prior_var = post_var)) %>%
  filter(strategy != "prior") %>% 
  mutate(gain_prior = (post_var/prior_var-1)*100, 
         gain_prior_upp_ci = ((post_var+2*post_var_sd)/prior_var-1)*100,
         gain_prior_low_ci = ((post_var-2*post_var_sd)/prior_var-1)*100) %>% 
  select(model,Query,starts_with("strategy"),given,N,everything()) %>% 
  arrange(model,Query,strategy,given)

# Lists for loops
mq <- as.list(as.data.frame(t(df %>% select(model,Query) %>% distinct()), stringsAsFactors = F))

# GRAPHS
for (i in 1:length(mq)) {
print(
  ggplot(df %>%
         filter(model == mq[[i]][1]   &
         Query == mq[[i]][2] & 
         strategy2 != "spread"),
         aes(x=strategy22, y=gain_prior, group = N, colour = N)) +
    # Line at 0 
    geom_hline(yintercept=0, linetype="dashed", size=0.5) +
    # Add point plot
    geom_point(position = position_dodge(width = 0.4)) + 
    # Add error bars
    geom_errorbar(
      width=.1, 
      aes(ymin=gain_prior_upp_ci, ymax=gain_prior_low_ci),
      position = position_dodge(width = 0.4)) +
    # Faceting
    facet_grid(given ~ strategy21, scale = "free_x") +
    # Labeling & Styling
    ylab("Gain in posterior variance over prior (%)") +
    labs(title = paste0("Strategy diagnosis: ",mq[[i]][1]," & ",mq[[i]][2]," estimand"),
         subtitle = "Grouped by strategy & given") + 
    theme_bw() + scale_colour_hue(l = 50, c = 50)  + 
    theme(
      legend.title.align = 0.5,
      axis.title.x = element_blank(),
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank())
)
}

```



## Principles

### Sometimes one case is not enough

For on regression line cases, you can see cases where M correlates positively with X and Y correlates positively to Y; or cases where M is correlated with neither

For off regression line cases, you can see cases where M correlates negatively with X and positively with Y, or vice versa; and you can see cases where M is correlated with neither. 

With no restrictions, you can't actually learn from a single data point  because you have flat priors over X->M and M->Y effects. You're only able to learn from correlations. 




### Different strategies for different estimands

### Where the probative value is

<!-- For the general intuition, recall that the probative value of a process-tracing test hinges on the difference in clue likelihoods associated with the alternative hypotheses in play for a given case. Recall that for different values of $X$ and $Y$ cell, we want to use process tracing to help us distinguish between two specific types that are consistent with the $X, Y$ pattern. Which types are in question varies across $X,Y$ combinations. Table \@ref(tab:FP) illustrates. -->

<!-- | \small      |          $Y=0$           |           $Y=1$             |   -->
<!-- |-------------|:------------------------:|:---------------------------:| -->
<!-- |     $X=0$   |     $b$ or $c$           |       $a$ or $d$            | -->
<!-- |     $X=1$   |     $a$ or $c$           |       $b$ or $d$            | -->
<!-- Table: (\#tab:FP). The ambiguity about types in each $X, Y$ cell. -->

<!-- Thus, in the $X=0, Y=0$ cell, what would be most useful is a clue that has high probative value in distinguishing between an untreated ($X=0$) $b$ type and and an untreated $c$ type. For a case in the $X=1, Y=0$ cell, on the other hand, what matters is how well the clue can discriminate between treated ($X=1$) $a$ and $c$ types. In our notation, it is the difference in $\phi_{jx}$ values for that indicates these cell-specific degrees of leverage.  -->

<!-- To illustrate, consider a situation in which for a given clue we have  $\phi_{b1}$=0.5^[The probability of observing the clue for a $b$ type (positive causal effect) case with $X=1$.]; $\phi_{d1}$=0.5 ^[The probability of observing the clue for a $d$ type (zero causal effect, $Y$ fixed at 1) case with $X=1$.]; $\phi_{b0}$=0.5^^[The probability of observing the clue for a $b$ type (positive causal effect) case with $X=0$.]; and $\phi_{c0}$=0.1^[The probability of observing the clue for a $c$ type (zero causal effect, $Y$ fixed at 0) case with $X=0$.]. In this situation, searching for the clue in $X=Y=1$ cases will yield no leverage since the clue does not discriminate between the two types ($b$ and $d$) that need to be distinguished given $X=Y=1$. Here there is no additional learning about $\lambda_b$ that can be gained from looking for the clue. In contrast, $X=0, Y=0$ cases will be informative since the clue is much better at distinguishing between $b$ and $c$ types---the two types in contention for this kind of case. Thus, although process tracing here does not provide information on the prevalence of positive causal effects ($b$ types) for an $X=Y=1$ case, it does provide information when $X=Y=0$.  -->

<!-- While it is common practice for mixed-method researchers to perform their process tracing "on the regression line," the BIQQ framework suggests that the gains to process tracing for different $X$ and $Y$ values in fact depend on the particular constellations of $\phi$ values for the potentially available clues. More generally, the framework allows one to assess the expected gains from any given case-selection strategy *ex ante* once priors have been specified.  -->




<!-- ## Appendix: Stan uncertainty -->

```{r appendix, echo = FALSE, eval = FALSE}
# manual check of base model with refr data and checking off regression line with data pattern 1

data_check <- M_base_model_2_regr$possible_data_list$all_off[,c(1,3)] %>% expand_data(models$base_model)
update_check <- update_model(models$base_model, data_check)
query_check <- query_model(update_check, "Y[X=1] - Y[X=0]", using = "posteriors")

# Prior check

prior_data <- gen_observed_data(12,4,4,12)  %>% expand_data(models$base_model)
update_prior <- update_model(models$base_model, prior_data, chains = 8, iter = 10000)
query_prior <- query_model(update_prior, "Y[X=1] - Y[X=0]", using = "posteriors")
query_prior


posterior <- query_distribution(update_prior, "Y[X=1] - Y[X=0]", using = "posteriors")

sd(posterior)

n = 8000

resample <- function(posterior, n) {
  sample(1:length(posterior), n, replace = TRUE)
}
resample(posterior)


sds <- replicate(2000, sd(posterior[resample(posterior, n)]))

hist(sds)



iters <- seq(1:12)*1000
sdss <- sapply(iters, function(n) sd(replicate(5000, sd(posterior[resample(posterior, n)]))))

plot(iters, sdss, ylim = c(0,.003 ))
```
