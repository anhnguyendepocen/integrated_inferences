# Going wide and going deep {#wide}

***

Researchers often need to choose between collecting data on a greater number of cases or collecting more data within a given set of cases. This is a choice, we might say, between going "wide" and going "deep." We discuss the tradeoffs and communicate an intuition that clue data, even on a small number of cases, can be informative even when there is $X, Y$ data on a very large number of cases, but only if it provides information that cannot be gathered from $X,Y$ data, such as selection into treatment. Simulations suggest that going deep is especially valuable for observational research, situations with homogeneous treatment effects, and, of course, when clues have strong probative value.

***

```{r packagesused12, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```

## Motivation

Let us continue our journey through the space of research-design choices. Suppose, now, that we have identified those clues that will be most informative, given our beliefs about the world. A further question that we face is the quintessential dilemma of *mixing* methods: what mixture of quantitative and qualitative evidence is optimal? We have, of course, argued in in Chapter \ref(mixing) that the distinction between quantitative and qualitative inference is, in a causal-model framework, without much of a difference. But here we are framing a more precise question: given finite resources, how should we trade off between studying a larger number of cases and drilling down to learn more about some subset of the cases in our sample? How should we decide between going "wide" and going "deep"?

As we did for clue selection, we will show that this is a question that we can use our causal model to answer. In other words, how much we should expect to learn from going wide versus going deep will *depend* on how we think the world works --- that is, on the beliefs embedded in our model. It is straightforward to use the `gbiqq` machinery to estimate the expected learning that will arise from different strategies.

For the most part, the question of going wide versus going deep is one that applies to population-level estimands. If our goals are defined by case-level estimands, then going deep within the cases of interest will generally be the best approach. We can also show, however, the existence of gains from extensive analysis for case-level estimands: knowing more about the population can help us get individual cases right.


## Developing some intuitions

To build up our intuitions about how the optimal mix of strategies might depend on how the world works, let us explore a simple example. We focus here on the question of how much we can learn from drilling deeper, given an initial set of $X,Y$ data and beliefs about the world. To simplify the exposition, we revert here to using our four basic causal types from Chapter \ref(models): $a, b, c, and d$.^[For this illustration, we just need to keep track of $b$ (positive effect), $a$ (negative effect), and $d$ (no effect, $Y$ fixed at $1$).]

<!-- e begin by considering the learning that occurs upon observing outcomes from varying numbers of cases given different $XY$ data ranging from small to quite large.  -->

<!-- The goal here is to build up intuitions on how beliefs change given different observations and how this affects posterior variance. We address the question in a very controlled setting in which  -->

Suppose that:

* a researcher is confronted with $X,Y$ data that exhibits no correlation; observations are balanced across the 4 cells defined by possible combinations of $X, Y$ values.
* the researcher can seek information on a highly informative ("doubly decisive") clue, $K$, within cases in the $X=Y=1$ cell; thus, we are imagining a scenaior in which the information we will get about the case in question is about as informative about that case as it could possibly be.
* although not known in advance, each time the researcher collects a within-case clue, she finds evidence suggesting that the case is a $b$ type.

We consider two different data-generating processes: 

1 There may be unobserved confounding between $X$ and $Y$, and we have flat priors over this confounding. Put differently, assignment propensities are unknown.

2 There is no confounding; $X$ can be treated as randomly assigned. 

We consider what happens as the number of cases on which we collect $K$ increases from 0 to 5. We also consider different amounts of initial $X,Y$ data, considering situations in which we have 5, 10, 50, and 5000 observations in each $X,Y$ cell.

We would expect that seeking a clue in a case---which, in this simulation, always delivers evidence consistent with a positive causal effect in that case---will lead the researcher to believe there are more $b$ types and that there is thus a higher average causal effect. But how strong will these shifts be? And how does the amount of belief change depend on the amount of $X, Y$ data available and the underlying data-generating process? When does the signal from the $X,Y$ data drown out any signal from the $K$ data, and when does $K$ data add value?

Figure \ref{morn} reports answers to these questions. In the top row, we report the average causal effect that we will estimate for different combinations of $X,Y$ and $K$ data. In the bottom row, we show the reductions in uncertainty (posterior variance) that we get with each strategy. On the left, we allow for unobserved confounding, and on the right we have random assignment. Each curve represents a different sample size for which we have $X,Y$ data. The number of cases in which we go "deep," collecting $K$ data, is represented on the horizontal axis. 

1. With unobserved confounding, we see clear gains to collecting clues on a greater number of cases across the 1 to 5 range. Collecting clue information on a greater number of cases shifts our beliefs about the average causal effect and reduces our uncertainty about that quantity. Moreover, with unobserved confounding, the value of the clue information is *independent* of how many $X,Y$ cases there are. 

What is happening here is that the clues are providing information on assignment propensities, which are informative about the share of each type in each cell. With flat priors over assignment propensities, our beliefs are centered around equal propensities for all types (though we're also very uncertain about this). Moreover, given equal assignment propensities, the flat data $X,Y$ pattern has us believing that there the average treatment effect is 0 (also with great uncertainty). For every additional $X=1, Y=1$ for which we observe $K=1$, however, we shift upward our belief about the share of $b$'s in the cell. We are, thus, now learning about assignment propensities: now it looks like $b$ types were more commonly assigned to $X=1$, implying that $d$'s must have been more commonly assigned to $X=0$. Put differently, we are learning that the flat data pattern has arisen via confounding that is "suppressing" a positive treatment effect. 

The value of the clue data, moreover, does not depend on how many $X,Y$ cases we've observed because no amount of $X,Y$ data can tell us about $X$<->$Y$ confounding. As we see in the bottom-left graph, we end up more certain, the more $X, Y$ data we have. But there's just as much to be *gained* from a given amount of clue data whether we've started with 5 $X,Y$ cases or 5000.

2. When assignment propensities are known (and so we can treat the data as experimental), the learning from clue data depends heavily on how many $X,Y$ cases we start out with. Where we have a large amount of $X,Y$ data, clue evidence adds little to nothing to our inferences about average treatment effects. There is nothing to be learned from the clues about assignment propensities since these are already known. And, with assignment propensities known, $X,Y$ information alone are sufficient for convergence on the average treatment effect as sample size increases. Clue information shifts beliefs about the types of the particular cases for which clue data is gathered --- i.e., for this case-level-estimand --- but has almost no effect on estimates of the population estimand. 

However, we can learn about average effects from clue data when we have few $X,Y$ cases. While we can infer the average causal effect from a known $X,Y$ distribution (and known assignment propensities), we have a lot of uncertainty about that distribution when we only have a handful of $X,Y$ cases. Thus, clue data help us by uniquely providing information about the types of individual cases. It is still only from observing $K=1$ in a case that we can learn that that case is a $b$ type, allowing us to update upwardly on the relative proportions of $b$'s and $d$'s. With little other information on the average effect, this pushes our belief about average effects upward.^[We also, by implication, update upwardly on the share of $a$'s relative to $d$'s in the $X=0, Y=1$ cell; but the direct learning about the $X=1, Y=1$ cell will be sharper than the indirect learning about the $a$'s, generating a net increase in the estimated average causal effect.] And, in the absence of a large amount of of $X,Y$ data, knowing there is one more $b$ and one less $d$ type still benefits from learning  

3. Though not visible from the figure, clue data help us learn about a different population-level estimand --- the *distribution* of causal effects in the population --- even when we have a large $N$ and known propensities. The same average causal effect could be consistent with a large share of $a$'s and $b$'s combined, with few no-effect cases, or with a small share of $a$'s and $b$'s combined, with many no-effect cases. In other words, we don't actually learn from $X,Y$ data about the distribution of types, even with a large $X,Y$ sample and known propensities. But observation of clues identifying $b$ types in the $X=Y=1$ cell, while it does not change estimates of *average* treatment effects, tells us that there is greater *heterogeneity* of effects in the population. More $b$ types, holding the average effect constant, means that there must also be more $a$ types. Thus, we have learned that there are more offsetting effects playing out in the population --- positive effects alongside negative effects --- than we knew before we saw the clue data. In fact, we learn *more* about heterogeneity from clue data where the average causal effect is already known than where we are highly uncertain about the average effect.

FLAG: Can we show the heterogenity point graphically, too? Seems odd to say it's not visible when we could make it so.


## Diagnosing mixes

The stylized example above is intended to help us see how our choices about depth vs. breadth can depend on the process through which we believe the data have been generated. Our more general point is that we can systematically assess different possible mixes of extensiveness and intensiveness given a causal model and any data that have already been observed. 

The basic procedure, in `gbiqq`, is as follows. 

1. **Define a model.** Specify the causal graph (possibly with unobserved confounding), any restrictions on causal effects, and any priors over parameters. We consider five different models: 

* One-path model, with a single, mediated path
* Two-path model, with a direct and an indirect path 
* Restricted model, the two-path model with monotonicity restrictions, excluding negative effects at each step
* Observed-confound model, in which there is only a direct $X \rightarrow Y$ path and the clue is a confound 
* Unobserved-confound model, the chain model with unobserved $X$ <-> $Y$ confounding

2. **Specify the given data.** We imagine starting with a certain amount of $X,Y$ data, from 32 cases, and we vary the pattern in those data. The given data patterns we examine are a strong positive $X,Y$ relationship; the absence of any $X,Y$ relationship; a pattern suggestive of $X=1$ being almost sufficient for $Y=1$; and a pattern suggestive of $X=1$ being almost necessary for $Y=1$.

3. **Specify wide and deep data strategies.** In examples below, we assume a baseline set of $X, Y$ data that have already been observed. We pose the wide-vs.-deep question at the margins: how much do we expect to gain from collecting $X,Y$ data for a given number of additional, randomly selected cases, and how much from looking for a clue, $M$, within a given number of cases in our sample (for which we already have $X,Y$ data)? To simplify the analysis, we assume that clue data are sought on a positive regression line, with half sought in the $X=0, Y=0$ cell and half in the $X=1, Y=1$ cell. A data strategy is defined as a combination of width and depth: adding $X,Y$ data on 0, 2, or 4 cases, and hunting for a clue within 0, 2, or 4 cases.

4. **Formulate queries.** As our discussion in the last section already suggests, the optimal mix may depend on the causal question we are interested in answering. We assess learning about three distinct causal queries:

* Average treatment effect
* Probability of positive causation: the probability that $X=1$ caused $Y=1$ in an $X=Y=1$ case? 
* Probability of mediated positive causation: the probability that $X=1$ caused $Y=1$ through a particular mechanism --- a positive effect of $X$ on $M$ and a positive effect of $M$ on $Y$ --- in an $X=Y=1$ case.

It is worth noting that the second two queries are conditioned on particular $X,Y$ values but that the data strategies we are examining in these analyses do not limit our inquiry to those values. The "wide" component of any strategy is a random draw from the population; and the "deep" componeny involves looking *both* in the $X=Y=1$ cell *and* in the $X=Y=0$ cell. Of course, it is not hard to imagine that seeing what is going on in an $X=Y=0$ case could be informative about causal relations in an $X=Y=1$ case.

5. **Diagnose each strategy, conditional on model and given data.** For each strategy, we implement a separate diagnosis under each combination of model and given data, using `gbiqq`'s `diagnose_strategy` function. For each strategy, model, and given data pattern, the function:

* identifies all possible new data realizations
* calculates the probability of each possible data realization
* updates posterior distributions on all parameters for each possible data realization
* calculates expected posteriors on all parameters by averaging across posteriors from all possible data-realizations, weighted by their probability of arising
* uses the expected posteriors on parameters to generate expected posterior distributions on each of the specified queries.

What we are interested in is the expected variance of the posteriors, which tells us how uncertain we expect to be about our estimate after implementing the strategy, given the model and the given data.^[While we also might expect the posterior mean to change depending on what data-realization we encounter, the *expected* posterior will be equal to the prior mean since any beliefs about what we are most likely to find are already reflected in the prior.] A greater reduction in expected posterior variance implies greater expected learning from the strategy. 

The results of these diagnoses are represented in Figures **flag: labels here**. In each figure, we consider learning about all three queries under a single model, with each row of subplots considering a different pattern of given data as the starting point. The "wide" data strategies (collecting $X, Y$ data for an additionl 0, 2, or 4 randomly selected cases) are represented by movement across the columns of subplots; the "deep" strategies (collecting a clue for 0, 2, or 4 $X=Y$ cases within the sample) are represented within each subplot. Within each subplot, we provide the expected posterior variance for a particular level of "width" and given data, providing error bars representing 95 percent of the simulation variability.



Findings:

**1-path model**

Learning from clues is hard because you don't have any prior on whether M=0 or M=1 is more consistent with a b type. Could be linked positive effects or linked negative effects. However, when you see, say, a pattern of multiple M=1's in X=1, Y=1 cases, which cannot be a types, this shifts you toward the joint beliefs that b types operate via linked *positive* effects and that there are more b types in this cell. 

*ATE*

Regr: Comparable learning from adding 2D than adding 2W Hints that 4D may be better than 4W, and that 2W+2D may be better than 4W.

Learning about ATE much weaker for other given data patterns, with no learning from flat and slight learning with necs/suff. You are already quite certain there's an ATE of 0 with flat data (and flat priors on the original model). So even if you see a b type, so you now think there are more b's than d's, you also upwardly update your beliefs about a types. You have a much more dispersed prior given regr data and flat original priors on the model, which allows the data to move your beliefs much more.

*ProbPos*

Regr: For W, similar to ATE. For D, more learning than for ATE. Much steeper learning from D than from W. Also, there a much stronger case for investing in depth (on the regression line) over width if you're estimating ProbPos than if you're estimating ATE. Probably because we can learn directly about case type (is it a b?) from clues, but getting a case's type gives us only indirect leveage on the ATE, which is share of b's minus share of a's. In fact, given beliefs about the ATE from the given data, once you see a couple of b's, you're likely to think there are also more a's.

Learning much more muted for nec and suff, and almost undetectable for flat data. Note slightly more learning for nec given, probably simply because you start out with a higher estimate (fewer X=0, Y=1 cases than with suff given), so more uncertainty.


*Via_M* 

Regr: Massive learning from D. Moderate learning from W; but more learning from W about Via-M than about the ATE. (Why?) Outsized benefits from D because only clues can tell us about the mechanism: here, it's not a question of which path but whether it's via linked positive effects or linked negative effects.

Flat: even some learning here -- much more on this query than other queries. And here you learn nothing from W, because you don't expect to see anything but additional flat X,Y data. 

Necs and suff: between regr and flat, with necs being stronger learning (necs has higher estimate to begin with since more evidence of positive effects).

Strongly diminishing returns to D, for all givens.


**2-path mode**

Big takeaway: it's very hard to learn from clues with a 2-path model with flat priors. Whatever you learn about the mediated path doesn't tell you anything about the direct path.

*ATE* 

Regr: W is about as valuable here as it is for 1-path model. D is useless.

As with 1-path model, much less little learning about ATE from W given necs, suff, or flat data (in order of diminishing learning opportunities). D still useless.



*ProbPos*  

Regr: Very similar to 1-path model: Learning from W is similar to, though a bit less than, for ATE. No learning from D.

No detectable learning from W or D from other data patterns (because of weaker X,Y patterns).


*Via_M*

Most surprising results are here: in contrast to 1-path model, the learning about pathway is almost undetectable here, from D or from W. Why??? Suppose you see a set of M=1's in X=Y=1 cases. Isn't that evidence for X having a positive effect on M, M having a positive effect on Y, and X having a positive effect on Y that depends on this chain of effects? As compared to seeing a mix of M=0's and M=1's or a bunch of M=0's? Seems like there should definitely still be learning from D here.



**2-path restricted model**

*ATE*

D is no more helpful here because you've still got the other path that M doesn't tell you anything about.




Full set of analyses

```{r modelsprepdiag, echo = FALSE, eval = TRUE}
# Models

models <- list(
  
  chain_model     = make_model("X->M->Y"),

  base_model     = make_model("X->M->Y<-X"),
  
  restricted_model = make_model("X->M->Y<-X") %>%
    set_restrictions(c(decreasing("X", "Y"), 
                       decreasing("X", "M"),
                       decreasing("M", "Y"))),
  
  obs_confound   = make_model("M->X->Y<-M"),
  
  unobs_confound = make_model("X->M->Y<-X; X<->Y"))

gen_observed <- function(n00=1, n01=1, n10=1, n11=1,times = 1) {
  data.frame(X = c(0,0,1,1), Y =c(0,1,0,1), M = NA, ns = c(n00, n01, n10, n11)*times) %>%
    tidyr::uncount(ns) %>%
    collapse_data(models$base_model, remove_family = TRUE)}

# Givens
observed <- list(
  regr   = gen_observed(12,4,4,12),
  flat   = gen_observed(8,8,8,8),
  suff   = gen_observed(8,8,4,12),
  necs   = gen_observed(12,4,8,8),
  conc   = gen_observed(8,4,4,16))


# Data strategies

data_strats  <- {

  out <- list()
  j <-1
  for(N_wide in c(0, 2, 4)) { for(N_deep in c(0,2,4)) {
    
  add <- list(N = c(N_wide, N_deep/2, N_deep/2), 
                     withins = c(FALSE, TRUE, TRUE), 
                     vars = list(c("X", "Y"), "M", "M"), 
                     conditions = list(TRUE, c("X==0 & Y==0"), c("X==1 & Y==1")))
  out[[j]] <- add 
  names(out)[[j]] <- paste0("w_", N_wide, "_d_", N_deep)
  j <- j+1
  }}
  out
}


# Queries

queries <- list(
  ATE = "Y[X=1]-Y[X=0]", 
  ProbPos = "Y[X=1]>Y[X=0]",
  via_M = "(Y[X=1] > Y[X=0]) & (M[X=1] > M[X=0]) & (Y[M=M[X=1]] > Y[M=M[X=0]]) & (Y[X=1, M=M[X=1]] == Y[X=0, M=M[X=1]])"
)

given <- list(TRUE, "X==1 & Y==1", "X==1 & Y==1")

# Diagnosis function 

case_selection_diagnose <- function(model, observed,  sims = 4000, ...)
  
  diagnose_strategies(
    analysis_model = model,
    data_strategies = data_strats,
    observed = observed,
    queries = queries,
    given = given,
    sims = sims,
    fit = fit, 
    iter = 4000,
    refresh = 1000,
    ...)

```


```{r dodiagnosis, echo = FALSE, eval = FALSE}  
## Implement

if(do_diagnosis){
  
  {if(!exists("fit")) fit <- fitted_model()}
  
for(m in  1:length(models)) {for(g in 1:length(observed)) {	
#  for(m in  1) {for(g in 1) {	
    filename <-  paste0("saved/Ch12_", names(models)[m], "_",  names(observed)[g], ".rds")
    
    print(filename)
    
    write_rds(case_selection_diagnose(models[[m]], observed[[g]]), 
              filename)
    
  }}
}
```

 Graph full set

```{r basemodel, echo = FALSE, eval = FALSE}

#Graph
library(tidyverse)

observed <- c("regr", "suff", "conc", "flat", "necs")

process <- function(observed) {

 readRDS(paste0("saved/Ch12_base_model_", observed, ".rds"))$diagnoses_df %>%
    dplyr::mutate(Wide = as.numeric(sub(".*w[_]([^.]+)[_]d.*", "\\1", strategy))) %>%
    dplyr::mutate(Deep = as.numeric(sub(".*_d_", "", strategy))) %>%
    dplyr::filter(!is.na(Deep)) %>%
    dplyr::mutate(upper = post_var + 2*post_var_sd,
                   lower = post_var - 2*post_var_sd,
                   observed = observed) %>%
    dplyr::mutate(Wide=paste0("Wide N=", Wide))
}

df <- lapply(observed, process) %>%
  bind_rows()


df$observed<- factor(df$observed,levels=c("flat","regr","conc", "necs", "suff"))

ggplot(df, aes(x= Deep, y=post_var)) +
  geom_point(aes(shape=Query), size=2) +
  geom_line(aes(linetype=Query)) +
  facet_grid(vars(observed), vars(Wide)) +
  geom_errorbar(aes(ymin  = lower, ymax  = upper, width = 0.15))+
  theme_bw() +
  ylab("Expected Posterior Variance") +
  ggtitle("Wide and Deep Stategies for a 2-path model")



```

```{r chainmodel, echo = FALSE, eval = FALSE}

#Graph

process <- function(observed) {

 readRDS(paste0("saved/Ch12_chain_model_", observed, ".rds"))$diagnoses_df %>%
    dplyr::mutate(Wide = as.numeric(sub(".*w[_]([^.]+)[_]d.*", "\\1", strategy))) %>%
    dplyr::mutate(Deep = as.numeric(sub(".*_d_", "", strategy))) %>%
    dplyr::filter(!is.na(Deep)) %>%
    dplyr::mutate(upper = post_var + 2*post_var_sd,
                   lower = post_var - 2*post_var_sd,
                   observed = observed) %>%
    dplyr::mutate(Wide=paste0("Wide N=", Wide))
}

df <- lapply(observed, process) %>%
  bind_rows()


df$observed<- factor(df$observed,levels=c("flat","regr","conc", "necs", "suff"))

ggplot(df, aes(x= Deep, y=post_var)) +
  geom_point(aes(shape=Query), size=2) +
  geom_line(aes(linetype=Query)) +
  facet_grid(vars(observed), vars(Wide)) +
  geom_errorbar(aes(ymin  = lower, ymax  = upper, width = 0.15))+
  theme_bw() +
  ylab("Expected Posterior Variance") +
  ggtitle("Wide and Deep Stategies for a 1-path model")



```


```{r restrictedmodel, echo = FALSE, eval = FALSE}

#Graph

observed <- c("regr", "suff", "flat", "necs")

process <- function(observed) {

 readRDS(paste0("saved/Ch12_restricted_model_", observed, ".rds"))$diagnoses_df %>%
    dplyr::mutate(Wide = as.numeric(sub(".*w[_]([^.]+)[_]d.*", "\\1", strategy))) %>%
    dplyr::mutate(Deep = as.numeric(sub(".*_d_", "", strategy))) %>%
    dplyr::filter(!is.na(Deep)) %>%
    dplyr::mutate(upper = post_var + 2*post_var_sd,
                   lower = post_var - 2*post_var_sd,
                   observed = observed) %>%
    dplyr::mutate(Wide=paste0("Wide N=", Wide))
  
}


df <- lapply(observed, process) %>%
  bind_rows()


df$observed<- factor(df$observed,levels=c("flat","regr", "necs", "suff"))

ggplot(df, aes(x= Deep, y=post_var)) +
  geom_point(aes(shape=Query), size=2) +
  geom_line(aes(linetype=Query)) +
  facet_grid(vars(observed), vars(Wide)) +
  geom_errorbar(aes(ymin  = lower, ymax  = upper, width = 0.15))+
  theme_bw() +
  ylab("Expected Posterior Variance") +
  ggtitle("Wide and Deep Stategies for a Restricted Model")



```


How does this approach guide researchers in making choices about research designs?

We address this question with a focus  on characterizing the kind of learning that emerges from gathering different sorts of data---such ,  *under different research conditions*. We report the results here of simulation-based experiments designed to tell us under what research conditions different mixes of methods can be expected to yield more accurate inferences. We also discuss, at a high level, the implications of the framework for strategies of qualitative case-selection.  




```{r model393}
model <- make_model("X -> Y <- K") %>%
  set_restrictions("(Y[X=0, K=1]==1) | (Y[X=0, K=0]==0)") %>%
  set_parameters(c(0.01, .99, .5, .5, .25, .25, .25, .25))
```

We see that prior beliefs are for a 0 average effect which rises to approximately .5 for cases in which $K=1$ is observed and falls to -.5 for cases in which $K=0$ is observed.  

```{r querymodel, echo = FALSE}
kable(
  query_model(model, queries = list(ATE = "Y[X=1] - Y[X=0]"), 
              using =  "priors", 
              given = list(TRUE, "K==1", "K==0")))
```

We see little  difference in the prior on estimands. Despite this an important difference is that the model that allows for confounding also allows for updating on confounding, which the simple model does not.

```{r ch12allowconfound, echo  = FALSE}
model_confound <-   set_confound(model, list(X = "(Y[X=1] > Y[X=0])")) %>%
  set_parameters(c(0.001, .999, .5, .5, .2, .8, .25, .25, .25, .25)) 

  q2 <- query_model(model_confound, 
              queries = list(ATE = "Y[X=1] - Y[X=0]"), 
              using = "priors",
              given = list(TRUE, "K==1", "K==0"))
kable(q2)

#x <- (get_types(model_confound, query = "Y[X=1] - Y[X=0]", join_by = "|")$types)
```

We now examine inferences given different data strategies:


```{r ch12diagnosiswd, echo = FALSE, message = FALSE}

obs_fold <- function(n_fold) 
  data.frame(X = c(0,0,0,1,1,1), Y = c(0,0,1,0,1,1), K = NA) %>%
        slice(rep(row_number(), n_fold)) %>%
        collapse_data(model, remove_family = TRUE)


wd <- function(n_fold=2, sims = 600)  {
      
  diagnose_strategies(
    
   analysis_model = model,
   
   observed = obs_fold(n_fold),

   data_strategies = list(
     QL1 =  list(N = 1, within = TRUE, vars = "K", conditions = c("X==1 & Y==1")),
     QL2 =  list(N = 2, within = TRUE, vars = "K", conditions = c("X==1 & Y==1")),
     QL3 =  list(N = 3, within = TRUE, vars = "K", conditions = c("X==1 & Y==1")),
     QL4 =  list(N = 4, within = TRUE, vars = "K", conditions = c("X==1 & Y==1"))),
   
   queries = "Y[X=1] - Y[X=0]",
   
   sims = sims
   
   )
}

if(do_diagnosis) {
  diagnosis <- wd(2)
  write_rds(diagnosis, "saved/ch12diagnosiswd.rds")
  }


# diagnosis <- read_rds("saved/ch12diagnosiswd.rds")

# plot(0:4, diagnosis$diagnoses_df$MSE, ylim = c(0, max(diagnosis$diagnoses_df$MSE)), ylab = "MSE", xlab = "Cases with within case data", type = "b")
```

```{r wdXMY2, eval = FALSE}
if(do_diagnosis){
  wd_1_2 <- wide_or_deep(model, 1, 2)
  write_rds( wd_1_2, "saved/wd_1_2.rds")
}

wd_1_2 <- read_rds("saved/wd_1_2.rds")


wd_sim <- function(model, n_K, n_fold) {
  
    df <- data.frame(X = c(0,0,1,1), Y = c(0,0,0,1), K = NA) %>%
      slice(rep(row_number(), n_fold))
    df <- mutate(df, K = c(rep(1, n_K), rep(NA, n()-n_K)))
    
    given <-  collapse_data(df, model)

    updated <- gbiqq::gbiqq(model, data = df, stan_model = fit)
    query_model(updated, 
                  queries = list(ATE = "Y[X=1] - Y[X=0]"), 
                  using = "posteriors")
  
}

if(do_diagnosis){
  if(!exists("fit")) fit <- gbiqq::fitted_model()
  out <- sapply(c(4,8), function(n_K) {sapply(c(2, 10, 20), function(k) wd_sim(model, n_K, k))})
  write_rds(out, "saved/wide_or_deep_XMY.rds")
  }

wd <- read_rds("saved/wide_or_deep_XMY.rds")
wd <- t(wd[c(4,9,14), ])
rownames(wd) <- c("Clues on 4 cases", "Clues on 8 cases")
colnames(wd) <- c("N=8", "N=40", "N=80")
kable((wd))

if(do_diagnosis){
  if(!exists("fit")) fit <- gbiqq::fitted_model()
  out <- sapply(c(4,8), function(n_K) {sapply(c(2, 10, 20), function(k) wd_sim(model_confound, n_K, k))})
  write_rds(out, "saved/wide_or_deep_XMY2.rds")
  }

wd2 <- read_rds("saved/wide_or_deep_XMY2.rds")
wd2 <- t(wd2[c(4,9,14), ])
rownames(wd2) <- c("Clues on 4 cases", "Clues on 8 cases")
colnames(wd2) <- c("N=8", "N=40", "N=80")
kable((wd2))
```



## Evaluating strategies

As a metric of the returns from different research strategies we calculate the *expected* inaccuracy in the estimation of the average treatment effect, as given by:

$$\mathcal{L}=\mathbb{E}_\theta(\mathbb{E}_{\mathcal{D}|\theta}(\tau(\theta)-\hat{\tau}(\mathcal{D}))^2) $$


where $\tau(\theta)$ is the value of $\lambda_b-\lambda_a$ (the average treatment effect) given $\theta$, and $\hat{\tau}(\mathcal{D})$  is the *estimate* of this treatment effect (the mean posterior value) that is generated following some realization of data $\mathcal{D}$. Thus, if some $\theta$ characterized the true state of the world, then $\mathbb{E}_{\mathcal{D}|\theta}(\tau^\theta-\hat{\tau})^2$ is the expected error in estimation of the causal effect given different realizations of the data, $\mathcal{D}$,  that could obtain in this state of the world.  $\mathcal{L}$ is then the expected value of these errors given prior beliefs over possible values of $\theta$.

Note that, while we focus on errors on estimated average causal effects, similar exercises could assess how cross- and within-case observations distinctively contribute to other estimands | including the causal explanations for individual cases and the validity of causal theories | as well as to learning about inferential assumptions themselves (assignment and clue probabilities). 
For all simulations, prior distributions are drawn with parameters as described in the Supplementary Materials (\S \ref{AppSimNotes}, Table \@ref(tab:sims)). Priors on the type distribution are drawn from a Dirichlet distribution; priors for each of the $\pi$ and $\phi$ values are drawn independently from Beta distributions. We note that, while by construction priors on each parameter are independent, this will not generally be the case for posterior distributions. In most cases we simulate the prior distribution using 5200 draws of each parameter. For most experiments we then systematically vary the prior distribution for one parameter of the research situation between two extreme positions. We then calculate the expected posterior from each possible data realization and, in turn, the expected loss in estimates of treatment effects for a range of levels of investment in qualitative and quantitative evidence. 


A few further features of the experiments below are worth noting. First, our illustrations focus on learning about population-level causal effects; however, the model can yield results about the benefits of alternative research designs for estimating a wide range of other quantities of interest, such as case-specific causal explanations or clue probabilities. Second, while we focus on the search for a *single* clue in each case, the analysis can be extended to the case of an arbitrarily large set of clues. Third, in many of these experiments, the probative values are set at doubly decisive levels for all $\phi$ parameters, and thus focus on the very optimistic case of maximally informative process tracing. Fourth, we illustrate tradeoffs at low levels of $n$, but the model can be employed to make choices for arbitrarily large numbers of cases. Finally, we note that some results may be sensitive to the choice of priors. The results below should thus be understood as an illustration of the utility of the BIQQ framework for guiding research choices, rather than as a set of more general prescriptive design rules.



## Varieties of mixing {#varieties}

What are the marginal gains from additional pieces of correlational and process-tracing evidence for the  accuracy of causal estimates? Figure \ref{morn} displays  results,  plotting the errors associated with different mixes of correlational and process data. 

*  **Qualitative and quantitative data can act as partial substitutes for assessing causal effects**.
*  **The *relative* marginal gains from going wider and going deeper vary with the study design**. 
* **Optimal strategies might involve going deep in a subsample of cases only.**




\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/m_or_n.pdf}
\caption{{Expected errors in the estimation of average treatment effects for designs in which $X, Y$, data is sought in $n$ studies (horizontal axis) and clue data is sought within $m$ of these. The shading of dots indicates the proportion of cases for which within-case data is sought (white = none; black = all). For small sample sizes ($n \in \{1,2,3,4\}$) we show results for all designs ($m \in \{1,2,\dots, n\})$. For larger sample sizes, we show only designs with clues sought in 0, half, and all cases.}}
\label{morn}
\end{figure}




```{r ch13diagnosis, echo = FALSE}

model <- make_model("K-> X -> Y <- K")

if(do_diagnosis){

 data_strategies = list(
 		N4L0 =  list(N=4, withins = FALSE, vars = list(c("X", "Y")), conditions = TRUE),
 		N2L2 =  list(N=2, withins = FALSE, vars = list(c("X", "K", "Y")), conditions = TRUE)
 		#,
 		#N3L1 =  list(N=list(1,2), withins = FALSE, vars = list(c("X", "K", "Y"), c("X", "Y")), conditions = TRUE)
 		)
 	
 	possible_data_list = lapply(data_strategies, function(ds)
 		with(ds, make_possible_data(model = model, given = NULL, 
 		N = N, withins = withins, conditions = conditions, vars = vars)))
 	

diagnosis <- 
  
  diagnose_strategies(
    
   analysis_model = model,
   
   possible_data_list = possible_data_list,
   
   data_strategies = data_strategies,

   queries = "Y[X=1] - Y[X=0]",
   
   sims = 400
   
   )

write_rds(diagnosis, "saved/ch11diagnosisa.rds")
}

diagnosis <- read_rds("saved/ch13diagnosis.rds")
kable(diagnosis$diagnoses_df, digits = 3)

```



## Probative value of clues

## Effect Heterogeneity

## Uncertainty Regarding Assignment Processes


## Uncertainty regarding the probative value of clues

