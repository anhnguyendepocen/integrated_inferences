\documentclass[12pt,]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Integrated Inferences},
            pdfauthor={Macartan Humphreys and Alan Jacobs},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Integrated Inferences}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Macartan Humphreys and Alan Jacobs}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Draft!: 2020-01-14}

\usepackage{booktabs}
\usepackage{color}
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\emph{Quick Guide}

This book has four main parts:

\begin{itemize}
\item
  Part I introduces causal models and a Bayesian approach to learning about them and drawing inferences from them.
\item
  Part II applies these tools to strategies of learning from process tracing and mixed methods research, with applications.
\item
  Part III turn to design decisions, exploring strategies for assessing what kind of data is most useful for addressing different kinds of research questions given knowledge to date about a population or a case.
\item
  Everything up to Part IV assumes that we have access to models we are happy with. In Part IV we turn to the difficult question of model justification and outline a range of strategies on can use to justify causal models.
\end{itemize}

There is also an accompanying \texttt{R} package currently hosted on github. This can be downloaded and installed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"remotes"}\NormalTok{)}
\NormalTok{remotes}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"macartan/gbiqq"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

An \protect\hyperlink{examplesappendix}{appendix} shows how to use the package for defining and learning from a set of canonical causal models.

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We describe the book's general approach, and explain how it differs from current approaches in the social sciences. We preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences from evidence.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The engineer pressed the button, but the light didn't turn on.

``Maybe the bulb is blown,'' she thought.

She replaced the bulb, pressed the button and, sure enough, the light turned on.

``What just happened?'' asked her philosopher friend.

``The light wouldn't turn on because the bulb was busted, but I replaced the bulb and fixed the problem.''

``Such hubris!'' remarked her friend. ``If I understand you, you are saying that pressing the button \emph{would have} caused a change in the light \emph{if the bulb had not been busted}.''

``That's right.''

``But hold on a second. That's a causal claim about counterfactual events in counterfactual conditions that you couldn't have observed. I don't know where to begin. For one thing, you seem to be inferring from the fact that the light did not go on when you pressed the button the first time that pressing the button the first time had no effect at all. What a remarkable conclusion. Did it never occur to you that the light might have about to turn on anyway---and that your pressing that button at just that moment is what \emph{stopped} the light going on?''

``What's more,'' the philosopher went on, ``you seem also to be saying that pressing the button the second time \emph{did} have an effect because you saw the light go on that second time. That's rather incredible. That light could be controlled by a different circuit that was timed to turn it on at just the moment that you pressed the button the second time. Did you think about that possibility?''

``On top of those two unsubstantiated causal claims,'' the philosopher continued, ``you are \emph{also} saying, I think, that the difference between what you \emph{believe} to be a non-cause on the first pressing and a cause on the second pressing is itself due to the bulb. But, of course, countless other things could have changed! Maybe there was a power outage for a few minutes.''

``That hardly ever happens.''

``Well, maybe the light only comes on the second time the button is pressed.''

``It's not that kind of button.''

``So you say. But even if that's true, there are still so many other possible factors that could have mattered here---including things that neither of us can even imagine!''

The philosopher paused to ponder her friend's chutzpah.

``Come to think of it,'' the philosopher went on, ``how do you even know the bulb was busted?''

``Because the light worked when I replaced the bulb.''

``But that means,'' the philosopher responded, ``that your measurement of the state of the bulb depends on your causal inference about the effects of the button. And we know where that leads. Really, my friend, you are lost.''

``So do you want me to put the old bulb back in?''

\hypertarget{the-case-for-causal-models}{%
\section{The Case for Causal Models}\label{the-case-for-causal-models}}

In the conversation between the philosopher and the engineer, the philosopher disputes what seems a simple inference. Some of her arguments suggest a skepticism bordering on paranoia and seem easily dismissed. Others seem closer to hitting a mark: perhaps there was nothing wrong with the bulb and the button was just the kind that has to be pressed twice. For an objection like this, we have to rely on the engineer's knowledge of how the button works.

While the philosopher's skepticism guards against false inferences, it is also potentially paralyzing.

Social scientists have been shifting between the poles staked out by the philosopher and the engineer for many years. The engineers bring background knowledge to bear on a question and deploy models of broad processes to make inferences about particular cases. The philosophers bring a skeptical lense and ask for justifications that depend as little as possible on imported knowledge.

This book is written for would-be engineers. It is a book about how we can mobilize our background knowledge about how the world works to learn more about the world. It is, more specifically, a study in how we can use causal models of the world to design and implement empirical strategies of causal inferences. But we will try to imagine throughout that the engineers have philosophers looking over their shoulders and will try to figure out ways to equip the engineers with plausible answers to the the philosophers' worries.

By causal models, we mean systems of statements reflecting background knowledge of and uncertainty about possible causal linkages in a domain of interest. There are three closely related motivations for our move to examine the important role that models can play in empirical social inquiry. One is an interest in integrating qualitative knowledge with quantitative approaches, and a view---possibly a controversial one---that process tracing is a model-dependent endeavor. A second is concern over the limits of design-based inference. A third and related motive is an interest in better connecting empirical strategies to theory.

\hypertarget{the-limits-to-design-based-inference}{%
\subsection{The limits to design-based inference}\label{the-limits-to-design-based-inference}}

The engineer in our story tackles the problem of causal inference using models: theories of how the world works, generated from past experiences and applied to the situation at hand. The philosopher maintains a critical position, resisting models and the importation of beliefs not supported by evidence in the case at hand.

The engineer's approach recalls the dominant orientation among social scientists until rather recently. At the turn of the current century, multivariate regression had become a nearly indispensable tool of quantitative social science, with a large family of statistical models serving as political scientists' and economists' analytic workhorses for the estimation of causal effects.

Over the last two decades, however, the philosophers have raised a set of compelling concerns about the assumption-laden nature of standard regression analysis, while also clarifying how valid inferences can be made with limited resort to models in certain research situations. The result has been a growth in the use of design-based inference techniques that, in principle, allow for model-free estimation of causal effects (see \citet{dunning2012natural}, \citet{GerGreKap04}, \citet{druckman2011experimentation}, \citet{palfrey2009laboratory} among others). These include lab, survey, and field experiments and natural-experimental methods exploiting either true or ``as-if'' randomization by nature. With the turn to experimental and natural-experimental methods has come a broader conceptual shift, with a growing reliance on the ``potential outcomes'' framework as a model for thinking about causation (see \citet{Rubin1974}, \citet{splawa1990application} among others) and a reduced reliance on models of data-generating processes.

The ability to estimate average effects and to calculate \(p\)-values and standard errors without resort to models is an extraordinary development. In Fisher's terms, with these tools, randomization processes provide a ``reasoned basis for inference,'' placing empirical claims on a powerful footing.

While acknowledging the strengths of these approaches, we also take seriously two points of concern.

The first concern---raised by many in recent years (e.g., \citet{thelen2015comparative})---is about design-based inference's scope of application. While experimentation and natural experiments represent powerful tools, the range of research situations in which model-free inference is possible is inevitably limited. For a wide range of causal conditions of interest to social scientists and to society, controlled experimentation is impossible, and true or ``as-if'' randomization is absent. Moreover, limiting our focus to those questions for, or situations in which, exogeneity can be established ``by design'' would represent a dramatic narrowing of social science's ken. It would be a recipe for, at best, learning more and more about less and less. To be clear, this is not an argument against experimentation or design based inference; yet it is an argument for why social science needs a broader set of tools.

The second concern is more subtle. The great advantage of design-based inference is that it liberates researchers from the need to rely on models to make claims about causal effects. The risk is that, in operating model-free, researchers end up learning about effect sizes but not about models. But models are what we want to learn about. Our goal as social scientists is to have a useful model for how the world works, not simply a collection of claims about the effects different causes have had in different times and places. It is through models that we derive an understanding of how things might work in contexts and for processes and variables that we have not yet studied. Thus, our interest in models is intrinsic, not instrumental. By taking models, as it were, out of the equation, we dramatically limit the potential for learning about the world.

\hypertarget{qualitative-and-mixed-method-inference}{%
\subsection{Qualitative and mixed-method inference}\label{qualitative-and-mixed-method-inference}}

Recent years have seen the elucidation of the inferential logic behind ``process tracing'' procedures used in qualitative political science and other disciplines. In our read, the logic provided in these accounts depends on a particular form of model-based inference.\footnote{As we describe in \citet{humphreys2015mixing}, the term ``qualitative research'' means many different things to different scholars, and there are multiple approaches to mixing qualitative and quantitative methods. There we distinguish between approaches that suggest that qualitative and quantitative approaches address distinct, if complementary, questions; those that suggest that they involve distinct measurement strategies; and those that suggest that they employ distinct inferential logics. The approach that we employ in \citet{humphreys2015mixing} connects most with the third family of approaches. Most closely related, in political science, is work in \citet{GlynnQuinn2011}, in which researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. In the present book, however, we move toward a position in which fundamental differences between qualitative and quantitative inference tend to dissolve, with all inference drawing on what might be considered a ``qualitative'' logic in which the researcher's task is to confront a pattern of evidence with a theoretical logic.}

While process tracing as a method has been around for more than three decades (e.g., \citet{george1985case}), its logic has been most fully laid out by qualitative methodologists over the last 15 years (e.g., \citet{bennett2014process}, \citet{george2005case}, \citet{brady2010rethinking}, \citet{Hall2003aligning}, \citet{mahoney2010after}). Whereas \citet{king1994designing} sought to derive qualitative principles of causal inference within a correlational framework, qualitative methodologists writing in the wake of ``KKV'' have emphasized and clarified process-tracing's ``within-case'' inferential logic: in process tracing, explanatory hypotheses are tested based on observations of what happened within a case, rather than on covariation between causes and effects across cases. The process tracing literature has also advanced increasingly elaborate conceptualizations of the different kinds of probative value that within-case evidence can yield.

For instance, qualitative methodologists have explicated the logic of different test types (``hoop'', ``smoking gun'', etc.) involving varying degree of specificity and sensitivity (\citet{collier2011understanding}, \citet{Mahony:Logic:2012}). A smoking-gun test is a test that seeks information that is only plausibly present if a hypothesis is true (thus, generating strong evidence for the hypothesis if passed), a hoop test seeks data that should certainly be present if a proposition is true (thus generating strong evidence against the hypothesis if failed), and a doubly decisive test is both smoking-gun and hoop (for an expanded typology, see also \citet{rohlfing2013comparative}). Other scholars have expressed the leverage provided by process-tracing evidence in Bayesian terms, moving from a set of discrete test types to a more continuous notion of probative value (\citet{fairfield2017explicit}, \citet{BennettAppendix}, \citet{humphreys2015mixing}).\footnote{In \citet{humphreys2015mixing}, we use a fully Bayesian structure to generalize Van Evera's four test types in two ways: first, by allowing the probative values of clues to be continuous; and, second, by allowing for researcher uncertainty (and, in turn, updating) over these values. In the Bayesian formulation, use of process-tracing information is not formally used to conduct tests that are either ``passed'' or ``failed'', but rather to update beliefs about different propositions.}

Yet, conceptualizing the different ways in which probative value might operate leaves a fundamental question unanswered: what gives within-case evidence its probative value with respect to causal relations? We believe that, fundamentally, the answer lies in researcher beliefs that lies outside of the analysis in question. We enter a research situation with a model of how the world works, and we use this model to make inferences given observed patterns in the data---while at the same time updating those models based on the data. A key aim of this book is to demonstrate how models can --- and, in our view, must --- play in drawing case-level causal inferences.

As we will also argue, along with clarifying the logic of qualitative inference, causal models can also enable the systematic integration of qualitative and quantitative forms of evidence. Social scientists are increasingly pursuing mixed-method research designs. It is becoming increasingly common for scholars to pursue research strategies that combine quantitative with qualitative forms of evidence. A typical mixed-methods study includes the estimation of causal effects using data from many cases as well as a detailed examination of the processes taking place in a few. Prominent examples include Lieberman's study of racial and regional dynamics in tax policy (\citet{lieberman2003race}); Swank's analysis of globalization and the welfare state (\citet{swank2002global}); and Stokes' study of neoliberal reform in Latin America (\citet{stokes2001mandates}). Major recent methodological texts provide intellectual justification of this trend toward mixing, characterizing small-\(n\) and large-\(n\) analysis as drawing on a single logic of inference and/or as serving complementary functions (King, Keohane, and Verba, 1994; Brady and Collier, 2004). The American Political Science Association now has an organized section devoted in part to the promotion of multi-method investigations, and the emphasis on multiple strategies of inference research is now embedded in guidelines from many research funding agencies (Creswell and Garrett, 2008).

However, while scholars frequently point to the benefits of mixing correlational and process-based inquiry (e.g., \citet{collier2010sources}, p.\textasciitilde{}181), and have sometimes mapped out broad strategies of multi-method research design (\citet{Lieberman2005nested}, \citet{SeawrightGerring2008}), they have rarely provided specific guidance on how the integration of inferential leverage should unfold. In particular, the literature does has not supplied specific principles for aggregating findings---whether mutually reinforcing or contradictory---across different modes of analysis.A small number of exceptions stand out. In the approach suggested by \citet{gordon2004quantitative}, for instance, available expert (possibly imperfect) knowledge regarding the operative causal mechanisms for a small number of cases can be used to anchor the statistical estimation procedure in a large-N study. \citet{WesternJackman1994} propose a Bayesian approach in which qualitative information shapes subjective priors which in turn affect inferences from quantitative data. Relatedly, in \citet{GlynnQuinn2011}, researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. \citet{seawrightbook} presents an informal framework in which case studies are used to test the assumptions underlying statistical inferences, such as the assumption of no-confounding or the stable-unit treatment value assumption (SUTVA).

Yet we still lack a comprehensive framework that allows us to enter qualitative and quantitative form of information into an integrated analysis for the purposes of answering the wide range of causal questions that are of interests to social scientists, including questions about case-level explanations and causal effects, average causal effects, and causal pathways. As we aim to demonstrate in this book, grounding inference in causal models provides a very natural way of combining information of the \(X,Y\) variety with information about the causal processes connecting \(X\) and \(Y\). The approach can be readily addressed to both the case-oriented questions that tend to be of interest to qualitative scholars and the population-oriented questions that tend to motivate quantitative inquiry. As will become clear, in fact, when we structure our inquiry in terms of causal models, the conceptual distinction between qualitative and quantitative inference becomes hard to sustain. Notably, this is not for the reason that ``KKV'''s framework suggests, i.e., that all causal inference is fundamentally about correlating causes and effects. To the contrary, it is that in a causal-model-based inference, what matters for the informativeness of a piece of evidence is how that evidence is connected to our query, given how we think the world works. While the apparatus that we present is formal, the approach---in asking how pieces of evidence drawn from different parts of a process map on to a base of theoretical knowledge---is arguably most closely connected to process tracing in its core logic.

\hypertarget{connecting-theory-and-empirics}{%
\subsection{Connecting theory and empirics}\label{connecting-theory-and-empirics}}

Theory and empirics have had a surprisingly uncomfortable relationship in political science. In a major recent intervention, for instance, \citet{clarke2012model} draw attention to and critique political scientists' extremely widespread reliance on the ``hypothetico-deductive'' (H-D) framework, in which a theory or model is elaborated, empirical predictions derived, and data sought to test these predictions and the model from which they derive.Clarke and Primo draw on decades of scholarship in the philosophy of science pointing to deep problems with the HD framework, including with the idea that the truth of a model logically derived from first principles can be \emph{tested} against evidence.

This book is also motivated by a concern with the relationship between theory and evidence in social inquiry. In particular, we are struck by the frequent lack of a clear link between theory, on the one hand, and empirical strategy and inference, on the other. We see this ambiguity as relatively common in both qualitative and quantitative work. We can perhaps illustrate it best, however, by reference to qualitative work, where the centrality of theory to inference has been most emphasized. In process tracing, theory is what justifies inferences. In their classic text on case study approaches, \citet{george2005case} describe process tracing as the search for evidence of ``the causal process that a theory hypothesizes or implies'' (6). Similarly, \citet{Hall2003aligning} conceptualizes the approach as testing for the causal-process-related observable implications of a theory, \citet{mahoney2010after} indicates that the events for which process tracers go looking are those posited by theory (128), and \citet{gerring2006case} describes theory as a source of predictions that the case-study analyst tests (116). Theory, in these accounts, is supposed to help us figure out where to look for discriminating evidence.

What we do not yet have, however, is a systematic account of how researchers can derive within-case empirical predictions from theory and how exactly doing so provides leverage on a causal question. From what elements of a theory can scholars derive informative within-case observations? Given a set of possible things to be observed in a case, how can theory help us distinguish more from less informative observations? Of the many possible observations suggested by a theory, how can we determine which would add probative value to the evidence already at hand? How do the evidentiary requisites for drawing a causal inference, given a theory, depend on the particular causal question of interest---on whether, for instance, we are interested in identifying the cause of an outcome, estimating an average causal effect, or identifying the pathway through which an effect is generated? In short, how exactly can we ground causal inferences from within-case evidence in background knowledge about how the world works?

Most quantitative work in political science features a similarly weak integration between theory and research design. The modal inferential approach in quantitative work, both observational and experimental, involves looking for correlations between causes and outcomes, with minimal regard for intervening or surrounding causal relationships.\footnote{One exception is structural equation modeling, which bears a close affinity to the approach that we present in this book, but has gained minimal traction in political science.}

In this book, we seek to show how scholars can make much fuller and more explicit use of theoretical knowledge in designing their research projects and analyzing their observations. Like Clarke and Primo, we treat models not as maps of sort: maps, based on prior theoretical knowledge, about causal relations in a domain of interest. Also as in Clarke and Primo's approach, we do not write down a model in order to test its veracity. Rather, we show how we can systematically use causal models with particular characteristics to guide our empirical strategies and inform our inferences. Grounding our empirical strategy in a model allows us, in turn, to learn about the model itself as we encounter the data.

\hypertarget{key-contributions}{%
\section{Key contributions}\label{key-contributions}}

This book draws on methods developed in the study of Bayesian networks, a field pioneered by scholars in computer science, statistics, and philosophy. Bayesian networks, a form of causal model, have had limited traction to date in political science. Yet the literature on Bayesian networks and their graphical counterparts, directed acyclic graphs (DAGs), is a body of work that addresses very directly the kinds of problems that qualitative and quantitative scholars routinely grapple with.\footnote{For application to quantitative analysis strategies in political science, \citet{glynn2007non} give a clear introduction to how these methods can be used to motivate strategies for conditioning and adjusting for causal inference; \citet{garcia2015graphical} demonstrate how these methods can be used to assess claims of external validity. With a focus on qualitative methods, \citet{Waldner2015completeness} uses causal diagrams to lay out a `'completeness standard'' for good process tracing. \citet{weller2014finding} employ graphs to conceptualize the different possible pathways between causal and outcome variables among which qualitative researchers may want to distinguish. Generally, in discussions of qualitative methodology, graphs are used to capture core features of theoretical accounts, but are not developed specifically to ensure a representation of the kind of independence relations implied by structural causal models (notably what is called in the literature the ``Markov condition''). Moreover, efforts to tie these causal graphs to probative observations, as in \citet{Waldner2015completeness}, are generally limited to identifying steps in a causal chain that the researcher should seek to observe.}

Drawing on this work, we show in the chapters that follow how a theory can be formalized as a causal model represented by a causal graph and a set of structural equations. Engaging in this modest degree of formalization, we seek to demonstrate, yields enormous benefits. It allows us, for a wide range of causal questions, to identify a.) a set of variables (or nodes) in the model, including unobservable factors, that represent the causal query and b.) a set of observable variables that are potentially informative about the nodes in the query.

For students engaging in process tracing, the payoffs of this approach are that it provides:

\begin{itemize}
\item
  A grounding for probative value for data of any arbitrary kind, from different parts of any causal network. Including giving guidance on where there can be probative value
\item
  A way of aggregating inferences from observations drawn from all different parts of the causal network.
\item
  An approach for assessing a wide variety of estimands: e.g., how does inference differ for causal effects compared to mechanisms?
\item
  Consistency of probative value, priors, and therefore inferences with how you think the world works.
\item
  Transparency, allowing for evaluation
\item
  Design: diagnosis of different evidentiary and case-selection strategies, conditional on how you think the world works and the question you want to answer.
\end{itemize}

For mixed method inference:

\begin{itemize}
\item
  Systematic integration --- using both qual and quant to both help answer any given query. in fact, no fundamental difference between quant and qual data --- which may discomfit some readers, who see qual research as fundamentally distinct, but offers big advantages, including:
\item
  Transparency: how exactly the qual and the quant enter into the analysis.
\item
  A way to justify the background assumptions you've used
\item
  Learning in both directions: from cases to populations, from populations to cases
\item
  Which provides a model for cumulation. Models get updated and become priors for new analyses.
\item
  Design: diagnosis of wide vs deep, as well as evidentiary and case-selection strategies
\end{itemize}

As we will show, using causal models has substantial implications for common methodological advice and practice. To touch on just a few of these: Our elaboration and application of model-based process tracing shows that, given plausible causal models, process tracing's common focus on intervening causal chains may be much less productive than other empirical strategies, such as examining moderating conditions. Our examination of model-based case-selection indicates that for many common purposes there is nothing particularly especially informative about ``on the regression line'' cases or those in which the outcome occurred, and that case selection should often be driven by factors that have to date received little attention, such as the population distribution of cases and the probative value of the available evidence. And an analysis of clue-selection as a decision problem shows that the probative value of a piece evidence cannot be assessed in isolation, but hinges critically on what we have already observed.

\textbf{MORE TO COME HERE ELABORATING IMPLICATIONS, COMPARING TO ADVICE AND PRACTICE IN LITERATURE}

We emphasize that the basic analytical apparatus that we employ in this book is not new, and the book's aim is not to generate fundamentally novel approaches to study causality in the world. Rather, we see the book's goals as being of three kinds. First, the book aims to import insight: to introduce political scientists to an approach that has received little attention in the discipline but that can be useful for addressing the sorts of causal causal questions with which political scientists are commonly preoccupied. As a model-based approach, it is a framework especially well suited to a field of inquiry in which exogeneity frequently cannot be assumed by design---that is, in which we often have no choice but to be engineers. Second, the book draws connections between the Bayesian networks approach and key concerns and challenges with which methodologists in our discipline routinely grapple. Working with causal models and DAGs most naturally connects to concerns about confounding and identification that have been central to much quantitative methodological development. Yet we also show how causal models can address issues central to process tracing, such as how to select cases for examination, how to think about the probative value of causal process observations, and how to structure our search for evidence, given finite resources. Third, the book provides a set of usable tools for implementing the approach. We provide intuition and software that researchers can use to make research design choices and draw inferences from the data.

\hypertarget{the-road-ahead}{%
\section{The Road Ahead}\label{the-road-ahead}}

The book is divided into four main parts.

The first part is about the basics. We start off by describing the kinds of causal estimands of interest. The main goal here is to introduce the key ideas in the study of Bayesian nets and to argue for a focus of interest away from average treatment effects as go-to estimands of interest and towards a focus on causal nets, or causal structures, as the key quantity of interest. The next chapter introduces key Bayesian ideas; what Bayes' rule is and how to use it. The third chapter connects the study of Bayesian networks to theoretical claims. The key argument here is that nets should be thought of as theories which are themselves supportable by lower level networks (theories). Lower level theories are useful insofar as they provide leverage to learn about processes on higher level networks.

The second part applies these ideas to process tracing and mixed methods designs. Rather than conceptualizing process tracing as has been done in recent work as seeking process level data that is known to be informative about a causal claim, the approach suggested here is one in which the probative value of a clue is derived from its position in a causal network connecting variables of interest. Chapter 5 lays out the key logic of inference from clues and provides general criteria for assessing when it is and is not possible. Chapter 6 provides specific tools for assessing which collections of clues are most informative for a given estimand of interest and outlines a strategy for assessing which clues to gather when in a research process. Chapter 7 applies these tools to the problem of assessing the effects of economic inequality on democratization.

Chapter 8 moves to mixed data problems --- situations in which a researcher contains ``quantitative'' (\(X,Y\)) data on a set of cases and is considering gathering within case (``qualitative'') data on some of these. We argue that this situation is formally no different to the single case process tracing problem since a collection of cases can always be conceptualized as a single case with vector valued variables. The computational complexity is however greater in these cases and so in this chapter we describe a set of models that may be useful for addressing these problems. We conclude this part by revisiting the problem of inequality and democracy introduced in Chapter 7.

The third part focuses on reesarch design. In this framework the problem of case selection is equivalent to the kind of problem of clue selection discussed in Chapter 6. For a canonical multicase model however we use simulation approaches to provide guidance for how cases should be selected. The broad conclusion here is that researchers should go where the probative value lies, and all else equal, should select cases approximately proportional to the size of \(XY\) strata---whether or not these are ``on the regression line.''

The fourth part steps back and puts the model-based approach into question. We have been advocating an embrace of models to aid inference. But the dangers of doing this are demonstrably large. The key problem is that with model-based inference, the inferences are only as good as the model. In the end, while we are supporting the efforts of engineers, we know that the philosopher is right. This final part provides four responses to this (serious) concern. The first is that the dependence on models can sound more extreme than it is. Seemingly fixed parameters of models can themselves become quantities of interest in lower-level models, and there can be learning about these when higher-level models are studied. Thus models are both put to use and objects of interest. The second is that different types of conditional statements are possible; in particular as shown in work qualitative graphs. The third response points to the sort of arguments that can be made to support models, most importantly the importation of knowledge from one study to another. The last argument, presented in the last substantive chapter, highlights the tools to \emph{evaluate} models, using tools that are increasingly standard in Bayesian analysis.

Here we go.

\hypertarget{part-foundations}{%
\part{Foundations}\label{part-foundations}}

\hypertarget{models}{%
\chapter{Causal Models}\label{models}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We provide a lay language primer on the logic of causal models.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

While social scientific methods can be addressed to many sorts of questions, matters of causation have long been central to theoretical and empirical work in political science, economics, sociology, and psychology. Causality is also the chief focus of this book.

Causal knowledge, however, is not just the end goal of much empirical social science; it is also a key \emph{input} into causal inference. Rarely do we arrive at causal inquiry fully agnostic about causal relations in the domain of interest. Moreover, our beliefs about how the world works---as we show later in this book---have profound implications for how the research process and inference should unfold.

What we need is a language for expressing our prior causal knowledge such that we can full exploit it, drawing inferences and making research design decisions in ways that are logically consistent with our beliefs, and such that others can readily see and assess those underlying premises. Causal models provide such a language.

In this chapter we provide a basic introduction to causal models. Subsequent chapters in Part I layer on other foundational components of the book's framework, including a causal-model-based understanding of theory, the definition of common causal estimands within causal models, and the basics of Bayesian inference. While here we focus on the formal definition of causal models, in Chapter 10 we discuss strategies for generating them.

\hypertarget{the-counterfactual-model}{%
\section{The counterfactual model}\label{the-counterfactual-model}}

We begin with what we might think of as a meta-model, the counterfactual model of causation. The counterfactual model is the dominant approach to causal relations in the social sciences. At its core, a counterfactual understanding of causation captures a simple notion of causation as ``difference-making.''\footnote{The approach is sometimes attributed to David Hume, whose writing contains ideas both about causality as regularity and causality as counterfactual. On the latter the key idea is ``if the first object had not been, the second never had existed'' \citep[Section VIII]{hume2000enquiry}. More recently, the counterfactual view has been set forth by \citet{splawa1990application} and \citet{lewis1973counterfactuals}. See also \citet{lewis1986causation}.} In the counterfactual view, to say that \(X\) caused \(Y\) is to say: \emph{had} \(X\) been different, \(Y\) \emph{would have been} different. Critically, the antecedent, ``had \(X\) been different,'' imagines a \emph{controlled} change in \(X\)---an intervention that altered \(X\)'s value---rather than a naturally arising difference in \(X\). The counterfactual claim, then, is not that \(Y\) is different in those cases in which \(X\) is different; it is, rather, that if one could have \emph{made} \(X\) different, \(Y\) would have been different.

Turning to a substantive example, consider, for instance, the claim that India democratized (\(Y\)) because it had a relatively high level of economic equality (\(X\)) (drawing on the logic of \citet{boix2003democracy}). In the counterfactual view, this is equivalent to saying that, had India \emph{not} had a high level of equality---where we imagine that we \emph{made} equality in India lower---the country would not have democratized. High economic equality made a difference.

Along with this notion of causation as difference-making, we also want to allow for variability in how \(X\) acts on the world. \(X\) might sometimes make a difference, for some units of interest, yet sometimes not. High levels of equality might generate democratization in some countries or historical settings but not in others. Moroever, while equality might make democratization happen in some times in places, it might prevent that same outcome in others. In political science, we commonly employ the ``potential outcomes'' framework to describe the different kinds of counterfactual causal relations that might prevail between variables \citep{Rubin1974}. In this framework we characterize how a given unit responds to a causal variable by positing the outcomes that it \emph{would} take on at different values of the causal variable.

It is quite natural to think about potential outcomes in the context of medical treatment. Consider a situation in which some individuals in a diseased population are observed to have received a drug (\(X=0\)) while others have not (\(X=0\)). Assume that, subsequently, a researcher observes which individuals become healthy (\(Y=1\)) and which do not (\(Y=0\)). Let us further stipulate that each individual belongs to one of four unobserved response `'types,'' defined by the potential effect of treatment on the individual:\footnote{We implicitly invoke the assumption that the treatment or non-treatment of one patient has no effect on the outcomes of other patients. This is known as the stable unit treatment value assumption (SUTVA). See also \citet{HerronQuinn} for a similar classification of types.}

\begin{itemize}
\tightlist
\item
  \textbf{a}dverse: Those who would get better if and only if they do not receive the treatment
\item
  \textbf{b}eneficial: Those who would get better if and only if they do receive the treatment
\item
  \textbf{c}hronic: Those who will remain sick whether or not they receive treatment
\item
  \textbf{d}estined: Those who will get better whether or not they receive treatment
\end{itemize}

We can express this same idea by specifying the set of ``potential outcomes'' associated with each type of patient, as illustrated in Table \ref{tab:PO}.

\begin{longtable}[]{@{}lcccc@{}}
\caption{\label{tab:PO}. Potential outcomes: What would happen to each of four possible types of case if they were or were not treated.}\tabularnewline
\toprule
\small & Type a & Type b & Type c & Type d\tabularnewline
\midrule
\endfirsthead
\toprule
\small & Type a & Type b & Type c & Type d\tabularnewline
\midrule
\endhead
& \textbf{a}dverse effects & \textbf{b}eneficial Effects & \textbf{c}hronic cases & \textbf{d}estined cases\tabularnewline
Not treated & Healthy & Sick & Sick & Healthy\tabularnewline
Treated & Sick & Healthy & Sick & Healthy\tabularnewline
\bottomrule
\end{longtable}

In each column, we have simply written down the outcome that a patient of a given type would experience if they are not treated, and the outcome they would experience if they are treated.

Throughout the book, we generalize from this toy example. Whenever we have one causal variable and one outcome, and both variables are binary (i.e., each can take on two possible values, 0 or 1), then there are only four sets of possible potential outcomes, or ``causal types.'' More generally, for any pair of causal and outcome variables, we will use \(\theta^Y\) to denote the causal type at node \(Y\). We, further, add subscripts to denote particular types, as for instance with \(\theta^Y_{ij}\). Here \(i\) represents the case's potential outcome when \(X=0\) and \(j\) is the case's potential outcome when \(X=1\).

Incorporating this notation, when we have one binary causal variable and a binary outcome, the four types are:

\begin{itemize}
\tightlist
\item
  \textbf{a}: A negative causal effect of \(X\) on \(Y\). We write this as: \(\theta^Y = \theta^Y_{10}\).
\item
  \textbf{b}: A positive causal effect of \(X\) on \(Y\). We write this as: \(\theta^Y = \theta^Y_{01}\).
\item
  \textbf{c}: No causal effect, with \(Y\) ``stuck'' at 0. We write this as: \(\theta^Y = \theta^Y_{00}\).
\item
  \textbf{d}: No causal effect, with \(Y\) ``stuck'' at 1. We write this as: \(\theta^Y = \theta^Y_{11}\).
\end{itemize}

Table \ref{tab:POGEN} summarizes these types in terms of potential outcomes:

\begin{longtable}[]{@{}lcccc@{}}
\caption{\label{tab:POGEN}. Generalizing from Table \ref{tab:PO}, the table gives for each causal type the values that \(Y\) would take on if \(X\) is set at \(0\) and if \(X\) is set at 1.}\tabularnewline
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright
\small\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering
Type a\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering
Type b\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Type c\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Type d\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright
\small\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering
Type a\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering
Type b\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Type c\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Type d\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
\(\theta^Y=\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
\(\theta^Y=\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\(\theta^Y=\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\(\theta^Y=\theta^Y_{11}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright
Set \(X=0\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
\(Y(0)=1\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
\(Y(0)=0\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\(Y(0)=0\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\(Y(0)=1\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright
Set \(X=1\)\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
\(Y(1)=0\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
\(Y(1)=1\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\(Y(1)=0\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\(Y(1)=1\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Returning to our democratization example, let \(I=1\) represent a high level of economic equality and \(I=0\) its absence, with \(D=1\) representing democratization and \(D=0\) its absence. A \(\theta^D_{10}\) (\(a\)) type, then, is any case in which a high level of equality, if it occurs, \emph{prevents} democratization in a country that would otherwise have democratized. The causal effect of high equality in an \(a\) type is \(= -1\). A \(\theta^D_{01}\) (\(b\)) type is a case in which high equality, if it occurs, generates democratization in a country that would otherwise have remained non-democratic (effect \(= 1\)). A \(\theta^D_{00}\) (\(c\)) type is a case that will not democratize regardless of the level of equality (effect \(= 0\)); and a \(\theta^D_{11}\) (\(d\)) type is one that will democratize regardless of the level of equality (again, effect \(= 0\)).

In this setting, a causal \emph{explanation} of a given case outcome amounts to a statement about its type. The claim that India democratized because of a high level of equality is equivalent to saying that India democratized and is \(\theta^D_{01}\) type. To claim that Sierra Leone democratized because of low inequality is equivalent to saying that Sierra Leone democratized and is \(\theta^D_{10}\) type. To claim, on the other hand, that Malawi democratized for reasons having nothing to do with its level of economic equality is to characterize Malawi as a \(\theta^D_{11}\) type (which alreqdy specifies its outcome).

We can also use potential-outcomes reasoning for more complex causal relations. For example, supposing there are two binary causal variables \(X_1\) and \(X_2\), we can specify any given case's potential outcomes for each of the different possible combinations of causal conditions---there now being four such conditions (as each causal variable may take on \(0\) or \(1\) when the other is at \(0\) or \(1\)).

As for notation, we now need to expand \(\theta\)'s subscript since we need to represent the value that \(Y\) takes on under each of the four possible combinations of \(X_1\) and \(X_2\) values. We construct the four-digit subscript to with the ordering: \[\{Y|(X_1=0, X_2=0),Y|(X_1=1, X_2=0),Y|(X_1=0, X_2=1),Y|(X_1=1, X_2=1)\}\].
Thus, for instance, \(\theta^Y_{0100}\) means that \(Y\) is 1 if \(X_1=1\) and \(X_2=0\) and is 0 otherwise. We now have 16 causal types: 16 different patterns that \(Y\) might display in response to changes in \(X_1\) and \(X_2\). The full set is represented in Table \ref{tab:PO16}, which also makes clear how types are read off of four-digit subscripts. (The type numberings in the first column are, of course, arbitrary here and included for ease of reference.)

We can read off this table that for nodal type \(\theta^Y_{0101}\), \(X_1\) has a positive causal effect on \(Y\) but \(X_2\) has no effect, whereas for \(\theta^Y_{0011}\), \(X_2\) has a positive effect but \(X_1\) has none. We also capture interactions here. For instance, \(\theta^Y_{0001}\), \(X_2\) has a postive causal effect if and only if \(X_1\) is 1.

As one might imagine, the number of causal types increases rapidly as the number of considered causal variables increases, as it also would if we allowed \(X\) or \(Y\) to take on more than 2 possible values. However, the basic principle of representing possible causal relations as patterns of potential outcomes remains unchanged as long as variables are discrete.

\begin{table}[t]

\caption{\label{tab:PO16}With two binary causal variables, there are 16 causal types: 16 ways in which $Y$ might respond to changes in the two variables.}
\centering
\begin{tabular}{l|r|r|r|r}
\hline
\$\textbackslash{}theta\textasciicircum{}Y\$ & if \$X\_1=0, X\_2=0\$ & if \$X\_1=1,X\_2=0\$ & if \$X\_1=0,X\_2=1\$ & if \$X\_1=1, X\_2=1\$\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0000\}\$ & 0 & 0 & 0 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1000\}\$ & 1 & 0 & 0 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0100\}\$ & 0 & 1 & 0 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1100\}\$ & 1 & 1 & 0 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0010\}\$ & 0 & 0 & 1 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1010\}\$ & 1 & 0 & 1 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0110\}\$ & 0 & 1 & 1 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1110\}\$ & 1 & 1 & 1 & 0\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0001\}\$ & 0 & 0 & 0 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1001\}\$ & 1 & 0 & 0 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0101\}\$ & 0 & 1 & 0 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1101\}\$ & 1 & 1 & 0 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0011\}\$ & 0 & 0 & 1 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1011\}\$ & 1 & 0 & 1 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{0111\}\$ & 0 & 1 & 1 & 1\\
\hline
\$\textbackslash{}theta\textasciicircum{}Y\_\{1111\}\$ & 1 & 1 & 1 & 1\\
\hline
\end{tabular}
\end{table}

Readers will note that, in the counterfactual framework, causal relations are conceptualized as deterministic. A given case has a set of potential outcomes. Any randomness enters the analysis as incomplete knowledge of the factors influencing an outcome. But, in principle, if we knew all of the relevant causal conditions and the complete set of potential outcomes for a case, we could perfectly predict the actual outcome in that case. This understanding of causality---as ontologically deterministic, but empirically imperfectly understood---is compatible with views of causation commonly employed by qualitative researchers (see, e.g., \citet{mahoney2008toward}), and with understandings of causal determinism going back at least to \citet{laplace1901philosophical}. As we will see, we can readily express this kind of incompleteness of knowledge within a causal model framework; indeed, the way in which causal models manage uncertainty is central to how they allow us to pose questions of interest and to learn from evidence.

A further important, if somewhat counter-intuitive implication, of the counterfactual framework lies in how it forces us to think about multiple causes. When seeking to explain the outcome in a case, researchers sometimes proceed as though competing explanations amount to rival causes, where \(X_1\) being a cause of \(Y\) implies that \(X_2\) was not. Did Malawi democratize because it was a relatively economically equal society \emph{or} because of international pressure to do so? In the counterfactual model, however, causal relations are non-rival. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if \emph{either} supporter's vote were different. Put differently, when we say that \(X\) caused \(Y\) in a given case, we will generally mean that \(X\) was \emph{a} cause, \(X\) will rarely be \emph{the} cause in the sense of being the \emph{only} thing a change in which would have changed the outcome. Malawi might not have democratized if \emph{either} a relatively high level of economic equality or international pressure had been absent. For most social phenomena that we study, there will be multiple, and sometimes a great many, difference-makers for any given case outcome.

\hypertarget{causal-models-and-directed-acyclic-graphs}{%
\section{Causal Models and Directed Acyclic Graphs}\label{causal-models-and-directed-acyclic-graphs}}

Potential outcomes tables can capture quite a lot. We could, for instance, summarize our beliefs about the relationship between economic equality and democratization by saying that we think that the world is comprised of a mixture of \(a\), \(b\), \(c\), and \(d\) types, as defined above. We could get more specific and express a belief about what proportions of cases in the world are of each of the four types. For instance, we might believe that \(a\) types and \(d\) types are quite rare while \(b\) and \(c\) types are more common. Moreover, our belief about the proportions of \(b\) (positive effect) and \(a\) (negative effect) cases imply a belief about equality's \emph{average} effect on democratization as, in a binary setup, this quantity is imply the proportion of \(b\) types minus the proportion of \(a\) types.

As we have seen, beliefs about even more complex causal relations can be fully expressed in potential-outcomes notation. However, as causal structures become more complex---especially, as the number of variables in a domain increases---a causal model can be a powerful organizing tool. In this section, we show how causal models and their visual counterparts, directed acyclic graphs (DAGs), can represent substantive beliefs about counterfactual causal relationships in the world. The key ideas in this section can be found in many texts (see, e.g., Halpern and Pearl (2005) and Galles and Pearl (1998)), and we introduce here a set of basic principles that readers will need to follow the argumentation in this book.

To slightly shift the frame of our running example, suppose that we believe the level of economic inequality can have an effect on whether a country democratizes. We might believe inequality affects the likelihood of democratization by generating demands for redistribution, which in turn can cause the mobilization of lower-income citizens, which in turn can cause democratization. We might also believe that mobilization itself is not just a function of redistributive preferences but also of the degree of ethnic homogeneity, which shapes capacities of lower-income citizens for collective action. We can visualize this model in Figure \ref{fig:simpleDAG}.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/simpleDAG-1} 

}

\caption{A simple causal model in which high inequality ($I$) affects the democratization ($D$) via redistributive demands and mass mobilization ($M$), which is also a function of ethnic homogeneity ($E$). The arrows show relations of causal dependence between variables.  The graph does not capture the ranges of the variables and the functional relations between them.}\label{fig:simpleDAG}
\end{figure}

\hypertarget{components-of-a-causal-model}{%
\subsection{Components of a Causal Model}\label{components-of-a-causal-model}}

In the context of this example, let us now consider the three components of a causal model: variables, functions, and distributions.

\hypertarget{the-variables.}{%
\subsubsection{The variables.}\label{the-variables.}}

The first component of a causal model is the set of variables across which the model characterizes causal relations. On the graph in Figure \ref{fig:simpleDAG}, the 6 included variables are represented by the 6 nodes.

Notice that some of these variables have arrows pointing \emph{into} them: \(R, M\), and \(D\) are endogenous variables, meaning that their values are determined entirely by other variables in the model.

Other variables have arrows pointing out of them but no arrows pointing into them: \(I, E\) and \(U_D\) are exogenous variables. Exogenous variables are those that influence other variables in the model but themselves have no causes specified in the model. While \(I\) and \(E\) have natural interpretations, we might wonder what \(U_D\) represents as it does not feature in our substantive claims about how democratization arises. In the world of causal models, \(U\) terms are typically used to capture unspecified exogenous influences. Far from being nuisance terms, \(U\) variables constitute a key way in which we express uncertainty about the world and, in turn, are often the locus of learning about the questions we are asking. In the present example, we believe democratization to be potentially affected by mobilization, but we also know that democratization is affected by other things, even if we do not know what they are. We can thus think of \(U_D\) as a set of unknown factors---factors other than mobilization---that affect democratization.\footnote{Conventionally, we denote the set of exogenous variables as \(\mathcal{U}\) and the set of endogenous variables as \(\mathcal{V}\).}

In a causal-model framework, we sometimes use familial terms to describe relations among variables. For instance, two nodes directly connected by an arrow are known as ``parent'' and ``child,'' while two nodes with a child in common (both directly affect the same variable) are ``spouses.'' We can also say that \(I\) is an ``ancestor'' of \(D\) (a node upstream from \(D\)'s parent) and conversely that \(D\) is a descendant of \(I\) (a node downstream from \(I\)'s child).

In identifying the variables, we also need to specify the \emph{ranges} across which they can potentially vary. We might specify, for instance, that all variables in the model are binary, taking on the values 0 or 1. We could, alternatively, define a set of categories across which a variable ranges or allow a variable to take on any real number value or any value between a set of bounds. \footnote{If we let \(\mathcal{R}\) denote a set of ranges for all variables in the model, we can indicate \(X\)'s range, for instance, by writing \(\mathcal{R}(X)=\{0,1\}\). The variables in a causal model together with their ranges---the triple \((\mathcal{U}, \mathcal{V}, \mathcal{R})\)---are sometimes called a \emph{signature}, \(\mathcal{S}\).}

\hypertarget{the-functions.}{%
\subsubsection{The functions.}\label{the-functions.}}

Next, we need to specify our beliefs about the causal relations among the variables in our model. How is the value of one variable affected by, and how does it affect, the values of others? For each endogenous variable---each variable influenced by others in the model---we need to express beliefs about how its value is affected by its parents, its immediate causes.

The graph already represents some aspects of these beliefs: the arrows, or directed edges, tell us which variables we believe to be direct causal inputs into other variables. So, for instance, we believe that democratization (\(D\)) is determined jointly by mobilization (\(M\)) and some exogenous, unspecified factor (or set of factors), \(U_D\). We can think of \(U_D\) as all of the other influences on democratization, besides mobilization, that we either do not know of or have decided not to explicitly include in the model. We believe, likewise, that \(M\) is determined by \(I\) and an unspecified exogenous factor (or set of factors), \(U_M\). And we are conceptualizing inequality (\(I\)) as shaped solely by a factors exogenous to the model, captured by \(U_I\). (For all intents and purposes, \(I\) behaves as an exogenous variable here since its value is determined solely by an exogenous variable.)

We can also, however, express more specific beliefs about causal relations in the form of a causal function.\footnote{The collection of all causal functions in the model can be denoted as \(\mathcal{F}\).} Specifying a function means writing down whatever general or theoretical knowledge we have about the direct causal relations between variables. A function specifies how the value that one variable takes on is determined by the values that other variables---its parents---take on.

We can specify this relationship in a vast variety of ways. It is useful however to distinguish broadly between parametric and non parameteric approaches.

\begin{itemize}
\tightlist
\item
  A \emph{parametric} approach specifies a functional form that relates parents to children. For instance we might model one variable as a linear function of another. For instance, we can write \(R=\beta I\), where \(\beta\) is a parameter that we do not know the value of at the outset of a study but which we wish to learn about. If we believe \(D\) to be linearly affected by \(M\) but also subject to forces that we do not yet understand and have not yet specified in our theory, then we can write: \(D=\beta M+U_D\), where \(U_D\) represents a random disturbance. We can be still more agnostic by, for example including parameters that govern how other parameters operate. Consider, for instance the function, \(D=\beta M^{U_D}\). Here, \(D\) and \(M\) are linearly related if \(U_D=1\), but exponentially if \(U_D\) is anything other than \(1\). The larger point is that functions can be written to be quite specific or extremely general, depending on the state of prior knowledge about the phenomenon under investigation. The use of a structural model \emph{does not require precise knowledge of specific causal relations}, even of the functional forms through which two variables are related.
\end{itemize}

\begin{itemize}
\tightlist
\item
  With discrete data, causal functions can also take fully \emph{non-parametric} form, allowing for \emph{any possible relation} between parents and children. Let us, for instance, allow \(U_D\) to range across the four possible values, yielding the following causal function for \(D\):

  \begin{itemize}
  \tightlist
  \item
    if \(U_D=\theta^D_{10}\), then \(D=1-M\)
  \item
    if \(U_D=\theta^D_{01}\), then \(D=M\)
  \item
    if \(U_D=\theta^D_{00}\), then \(D=0\)
  \item
    if \(U_D=\theta^D_{11}\), then \(D=1\)
  \end{itemize}
\end{itemize}

We are, of course, drawing on our original four causal types from earlier in this chapter. Here, \(U_D\) is essentially a placeholder for causal types. We can think of it as an unknown factor that conditions the effect of mobilization on democratization, determining whether \(M\) has a negative effect, a positive effect, no effect with democratization never occurring, or no effect with democratization bound to occur regardless of mobilization.

Using our causal type framework, we can similarly use \(U\) terms to designate causal relations involving of any number of parent nodes. With two parent nodes, for instance, we simply use causal types of the form \(\theta^Y_{xxxx}\), as illustrated above.

The chapters to come operate in a non-parametric vein, with \(U\) terms as receptacles for causal types. To emphasize this feature, we switch notation and use \(\theta\) instead of \(U\). Thus, every substantively defined node, \(J\), in a graph has a \(\theta^J\) term pointing into it, and the value of \(\theta^J\) gives the mapping from \(J\)'s parents (if it has any) to the value of \(J\). The basic idea, applied to the binary variables in Figure @\ref(simpleDAG), is as follows:

\begin{itemize}
\tightlist
\item
  \textbf{For a node with no parents}: For an exogenous node like \(I\), \(\theta^I\) represents an ``assignment'' process and can take on one of two values, \(\theta^I_{0}\), meaning that \(I\) is ``assigned'' to \(0\) or \(\theta^I_{1}\), meaning that \(I\) is assigned to 1.
\item
  \textbf{For a node with 1 parent}: For endoegenous node \(R\), with only one parent (\(I\)), \(\theta^R\) takes on one of four values of the form \(\theta^R_{ij}\) (our four original types, \(\theta^R_{10}\), \(\theta^R_{01}\), etc.).
\item
  \textbf{For a node with 2 parents}: \(\theta^M\) will take on a possible 16 values of the form \(\theta^M_{ijkl}\) (\(\theta^M_{0000}\), \(\theta^M_{0001}\), etc.).
\item
  \textbf{And so on}
\end{itemize}

For analytic applications later in the book, we will want to be able to think both about the causal type operation at a particular \emph{node} and about \emph{collections} of causal types across a model. We thus refer to \(\theta^J\) as a unit's \emph{nodal} causal type, or simply nodal type, for \(J\).\footnote{The types here map directly into the four types, \(a\), \(b\), \(c\), \(d\), used in \citet{humphreys2015mixing} and into principal strata employed by Rubin and others. The literature on probabilistic models also refers to such strata as ``canonical partitions'' or ``equivalence classes.'' Note that this model is not completely general as the multinomial distribution assumes that errors are iid.} We refer to the collection of nodal types across all nodes for a given unit (i.e., a case) as the case's \emph{unit causal type}, or unit type, denoted by \(\theta\). Since the nodal types of exogenous include values of exogenous nodes, then the unit type fully specifies all node values as well as all \emph{counterfactual} node values for a unit.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Box:

DEFINITION OF NODAL CAUSAL TYPES

DEFINITION OF UNIT CAUSAL TYPES

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

It is thus worth dwelling for a moment on what this kind of function is doing. We have started with a graph in which mobilization can have an effect on democratization and the understanding that this effect, both its existence and its sign, may vary across cases. Cases, in other words, may be of different causal types. Further, we do not know what it is that shapes \(D\)'s response to \(M\)---what makes a case one type versus another. We thus use \(\theta_D\) as a stand-in for the unknown and unspecified moderators of \(M\)'s effect. We might, at this stage, wonder what the point is of including \(\theta_D\) in the model; are we not essentially just placing a question mark on the graph? We are, and that is precisely the point. As we will see in later chapters, non-substantive, causal-type nodes can play a key role in specifying (a) what we are uncertain about in a causal network and (b) what we would like to find out. Embedding our questions about the world directly into a model of the world, in turn, allows us to answer those questions in ways systematically and transparently guided by prior knowledge.

\hypertarget{interpretation-of-functional-equations}{%
\subsection{Interpretation of functional equations}\label{interpretation-of-functional-equations}}

A few important aspects of causal functions stand out. First, unlike regression equations and other equations describing data patterns, these functions express \emph{causal} beliefs. When we write \(D=\beta M\) as a function, we do not just mean that we believe the values of \(M\) and \(D\) in the world to be linearly related. We mean that we believe that the value of \(M\) \emph{determines} the value of \(D\) through this linear function. Functions are, in this sense, meant as \emph{directional} statements, with causes on the righthand side and an outcome on the left.

Second, to specify functions is to unpack a potentially complex web of causal relations into its constituent causal links. For each variable, we do not need to think through entire sequences of causation that might precede it. We need only specify how we believe it to be affected by its parents---that is to say, those variables pointing directly into it. Our outcome of interest, \(D\), may be a shaped by multiple, long chains of causality. To theorize how \(D\) is generated, however, we write down how we believe \(D\) is shaped by its immediate causes, \(M\) and \(U_D\). We then, separately, express a belief about how \(M\) is shaped by \emph{its} direct causes, \(R\) and \(E\). A variable's function must include as inputs all, and only, those variables that point directly into that variable.\footnote{The set of a variable's parents is required to be minimal in the sense that a variable is not included among the parents if, given the other parents, the child does not depend on it in any state that arises with positive probability.}

Third, as in the general potential-outcomes framework, all relations in a causal model are conceptualized as in principle deterministic. There is not as much at stake here though as you might think at first; by this we simply mean that a variable's value is \emph{determined} by the values of its parents \emph{along with} any stochastic or unknown components. We express uncertainty about causal relations, however, either as unknown paramaters (e.g., \(\beta\), above) or as random disturbances, the \(U\) terms, or the causal types \(\theta\).

Fourth, in a properly specified causal model \emph{the values of the exogenous variables}---those with no arrows pointing in to them---\emph{are sufficient to determine the values of all other variables in the model.} Consistent with more informal usage, we refer to a given set of values for all exogenous terms in a model as a \emph{context}. In causal model, context determines all other values. For instance, in Figure \ref{fig:simpleDAG}, knowing the values of \(I\), \(E\), and \(U_D\) as well as the causal functions (including the values of any parameters they contain) would tell us the values of \(R\), \(M\), and \(D\).

\hypertarget{the-distributions}{%
\subsubsection{The distributions}\label{the-distributions}}

Putting these components together gives what is termed a \emph{structural causal model.} In a structural causal model, all endogenous variables are, either directly or by implication, functions of a case's context (the values of the set of exogenous variables).\footnote{More formally, a \textbf{structural causal model} \emph{over} signature \(\mathcal{S}=<\mathcal{U},\mathcal{V},\mathcal{R}>\) is a pair \(<\mathcal{S}, \mathcal{F}>\), where \(\mathcal{F}\) is a set of ordered structural equations containing a function \(f_i\) for each element \(Y\in \mathcal{V}\). We say that \(\mathcal{F}\) is a set of ordered structural equations if no variable is its own descendant and if no element in \(\mathcal{U}\) is parent to more than one element of \(\mathcal{V}\). This last condition can be achieved by shifting any parent of multiple children in \(\mathcal{U}\) to \(\mathcal{V}\). This definition thus includes an assumption of acyclicity, which is not found in all definitions in the literature.} What we have not yet inscribed into the model, however, is any beliefs about how \emph{likely} or \emph{common} different kinds of contexts might be. Thus, for instance, a structural causal model consistent with Figure \ref{fig:simpleDAG} stipulates that \(I\), \(E\), and \(U_D\) may have effects on \(D\), but it says nothing in itself about the distribution of \(I\), \(E\), and \(U_D\) themselves, beyond limitations on their ranges.\footnote{Thus \(P(d|i,e, u_D)\) would defined by this structural model (as a degenerate distribution), but \(P(i)\), \(P(e)\), \(P(u_D)\), and \(P(i,e, u_D)\) would not be.} We have not said anything, for instance, about how common high inequality is across the relevant domain of cases, how common ethnic homogeneity is, or how unspecified inputs are distributed.

In many research situations, we will have beliefs not just about how the world works under different conditions, but also about what kinds of conditions are more likely than others. We can express these beliefs about context as probability distributions over the models exogenous terms.\footnote{We assume that the exogenous terms, the elements of \(\mathcal{U}\), are generated independently of one another. While this is not without loss of generality, it is not as constraining as it might at first appear: any graph in which two exogenous variables are not independent can be replaced by a graph in which these two terms are listed as endogenous (possibly unobserved) nodes, themselves generated by a third variable. Note also that one could envision ``incomplete probabilistic causal models'' in which researchers claim knowledge regarding distributions over \emph{subsets} of \(\mathcal{U}\).} For instance, a structural causal model might support a claim of the form: ``\(R\) has a positive effect on \(M\) if and only if \(E=1\) holds.'' We might, then, add to this a belief that \(E=1\) in 25\% of cases in the population of interest. Including this belief about context implies, in turn, that \(R\) has a positive effect on \(M\) a quarter of the time. As with the functions, we can also (and typically would) build uncertainty into this belief by specifying a \emph{distribution} over possible shares of cases with ethnic homogeneity, with our degree of uncertainty captured by the distribution's variance.

With our non parametric representation of functional forms, we let \(\lambda_j^X\) denote the probability that \(\theta^X = \theta^X_j\). For instance in a simple \(X \rightarrow Y\) model, \(\lambda^Y_{01}\) denotes the probability that \(\theta^Y = \theta^Y_{01}\). FLAG: FLESH OUT INCLUDING HOW CONFOUNDING IS TREATED

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Technical Note on the Markov Property
The assumptions that no variable is its own descendant and that the \(U\) terms are generated independently make the model \emph{Markovian}, and the parents of a given variable are Markovian parents. Knowing the set of Markovian parents allows one to write relatively simple factorizations of a joint probability distribution, exploiting the fact (``the Markov condition'') that all nodes are \emph{conditionally independent} of their nondescendants, conditional on their parents. Variables \(A\) and \(B\) are ``conditionally independent'' given \(C\) if \(P(a|b,c) = P(a|c)\) for all values of \(a, b\) and \(c\).\\
To see how this Markovian property allows for simple factorization of \(P\) for Figure \ref{fig:simpleDAG}, note that \(P(X, R, Y)\) can always be written as:
\[P(X, R, Y) = P(X)P(R|X)P(Y|R, X)\]
If we believe, as in the figure, that \(X\) causes \(Y\) only through \(R\) then we have the slightly simpler factorization:
\[P(X, R, Y) = P(X)P(R|X)P(Y|R)\]
Or, more generally:

\begin{equation} 
P(v_1,v_2,\dots v_n) = \prod P(v_i|pa_i)
\label{eq:markov}
\end{equation}

The distribution \(P\) on \(\mathcal{U}\) induces a joint probability distribution on \(\mathcal{V}\) that captures not just information about how likely different states are to arise but also the relations of conditional independence between variables that are implied by the underlying causal process. For example, if we thought that \(X\) caused \(Y\) via \(R\) (and only via \(R\)), we would then hold that \(P(Y | R) = P(Y | X, R)\): in other words if \(X\) matters for \(Y\) only via \(R\) then, conditional on \(R\), \(X\) should not be informative about \(Y\).\\
In this way, a probability distribution \(P\) over a set of variables can be consistent with some causal models but not others. This does not, however, mean that a specific causal model can be extracted from \(P\). To demonstrate with a simple example for two variables, any probability distribution on \((X,Y)\) with \(P(x)\neq P(x|y)\) is consistent both with a model in which \(X\) is a parent of \(Y\) and with a model in which \(Y\) is a parent of \(X\).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Once we introduce beliefs about the distribution of values of the exogenous terms in a model, we have specified a \emph{probabilistic causal model.} We need not say much more, for the moment, about the probabilistic components of causal models. But to foreshadow the argument to come, our prior beliefs about the likelihoods of different contexts play a central role in the framework that we present in this book. We will see how the encoding contextual knowledge---beliefs that some kinds of conditions are more common than others---forms a key foundation for causal inference. At the same time, our expressions of \emph{uncertainty} about context represent scope for learning: it is the very things that we are, at a study's outset, uncertain about that we can update our beliefs about as we encounter evidence.

\hypertarget{rules-for-graphing-causal-models}{%
\subsection{Rules for graphing causal models}\label{rules-for-graphing-causal-models}}

The diagram in Figure \ref{fig:simpleDAG} is a causal DAG \citep{hernan2006instruments}. We endow it with the interpretation that an arrow from a parent to a child that a change in the parent can, under some circumstances, induce a change in the child. Though we have already been making use of this causal graph to help us visualize elements of a causal model, we now explicitly point out a number of general features of causal graphs as we will be using them throughout this book. Causal graphs have their own distinctive ``grammar,'' a set of rules that give them important analytical features.

\textbf{Directed, acyclic.} A causal graph represents elements of a causal model as a set of nodes (or vertices), representing variables, connected by a collection of single-headed arrows (or directed edges). We draw an arrow from node \(A\) to node \(B\) if and only if we believe that \(A\) can have a direct effect on \(B\). The resulting diagram is a \emph{directed acyclic} graph (DAG) if there are no paths along directed edges that lead from any node back to itself---i.e., if the graph contains no causal cycles. The absence of cycles (or ``feedback loops'') is less constraining than it might appear at first. In particular if one thinks that \(A\) today causes \(B\) tomorrow which in turn causes \(A\) today, we can represent this as \(A_1 \rightarrow B \rightarrow A_2\) rather than \(A \leftrightarrow B\). That is, we timestamp the nodes, turning what might informally sppear as feedback into a non cyclical chain.

\textbf{Meaning of missing arrows.} The \emph{absence} of an arrow between \(A\) and \(B\) means that \(A\) is not a direct cause of \(B\).\footnote{By ``direct'' we mean that the \(A\) is a parent of \(B\): i.e., the effect of \(A\) on \(B\) is not fully mediated by one or more other variables in the model.} Here lies an important asymmetry: drawing such an arrow does not mean that we know that \(A\) \emph{does} directly cause \(B\); but omitting such an arrow implies that we know that \(A\) does \emph{not} directly cause \(B\). We say more, in other words, with the arrows we omit than with the arrows that we include.

Returning to Figure \ref{fig:simpleDAG}, we have here expressed the belief that redistributive preferences exert no direct effect on democratization; we have done so by \emph{not} drawing an arrow directly from \(R\) to \(D\). In the context of this model, saying that redistributive preferences have no direct effect on democratization is to say that any effect of redistributive preferences on democratization \emph{must} run through mobilization; there is no other pathway through which such an effect can operate. This might be a way of encoding the knowledge that mass preferences for redistribution cannot induce autocratic elites to liberalize the regime absent collective action in pursuit of those preferences.

The same goes for the effects of \(I\) on \(M\), \(I\) on \(D\), and \(E\) on \(D\): the graph in Figure \ref{fig:simpleDAG} implies that we believe that these effects also do not operate directly, but only along the indicated, mediated paths.

\textbf{Sometimes causes.} The existence of an arrow from \(A\) to \(B\) does not imply that \(A\) always has a direct effect on \(B\). Consider, for instance, the arrow running from \(R\) to \(M\). The existence of this arrow requires that \(M\) appear somewhere in the \(M\)'s functional equation as a variable's functional equation must include all variables pointing directly into it. Imagine, though, that \(M\)'s causal function is specified as: \(M = RE\). This function allows for the \emph{possibility} that \(R\) affects \(M\), as it will whenever \(E=1\). However, it also allows that \(R\) will have no effect, as it will when \(E=0\).

This example also, incidentally, demonstrates another important consequence of context, the values of the exogenous variables: a case's context determines not just the settings on the endogenous variables, but also the causal \emph{effects} that prevail among the variables. Under the functional equation \(M=RE\), a case's ethnic-compositional context determines whether or not redistributive preferences will have an effect on mobilization.

\textbf{Representing \(U\) on the graph}. As a matter of convention, explicitly including \(U\) terms is optional. In practice, \(U\)'s are often excluded from the visual representation of a model on the understanding that every variable on the graph is subject to some unaccounted-for influence and thus, implicitly, has a \(U\) term pointing into it. In this book, we will generally draw the \(U\) terms where they are of particular theoretical or analytical interest but will otherwise omit them. Whether we include or omit \(U\) terms, we will generally treat those nodes in a graph that have no arrows pointing into them as the exogenous variables that define the context.

\textbf{No excluded common causes, no unobserved confounding please.} Any cause common to multiple variables on the graph must itself be represented on the graph. If \(A\) and \(B\) on a graph are both affected by some third variable, \(C\), then we must represent this common cause. Put differently, any two variables without common causes on the graph are taken to be indepedent of one another. Thus, the graph in Figure \ref{fig:simpleDAG} implies that the values of \(I\), \(E\), and \(U_D\) are all determined independently of one another. If in fact we believed that a country's level of inequality and its ethnic composition were both shaped by, say, its colonial heritage, then this DAG would \emph{not} be an accurate representation of our beliefs about the world. To make it accurate, we would need to add to the graph a variable capturing that colonial heritage and include arrows running from colonial heritage to both \(I\) and \(E\).

This rule ensures that the graph captures all potential correlations among variables that are implied by our beliefs. If \(I\) and \(E\) are in fact driven by some common cause, then this means not just that these two variables will be correlated but also that each will be correlated with any consequences of the other. For instance, a common cause of \(I\) and \(E\) would also imply a correlation between \(R\) and \(E\). \(R\) and \(E\) are implied to be independent in the current graph but would be implied to be correlated if a common node pointed into both \(I\) and \(E\).

Of particular interest in Figure \ref{fig:simpleDAG} is the implied independence of \(U_D\) from every other node. Imagine, for instance, an additional node pointing into both \(I\) and \(U_D\). This would represent a classic form of confounding: the assignment of cases to values on the explanatory variable would be correlated with case's potential outcomes on \(D\). The omission of any such pathway is precisely equivalent to expressing the belief that \(I\) is exogenous, or (as if) randomly assigned.

\textbf{Representing excluded common causes, unobserved confounding if you have it.} It may be however that there are common causes for nodes that we simply do not understand. We are open to the idea that some unknown feature determines both \(I\) and \(D\). In this case it is as if \(U_I\) and \(U_D\) are not independently distributed. This is often represented by adding a dotted line, or a two headed arrow, connecting nodes whose shocks are not independent. Figure \ref{fig:simpleDAGb} illustrates. In general we will allow for this kind of unobserved confounding in the models in this book and seek to learn about the joint distribution of errors in such cases.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/simpleDAGb-1.pdf}
\caption{\label{fig:simpleDAGb}A DAG with unobserved confounding}
\end{figure}

\textbf{Licence to exclude variables.} The flip side of this rule is that a causal graph, to do the work it must do, does not need to include everything we know about a substantive domain of interest. We may know quite a lot about the causes of economic inequality, for example. But we can safely omit any other factor from the graph as long as it does not affect multiple variables in the model. Indeed, we can choose to capture any number of unspecified factors in a \(U\) term. We may be aware of a vast range of forces shaping whether countries democratize, but choose to bracket them for the purposes of an examination of the role of economic inequality. This bracketing is permissible as long as none of these unspecified factors also act on other variables included in the model.

\textbf{You can't read functional equations from a graph.} As should be clear, a DAG does not represent all features of a causal model. What it does record is which variables enter into the structural equation for every other variable: what can directly cause what. But the DAG contains no other information about the form of those causal relations. Thus, for instance, the DAG in Figure \ref{fig:simpleDAG} tells us that \(M\) is function of both \(R\) and \(E\), but it does not tell us whether that joint effect is additive (\(R\) and \(E\) separately increase mobilization), interactive (the effect of each depends on the value of the other), or whether either effect if linear, curvilinear or something else. This lack of information about functional forms often puzzles those encountering causal graphs for the first time; surely it would be convenient to visually differentiate, say, additive from conditioning effects. As one thinks about the variety of possible causal functions, it quickly becomes clear that there would be no simple visual way of capturing all possible functional relations. Moreover, as we shall now see, causal graphs are a tool designed with a particular analytic purpose in mind---a purpose to which we now turn.

\hypertarget{conditional-independence-from-dags}{%
\subsection{Conditional independence from DAGs}\label{conditional-independence-from-dags}}

If we encode our prior knowledge using the grammar of a causal graph, we can put that knowledge to work for us in powerful ways. In particular, the rules of DAG-construction allow for an easy reading of the \emph{conditional independencies} implied by our beliefs.

To begin thinking about conditional independence, it can be helpful to conceptualize dependencies between variables as generating \emph{flows of information}. Let us first consider a simple relationship of dependence. Returning to Figure \ref{fig:simpleDAG}, the arrow running from \(I\) to \(R\), implying a direct causal dependency, means that if we expect \(I\) and \(R\) to be correlated. Put differently, observing the value of one of these variables also gives us information about the value of the other. If we measured redistributive preferences, the graph implies that we would also be in a better position to infer the level of inequality, and vice versa. Likewise, \(I\) and \(M\) are also linked in a relationship of dependence: since inequality can affect mobilization (through \(R\)), knowing the the level of inequality would allow us to improve our estimate of the level of mobilization and vice versa.

In contrast, consider \(I\) and \(E\), which are in this graph indicated as being \emph{independent} of one another. Learning the level of inequality, according to this graph, would give us no information whatsoever about the degree of ethnic homogeneity, and vice-versa.

Moreover, sometimes what you learn depends on \emph{what you already know.} Suppose that we already knew the level of redistributive preferences. Would we then be in a position to learn about the level of inequality by observing the level of mobilization? According to this graph we would not: since the causal link---and, hence, flow of information between \(I\) and \(M\)---runs through \(R\), and we already know \(R\), there is nothing left to be learned about \(I\) by also observing \(M\). Anything we could have learned about inequality by observing mobilization is already captured by the level of redistributive preferences, which we have already seen. In other words, if we were not to include \(R\) in the causal model, then \(I\) and \(M\) would be dependent and informative about each other. When we do include \(R\) in the causal graph, \(I\) and \(M\) are independent of one another, hence uninformative about each other. We can express this idea by saying that \(I\) and \(M\) are \emph{conditionally independent given \(R\)}.

We say that two variables, \(A\) and \(C\), are ``conditionally independent'' given a set of variables \(\mathcal B\) if, once we have knowledge of the values in \(\mathcal B\), knowledge of \(A\) provides no information about \(C\) and vice-versa. Taking \(\mathcal B\) into account thus ``breaks'' any relationship that might exist unconditionally between \(A\) and \(C\).

To take up another example, suppose that war is a cause of both military casualties and price inflation, as depicted in Figure \ref{fig:warDAG}. Casualties and inflation will then be (unconditionally) correlated with one another because of their shared cause. If I learn that there have been military casualties, this information will lead me to think it more likely that there is also war and, in turn, price inflation (and vice versa). However, assuming that war is their only common cause, we would say that military casualties and price inflation are \emph{conditionally independent given war.} If we already know that there is war, then we can learn nothing further about the level of casualties (price inflation) by learning about price inflation (casualties). We can think of war, when observed, as blocking the flow of information between its two consequences; everything we would learn about inflation from casualties is already contained in the observation that there is war. Put differently, if we were just to look at cases where war is present (i.e., if we hold war constant), we should find no correlation between military casualties and price inflation; likewise, for cases in which war is absent.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/warDAG-1} 

}

\caption{This graph represents a simple causal model in which war ($W$) affects both military casualties ($C$) and price inflation ($P$).}\label{fig:warDAG}
\end{figure}

Relations of conditional independence are central to the strategy of statistical control, or covariate adjustment, in correlation-based forms of causal inference, such as regression. In a regression framework, identifying the causal effect of an explanatory variable, \(X\), on a dependent variable, \(Y\), requires the assumption that \(X\)'s value is conditionally independent of \(Y\)'s potential outcomes (over values of \(X\)) given the model's covariates. To draw a causal inference from a regression coefficient, in other words, we have to believe that including the covariates in the model ``breaks'' any biasing correlation between the value of the causal variable and its unit-level effect.

As we will explore, however, relations of conditional independence are of more general interest in that they tell us, given a model, \emph{when information about one feature of the world may be informative about another feature of the world, given what we already know}. By identifying the possibilities for learning, relations of conditional independence can thus guide research design.

To see more systematically ow a DAG can reveal conditional independencies, it is useful spell out three pairs of features of the flow of information in causal graphs:

\begin{figure}

{\centering \includegraphics[width=.9\textwidth]{ii_files/figure-latex/unnamed-chunk-6-1} 

}

\caption{\label{fig:CI} Three elementary relations of conditional independence.}\label{fig:unnamed-chunk-6}
\end{figure}

(1a) Information can flow unconditionally along a path of arrows pointing in the same direction. In Panel 1 of Figure \ref{fig:CI}, information flows across all three nodes. Learning about any one will tell us something about the other two.

(1b) Learning the value of a variable along a path of arrows pointing in the same direction \emph{blocks} flows of information across that variable. Knowing the value of \(B\) in Panel 1 renders \(A\) no longer informative about \(C\), and vice versa: anything that \(A\) might tell us about \(C\) is already captured by the information about \(B\).

(2a) Information can flow unconditionally across the branches of any forked path. In Panel 2 learning only \(A\) can provide information about \(C\) and vice-versa.

(2b) Learning the value of the variable at the forking point blocks \emph{flows} of information across the branches of a forked path. In Panel 2, learning \(A\) provides no information about \(C\) if we already know the value of \(B\).\footnote{Readers may recognize this statement as the logic of adjusting for a confound that is a cause of both an explanatory variable and a dependent variable in order to achieve conditional independence.}

(3a) When two or more arrowheads collide, generating an inverted fork, there is no unconditional flow of information between the incoming sequences of arrows. In Panel 3, learning only \(A\) provides no information about \(C\), and vice-versa.

(3b) Collisions can be sites of \emph{conditional} flows of information. In the jargon of causal graphs, \(B\) in Panel 2 is a ``collider'' for \(A\) and \(C\).\footnote{In the familial language of causal models, a collider is a child of two or more parents.} Although information does not flow unconditionally across colliding sequences, it does flow across them \emph{conditional} on knowing the value of the collider variable or any of its downstream consequences. In Panel 2, learning \(A\) \emph{does} provide new information about \(C\), and vice-versa, \emph{if} we also know the value of \(B\) (or, in principle, the value of anything that \(B\) causes).

The last point is somewhat counter-intuitive and warrants further discussion. It is easy enough to see that, for two variables that are correlated unconditionally, that correlation can be ``broken'' by controlling for a third variable. In the case of collision, two variables that are \emph{not} correlated when taken by themselves \emph{become} correlated when we condition on (i.e., learn the value of) a third variable, the collider. The reason is in fact quite straightforward once one sees it: if an outcome is a joint function of two inputs, then if we know the outcome, information about one of the inputs can provide information about the other input. For example, if I know that you have brown eyes, then learning that your mother has blue eyes makes me more confident that your father has brown eyes.

Looking back at our democratization DAG in Figure \ref{fig:simpleDAG}, \(M\) is a collider for \(R\) and \(E\), its two inputs. Suppose that we again have the functional equation \(M=RE\). Knowing about redistributive preferences alone provides no information whatsoever about ethnic homogeneity since the two are determined independently of one another. On the other hand, imagine that you already know that there was no mobilization. Now, if you observe that there \emph{were} redistributive preferences, you can figure out the level of ethnic homogeneity: it must be 0. (And likewise in going from homogeneity to preferences.)

Using these basic principles, conditional independencies can be read off any DAG. We do so by checking every path connecting two variables of interest and ask whether, along those paths, the flow of information is open or blocked, given any other variables whose values are already observed. Conditional independence is established when \emph{all} paths are blocked given what we already know; otherwise, conditional independence is absent.

\hypertarget{a-simple-running-example}{%
\subsection{A simple running example}\label{a-simple-running-example}}

We will illistrate these core ideas with a simple running example of a model of government corruption and survival.

We begin with two binary features of context. Consider, first, that a country may or may not have a free press (\(X\)). Second, the country's government may or may not be sensitive to public opinion (\(S\)).\footnote{Government sensitivity here can be thought of as government sophistication (does it take the actions of others into account when making decisions?) or as a matter of preferences (does it have a dominant strategy to engage in corruption?).} Let us then stipulate what follows from these conditions. The government will engage in corruption (\(C=1\)) unless it is sensitive to public opinion and there is a free press. Moreover, if and only if there is both government corruption and a free press, the press will report on the corruption (\(R=1\)). Finally, the government will be removed from office (\(Y=1\)) if it has acted corruptly and this gets reported in the press; otherwise, the government remains in office.

As a set of equations, this simple causal model may be written as follows:

\(\begin{array}{ll} C = 1-X\times S & \mbox{Whether the government is corrupt}\\ R = C\times X & \mbox{Whether the press reports on corruption}\\ Y = C\times R & \mbox{Whether the government is removed from office} \end{array}\)

One thing that these equations make clear is that the variables in our model function in various places as causal-type nodes for one another. For instance, we can see from equation for \(C\) that the causal effect of a free press (\(X\)) on corruption (\(C\)) depends on whether the government is sensitive to public opinion (\(S\)): \(S\) determines \(C\)'s response to \(X\) (as does \(X\) for \(S\)'s effect on \(C\)). A similar relationship holds for \(C\) and \(X\) in their effect on \(R\) and for \(C\) and \(R\) in their effect on \(Y\). As we will see below, the model also implies more complex causal-type relationships. We can, further, substitute through the causal processes to write down the functional equation for the outcome in terms of the two initial causal variables: \(Y=(1-S)X\).\footnote{In Boolean notation (but preserving a structural equation interpretation), where \(Y\) stands for the occurrence of government removal, \(Y= \neg S \land X\); and the function for the outcome ``government retained'' can be written \(\neg Y = (S\land X) \lor (S\land\neg X) \lor (\neg S \land \neg X)\) or, equivalently, \(\neg Y = S + \neg S \neg X\).}

Let us, further, allow the two primary causal variables---the existence of a free press and the existence of a sensitive government---to vary probabilistically. In particular, we represent the probability of a free press with the population parameter \(\lambda^X_1\) and the probability of a sensitive government with the parameter \(\lambda^S_1\).

Note that in this model, only the most ``senior'' specified variables, \(X\) and \(S\), have a stochastic component (i.e., \(\lambda^X_1\) and \(\lambda^S_1\) lie between 0 and 1). All other endogenous variables are deterministic functions of other specified variables (put differently: each node has only a single nodal type).

\begin{figure}

{\centering \includegraphics[width=\textwidth]{ii_files/figure-latex/running-1} 

}

\caption{The figure shows a simple causal model. $S$ and $X$ are stochastic, other variables fully determined by their parents, as shown in bottom right panel.}\label{fig:running}
\end{figure}

The corresponding causal diagram for this model is shown in Figure \ref{fig:running}.

In later chapters we will develop this model and use it to illustrate different estimands and different strategies for case level inference.

\hypertarget{illustrations}{%
\section{Illustrations}\label{illustrations}}

We can provide more of a sense of how one might encode prior knowledge in a causal model by asking how we might construct models in light of extant scholarly works. We undertake this exercise here for three well-known works in comparative politics and international relations: Pierson's seminal book on welfare-state retrenchment (\citet{pierson1994dismantling}); Elizabeth Saunders' research on leaders' choice of military intervention strategies (\citet{saunders2011leaders}); and Przeworski and Limongi's work on democratic survival (\citet{przeworski1997modernization}), an instructive counterpoint to Boix's (\citet{boix2003democracy}) argument about a related dependent variable. For each, we represent in the form of a causal model the causal knowledge that we might plausibly think we take away from the work in question. Readers might represent these knowledge bases differently; our present aim is merely to illustrate how causal models are constructed, rather than to defend a particular representation (much less the works in question) as accurate.

\hypertarget{welfare-state-reform-pierson-1994}{%
\subsection{Welfare state reform: Pierson (1994)}\label{welfare-state-reform-pierson-1994}}

The argument in Pierson's 1994 book \emph{Dismantling the Welfare State?} challenged prior notions of post-1980 welfare-state retrenchment in OECD countries as a process driven primarily by socioeconomic pressures (slowed growth, rising unemployment, rising deficits, aging populations) and the rise of market-conservative ideologies (embodied, e.g., the ascendance of Thatcher and Reagan). Pierson argues that socioeconomic and ideological forces put retrenchment on the policy agenda, but do not ensure its enactment because retrenchment is a politically perilous process of imposing losses on large segments of the electorate. Governments will only impose such losses if they can do so in ways that allow them avoid blame for doing so---by, for instance, making the losses hard to perceive or responsibility for them difficult to trace. These blame-avoidance opportunities are themselves conditioned by the particular social-program structures that governments inherit.

\begin{figure}

{\centering \includegraphics[width=.7\textwidth]{ii_files/figure-latex/unnamed-chunk-7-1} 

}

\caption{\label{fig:DAGPierson} A graphical representation of Pierson (1994).}\label{fig:unnamed-chunk-7}
\end{figure}

While the argument has many more specific features (e.g., different program-structural factors that matter, various potential strategies of blame-avoidance), its essential components can be captured with a relatively simple causal model. We propose such a model in graphical form in Figure \ref{DAGPierson}. Here, the outcome of retrenchment (\(R\)) hinges on whether retrenhcment makes it onto the agenda (\(A\)) and on whether blame-avoidance strategies are available to governments (\(B\)), and on some unspecified random input (\(U_R\)). Retrenchment emerges on the policy agenda as a consequence of both socioeconomic developments (\(S\)) and the ascendance of ideologically conservative political actors (\(C\)). Inherited program structures (\(P\)), meanwhile, determine the availability of blame-avoidance strategies.

A few features of this graph warrant attention. As we have discussed, it is the omitted arrows in any causal graph that imply the strongest statements. The graph in Panel (a) implies that \(C\), \(S\), \(P\), and \(U_R\)---which are neither connected along a directed path nor downstream from a common cause---are independent of one another. This implies, for instance, that whether conservatives govern is independent of whether program structures will allow for blame-free retrenchment. Thus, as Pierson argues, a Reagan or Thatcher can come to power but nonetheless run up against an opportunity structure that would makes retrenchment politically perilous. Further, in this graph any effect of program structures on retrenchment \emph{must} run through their effects on blame-avoidance opportunities. One could imagine relaxing this restriction by, for instance, drawing an arrow from \(P\) to \(A\): program structures might additionally affect retrenchment by conditioning the fiscal costliness of the welfare state, thus helping to determine whether reform makes it onto the agenda.

Where two variables \emph{are} connected by an arrow, moreover, this does not imply that a causal effect will always operate. Consider, for instance, the arrow pointing from \(A\) to \(R\). The fact that \(A\) sometimes affects \(R\) and sometimes does not is, in fact, central to Pierson's argument: conservatives and socioeconomic pressures forcing retrenchment on the agenda will \emph{not} generate retrenchment if blame-avoidance opportunities are absent.

The graph also reflects a choice about where to begin. We could, of course, construct a causal account of how conservatives come to power, how socioeconomic pressures arose, or why programs were originally designed as they were. Yet it is perfectly permissible for us to bracket these antecedents and start the model with \(C\), \(S\), and \(P\), as long as we do not believe that these variables have any antecedents in common. If they do have common causes, then this correlation should be captured in the DAG.\footnote{In DAG syntax, this correlation can be captured by placing the common cause(s) explicitly on the graph or by drawing a dashed line between the correlated nodes, leaving the source of the correlation unspecified.}

The DAG itself tells us about the possible direct causal dependencies but is silent on the ranges of and functional relations among the variables. How might we express these? With three endogenous variables, we need three functions indicating how their values are determined. Moreover, every variable pointing directly into another variable must be part of that second variable's function. Let us assume all variables are binary, with each condition either absent or present. We can capture quite a lot of Pierson's theoretical logic with the following quite simple functional equations:

\begin{itemize}
\tightlist
\item
  \(A=CS\), implying that retrenchment makes it on the agenda if and only if both conservatives are in power and socioeconomic pressures are high.
\item
  \(B=P\), implying that blame-avoidance opporunities arise when and only when program structures take a particular form
\item
  \(R=ABU_R\).
\end{itemize}

This last functional equation requires a little bit of explanation. Here we are saying that retrenchment will only occur if retrenchment is on the agenda and blame-avoidance opportunities are present (as the expression zeroes out if either of these are 0). Yet even if both are present, the effect on retrenchment also hinges on the value of \(U_R\). \(U_R\) thus behaves as a causal-type variable with respect to the effect of an \(AB\) combination on \(R\) and allows for two possible types. When \(U_R=1\), the \(AB\) combination has a positive causal effect on retrenchment. When \(U_R=0\), \(AB\) has no causal effect: retrenchment will not occur regardless of the presence of \(AB\). A helpful way to conceptualize what \(U_R\) is doing is that is capturing a collection of features of a case's context that might render the case susceptible or not susceptible to an \(AB\) causal effect. For instance, Pierson's analysis suggests that a polity's institutional structure might widely diffuse veto power such that stakeholders can block reform even when retrenchment is on the agenda and could be pursued without electoral losses. We could think of such a case as having a \(U_R\) value of 0, implying that \(AB\) has no causal effect. A \(U_R=1\) case, with a positive effect, would be one in which the government has the institutional capacity to enact reforms that it has the political will to pursue.

\hypertarget{military-interventions-saunders-2011}{%
\subsection{Military Interventions: Saunders (2011)}\label{military-interventions-saunders-2011}}

\citet{saunders2011leaders} asks why, when intervening militarily abroad, do leaders sometimes seek to transform the \emph{domestic} political institutions of the states they target but sometimes seek only to shape the states' external behaviors. Saunders' central explanatory variable is the nature of leaders' causal beliefs about security threats. When leaders are ``internally focused,'' they believe that threats in the international arena derive from the internal characteristics of other states. Leaders who are ``externally focused,'' by contrast, understand threats as emerging strictly from other states' foreign and security policies. These basic worldviews, in turn, affect the cost-benefit calculations they make about intervention strategies, via two mechanisms. Most simply, these beliefs affect perceptions of the likely security gains from a transformative intervention strategy. In addition, these beliefs affect the kinds of strategic capabilities in which leaders invest, which in turn effects the costliness and likelihood of success of alternative intervention strategies. Calculations about the relative costs and benefits of different strategies then shape the choice between a transformative and non-transformative approach to intervention. Yet leaders can, of course, only choose one of these options if they decide to intervene at all. The decision about whether to intervene depends, in turn, on at least two kinds of considerations. A leader is more likely to intervene against a given target when the nature of the dispute makes the leader's preferred strategy---given their causal beliefs---appear feasible in this situation; yet leaders may also be pushed to intervene by international or domestic audiences.

Figure \ref{fig:DAGSaunders} depicts the causal dependencies in Saunders' argument in DAG form. Working from left to right, we see that causal beliefs (\(C\)) affect the expected net relative benefits of the two strategies (\(B\)) both via a direct pathway and via an indirect pathway running through preparedness investments (\(P\)). Characteristics of a given target state or dispute (\(T\)) likewise influence \(B\). The decision about whether to intervene (\(I\)) is then a function of three factors: causal beliefs (\(C\)), the expected relative net benefits of the strategies (\(B\)), and audience pressures (\(A\)). Finally, the choice of strategy (\(S\)) is a function of whether or not intervention occurs at all (\(I\)), cost-benefit comparisons between the two strategies (\(B\)), and other, idiosyncratic factors that may operate in various cases (\(U_S\)).

\begin{figure}

{\centering \includegraphics[width=.7\textwidth]{ii_files/figure-latex/unnamed-chunk-8-1} 

}

\caption{\label{fig:DAGSaunders} A graphical representation of Saunders' (2011) argument.}\label{fig:unnamed-chunk-8}
\end{figure}

This relatively complex DAG illustrates how readily DAGs can depict the multiple pathways through which a given variable might affect another variable, as with the multiple pathways linking \(C\) to \(I\) and \(B\) (and, thus, all of its causes) to \(S\). In fact, this graphical representation of the dependencies in some ways throws the multiplicity of pathways into even sharper relief than does a narrative exposition of the argument. For instance, Saunders draws explicit attention to how causal beliefs operate on expected net benefits via both a direct and indirect pathway, both of which are parts of an indirect pathway from \(C\) to the outcomes of interest, \(I\) and \(S\). What is a bit easier to miss without formalization is that \(C\) also acts \emph{directly} on the choice to intervene as part of the feasibility logic: when leaders assess whether their generally preferred strategy would be feasible if deployed against a particular target, the generally preferred strategy is itself a product of their causal beliefs. The DAG also makes helpfully explicit that the two main outcomes of interest---the choice about whether to intervene and the choice about how---are not just shaped by some of the same causes but are themselves causally linked, with the latter depending on the former.

Omitted links are also notable. For instance, the lack of an arrow between \(T\) and \(A\) suggests that features of the target that affect feasibility have no effect on audience pressures. If instead we believed, for instance, that audiences take feasibility into account in demanding intervention, we would want to include a \(T \rightarrow A\) arrow.

Turning to variable ranges and functional equations, it is not hard to see how one might readily capture Saunders' logic in a fairly straightforward set-theoretic manner. All variables except \(S\) could be treated as binary with, for instance, \(C=1\) representing internally focused causal beliefs, \(P=1\) representing preparedness investments in transformation, \(B=1\) representing expectations that transformation will be more net beneficial than non-transformation, \(T=1\) meaning that a target has characteristics that make transformation a feasible strategy, and so on. Although there are two strategies, we in fact need three values for \(S\) because it must be defined for all values of the other variables---i.e., it must take on a distinct categorical value if there is no intervention at all. We could then define functions, such as:

\begin{itemize}
\tightlist
\item
  \(B=CPT\), implying that transformation will only be perceived to be net beneficial in a case if and only if the leader has internally focused causal beliefs, the government is prepared for a transformative strategy, and the target has characteristics that make transformation feasible
\item
  \(I=(1-|B-C|)+(1-(1-|B-C|))A\), implying that intervention can occur under (and only under) either of two alternative sets of conditions: if the generally preferred strategy and the more net-beneficial strategy in a given case are the same (i.e., such that \(B-C=0\)) or, when this alignment is absent (i.e., such that \(|B-C|=0\)), where audiences pressure a leader to intervene.
\end{itemize}

\hypertarget{development-and-democratization-przeworski-and-limongi-1997}{%
\subsection{Development and Democratization: Przeworski and Limongi (1997)}\label{development-and-democratization-przeworski-and-limongi-1997}}

\citet{przeworski1997modernization} argue that democratization occurs for reasons that are, with respect to socioeconomic or macro-structural conditions, largely idiosyncratic; but once a country has democratized, a higher level of economic development makes democracy more likely to survive. Economic development thus affects whether or not a country is a democracy, but only after a democratic transition has occurred, not before. Thus, unlike in \citet{boix2003democracy}, democratization in Przeworski and Limongi's argument is exogenous, rather than being determined by other variables in the model. Moreover, the dynamic component of Przeworski and Limongi's argument---the fact that both the presence of democracy and the causal effect of development on democracy depend on whether a democratic transition occurred at a previous point in time---forces us to think about how to capture over-time processes in a causal model.

We represent Przeworski and Limongi's argument in the DAG in Figure \ref{fig:DAGPL}. The first thing to note is that we can capture dynamics by considering democracy at different points in time as separate nodes. According to the graph, whether a country is a democracy in a given period (\(D_t\)) is a function, jointly, of whether it was a democracy in the previous period (\(D_{t-1}\)) and of the level of per capita GDP in the current period, as well as of other unspecified forces (\(U_{D_t}\)) that lie outside the model.

\begin{figure}

{\centering \includegraphics[width=.7\textwidth]{ii_files/figure-latex/DAGPL-1} 

}

\caption{A graphical representation of Przeworski and Limongi's argument, where $D_{t-1}$=democracy in the previous period; $GDP_t$=per capita GDP in the current period; $D_t$=democracy in the current period.}\label{fig:DAGPL}
\end{figure}

Second, the arrow running from \(GDP_{t-1}\) to \(D_t\) means that \(GDP\) \emph{may} affect democracy, not that it always does. Indeed, Przeworski and Limongi's argument is that development's effect depends on a regime's prior state: GDP matters for whether democracies continue to be democracies, but not for whether autocracies go on to become democracies. The \emph{lack} of an arrow between \(D_{t-1}\) and \(GDP_{t-1}\), however, implies a (possibly incorrect) belief that democracy and \(GDP\) in the last period are independent of one another.

Finally, we might consider the kind of causal function that could capture Przeworski and Limongi's causal logic. In this function, \(GDP\) should reduce the likelihood of a transition \emph{away} from democracy but not affect the probability of a transition \emph{to} democracy, which should be exogenously determnined. One possible translation of the argument into functional terms is:

\[d_t = 1 (p(1-d_{t-1}) + d_{t-1}(1-q(1-gdp)) > u_{D_t})\]

where

\begin{itemize}
\tightlist
\item
  \(d_t\) and \(d_{t-1}\) are binary, representing current and last-period democracy, respectively
\item
  \(p\) is a parameter, varying from 0 to 1, representing the probability that an autocracy democratizes
\item
  \(q\) is a parameter, varying from 0 to 1, representing the probability that a democracy with a GDP of 0 reverts to autocracy
\item
  \(gdp\) represents national per capita GDP, normalized on a 0 to 1 scale for the population of interest.
\item
  \(u_{D_t}\) represents a random, additional input into democracy with a uniform distribution on the 0 to 1 scale
\item
  the indicator function, \({1}\), evaluates the inequality and generates a value of \(1\) if and only if it is true
\end{itemize}

Unpacking the equation, the likelihood that a country is a democracy in a given period rises and falls with the expression to the left of the \(>\)-operator. This expression itself has two parts, reflecting the difference between the determinants of \emph{transitions to} democracy (captured by the first part) and the determinants of democratic \emph{survival} (captured by the second). The first part comes into play---i.e., is non-zero---only for non-democracies. For non-democracies, the expression evaluates simply to \(p\), the exogenous probability of democratization. The second part is non-zero only for democracies, where it evaluates to \(1-q\)---the inverse of the reversion parameter---times \(1-gdp\): thus, the reversion probability falls as national income rises. The inequality is then evaluated by ``asking'' whether the expression on the left (either \(p\) or \((1-q)gdp\)) is greater than a number (\(u_{D_t}\)) randomly drawn from a uniform distribution between 0 and 1. Thus, higher values for the expression increase the likelihood of democracy while the randomness of the \(u_{D_t}\) threshold captures the role of other, idiosyncratic inputs.

Note how, while the functional equation nails down certain features of the process, it leaves others up for grabs. In particular, the parameters \(p\) and \(q\) are assumed to be constant for all autocracies and for all democracies, respectively, but their values are left unspecified. And one could readily write down a function that left even more openness---by, for instance, including an unknown parameter that translates \(GDP\) into a change in the probability of reversion or allowing for non-linearities, with unknown parameters, in this effect.

\hypertarget{chapter-appendix}{%
\section{Chapter Appendix}\label{chapter-appendix}}

\hypertarget{steps-for-constructing-causal-models}{%
\subsection{Steps for constructing causal models}\label{steps-for-constructing-causal-models}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Box: \textbf{Steps for constructing causal models}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify a set of variables in a domain of interest
\end{enumerate}

\begin{itemize}
\tightlist
\item
  You should specify the range of each variable: is it continuous or discrete?
\item
  May include \(U\) terms representing unspecified, random influences
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Draw a causal graph (DAG) representing beliefs about causal dependencies among these variables
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Capture direct effects only
\item
  Arrows indicate \emph{possible}, not constant or certain, causal effects
\item
  The absence of an arrow between two variables indicates a belief of \emph{no} direct causal relationship between them
\item
  Ensure that the graph captures all correlations among variables. This means that either (a) any common cause of two or more variables is included on the graph (with implications for Step 1) or (b) correlated variables are connected with a dashed, undirected edge.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Write down one causal function for each endogenous variable
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Each variable's function must include all variables directly pointing into it on the graph
\item
  Functions may take any form, as long as each set of possible causal values maps onto a single outcome value
\item
  Functions may express arbitrary amounts of uncertainty about causal relations
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  State probabilistic beliefs about the distributions of the exogenous variables
\end{enumerate}

\begin{itemize}
\tightlist
\item
  How common or likely to do we think different values of the exogenous variables are?
\item
  Are they independently distributed? If in step 2 you drew an undirected edge between nodes then you believe that the connected variables are not independently distributed.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{model-construction-in-code}{%
\subsection{Model construction in code}\label{model-construction-in-code}}

Our \texttt{gbiqq} package provides a set of functions to implement all of these steps concisely for \emph{binary} models -- models in which all variables are dichotomous.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Steps 1 and 2 }
\CommentTok{# We define a model with three binary variables and specified edges between them:}
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> M -> Y"}\NormalTok{)}

\CommentTok{# Step 3}
\CommentTok{# Unrestricted functional forms are allowed by default, though these can }
\CommentTok{# also be reduced. Here we impose monotonicity at each step }
\CommentTok{# by removing one type for M and one for Y}
\NormalTok{model <-}\StringTok{ }\KeywordTok{set_restrictions}\NormalTok{(model, }\DataTypeTok{labels =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{M =} \StringTok{"M10"}\NormalTok{, }\DataTypeTok{Y=}\StringTok{"Y10"}\NormalTok{))}

\CommentTok{# Step 4}
\CommentTok{# We set priors over the distribution of (remaining) causal types.}
\CommentTok{# Here we set "jeffreys priors"}
\NormalTok{model <-}\StringTok{ }\KeywordTok{set_priors}\NormalTok{(model, }\DataTypeTok{distribution =} \StringTok{"jeffreys"}\NormalTok{)}

\CommentTok{# We now have a model defined as an R object. }
\CommentTok{# Later we will ask questions of this model and update it using data.}
\end{Highlighting}
\end{Shaded}

These steps are enough to fully describe a binary causal model. Later in this book we will see how we can ask questions of a model like this but also how to use data to train it.

\hypertarget{test-yourself-can-you-read-conditional-independence-from-a-graph}{%
\subsection{Test yourself! Can you read conditional independence from a graph?}\label{test-yourself-can-you-read-conditional-independence-from-a-graph}}

As an exercise, see whether you can identify the relations of conditional independence between \(A\) and \(D\) in Figure \ref{fig:CItest}.

\begin{figure}

{\centering \includegraphics[width=.9\textwidth]{ii_files/figure-latex/unnamed-chunk-10-1} 

}

\caption{\label{fig:CItest} An exercise: $A$ and $D$ are conditionally independent, given which other variable(s)?}\label{fig:unnamed-chunk-10}
\end{figure}

Are A and D independent:

\begin{itemize}
\tightlist
\item
  unconditionally?
\end{itemize}

Yes. \(B\) is a collider, and information does not flow across a collider if the value of the collider variable or its consequences is not known. Since no information can flow between \(A\) and \(C\), no information can flow between \(A\) and \(D\) simply because any such flow would have to run through \(C\).

\begin{itemize}
\tightlist
\item
  if you condition on \(B\)?
\end{itemize}

No.~Conditioning on a collider opens the flow of information across the incoming paths. Now, information flows between \(A\) and \(C\). And since information flows between \(C\) and \(D\), \(A\) and \(D\) are now also connected by an unbroken path. While \(A\) abnd \(D\) were independent when we conditioned on nothing, they cease to be independent when we condition on \(B\).

\begin{itemize}
\tightlist
\item
  if you condition on \(C\)?
\end{itemize}

Yes. Conditioning on \(C\), in fact, has no effect on the situation. Doing so cuts off \(B\) from \(D\), but this is irrelevant to the \(A\)-\(D\) relationship since the flow between \(A\) and \(D\) was already blocked at \(B\), an unobserved collider.

\begin{itemize}
\tightlist
\item
  if you condition on \(B\) and \(C\)?
\end{itemize}

Yes. Now we are doing two, countervailing things at once. While conditioning on \(B\) opens the path connecting \(A\) and \(D\), conditioning on \(C\) closes it again, leaving \(A\) and \(D\) conditionally independent.

Analyzing a causal graph for relations of independence represents one payoff to formally encoding our beliefs about the world in a causal model. We are, in essence, drawing out implications of those beliefs: given what we believe about a set of direct causal relations (the arrows on the graph), what must this logically imply about other dependencies and independencies on the graph, conditional on having observed some particular set of nodes? We show in a later chapter how these implications can be deployed to guide research design, by indicating which parts of a causal system are potentially informative about other parts that may be of interest.

\hypertarget{theory}{%
\chapter{Theories as causal models}\label{theory}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We introduce the idea of thinking of (applied) theoretical claims as claims within hierarchies of causal models. Lower level models serve as a theory for a higher level model if the higher level model can be deduced from the lower level model. The empirical content of a lower level model is the possible reduction in variance of the higher level model that it can provide.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Theory plays an important role in this book's use of causal models for causal inference. Yet the term ``theory'' in the empirical social sciences means very different things in different contexts. In this book, we will refer to a theory is an \emph{explanation} of a phenomeon: a theory provides an account of how or under what conditions a set of causal relationships operate. Moreover, we can express both a theory and the claims being theorized as causal models. A theory, then, is a model that explains and implies another model---possibly with the help of some data.

We discuss toward the end of the chapter how this definition of theory relates to common understandings of theory in the social sciences. First, however, we focus on unpacking our working definition. In embedding theorization within the world of causal models, we ultimately have an empirical objective in mind. Theorizing a causal relationship of interest, in our framework, means elaborating our causal beliefs about the world in greater detail. As we show in later chapters, theorizing in the form of a causal model allows us to generate research designs: to identify sources of inferential leverage and to explicitly and systematically link observations of components of a causal system to the causal questions we seek to answer.

\hypertarget{theory-as-a-lower-level-model}{%
\section{Theory as a ``lower-level'' model}\label{theory-as-a-lower-level-model}}

Let us say that a causal model, \(M^\prime\), is a \emph{theory} of \(M\) if \(M\) is implied by \(M^\prime\). Theory is, thus, all relative. \(M^\prime\) might itself sit atop a theory, \(M^{\prime\prime}\), that implies \(M^\prime\). To help fix the idea of theory as ``supporting'' or ``underlying'' the model(s) it theorizes, we refer to the theory, \(M^\prime\), as a \emph{lower}-level model relative to \(M\) and refer to \(M\) as a \emph{higher}-level model relative to its theorization, \(M^\prime\).\footnote{We note that our definition of theory differs somewhat from that given in \citet{pearl2009causality} (p207): there a theory is a (functional) causal model and a restriction over \(\times_j \mathcal{R}(U_j)\), that is, over the collection of contexts envisionable. Our definition also considers probabilistic models as theories, allowing statements such as `'the average effect of \(X\) on \(Y\) is 0.5.''}

We illustrate showing two models, \(M^\prime\), \(M^{\prime\prime}\) that each imply a model \(M\). In each case the lower level models contain additional nodes in a way that allows for a kind of ``disaggregation'' of exogenous nodes.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/Highlow-1} 

}

\caption{Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes.}\label{fig:Highlow}
\end{figure}

We start with the higher-level model, \(M\), represented in Figure \ref{fig:Highlow}(a). We can then offer the model, \(M^\prime\) in panel (b) as a \emph{theory}, a lower-level model, of \(M\). We have added a node, \(K\), in the causal chain between \(X\) and \(Y\), a familiar mode of theorization. In doing do we have in fact \emph{split} the error \(\theta^Y\) into two parts: \(\theta^{Y_\text{lower}}\) and \(\theta^K\).

Intuitively, in the higher-level model, (a), \(Y\) is a function of \(X\) and a disturbance \(\theta^Y\), the latter representing all things other than \(X\) than can affect \(Y\). In our four-type setup, \(\theta^Y\) represents all of the (unspecified) sources of variation in \(X\)'s effect on \(Y\). When we add \(K\), \(X\) now does not directly affect \(Y\) but only does so via \(K\). Further, we model \(X\) as acting on \(K\) ``with error,'' with \(\theta^K\) representing all of the (unspecified) factors determining \(X\)'s effect on \(K\). The key thing to notice here is that \(\theta^K\) now represents \emph{a portion of the variance that \(\theta^Y\) represented in the higher-level graph}: some of the variation in \(X\)'s effect on \(Y\) now arises from \(X\)'s effect on \(K\), which is captured by \(\theta^K\). So, for instance, \(X\) might have no effect on \(Y\) because \(\theta^K\) takes on a value such that \(X\) has no effect on \(K\). Likewise, any effect of \(X\) on \(Y\) must arise from an effect of \(X\) on \(K\), captured in \(\theta^K\)'s value. \footnote{As we emphasize further below, it is in fact only this ``error'' in the \(X\rightarrow K\) link that makes the addition of \(K\) potentially informative as a matter of research design: if \(K\) were a deterministic function of \(X\) only, then knowledge of \(X\) would provide full knowledge of \(K\), and nothing could be learned from observing \(K\).} What \(\theta^K\) represents, then, is that part of the original \(\theta^Y\) that arose from some force other than \(X\) operating at the \emph{first} step of the causal chain from \(X\) to \(Y\).

So now, \(\theta^Y\) is not quite the same entity in the lower-level graph that it was in the higher-level graph. In the original graph, \(\theta^Y\) represented \emph{all} sources of variation in \(X\)'s effect on \(Y\). In the lower-level model, with \(K\) as mediator, \(\theta^Y\) represents only random variation in \(K\)'s effect on \(Y\). \(\theta^Y\) has been expunged of any factors shaping the first stage of the causal process, which now reside in \(\theta^K\). Reflecting a convention that we use throughout the book, we highlight this change in \(\theta^Y\)'s meaning by referring in the second model to \(\theta^{Y_\text{lower}}\).

Theorization here thus starts with the proliferation of substantive variables---adding beliefs about intervening steps in a causal process. But, critically, it also involves an accompanying disaggregation of unexplained variation. Addition and splitting thus go hand-in-hand: the \emph{insertion} of a mediator between \(X\) and \(Y\) also involves the \emph{splitting} of \(Y\)'s unspecified parent (\(\theta_Y\)).

Consider next model \(M''\) panel (c) in Figure \ref{fig:Highlow}, which also supports (implies) the higher-level theory in panel \((a)\). The logical relationship between models \((a)\) and \((c)\), however, is somewhat different. Here the lower-level model \emph{specifies} one of the conditions that comprised \(\theta^Y\) in the higher-level model. In specifying a moderator, \(C\), we have extracted \(C\) from \(\theta^Y\), leaving \(\theta^{Y_\text{lower}}\) to represent all factors \emph{other than \(C\)} that condition \(X\)'s effect on \(Y\). (Again, the relabeling as \(\theta^{Y_\text{lower}}\) reflects this change in the term's meaning.) While we might add a \(\theta^C\) term pointing into \(C\), this is not necessary. Whereas in Model (b) we have extracted \(\theta^K\) from \(\theta^Y\), in Model (c), it is \(C\) itself that we have extracted from \(\theta^Y\), substantively specifying what had been just a random disturbance.

Critically, notice that since lower level models imply higher level models we think of theories as implying the models they are theorizing. If you believe Model \(M'\), then you also must believe Model \(M\). If it is possible that \(X\) can affect \(K\) and possible that \(K\) can affect \(Y\) then it is possible that \(X\) can affect \(Y\). The converse is not true, however. It is not possible to still believe that \(X\) can effect \(Y\) if you do not think that \(X\) can affect \(K\). Similarly, if you believe Model (c), then you must also believe Model (a): if it is true that \(X\) can affect \(Y\), possibly in ways that are moderated by \(C\), then it is trivially true, more simply, that \(X\) can affect \(Y\).

\hypertarget{illustration-of-unpacking-causal-types}{%
\section{Illustration of unpacking causal types}\label{illustration-of-unpacking-causal-types}}

We now show more specifically how causal types in lower level models map into causal types in higher level models.

For concreteness, let us return to our democratization example and consider first the very basic claim that inequality can have an affect on democratization. We represent this simple claim in Figure \ref{fig:demtheory5}, Panel (a). In this simple model, \(I\) may sometimes have an effect of \(D\), and sometimes not; and that effect may be positive or negative. All of this will depend on the case's causal type.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/demtheory5-1} 

}

\caption{DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a).}\label{fig:demtheory5}
\end{figure}

In addition the figure shows two models that each \emph{explain} Model (a), though in different ways. Model (b) answers the explanatory question, ``\emph{How} does inequality affect democratization?'' Model (c) answers the explanatory question, ``\emph{Why} does inequality's effect on democratization vary?'' Both theories provide richer, more interpretable accounts of the phenomenon of interest than the simpler model that they are theorizing.

These lower level models imply a set of causal types that are richer than that implied by (a). Recall that in Chapter \ref{models}, we considered the idea that at any node, a causal type may be conceptualized as a case specific disturbance, that governs the mapping from input variables to outcome variables.

In particular if we deploy our four-causal-type function from Chapter \ref{models} we have:

\begin{itemize}
\tightlist
\item
  \(a\): \(\theta^D=\theta^D_{10}\), then \(D=1-I\) (\(I\) has a negative effect on \(D\))
\item
  \(b\): \(\theta^D=\theta^D_{01}\), then \(D=I\) (\(I\) has a positive effect on \(D\))
\item
  \(c\): \(\theta^D=\theta^D_{00}\), then \(D=0\) (\(I\) has no causal effect)
\item
  \(d\): \(\theta^D=\theta^D_{11}\), then \(D=1\) (\(I\) has no causal effect)
\end{itemize}

Knowing \(\theta\) tells us how \(D\) responds to \(I\) and it ignores any heterogeneity between units as long they respond in the same way. For any causal type the model is \emph{consistent} with \(I\)'s causal effect operating for different reasons for different units, but these differences are left entirely unaccounted for.

\hypertarget{type-disaggregation-in-a-mediation-model}{%
\subsection{Type disaggregation in a mediation model}\label{type-disaggregation-in-a-mediation-model}}

Model (b) has causal types defined for nodes \(M\) and for \(D\). As with the overall \(I,D\) relationship. We thus allow \(I\) to have a positive, negative, or no effect on \(M\), with \(\theta^M\) taking on four possible values, again corresponding to \(a,b,c,d\) nodal types (now: \emph{a}: \(\theta_{10}^M\), \emph{b}: \(\theta_{01}^M\), \emph{c}: \(\theta_{00}^M\), \emph{d}: \(\theta_{11}^M\)).

And we allow for \(M\) to have a positive, negative, or no effect on \(D\), with \(\theta^D_{\text{lower}}\) possible values again being one of four nodal types (\(\theta_{10}^D\), \(\theta_{01}^D\), \(\theta_{00}^D\), \(\theta_{11}^D\)).

We can now think about \emph{combinations} of types in the lower-level model as mapping onto types in the higher-level model. Table \ref{tab:highlowmapping} illustrates.

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:highlowmapping} Mapping from lower level nodal types on \(M\) and \(D\) to higher level causal types on \(D\).}\tabularnewline
\toprule
\begin{minipage}[b]{0.13\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{10}^{D_{lower}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{01}^{D_{lower}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{lower}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{lower}}\)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.13\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{10}^{D_{lower}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{01}^{D_{lower}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{lower}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{lower}}\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\theta_{10}^{M}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{01}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{10}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{higher}}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\theta_{01}^{M}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{10}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{01}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{higher}}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\theta_{00}^{M}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{higher}}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\theta_{11}^{M}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{00}^{D_{higher}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
\(\theta_{11}^{D_{higher}}\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

For instance, in a case in which both \(\theta^M=\theta^M_{01}\) (a positive effect of \(I\) on \(M\)) and \(\theta^{D_{\text{lower}}}=\theta_{01}^{D_{lower}}\) (a positive effect of \(M\) on \(D\)), we have a positive effect of \(I\) on \(D\)---meaning that, in the \emph{higher-level} model, \(\theta^{D_{higher}}=\theta^{D_{higher}}_{01}\). Two linked negative effects also generate a positive effect of \(I\) on \(D\) and so map onto the same higher-level type. Further, it is easy to see that if there is no causal effect at \emph{either} the \(I \rightarrow M\) step \emph{or} the \(M \rightarrow D\) step, we will have one of the null effect types at the higher level since, in this model, \(I\) cannot affect \(D\) unless there are causal effects at both constituent steps.\footnote{These mappings, of course, hinge on the fact that \(I\) affects \(D\) \emph{only} through \(M\) in this model (no direct effects or other pathways).}

To foreshadow the discussion in later chapters, these mappings are critical: they allow us to use inferences drawn at a lower level to answer questions posed at a higher level.

\hypertarget{type-disaggregation-in-a-moderation-model}{%
\subsection{Type disaggregation in a moderation model}\label{type-disaggregation-in-a-moderation-model}}

Alternatively, we might wonder \emph{when} inequality causes democratization. Our simple claim, in panel (a), allows that \(I\) \emph{can} cause \(D\), but provides no information about the conditions under which it does so. Those conditions are implicitly embedded within \(\theta^D\), where they are left unspecified. We could, however, theorize some of what is left unsaid in in panel (a). We do this in panel (c), where we posit ethnic homogeneity (\(E\)) as a moderator of inequality's effect on democratization. Panel (c) represents a theory of panel (a) in that it can help account for variation in causal effects that is unaccounted for by the model in (a).

Model (c) has thus given substantive meaning to an aspect of the phenomenon that was merely residual variation in Model (a). Model (a) provides no account of why inequality has the effects it does, relying fully on \(\theta^D\) as a placeholder for this uncertainty. In Model (c), \(\theta^D\) plays a more modest role, with ethnic homogeneity doing a good deal of the work of determining inequality's possible effects.

In this graph, we again have a \(\theta_D^{\text{lower}}\) term, but it is a different object from \(\theta_D^{\text{lower}}\) in the mediation graph. In this moderation model, \(\theta_D^{\text{lower}}\) is more complex as it determines the mapping from two binary variables into \(D\). A causal type in this setup now represents how a case will respond to four different possible combinations of \(I\) and \(E\) values. Rather than four causal types, we now have 16, as there are 16 possible ways in which a case might respond to two binary variables (see Table \ref{tab:PO16} in Chapter \ref{models}).

In Table \ref{tab:PO16b} we give a mapping from a subset of these lower level types to the upper level types corresponding to the model in (a).

\begin{longtable}[]{@{}cccccl@{}}
\caption{\label{tab:PO16b} Values for \(D\) given \(E\) and \(I\). With two binary causal variables, there are 16 nodal types: 16 ways in which \(Y\) depends on \(I\) and \(E\). These lower level types map into higher level types for a model in which \(Y\) depends on \(I\) only, as shown in the final column.}\tabularnewline
\toprule
\begin{minipage}[b]{0.17\columnwidth}\centering
Low Type\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=0,E=0\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=0,E=1\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=1,E=0\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=1, E=1\)\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
High Type\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.17\columnwidth}\centering
Low Type\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=0,E=0\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=0,E=1\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=1,E=0\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
\(I=1, E=1\)\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
High Type\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\centering
\(\theta^{D}_{0000}\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\theta^{D}_{00}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\centering
\(\theta^{D}_{0001}\)
\(\theta^{D}_{0010}\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1
0\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\theta^{D}_{01}\) if \(E=1\), else \(\theta^D_{00}\)
\(\theta^D_{00}\) if \(E=1\), else \(\theta^D_{01}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\centering
\(\theta^{D}_{0011}\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\theta^{D}_{01}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\centering
\(\vdots\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
\(\vdots\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
\(\vdots\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
\(\vdots\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
\(\vdots\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\vdots\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\centering
\(\theta^{D}_{1110}\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\theta^D_{11}\) if \(E=0\), else \(\theta^D_{10}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\centering
\(\theta^{D}_{1111}\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\theta^D_{11}\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Importantly we see that the mapping between lower- and higher-level types can depend on the value of the moderator. More generally, since we can think of the value of exogeneous nodes, \(E\) and \(I\), as being nodal types for those nodes, we can think of the lower level nodal type as a concatenation of the upper level nodal types for \(E\) and \(D\). Thus we can think of the the higher level type as depending uniquely on the fully specified lower level type.

For instance, a case that has type \(\theta_{01}\) in the higher-level model if it has type \(\theta_{0010}\) in the lower-level model \emph{and} \(E=0\). This is a case for which \(I\) has a positive effect on \(D\) when \(E=0\) \emph{and} in which \(E\) \emph{is in fact} 0. On the other hand, the same lower level type, in combination with \(E=1\) maps onto the type \(\theta_{10}\) in the higher-level model---a type in which \(D\) responds negatively to \(I\).

In later chapters, we represent all lower- to higher-level mappings relevant to a question of interest with the use of ``type-reduction'' tables that allow one to readily see how inferences drawn at one level inform causal questions posed at another level.

\hypertarget{rules-for-moving-between-higher--and-lower-level-models}{%
\section{Rules for moving between higher- and lower-level models}\label{rules-for-moving-between-higher--and-lower-level-models}}

Thinking about models as conditionally nested within one another can be empirically useful. It provides a way of generating empirical leverage on a causal question by plumbing more deeply our background knowledge about a domain of interest. When we more fully specify higher-level claims via a more elaborate, lower-level model, we are a making explicit unspecified conditions on which the higher-level relationships depend. In doing this, we are identifying potentially observable nodes that might be informative about our research question.

As we develop lower-level models to support our claims, or determine which claims are supported by our theories, what kinds of moves are we permitted to make? One important thing to note is that the mappings between higher-level claims and theories may not be one-to-one. A single theory can support multiple higher-level theories. Moreover, a single higher-level relation can be supported by multiple, possibly incompatible lower-level theories.

To illustrate, consider two ``lower level'' theories of democratization:

\begin{itemize}
\tightlist
\item
  (\(L_1\)): \(Inequality \rightarrow Democratization \leftarrow Mobilization\)\\
\item
  (\(L_2\)): \(Inequality \rightarrow Mobilization \rightarrow Democratization\)
\end{itemize}

Note how these theories are incompatible with one another. While \(Inequality\) and \(Democratization\) are independent in \(L_1\), they are causally related in \(L_2\). Moreover, in \(L_2\), \(Inequality\) and \(Democratization\) are related only through \(Mobilization\), while in \(L_1\), \(Democratization\) is directly affected by \(Inequality\).\footnote{Put differently, these two theories record different relations of conditional independence: in \(L_1\), \(Inequality\) and \(Mobilization\) are unconditionally independent, but they are not unconditionally independent in \(L_2\). Also, in \(L_2\), \(Inequality\) is independent of \(Democratization\) conditional on \(Mobilization\); but this is not the case in \(L_1\).}

Now, consider the following three higher-level claims:

\begin{itemize}
\tightlist
\item
  (\(H_1\)): \(Inequality \rightarrow Democratization\)
\item
  (\(H_2\)): \(Mobilization \rightarrow Democratization\)
\item
  (\(H_3\)): \(Inequality \rightarrow Mobilization\)
\end{itemize}

\(H_1\) could be derived from (explained by) either theory, \(L_1\) or \(L_2\). Although the two theories are incompatible with one another, in both theories \(Inequality\) affects \(Democratization\). Both theories likewise imply \(H_2\), in which \(Mobilization\) affects \(Democratization\).

\(H_3\), however, can be supported only by one of these theories: only in \(L_2\), and not in \(L_1\), does \(Inequality\) cause \(Mobilization\).\footnote{In addition, the \emph{conditional} higher-level model \(((Inequality \rightarrow Democratization)|Mobilization=1)\) can be supported by model \(L_1\) but not by model \(L_2\), where holding \(Mobilization\) constant would sever the dependence of \(Democratization\) on \(Inequality\).}

Thus multiple (possibly \emph{incompatible}) theories can usually be proposed to explain any given causal effect. When seeking an explanation for, say, \(H_1\), the choice between \(L_1\) and \(L_2\) is not dictated by logic; it must be drawn from a substantive belief about which set of causal dependencies operates in the world. On the other hand, \(L_2\) \emph{is} logically ruled out as an explanation of \(H_3\). Further, any given theory logically implies multiple (necessarily \emph{compatible}) higher-level claims about causal relations.

What, more generally, are the permissible moves across levels?

\hypertarget{moving-down-levels}{%
\subsection{Moving down levels}\label{moving-down-levels}}

We have already discussed two possible forms of theorization --- moves down a level: (i) disaggregating existing nodes, i.e., by introducing beliefs about mediation or moderation, or (ii) adding nodes representing variation in a feature of context that is implicitly held constant in the higher-level model.

There are other possible ways of elaborating a model. For instance, we can add \emph{antecedent conditions}: causes of nodes that were exogenous in the higher-level model. Likewise, we can add \emph{downstream effects}: outcomes of nodes that were terminal in the higher-level model.

\begin{figure}

{\centering \includegraphics[width=.7\textwidth]{ii_files/figure-latex/incompat-1} 

}

\caption{A higher-level model and a lower-level model that is impermissible.}\label{fig:incompat}
\end{figure}

The central principle governing allowable elaborations is that a lower-level model \emph{must not introduce dependencies between variables that were omitted in the higher-level model.} We provide an example of a violation of this principle in Figure \ref{fig:incompat}.

We start with a higher-level model, in panel (a), in which inequality affects democratization through mobilization. We then elaborate the model in panel (b) by adding ethnic homogeneity as a moderator of mobilization's effect. However, because ethnic homogeneity is also modeled here as affecting inequality, we have now introduced a source of dependence between inequality and democratization that was omitted from the higher-level model. In panel (a), democratization and inequality were dependent only via mobilization; and so they are conditionally independent given mobilization. In panel (b), democratization and mobilization are additionally dependent via their common cause, ethnic homogeneity. By the rules governing causal graphs (see Chapter \ref{models}), the higher-level specifically \emph{prohibited} this second source of dependency---since all dependencies between variables must be represented.
Put differently, if two variables are independent--- or conditionally independent given a third variable---in one model, then this same relation of independence (or conditional independence) must be captured in any theory of that model. A theory can \emph{add} conditional independencies not present in the higher-level model. For instance, a mediation theory, \(X \rightarrow M \rightarrow Y\), implies a conditional independence that is not present in the higher-level model that it supports, \(X \rightarrow Y\): in the lower-level model only, \(X\) is conditionally independent of \(Y\) given \(M\). But we may not theorize away (conditional) independencies insisted on by our higher-level claim.

\hypertarget{moving-up-levels}{%
\subsection{Moving up levels}\label{moving-up-levels}}

Moving in the other direction, what, in general, are the permissible \emph{simplifications} of lower-level models? In other words, given a theory, what are the higher-level claims that it can support?

When we move up a level --- i.e., eliminate one or more nodes --- the key rule is that the higher-level graph must take into account:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  all \emph{dependencies} among remaining nodes and
\item
  all \emph{variation} generated by the eliminated node.
\end{enumerate}

We can work out what this means, separately, for eliminating \emph{endogenous} nodes and for eliminating \emph{exogenous} nodes.

\emph{Eliminating endogenous nodes}

Eliminating an endogenous node means removing a node with parents (direct causes) represented on the graph. If the node also has one or more children, then the node captures a dependency: it links its parents to its children. When we eliminate this node, preserving these dependencies requires that all of the eliminated node's parents adopt---become parents of---all of the eliminated node's children. Thus, for instance in panel (b) of Figure \ref{fig:Highlow}, if we were to eliminate \(M\), \(M\)'s parents (\(X\) and \(\theta^M\)) need to adopt \(M\)'s child, \(Y\). We see in panel (a) of the figure, the higher-level model, that \(X\) is now pointing directly into \(Y\).

As for \(\theta^M\), it too must now point directly into \(Y\)---though we can use a bit of shorthand to make this happen. Recall that \(\theta^M\) represents the part of \(M\) that is randomly determined. Rather than drawing two separate disturbance (\(\theta\)) terms pointing into \(Y\), however, we more simply represent the combined disturbance term as \(\theta^Y_{\text{higher}}\), with the `'higher'' signaling the aggregation of roots. (This is, of course, simply reversing the disaggregation that we undertook earlier to move from the higher- to the lower-level model.)

More intuitively, when we simplify away a mediator, we need to make sure that we preserve the causal relationships being mediated---both those among substantive variables and any random shocks at the mediating causal steps.\footnote{Eliminating endogenous nodes may also operate via ``encapsulated conditional probability distributions'' \citep{koller2009probabilistic} wherein a system of nodes, \(\{Z_i\}\) is represented by a single node, \(Z\), that takes the parents of \(\{Z_i\}\) not in \(\{Z_i\}\) as parents to \(Z\) and issues the children of \((Z_i)\) that are not in \((Z_i)\) as children. However, this is not a fundamental alteration of the graph.}

\emph{Eliminating exogenous nodes}

What about eliminating exogenous nodes---nodes with no parents? For the most part, exogenous nodes cannot be eliminated, but must either be replaced by or incorporated into \(U\) (or \(\theta\)) terms. The reason is that we need preserve any dependencies or variation generated by the exogenous node. Figure \ref{fig:elimrules} walks through four different situations in which we might want to simplify away the exogenous node, \(X\). (Here we use the more generic \(U\) notation, though the same principles apply if these are type-receptacles(\(\theta\).)

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/elimrules-1} 

}

\caption{Here we represent the basic principles for eliminating exogenous nodes.}\label{fig:elimrules}
\end{figure}

\begin{itemize}
\tightlist
\item
  \emph{Multiple children.} In (a1), we start with a lower-level model in which \(X\) has two children, thus generating a dependency between \(W\) and \(Y\). If we eliminate \(X\), we must preserve this dependency. We can do so, as pictured in (a2), by replacing \(X\) with a \(U\) term that also points into \(W\) and \(Y\).\footnote{By DAG convention, we could, alternatively, convey the same information with a dashed, undirected line between \(W\) and \(Y\).} Though we are no longer specifying what it is that connects \(W\) and \(Y\), the correlation itself is retained.
\item
  \emph{Substantive spouse.} In (b1), \(X\) has a spouse that is substantively specified, \(W\). If we eliminate \(X\), we have to preserve the fact that \(Y\) is not fully determined by \(W\); \emph{something} else also generates variation in \(Y\). We thus need to replace \(X\) with a \(U\) term, \(U_Y\), to capture the variation in \(Y\) that is not accounted for by \(W\).
\item
  \emph{\(U\)-term spouse.} In (c1), \(X\) has a spouse that is \emph{not} substantively specified, \(U^{Y_\text{lower}}\). Eliminating \(X\) requires, again, capturing the variance that it generates as a random input. As we already have a \(U\) term pointing only into \(Y\), we can substitute in \(U^{Y_\text{higher}}\), which represents both \(U^{Y_\text{lower}}\) and the variance generated by \(X\).\emph{IS THIS FOOTNOTE RIGHT?}\footnote{This aggregation cannot occur if \(U^{Y_\text{lower}}\) also has another child, \(W\), that is not a child of \(X\) since then we would be representing \(Y\)'s and \(W\)'s random components as identical, which they are not in the lower-level graph.}
\item
  \emph{One child, no spouse.} In (d1), \(X\) has only one child and no spouse. Here we can safely eliminate \(X\) with no loss of information. It is always understood that every exogenous node has some cause, and there is no loss of information in simply eliminating a node's causes if those causes are exogenous and do not affect other endogenous nodes in the model. In (d2) we are simply not specifying \(Y\)'s cause, but we have not lost any dependencies or sources of variance that had been expressed in (d1).
\end{itemize}

One interesting effect of eliminating a substantive exogenous node can be to render seemingly deterministic relations effectively probabilistic. In moving from (b1) to (b2), we have taken a component of \(Y\) that was determined by \(X\) and converting it into a random disturbance. Just as we can explain a more probabilistic claim with a less probabilistic theory, we can derive higher-level claims with greater probabilism from theories with greater determinism.

\begin{verbatim}
## Warning in text.default(x, y + text_shift, names, cex =
## cex): font metrics unknown for character 0xa

## Warning in text.default(x, y + text_shift, names, cex =
## cex): font metrics unknown for character 0xa

## Warning in text.default(x, y + text_shift, names, cex =
## cex): font metrics unknown for character 0xa

## Warning in text.default(x, y + text_shift, names, cex =
## cex): font metrics unknown for character 0xa
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/lowercomplexdem-1} 

}

\caption{A lower-level model  from which multiple higher level models can be derived.}\label{fig:lowercomplexdem}
\end{figure}

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/runningsubs-1.pdf}
\caption{\label{fig:runningsubs}Higher level models derived from the lower level model of Figure X. Nodes that are eliminated are marked in grey; circles denote exogenous nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two other nodes could equivalently be indicated as an undirected edge connecting the two.) Note that \(M\), \(R\), and \(D\) are deterministic functions of \(I\) and \(E\) in this example.}
\end{figure}

We can apply these principles to a model of any complexity. We illustrate a wider range of simplifications by starting with Figure \ref{fig:lowercomplexdem}, which represents a somewhat amended version of our inequality and democratization model from Chapter \ref{models}, with more complex causal relations. Then, in Figure \ref{fig:runningsubs}, we show all permissible reductions of the more elaborate model. We can think of these reductions as the full set of simpler claims (involving at least two nodes) that can be derived from the lower-level theory. In each subgraph,

\begin{itemize}
\tightlist
\item
  we mark eliminated nodes in grey;
\item
  those nodes that are circled must be replaced with \(U\) terms; and
\item
  arrows represent the causal dependencies that must be preserved.
\end{itemize}

Note, for instance, that neither \(E\) (because it has a spouse) nor \(I\) (because it has multiple children) can be simply eliminated; each must be replaced with a \(U\) term. Also, the higher-level graph with nodes missing can contain arrows that do not appear at all in the lower-level graph: eliminating \(M\), for instance, forces an arrow running from \(X\) to \(R\) and another running from \(X\) to \(Y\), as \(X\) must adopt \(M\)'s children. The simplest elimination is of \(D\) itself since it does not encode any dependencies between other variables.

We can also read Figure \ref{fig:runningsubs} as telling us the set of claims for which the lower-level graph in Figure \ref{fig:running} can serve as a theory. As we can see, the range of claims that a moderately complex model can theorize is vast. For each simpler claim, moreover, there may be other possible lower-level graphs---theories besides ---consistent with it.

\hypertarget{conditioning-on-nodes}{%
\subsubsection{Conditioning on nodes}\label{conditioning-on-nodes}}

A further permissible ``upward'' move is conditioning on a node. When we condition on a node, we are restricting the higher-level model in scope to situations in which that node's value is held constant. Doing so allows us to eliminate the node as well as all arrows pointing into it or out of it. Consider three different situations in which we might condition on a node:

\begin{itemize}
\tightlist
\item
  \emph{Exogenous, with multiple children.} In simplifying (a1) in Figure \ref{fig:elimrules}, we need to be sure we retain any dependence that \(X\) generates between \(W\) and \(Y\). However, recalling the rules of conditional independence on a graph (see Chapter \ref{models}), we know that \(W\) and \(Y\) are \emph{independent} conditional on \(X\). Put differently, if we restrict the analysis to contexts in which \(X\) takes on a constant value, the lower-level model implies that \(Y\) and \(W\) will be uncorrelated across cases. As fixing \(X\)'s value breaks the dependence between \(Y\) and \(W\), we can drop \(X\) (and the arrows pointing out of it) without having to represent that dependence.
\item
  \emph{Exogenous, with spouse.} In simplifying (b1) or (c1) in Figure \ref{fig:elimrules}, we need to account for the variation generated by \(X\). If we fix \(X\)'s value, however, then we eliminate this variation by assumption and do not need to continue to represent it (or the arrow pointing out of it) on the graph.
\item
  \emph{Endogenous.} When we condition on an endogenous node, we can eliminate the node as well the arrows pointing into and out of it. We, again, leverage relations of conditional independence here. If we start with graph (b) in Figure \ref{fig:Highlow}, and we condition on the mediator, \(M\), we sever the link between \(Y\) on \(X\), rendering them conditionally independent of one another. We can thus remove \(M\), the arrow from \(X\) to \(M\), and the arrow from \(M\) to \(Y\). In the new model, with \(M\) fixed, \(Y\) will be entirely determined by the random disturbance \(\theta^{Y_\text{lower}}\).\footnote{Note that such conditioning does not add any variance to the \(\theta^Y\) term, so we retain the notation \(\theta^{Y_\text{lower}}\).}
\end{itemize}

In sum, we can work with models that are simpler than our causal beliefs: we may believe a complex lower-level model to be true, but we can derive from it a sparer set of claims. There may be intervening causal steps or features of context that we believe matter, but that are not of interest for a particular line of inquiry. While these can be removed, we nonetheless have to make sure that their \emph{implications} for the relations remaining in the model are not lost. Understanding the rules of reduction allow us to undertake an important task: checking which simpler claims are and are not consistent with our full belief set.

\hypertarget{relation-to-technical-literature}{%
\subsubsection{Relation to technical literature}\label{relation-to-technical-literature}}

Formally moving from lower level DAG to a higher level DAG requires \emph{marginalization}: assessing the joint marginal distribution of observed nodes in the higher level graph over the distribution of unobserved nodes in the lower level graph.

Unfortunately there is no guarantee that the margin of a distribution that is consistent with a lower level DAG will be consistent with any higher level DAG (techically, ``DAGS are not closed under marginalization'').

In response, various types of richer graphs have been developed, such as ``acyclic directed mixed graphs'' (ADMGs) Maximal Ancestral Graphs (Spirtes and Richardson, Ancestral graph Markov models, 2002), or mDAGs (Evans 2015 (Graphs for Margins)). See also (Wermuth 2011)

ADMGs for example have directed and bidirected edges but no directed cycles and are closed under marginalization.

We use two approaches in our applications to engage with this problem. First we generally allow for \emph{unobserved confounding} in models. Second we will allow for the estimation of lower level models with unobserved nodes.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We close this chapter by considering how the understanding of theory that we work with in this book compares to other prominent understandings of theory.

\textbf{Theory as tautology.} The claim that the number of Nash equilibria is generically odd in finite games is often understood to be a theoretical claim. Unless there are errors in the derivation of the result, the claim is true in the sense that the conclusions follow from the assumptions. There is no evidence that we could go looking for in the world to assess the claim. The same can be said of the theoretical claims of many formal models in social sciences; they are theoretical deductions of the if-then variety \citep{clarke2012model}. Theory in this sense is true by tautology. By contrast, theory as we define it in this book refers to claims with \emph{empirical} content: a theory refers to causal relations in the world that might or might not hold, and is susceptible to empirical testing. The deductive \emph{logical} relations that hold in a causal model are those of conditional independence, as discussed in Chapter \ref{models}: for instance, if \(X\) causes \(Y\) only through \(M\) in a theory, then \(X\) and \(Y\) are conditionally independent given some value of \(M\).

\textbf{Theory as a collection of maps.} According to \citet{clarke2012model}, building on a semantic view of theory (\citet{giere2010explaining}), a theory is a collection of models, together with a set of hypotheses linking them to the real world. As in our usage, Clarke and Primo see theories and models as very similar objects: for them, a theory is a system of models; for us, a theory is a supporting model. In both frameworks, there is no real difference in kind between models and theories.

Our approach also shares with Clarke and Primo the idea that models are not full and faithful reflections of reality; they are maps designed for a particular purpose. In the case of causal models, the purpose is to capture relationships of independence and possible causal dependence. As we have shown, that is a purpose that allows for the stripping away of detail---though it also forbids certain simplifications (such as any simplification that removes a dependency between variables). Clarke and Primo see models as useful to the extent that they are similar to features of the real world in ways related to the model's purpose. Along these lines, a causal model will be useful to the extent that it posits relations of independence that are similar to those prevailing in the domain under investigation.

\textbf{Theory as a testable claim} In the hypothetico-deductive framework, often traced back to \citet{popper2014conjectures} and highly influential in empirical political science, empirical social science is an activity of theory-\emph{testing}. Having developed a theory, we then derive from it a set of empirical predictions and then test those predictions against evidence. In \citet{clarke2012model}, we also seek to confirm theories by developing and testing hypotheses about the similarity of a model or theory to particular features of the world. In both cases, a theory is posited---possibly on the basis of logic or background knowledge---and then assessed. The value (truth or usefulness) of the model itself is the object of inquiry.

In a causal-model framework, theories are always tentative, and we can subject any model or theory to empirical evaluation, a task to which we turn in Chapter \ref{evaluation}. However, in the book's setup, theories are first and foremost \emph{expressions of what we already know and don't know} about a given causal domain when inquiry begins. We encode this background knowledge in order to inform research-design choices and draw inferences from the data. Models and theories are thus, in this sense, the world within which inquiry unfolds. Indeed, as we explore in Chapter \ref{questions}, the very questions we ask live within---can be represented as parts of---our theories.

\textbf{Theory as generalization} In another of the many uses of ``theory,'' political scientists often think of theorization as generalization. For \citet{Van-Evera:1997} and \citet{przeworski1970logic}, for instance, theories are by their nature general statements that we can use to explain specific events. In this view, ``Diamond resources caused Sierra Leone's civil war'' is a case-specific explanation; ``Natural resource endowments cause civil war'' is a theoretical formulation.

In our treatment of theory as a lower-level causal model, however, there is no generic sense in which a theory is more or less general than the higher-level claim that it explains. In this book's framework, we \emph{can} theorize by generalizing: when we elaborate a model by building in variation in a factor that was held constant in the higher-level claim, we are making the model more general in scope. If our natural resources claim implicitly applies only to weak states, we can theorize this claim by allowing state strength to vary and articulating how the natural-resource effect hinges on that claim.

However, when we theorize by disaggregating nodes---say, by adding intervening causal steps---we have in fact made a more \emph{specific} claim. Natural resources may cause civil war under a broad set of circumstances. Natural resources will cause civil war \emph{through looting by rebel groups} under an almost certainly narrower set of circumstances. Here, the more elaborate argument---the theorization of \emph{why} \(X\) causes \(Y\)---is actually a stronger claim, with narrower scope, than the simpler one that it supports.

\textbf{The value of parsimony} \citet{Van-Evera:1997} and \citet{przeworski1970logic} also express a common view in characterizing \emph{parsimony} as a quality of good theory. While they recognize that parsimony must often be traded off against other goods, such as accuracy and generality, \emph{ceteris paribus} a more parsimonious theory---one that uses fewer causal variables to explain variation in a given outcome---is commonly understood to be a better theory.

We do not take issue with the idea that simpler models and explanations are, all else equal, better. But the succeeding chapters also demonstrate a distinctive and important way in which all else will often not be equal when we seek to use theory to guide research design and support causal inference. To foreshadow the argument to come, the elaboration of more detailed, lower-level models can direct us to new opportunities for learning. As we unpack a higher-level claim, we will often be identifying additional features of a phenomenon the observation of which can shed light on causal questions of interest. Moreover, our background beliefs---the prior knowledge on which causal inference must usually rest---are often more informative at lower levels than at higher levels: it will, for instance, often be easier for us express beliefs about causal effects for smaller steps along a causal chain than about an overarching \(X \rightarrow Y\) effect.

Making things more complicated, of course, still makes things more complicated. And we should avoid doing so when the payoff is small, as it will sometimes be. But in the pages to come, we will also see a distinct set of benefits that arise from drilling more deeply into our basis of prior knowledge when formulating inferential strategies.

\hypertarget{quantifying-the-gains-of-a-theory}{%
\subsection{Quantifying the gains of a theory}\label{quantifying-the-gains-of-a-theory}}

What are the gains of a thoery that introduces a node \(K\) relative to one that does not include \(K\). What is the value added of the more elaborate theory?

One approach to assessing the contribution of a theory is to calculate the mean reduction in Bayes risk:

\[\text{Gains from theory} = 1- \frac{E_{K|W}(Var(Q|K,W))}{Var(Q|W)}\]

This is a kind of \(R^2\) measure (see also \citet{gelman2006bayesian}).

Another approach is to ask: how much better are my guesses now compared to what I would have guessed before, given what I know now.

Expected wisdom.

\[Wisdom  = \int(q_0 - q)^2 - (q_k - q)^2 p(q | k)dq\]
This captures how much better off we are with the guess we have made given current data (\(q_k\)) compared to the guess we woudl have made without it (\(q_0\)), knowing what we know now (\(p(q|k)\). An advantage of this conceptualization is that you can record gains in learning even if posterior variance is larger than prior variance. Even still the implications for strategy are the same since wisdom is maximized by a strategy that reduces expected squared error.

Other possible measures of gains from theory might include the simple correlation between \(K\) and \(Q\), or entropy-based measures (see \citet{zhang2003properties} for many more possibilities).

For this problem the correlation is given by (see appendix):

\[\rho_{KQ} = \frac{(\phi_b+\phi_d)(1-2p)(p(1-p))^{.5}}{
(p\phi_b+(1-p)\phi_d)(1-(p\phi_b+(1-p)\phi_d)))^{.5}}\]

One might also use a measure of ``mutual information'' from information theory:

\[I(Q,K) = \sum_q \sum_k P(q,k)\log\left(\frac{P(q,k)}{P(q)P(k)}\right)\]

To express this mutual information as a share of variation explained, we could divide \(I(Q,K)\) by the entropy of \(Q\), \(H(Q)\) where \(H(Q) = -\sum_qP(q)\log(P(q))\). The resulting ratio can be interpreted as 1 minus the ratio of the entropy of \(Q\) conditional (on \(K\)) to the unconditional entropy of \(Q\).

For this example, Figure \ref{fig:probative_value} shows gains as a function of \(\phi_b\) given a fixed value of \(\phi_d\). The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely.

\begin{figure}

{\centering \includegraphics[width=.7\textwidth]{ii_files/figure-latex/unnamed-chunk-12-1} 

}

\caption{\label{fig:probative_value} The solid line shows gains in precision (reduced posterior variance) for different values of $\phi_b$ given $\phi_d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\phi_b - \phi_d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$.}\label{fig:unnamed-chunk-12}
\end{figure}

\hypertarget{chapter-appendices}{%
\section{Chapter Appendices}\label{chapter-appendices}}

\hypertarget{summary-boxes}{%
\subsection{Summary Boxes}\label{summary-boxes}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{BOX 1}

\textbf{Two kinds of theories.}

Theories are ``lower-level'' causal models that explain or provide an account of a ``higher-level'', simpler model. There are two forms of theorization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The disaggregation of nodes. A single node in a higher-level model can be split into multiple nodes. For instance, for a higher-level model in which \(X \rightarrow Y \leftarrow \theta^Y\):
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \emph{Mediation}: A mediator, \(M\), can be introduced between \(X\) and \(Y\), thus splitting \(\theta^Y\) into \(\theta^M\) and \(\theta^{Y_\text{lower}}\). The mediation theory thus explains the \(X \rightarrow Y\) relationship.
\item
  \emph{Moderation}: A component of \(\theta^Y\) can be extracted and specified as a substantive variable. This variable is now a substantively conceptualized moderator of the \(X \rightarrow Y\) relationship. The moderation theory thus provides a fuller explanation of why \(X\) has different effects on \(Y\) in different contexts.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Generalization. A feature of context omitted and implicitly held constant in a higher-level model can be explicitly included in the model. The higher-level model is now explained as a special case of a more general set of causal relations.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{BOX 2}

\textbf{Rules for moving between levels}

\emph{Moving down levels}:

All (conditional) independencies represented in a higher-level model must be preserved in the lower-level model.

When we disaggregate or add nodes to a model, new conditional independencies can be generated. But any variables that are independent or conditionally independent (given a third variable) in the higher-level model must also be independent or conditionally independent in the lower-level model.

\emph{Moving up levels}: We can move up levels by eliminating an exogenous node, eliminating an endogenous node, or conditioning on a node. When we eliminate a node from a model, we must preserve any variation and dependencies that it generates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When eliminating an endogenous node, that node's parents adopt (become direct causes of) that node's children.
\item
  When eliminating an exogenous node, we must usually replace it with a \(U\) term. If the node has more than one child, it must be replaced with a \(U\) term pointing into both children (or an undirected edge connecting them) to preserve the dependency between its children. If the node has a spouse, the eliminated node's variation must also be preserved using a \(U\) term. Where the spouse is (already) a \(U\) term with no other children, \(U\) terms can be combined.\\
\item
  Since conditioning on a node ``blocks'' the path through which it connects its children, we can simply eliminate the node and the arrows between it and its children.
\item
  An exogenous node with no spouse and only one child can be simply eliminated.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{illustration-of-a-mapping-from-a-game-to-a-dag}{%
\subsection{Illustration of a Mapping from a Game to a DAG}\label{illustration-of-a-mapping-from-a-game-to-a-dag}}

Our running example supports a set of higher level models, but it can also be \emph{implied} by a lower level models. Here we illustrate with an example in which the lower level model is a game theoretic model, together with a solution.\footnote{Such representations have been discussed as multi agent influence diagrams, for example in \citet{koller2003multi} or \citet{white2009settable} on ``settable systems''--- an extension of the ``influence diagrams'' described by \citet{dawid2002influence}.}

In Figure \ref{fig:tree} we show a game in which nature first decides on the type of the media and the politician -- is it a media that values reporting on corruption or not? Is the politician one who has a dominant strategy to engage in corruption or one who is sensitive to the risks of media exposure? In the example the payoffs to all players are fully specified, though for illustration we include parameter \(b\) in the voter's payoffs which captures utility gains from sacking a politician that has had a negative story written about them \emph{whether or not they actually engaged in corruption}. A somewhat less specific, though more easily defended, theory would not specify particular numbers as in the figure, but rather assume ranges on payoffs that have the same strategic implications.

The theory is then the game plus a solution to the game. Here for a solution the theory specifies subgame perfect equilibrium.

In the subgame perfect equilibrium of the game; marked out on the game tree (for the case \(b=0\)) the sensitive politicians do not engage in corruption when there is a free press -- otherwise they do; a free press writes up any acts of corruption, voters throw out the politician if indeed she is corrupt and this corruption is reported by the press.

As with any structural model, the theory says what will happen but also what \emph{would} happen if things that should not happen indeed happened.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/tree-1.pdf}
\caption{\label{fig:tree}\label{fig:tree} A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which \(b=0\).}
\end{figure}

To draw this equilibrium as a DAG we include nodes for every action taken, nodes for features that determine the game being played, and the utilities at the end of the game.

If equilibrium claims are justified by claims about the beliefs of actors then these could also appear as nodes. To be clear however these are not required to represent the game or the equilibrium, though they can capture assumed logics underlying the equilibrium choice. For instance a theorist might claim that humans are wired so that whenever they are playing a ``Stag Hunt'' game they play ``defect.'' The game and this solution can be represented on a DAG without reference to the beliefs of actors about the action of other players. However, if the \emph{justification} for the equilibrium involves optimization given the beliefs of other players, a lower level DAG could represent this by having a node for the game description that points to beliefs about the actions of others, that then points to choices. In a game with dominant strategies, in contrast, there would be no arrows from these beliefs to actions.

For our running example, nodes could usefully include the politician's expectations, since the government's actions depend on expectations of the actions of others. However, given the game there is no gain from including the media's expectations of the voter's actions since in this case the media's actions do not depend on expectations of the voters actions then these expectations should be included.

In Figure \ref{fig:gamedag} we provide two examples of DAGs that illustrate lower level models that support our running example.

The upper panel gives a DAG reflecting equilibrium play in the game described in Figure \ref{fig:tree}. Note that in this game there is an arrow between \(C\) and \(Y\) even though \(Y\) does not depend on \(C\) for some values of \(b\)---this is because conditional independence requires that two variables are independent for \emph{all} values of the conditioning set. For simplicity also we mark \(S\) and \(X\), along with \(b\) as features that affect which subgame is being played---taking the subgames starting after Nature's move. Note that the government's expectations of responses by others matters, but the expectations of other players do not matter given this game and solution. Note that the utilities appear twice in a sense. They appear in the subgame node, as they are part of the definition of the game--though here they are the utilities that players expect at each terminal node; when they appear at the end of the DAG they are the utilities that actually arise (in theory at least).

The lower level DAG is very low and much more general, representing the theory that in three player games of complete information, players engage in backwards induction and choose the actions that they expect to maximize utility given their beliefs about the actions of others. The DAG assumes that players know what game is being played (``Game''), though this could also be included for more fundamental justification of behavioral predictions. Each action is taken as a function of the beliefs about the game, the expectations about the actions of others, and knowledge of play to date. The functional equations---not shown---are given by optimization and belief formation assuming optimization by others.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{ii_files/figure-latex/gamedag-1} 

}

\caption{\label{fig:gamedag} The upper panel shows a causal graph that describes  relations between nodes suggested by analysis of  the  game  in Figure \ref{fig:tree} and which can imply the causal graph of  Figure \ref{fig:running}. The game itself  (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors.   The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''}\label{fig:gamedag}
\end{figure}

These lower level graphs can themselves provide clues for assessing relations in the higher level graphs. For instance, the lower level model might specify that the value of \(b\) in the game affects the actions of the government only through their beliefs about the behavior of voters, \(E\). These beliefs may themselves have a stochastic component, \(U_E\). Thus \(b\) high might be thought to reduce the effect of media on corruption. For instance if \(b \in \mathbb{R}_+\), we have \(C= 1-FG(1-\mathbb{1}(b>1))\). If \(X\) is unobserved and one is interested in whether \(S=0\) caused corruption, knowledge of \(b\) is informative. It is a root node in the causal estimand. If \(b>1\) then \(S=0\) did not cause corruption. However if \(b\) matters only because of its effect on \(E\) then the query depends on \(U_E\). In this case, while knowing \(b\) is informative about whether \(S=0\) caused \(C=1\), knowing \(E\) from the lower level graph is more informative.

Note that the model we have examined here involves no terms for \(U_C\), \(U_R\) and \(U_Y\)---that is, shocks to outcomes given action. Yet clearly any of these could exist. One could imagine a version of this game with ``trembling hands,'' such that errors are always made with some small probability, giving rise to a much richer set of predictions. These can be represented in the game tree as moves by nature between actions chosen and outcomes realized. Importantly in a strategic environment such noise could give rise to different types of conditional independence. For instance say that a Free Press only published its report on corruption with probability \(\pi^R\), then with \(\pi^R\) high enough the sensitive government might decide it is worth engaging in corruption even if there is a free press; in this case the arrow from \(X\) to \(C\) would be removed. Interestingly in this case as the error rate rises, \(R\) becomes less likely, meaning that the effect of a \(S\) on \(Y\) becomes gradually weaker (since governments that are not sensitive become more likely to survive) and then drops to 0 as sensitive governments start acting just like nonsensitive governments.

\hypertarget{questions}{%
\chapter{Causal Questions}\label{questions}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Although a lot of empirical work focuses on identifying average causal effects, there is a rich array of other well defined causal questions that can be asked about how variables relate to each other causally. We decribe major families of question and illustrate how these can all be described as questions about the values of nodes in a causal model.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The study of causation is central to most empirical social science, whether quantitative analyses of large sets of cases or qualitative, small-\(N\) case studies. Yet a general interest in causality masks tremendous heterogeneity in the kinds of causal questions that scholars tend to ask.

Returning to our inequality and democratization example, we might seek, for instance, to know inequality's average impact on democratization across some set of cases. Alternatively, we might be interested in a particular case---say, Mongolia in 1995---and want to know whether this is a context in which inequality has an effect---a question about causal effects at the case level. Relatedly---but distinctly---we might wonder whether the level of democracy in Mongolia in 1995 is causally attributable to the level of inequality in that case. And we may be interested in \emph{how} causal effects unfold, inquiring about the pathway or mechanism through which inequality affects democratization---a question we can also ask at two levels. We can ask whether inequality affected democratization in Mongolia through mobilization of the masses; and we can ask how commonly inequality affects democratization through mobilization across a broad set of cases.

Rather separate methodological literatures have been devoted to the study of average causal effects, the analysis of case-level causal effects and explanations, and the identification of causal pathways. It is typically understood that their analysis requires quite distinct sets of tools. In this chapter, we take a key integrative step in showing that each of these queries can be readily captured in a causal model. More specifically, we demonstrate how causal queries can be represented as question about one or more \emph{nodes} on a causal graph. When we assimilate our causal questions into a causal model, we are placing what we want to know in formal relation to both what we \emph{already} know and what we can potentially \emph{observe}. As we will see in later chapters, this move allows us then to deploy the model to generate strategies of inference: to determine which observations, if we made them, would be likely to yield the greatest leverage on our query, given our prior knowledge about the way the world works. And by the same logic, once we see the evidence, this integration allows us to ``update'' on our query---figure out in systematic fashion what we \emph{have} learned---in a manner that takes background knowledge into account.

In the remainder of this chapter, we walk through the conceptualization and causal-model interpretation of five key causal queries:

\begin{itemize}
\item
  Case-level causal effects
\item
  Case-level causal attribution
\item
  Case-level explanation
\item
  Average causal effects
\item
  Causal pathways
\end{itemize}

These five are not exhaustive of the causal questions that can be captured in causal graphs, but they are among the more common foci of social scientific investigation.

--\textgreater{}

\hypertarget{case-level-causal-effects}{%
\section{Case-level causal effects}\label{case-level-causal-effects}}

The simplest causal question is whether some causal effect operates in an individual case. Does \(X\) have an effect on \(Y\) in this case? For instance, is Yemen in 1995 a case in which a change in economic inequality would produce a change in whether or not the country democratizes? We could put the question more specifically as a query about a causal effect in a particular direction, for instance: Does inequality have a positive effect on democratization in the case of Yemen in 1995?

In counterfactual terms, a query about case-level causation is a question about what would happen if we could manipulate a variable in the case: if we could hypothetically manipulate \(X\)'s value in the case, would \(Y\)'s value also change? To ask whether a positive (or negative) effect operates for a case is to ask whether a particular counterfactual relation holds in that case. If we assume a binary setup for simplicity, to ask whether inequality has a positive effect on democratization is to ask: if we set \(I\) to \(0\) would \(D\) take on a value of \(0\), \emph{and} if we set \(I\) to \(1\), would \(D\) take on a value of \(1\)? (\emph{Both} of these conditions must hold for \(I\) to have a positive effect on \(D\).)

We can easily represent this kind of query in the context of a causal model. We show the DAG for such a model in Figure \ref{fig:casequery}. As introduced in Chapter \ref{theory}, \(\theta^Y\) here represents the causal type characterizing \(Y\)'s response to \(X\) and, if \(X\) and \(Y\) are binary, can take on one of four values: \(\theta^Y_{10}\), \(\theta^Y_{01}\), \(\theta^Y_{00}\), and \(\theta^Y_{11}\) (which map onto our original \(a, b, c\) and \(d\) types). Importantly, given that the value of nodes (or variables) is allowed to vary across cases, this setup allows for \(\theta_Y\)---the causal effect of \(X\) on \(Y\)---to vary across cases. Thus, \(X\) may have a positive effect on \(Y\) in one case (with \(\theta^Y=\theta^Y_{01}\)), \(X\) may have a negative (\(\theta^Y=\theta^Y_{10}\)) or no effect (\(\theta^Y=\theta^Y_{00}\) or \(\theta^Y_{11}\)) on \(Y\) in other cases.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/unnamed-chunk-14-1} 

}

\caption{\label{fig:casequery} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta^Y$. With a single binary causal variable of interest, we let $\theta_Y$ take on values $\theta^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta^Y$ ranges over the four values: $\theta^Y_{00}$, $\theta^Y_{10}$, $\theta^Y_{01}$ and $\theta^Y_{11}$.}\label{fig:unnamed-chunk-14}
\end{figure}

In this model, then, the query, ``What is \(X\)'s causal effect in this case?'' simply becomes \emph{a question about the value of \(\theta_Y\)}.

Interpreted as ``what is the expected effect of \(X\) on \(Y\)?'' the question becomes one of estimating \(\Pr(\theta^Y = \theta^Y_{01}) - \Pr(\theta^Y = \theta^Y_{10})\).

Similarly in a mediation model of the form \(X\rightarrow M \rightarrow Y\), like that discussed in Chapter 2, the question ``What is the the expected effect of \(X\) on \(Y\)?'' requires estimating
\[\Pr((\theta^M = \theta^M_{01} \& \theta^Y = \theta^Y_{01}) | (\theta^M = \theta^M_{10} \& \theta^Y = \theta^Y_{10}))   - \Pr((\theta^M = \theta^M_{01} \& \theta^Y = \theta^Y_{10}) | (\theta^M = \theta^M_{10} \& \theta^Y = \theta^Y_{01}))\]

Of course, these \(\theta\)s are not directly observable: causal types are intrinsically unobserved properties of cases. So, as we will see in later chapters, research design becomes a challenge of determining which \emph{observable} nodes in the graph are potentially informative about the unobservable nodes that constitute our causal queries.

FLAG: SPELL OUT ALL ESTIMANDS AS COLLECTIONS OF CAUSAL TYPES

\hypertarget{case-level-causal-attribution}{%
\section{Case-level causal attribution}\label{case-level-causal-attribution}}

A query about causal attribution is related to, but different from, a query about a case-level causal effect. When asking about \(X\)'s case-level effect, we are asking, ``\emph{Would} a change in \(X\) cause a change in \(Y\) in this case?'' The question of causal attribution is slightly different: ``\emph{Did} \(X\) cause \(Y\) to take on the value it did in this case?'' More precisely, we are asking, ``Given the values that \(X\) and \(Y\) \emph{in fact} took on in this case, would \(Y\)'s value have been different if \(X\)'s value had been different?''

For instance, given that we know that inequality in Taiwan was relatively low and that Taiwan democratized in 1996, was low inequality a \emph{cause} of Taiwan's democratization in 1996? Put differently, given low economic inequality and democratization in Taiwan in 1996, would the outcome in this case have been different if inequality had been high?

This goes beyond simply asking whether Taiwan is a case in which inequality has a causal effect on democratization. Whereas a case-level causal effect is defined in terms of a single \(\theta\) node, we define a causal-attribution query in terms of a larger set of nodes. To attribute \(Y\)'s value in a case to \(X\), we need to know not only whether this is the kind of case in which \(X\) could have an effect on \(Y\) but also whether the context is such that \(X\)'s value \emph{in fact} made a difference.

Consider, for instance, the general setup in Figure \ref{fig:attribquery}. Here, \(Y\) is a function of two variables, \(X\) and \(W\). This means that \(\theta^Y\) is somewhat more complicated than in a setup with one causal variable: \(\theta^Y\) must here define \(Y\)'s response to different combinations of two other variables, \(X\) and \(W\), since \emph{both} of these variables point directly into \(Y\). Thus, \(\theta^Y\) must cover the full set of possible causal interactions between two binary causal variables.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/attribquery-1} 

}

\caption{\label{fig:attribquery} This DAG is a graphical representation of the simple causal setup in which $Y$ depends on two variables $X1$ and $X2$. How $Y$ responds to X1 and X2 depnds on $\theta^Y$, the DAG itself does not provide information on whether or how X1 and X2 interact with each other.}\label{fig:attribquery}
\end{figure}

We already saw the set of causal types for a set up like this in Chapter 2 (see Table \ref{tab:PO16}). In the table, there are four column headings representing the four possible combinations of \(X1\) and \(X2\) values. Each row represents one possible pattern of \(Y\) values as \(X1\) and \(X2\) move through their four combinations.

Labelling is a little difficult with so many types. One approach used in Chapter \ref{models} is to represent change in \(X1\) on the horizontal axis, and change in the second variable, \(X2\), on the vertical axis. The value of \(X1\) increases from 0 to 1 as we move to the \emph{right} (from \(i\) to \(j\) or from \(g\) to \(h\)). And the value of \(X2\) increases from 0 to 1 as we move \emph{up} (from \(i\) to \(g\) or from \(j\) to \(h\)).

One way to conceptualize the size of the causal-type ``space'' is to note that \(X1\) can have any of our four causal effects (the four binary types) on \(Y\) when \(X2=0\); and \(X1\) can have any of four causal effects when \(X2=1\).\footnote{This is precisely equivalent to noting that \(X2\)'s effect on \(Y\) can be of any of the four types when \(X1=0\) and of any of the four types when \(X1=1\).} This yields 16 possible response patterns to combinations of \(X1\) and \(X2\) values.

A query about causal attribution---whether \(X1 = 1\) caused \(Y=1\)--for the model in in Figure \ref{fig:attribquery}, would be defined in terms of both \(X2\) and \(\theta_Y\). Parallel to our Taiwan example, suppose that we have a case in which \(Y=1\) and in which \(X1\) was also 1, and we want to know whether \(X1\) caused \(Y\) to take on the value it did. Answering this question requires knowing whether the case's type is such that \(X1\) would have had a positive causal effect on \(Y\), \emph{given the value of \(X2\)} (which we might think of as the context). Thus, given that we start with knowledge of \(X1\)'s and \(Y\)'s values, our query about causal attribution amounts to a query about two nodes on the graph: (a) the value of \(X2\) and (b) whether the value of \(\theta^Y\) is such that \(X1\) has a positive causal effect given \(X2\)'s value.

Suppose, for instance, that we were to observe \(X2=1\). We then need to ask whether the causal type, \(\theta_Y\), is such that \(X1\) has a positive effect when \(X2=1\). Consider type 8, or \(\theta_{01}^{11}\). This is a causal type in which \(X1\) has a positive effect when \(X2=0\) but no effect when \(X2=1\). Put differently, \(X2=1\) is a sufficient condition for \(Y=1\), meaning that \(X1\) makes no difference to the outcome when \(X2=1\).

In all we have four qualifying types: \(\theta_{00}^{01}\), \(\theta_{01}^{01}\), \(\theta_{10}^{01}\), and \(\theta_{11}^{01}\) (or 2, 4, 10, and 12). In other words, we can attribute a \(Y=1\) outcome to \(X1=1\) when \(X2=1\) and the causal type is one of these four. By parallel reasoning, we can also attribute a \(Y=1\) outcome to \(X1=1\) when \(X2=0\) and the causal type is any of \(\theta_{01}^{00}\), \(\theta_{01}^{01}\), \(\theta_{01}^{10}\), and \(\theta_{01}^{11}\).

Thus, a question about causal attribution is a question about the \emph{joint} value of a set of nodes: about whether the \emph{combination} of context and causal type is such that changing \(X\) would have changed the outcome.

\hypertarget{case-level-explanation}{%
\section{Case-level explanation}\label{case-level-explanation}}

So far we have been dealing with causes in the standard counterfactual sense: antecedent conditions a change in which would have produced a different outcome. Sometimes, however, we are interested in identifying antecedent conditions that were not counterfactual difference-makers but that nonetheless \emph{generated} or \emph{produced} the outcome. Consider, for instance, a situation in which an outcome was overdetermined: multiple conditions were present, each of which on their own, \emph{could} have generated the outcome. Then none of these conditions caused the outcome in the counterfactual sense; yet one or more of them may have been distinctively important in \emph{producing} the outcome. The concept of an \emph{actual cause} may be useful in putting a finer point on this kind of causal question.

Let us first approach the concept at an intuitive level. An antecedent condition, \(A\), that played a role in generating an outcome might not be a counterfactual cause because, had it not occurred, some second chain of events set in motion by \(B\) would have unfolded, generating the outcome anyway. In the standard counterfactual scenario, \(A\) is not a counterfactual cause: take away \(A\) and the outcome still happens because of the chain of events emanating from \(B\). Yet let us imagine that the fact that \(A\) \emph{did} occur \emph{prevented} part of \(B\)'s chain of consequences from unfolding and itself producing the outcome. Then let us imagine a tweaked counterfactual comparison in which we \emph{fix} the observed fact that \(B\)'s causal sequence did not fully unfold. We can then ask: \emph{conditional on \(B\)'s sequence not fully unfolding}, would \(A\) have been a counterfactual cause of the outcome? If so, then we say that \(A\) is an ``actual cause''" of the outcome. We have, in a sense, identified \(A\) as distinctively important in the production of the outcome, even if it was not a case-level cause in the usual sense.

More formally, and using the definition provided by \citep{halpern2015modification}, building on \citep{halpern2005causesa} and others, we say that a condition (\(X\) taking on some value \(x\)) was an \emph{actual cause} of an outcome (of \(Y\) taking on some value \(y\)), where \(x\) and \(y\) may be collections of events, if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(X=x\) and \(Y=y\) both happened
\item
  there is some set of variables, \(\mathcal W\), such that if they were fixed at the levels that they actually took in the case, and if \(X\) were to be changed, then \(Y\) would change (where \(\mathcal W\) can also be an empty set)
\item
  no strict subset of \(X\) satisfies 1 and 2 (there is no redundant part of the condition, \(X=x\))
\end{enumerate}

The definition thus describes a condition that \emph{would} have been a counterfactual cause of the outcome if we were to imagine holding constant some set of events that in fact occurred (and that, in reality, might not have been constant if the actual cause had not in fact occurred).

A motivating example used in much of the literature on actual causes \citep[e.g.~][]{hall2004two} imagines two characters, Sally and Billy, simultaneously throwing stones at a bottle. Both are great shots and hit whatever they aim at. Sally's stone hits first, and so the bottle breaks. However, Billy's stone \emph{would} have hit had Sally's not hit, and would have broken the bottle. Did Sally's throw cause the bottle to break? Did Billy's?

By the usual definition of causal effects, neither Sally's nor Billy's action had a causal effect: without either throw, the bottle would still have broken. We commonly encounter similar situations in the social world. We observe, for instance, the onset of an economic crisis and the breakout of war---either of which would be sufficient to cause the government's downfall---but with the economic crisis occurring first and toppling the government before the war could do so. Yet neither economic crisis nor war made a difference to the outcome.

To return to the bottle example, while neither Sally's nor Billy's throw is a counterfactual cause, there is an important sense in which Sally's action obviously broke the bottle, and Billy's did not. This intuition is confirmed by applying the definition above. Consider first the question: Did Sally's throw break the bottle? Conditions 1 and 3 are easily satisfied, since Sally \emph{did} throw and the bottle \emph{did} break (Condition 1), and ``Sally threw'' has no strict subsets (Condition 3).

Condition 2 is met if Sally's throw made a difference, counterfactually speaking; and in determining this, we are permitted to condition on (to fix in the counterfactual comparison) any event or set of events that actually happened (or on on none at all). To see why Condition 2 is satisfied, we have to think of there being three steps in the process: Sally and Billy throw, Sally's or Billy's rock hits the bottle, and the bottle breaks. In actuality, Billy's stone did not hit the bottle. And conditioning on this actually occurring event (Billy's stone not hitting), the bottle would \emph{not} have broken had Sally not thrown. From the perspective of counterfactual causation, it may seem odd to condition on Billy's stone not hitting the bottle when thinking about Sally not throwing the stone since Sally's throwing the stone was the very thing that prevented Billy from hitting the bottle. Yet Halpern argues that this is an acceptable thought experiment for establishing the importance of Sally's throw since conditioning is constrained to the actual facts of the case. Moreover, the same logic shows why Billy is not an actual cause. The reason is that Billy's throw is only a cause in those conditions in which Sally did not hit the bottle. But because Sally \emph{did} actually hit the bottle, we are not permitted to condition on Sally not hitting the bottle in determining actual causation. We thus cannot---even through conditioning on actually occurring events---construct any counterfactual comparison in which Billy's throw is a counterfactual cause of the bottle's breaking.

The striking result here is that there can be grounds to claim that a condition was the actual cause of an outcome even though, under the counterfactual definition, the effect of that condition on the outcome is 0. (At the same time, all counterfactual causes are automatically actual causes; they meet Condition 2 by conditioning on nothing at all, an empty set \(\mathcal W\).) One immediate methodological implication follows: since actual causes need not be causes, there are risks in research designs that seek to understand causal effects by tracing back actual causes---i.e., the way things actually happened. If we traced back from the breaking of the bottle, we might be tempted to identify Sally's throw as the cause of the outcome. We would be right only in an actual-causal sense, but wrong in the standard, counterfactual causal sense. Chains of events that appear to ``generate'' an outcome are not always causes. \footnote{Perhaps more surprising, it is possible that the expected causal effect is negative but that \(X\) is an actual cause in expectation. For instance, say that 10\% of the time Sally's shot intercepted Billy's shot but without hitting the bottle. In that case the average causal effect of Sally's throw on bottle breaking is \(-0.1\) yet 90\% of the time Sally's throw is an actual cause of bottle breaking (and 10\% of the time it is an actual cause of non-breaking). For related discussions see \citet{menzies1989probabilistic}.}

As with other causal queries, the question ``Was \(X=x\) the actual cause of \(Y=y\)?'' can be redefined as a question about which values for exogenous nodes produce conditions under which \(X\) could have made a difference. To see how, let us run through the Billy and Sally example again, but formally in terms of a model. Consider Figure \ref{fig:actualquery}, where we represent Sally's throw (\(S\)), Billy's throw (\(B\)), Sally's rock hitting the bottle (\(H^S\)), Billy's rock hitting the bottle (\(H^B\)), and the bottle cracking (\(C\)). Each endogenous variable has a \(\theta\) term associated with it, capturing its response to its parents. We capture the possible ``preemption'' effect with the arrow pointing from \(H^S\) to \(H^B\), allowing that whether Sally's rock hits to affect whether Billy's rock hits.

Let us again imagine that Sally threw (\(S=1\)), Billy threw (\(B=1\)), and the bottle cracked (\(C=1\)). Let us say that \(\theta^{H^B}\) takes on a value such that (a) \(H^B=0\) whenever \(H^S=1\) (Sally's hit preempts Billy's) and (b) \(B\) has a positive effect on \(H^B\) when \(H^S=0\) (Billy's throw hits if Sally's doesn't). Further, assume that \(S\) has a positive effect on \(H^S\). Let us finally posit that \(\theta^C\) takes on a value such that \(C=1\) if \(H^B\) equals \(1\).\footnote{That is, \(\theta^C\) equals some value \(\theta_{ij}^{11}\), where \(H^S\) operates along the horizontal axis and \(H^B\) along the vertical and \(i\) and \(j\) can be any 0 or 1 values.} This is a set of \(\theta\) values under which the query, ``Does \(S\) have a causal effect on \(C\)?'' must be answered in the negative. Similarly, this is a context in which \(C=1\) cannot be causally attributed to \(S=1\). If Sally had not thrown, then Sally's rock would not have hit the bottle, which means that Billy's rock would have hit, and the bottle would still have cracked---still, \(C=1\).

However, it is still possible that \(S=1\) was an actual cause of \(C=1\). To complete this query, we need to ask whether there is some node value that we can hold fixed at the value that it \emph{actually} assumed in the case such that \(S\) would have a causal effect on the outcome. Fixing \(B=1\) (Billy throws) cannot help (since if Billy throws, Billy hits, and the bottle cracks anyway). However, under \(S=1\) and \(B=1\), given the \(\theta\) values we have posited, \(H^B=0\): Billy's rock does not hit. If we hold constant that \(H^B=0\), then there is an ``opportunity'' for \(S\) to matter in that \(C\) is no longer forced to 1 (by Billy's rock hitting). But for \(S\) to matter under his scenario, something else has to be true: \(\theta^C\)'s value must allow for \(H^S\) to have a positive effect on \(C\) when \(H^B=0\).

Using our two-cause notation (with \(H^S\) on the horizontal axis, and \(H^B\) on the vertical), and given that we have already stipulated that \(C=1\) when \(H^B=1\), the one permissible value for \(\theta^C\) is \(\theta^{11}_{01}\). This is causal type in which neither \(H^B\) nor \(H^S\) can be causal if both Billy and Sally throw: whenever one variable is 1, the other has no effect. But it is also a type in which each has a causal effect if the other is held at 0.

It is also the case, as we have said, that all counterfactual causes are actual causes. They are, quite simply, counterfactual causes when we hold \emph{nothing} fixed (\(\mathcal W\) is the empty set). Thus, in fact, any \(\theta^S\), \(\theta^{H^S}\) and \(\theta^C\) values in which \(S\) has a positive effect when \(B=1\) will do. This includes, for instance, a \(\theta^C\) value in which Billy's hitting has no effect on the bottle (perhaps Billy doesn't throw hard enough!): e.g., \(\theta^{01}_{01}\). Here, Sally's throw is both a counterfactual cause and an actual cause of the bottle's cracking. The larger point is that actual cause queries can, like all other causal queries, be defined as questions about the values of nodes in a causal model.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/unnamed-chunk-15-1} 

}

\caption{\label{fig:actualquery} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\theta^Y$. With a single binary causal variable of interest, we let $\theta_Y$ take on values $\theta^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\theta^Y$ ranges over the four values: $\theta^Y_{00}$, $\theta^Y_{10}$, $\theta^Y_{01}$ and $\theta^Y_{11}$.}\label{fig:unnamed-chunk-15}
\end{figure}

Actual causes are conceptually useful whenever there are two sufficient causes for an outcome, but one preempts the operation of the other. For instance, we might posit that both the United States' development of the atomic bomb was a sufficient condition for U.S. victory over Japan in World War II, and that U.S. conventional military superiority was also a sufficient condition and would have operated via a land invasion of Japan. Neither condition was a counterfactual cause of the outcome because both were present. However, holding constant the \emph{absence} of a land invasion, the atomic bomb was a difference-maker, rendering it an actual cause. The concept of actual cause thus helps capture the sense in which the atomic bomb contributed to the outcome, even if it was not a counterfactual cause.

Similarly, the question of how \emph{common} it is for a condition to be an actual cause can be expressed as values of nodes, possibly including nodes that record parameter values for the relevant exogenous nodes.

An extended notion \citep[p 81]{halpern2016actual} of actual causes restricts the imagined counterfactual deviations to states that are more likely to arise (more ``normal'') than the factual state. We will call this notion a `'notable cause.'' Similarly, one cause, \(A\), is ``more notable'' than another cause, \(B\), if a deviation in \(A\) from its realized state is (believed to be) more likely than a deviation in \(B\) from its realized state.

For intuition, we might wonder why a Republican was elected to the presidency in a given election. In looking at some minimal winning coalition of states that voted Republican, we might distinguish between a set of states that \emph{always} vote Republican and a set of states that usually go Democratic but voted Republican this time. If the coalition is minimal winning, then every state that voted Republican is a cause of the outcome in the standard (difference making) sense. However, only the states that usually vote Democratic are notable causes since it is only for them that the counterfactual scenario (voting Democratic) was more likely to arise than the factual scenario. In a sense, we take the ``red'' states' votes for the Republican as given---placing it, as it were, in the causal background---and identify as ``notable'' those conditions that mattered and easily could have gone differently. By the same token, we can say that, among those states that voted Republican this time, those that more commonly vote Democratic are \emph{more} notable causes than those that less commonly vote Democratic.

Again, whether something is a notable cause, or the likelihood in some population that a condition is a notable cause, can be expressed as a claim about the value of a set of root nodes.

Though not a focus of our applied examples we show formally how to estimate these estimands in the Appendix, section XXX.

\hypertarget{average-causal-effects}{%
\section{Average causal effects}\label{average-causal-effects}}

A more general query asks about an average causal effect in some population. In counterfactual terms, a question about average causal effects is: if we manipulated the value of \(X\) for all cases in the population---first setting \(X\) to one value for all cases, then changing it to another value for all cases---by how much would the average value of \(Y\) in the population change? Like other causal queries, a query about an average causal effect can be conceptualized as learning about a node in a causal model.

We can do this by conceiving of any given case as being a member of a population composed of different causal types. When we seek to estimate an average causal effect, we seek information about the \emph{shares} of these causal types in the population.

More formally and adapted from \citet{humphreys2015mixing}, we can use \(\lambda^Y_{ij}\) to refer to the \emph{share} of cases in a population that has causal type \(\theta^Y_{ij}\). Thus, given our four causal types above, \(\lambda^Y_{10}\) is the proportion of cases in the population with negative effects; \(\lambda_{01}\) is the proportion of cases with positive effects; and so on. We can, of course, also think of these shares as probabilities; that is, we can think of any given case as being ``drawn'' from a multinomial distribution with probabilities \(\lambda = (\lambda^Y_{10}, \lambda^Y_{01}, \lambda^Y_{00}, \lambda^Y_{11})\). One nice feature of this setup, with both \(X\) and \(Y\) as binary, the average causal effect can be simply characterized as the share of positive-effect cases less the share of negative-effect cases: \(\lambda^Y_{01} - \lambda^Y_{10}\).

Graphically, we can represent this setup by including \(\lambda^Y\) in a more complex causal graph as in Figure \ref{fig:DAGace}. As in our setup for case-level causal effects, \(X\)'s effect on \(Y\) in a case depends on (and only on) the case's causal type, \(\theta^Y\). The key difference is that we now model the case's type not as exogenously given, but as a function of two additional variables: the distribution of causal types in a population and a random process through which the case's type is ``drawn'' from that distribution. We represent the type distribution as \(\lambda^Y\) (a vector of values for the proportions \(\lambda^Y_{10}, \lambda^Y_{01}, \lambda^Y_{00}, \lambda^Y_{11}\)) and the random process drawing a \(\theta^Y\) value from that distribution as \(U_\theta\).

FLAG: CLARIFY PHILOSOPHOICAL INTERPREATAION OF LAMBDA AS SHARES

In this model, our causal query---about \(X\)'s average causal effect---is thus defined by the vector \(\lambda^Y\), and specifically by the shares of negative- and positive-causal-effect cases, respectively, in the population. What is \(X\)'s average effect on \(Y\) amounts to asking: what are the values of \(\lambda^Y_{10}\) and \(\lambda^Y_{01}\)? As with \(\theta^Y\), \(\lambda^Y\) is not directly observable. And so the empirical challenge is to figure out what we \emph{can} observe that would allow us to learn about \(\lambda^Y\)'s component values?\footnote{Note also that \(\lambda^Y\) can be thought of as itself drawn from a distribution, such as a Dirichlet. The hyperparameters of this underlying distribution of \(\lambda\) would then represent our uncertainty over \(\lambda\) and hence over average causal effects in the population.}

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/unnamed-chunk-16-1} 

}

\caption{\label{fig:DAGace} This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different causal types. As before, $X$'s effect on $Y$ is a function of a causal-type variable, $\theta^Y$. Yet here we explicitly model the process through which the case's type is drawn from a distribution of types in a population. The variable $\lambda$ is a vector representing the multinomial distribution of causal types in the population while $U_\theta$ is a random variable representing the draw of each case from the distribution defined by $\lambda$. A case's causal type, $\theta^Y$, is thus a joint function of $\lambda^Y$ and $U^{\theta_Y}$.}\label{fig:unnamed-chunk-16}
\end{figure}

We can, of course, likewise pose queries about other population-level causal quantities. For instance, we could ask for what proportion of cases in the population \(X\) has a positive effect: this would be equivalent to asking the value of \(\lambda^Y_{01}\), one element of the \(\lambda^Y\) vector. Or we could ask about the proportion of cases in which \(X\) has no effect, which would be asking about \(\lambda^Y_{00} + \lambda^Y_{11}\).

\hypertarget{causal-paths}{%
\section{Causal Paths}\label{causal-paths}}

To develop richer causal understandings, researchers often seek to describe the causal path or paths through which effects propagate. Consider the DAG in Figure \ref{fig:DAGpaths}, in which \(X\) can affect \(Y\) through two possible pathways: directly and via \(M\). Assume again that all variables are binary, taking on values of \(0\) or \(1\). As we have seen in Chapter \ref{theory}, mediation models require causal-type nodes that point into any mediators as well as into the outcome variable. So here we have drawn in a causal-type variable defining \(M\)'s response to \(X\), \(\theta^M\), and a causal-type variable capturing \(Y\)'s response, \(\theta^Y\). Importantly, \(\theta^Y\) defines \(Y\)'s response to \emph{two} parent variables: \(M\) and \(X\).

Suppose that we observe \(X=1\) and \(Y=1\) in a case. Suppose, further, that we have reasonable confidence that \(X\) has had a positive effect on \(Y\) in this case. We may nonetheless be interested in knowing whether that causal effect ran \emph{through} \(M\). We will refer to this as a query about a causal path. A causal path query, of course, goes beyond assessing whether some mediating event along the path occurred. We cannot, for instance, establish that the top path in Figure \ref{fig:DAGpaths} was operative simply by determining the value of \(M\) in this case---though that will likely be useful information.

Rather, the question of whether the top (mediated) causal path is operative is a composite question of two parts: First, does \(X\) have an effect on \(M\) in this case? Second, does that effect---the difference in \(M\)'s value caused by a change in \(X\)---in turn \emph{cause} a change in \(Y\)'s value? In other words, what we want to know is whether the effect of \(X\) on \(Y\) depends on---\emph{will not operate without}---the effect of \(X\) on \(M\).\footnote{A very similar question is taken up in work on mediation where the focus goes to understanding quantities such as the ``indirect effect''" of \(X\) on \(Y\) via \(M\). Formally, the indirect effect would be \[Y(X=1, M = M(X=1,\theta^M), 
  \theta^Y) - Y(X = 1, M = M(X=0, \theta^M), \theta^Y))\], which captures the difference to \(Y\) if \(M\) were to change in the way that it would change due to a change in \(X\), but without an actual change in \(X\) \citep[ p 132, \citet{imai2010general}]{pearl2009causality}.} Framing the query in this way makes clear that asking whether a causal effect operated via a given path is in fact asking about a specific set of causal effects lying along that path.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/unnamed-chunk-17-1} 

}

\caption{\label{fig:DAGpaths} Here $X$ has effects on $Y$ both indirectly through $M$ and directly.}\label{fig:unnamed-chunk-17}
\end{figure}

As we can show, we can also define a causal-path query as a question about specific nodes on a causal graph. In particular, just as we have defined other questions about causal effects in terms of causal-type nodes, a causal path can also be defined in terms of the values of type nodes: specifically, in the present example, in terms of the nodes \(\theta^M\) and \(theta^Y\). To see why, let us first note that there are two combinations of effects that would allow \(X\)'s positive effect on \(Y\) to operate via \(M\): (1) \(X\) has a positive effect on \(M\), which in turn has a positive effect on \(Y\); or (2) \(X\) has a negative effect on \(M\), which has a negative effect on \(Y\).

Thus, in establishing whether \(X\) affects \(Y\) through \(M\), the first question is whether \(X\) affects \(M\) in this case. Whether or not it does is a question about the value of the causal-type node, \(\theta^M\). Let us assume that \(\theta^M\) can take on four possible values corresponding to the four possible responses to \(X\): \(\theta^M_{10}, \theta^M_{01}, \theta^M_{00}, \theta^M_{11}\).\footnote{In other words, \(X\)'s effect on \(M\) could be negative, positive, absent with \(M\) stuck at \(0\), or absent with \(M\) stuck at \(1\), respectively.} For sequence (1) to operate, \(\theta^M\) must take on the value \(\theta^M_{01}\), representing a positive effect of \(X\) on \(M\). For sequence (2) to operate, \(\theta^M\) must take on the value \(\theta^M_{10}\), representing a negative effect of \(X\) on \(M\).

\(\theta^Y\), as for our causal-attribution example, defines \(Y\)'s response to different combinations of two other variables---here, \(X\) and \(M\)---since \emph{both} of these variables point directly into \(Y\). Another way to think about this setup is that \(M\) is not just a possible mediator of \(X\)'s indirect effect; \(M\) is also a potential \emph{moderator} of \(X\)'s direct effect. Where \(X\) can have both an mediated effect through \(M\) and a direct effect, \(X\) and \(M\) also potentially \emph{interact} in affecting \(Y\).

This results in sixteeen possible values for \(\theta^Y\)---again as shown above in Table \ref{tab:PO16}.

What values of \(\theta^Y\)
then are compatible with the operation of the \(M\) causal path? Let us first consider this question with respect to sequence (1), in which \(X\) has a positive effect on \(M\), and that positive effect is necessary for \(X\)'s positive effect on \(Y\) to occur. For this sequence to operate, \(\theta^M\) must take on the value of \(\theta^M_{01}\). When it comes to \(\theta^Y\), then, what we need to look for types in which \(X\)'s effect on \(Y\) \emph{depends on \(M\)'s taking on the value it does as a result of \(X\)'s positive effect on \(M\)}.

We are thus looking for causal types that represent two kinds of counterfactual causal relations operating on nodes. First, \(X\) must have a positive effect on \(Y\) when \(M\) changes as it should given \(X\)'s positive effect on \(M\). Second, that change in \(M\), generated by a change in \(X\), must be \emph{necessary} for \(X\)'s positive effect on \(Y\) to operate. The thought experiment here thus imagines a situation in which \(X\) changes from \(0\) to \(1\),\footnote{This is the natural thought experiment when explaining a case with realized value of \(X=1\), in which the outcome can be thought of as having been generated by a change from \(X=0\). The identification of types does hinge, however, on the direction in which we imagine types changing. In other situations, we might observe \(X=Y=0\) and thus conceive of the outcome as having been generated by a change from \(X=1\) to \(X=0\) (again, assuming a positive effect of \(X\) on \(Y\)). When we do this, query 2 below changes: we are now looking for types in which \(Y=1\) when \(X=0\) but \(M=1\). (Does \(Y\) stay at \(1\) when \(X\) moves to \(0\) but \(M\) doesn't?) The queries are then satisfied by types \(6\) and \(8\), rather than \(2\) and \(6\).} but \(M\) does \emph{not} change to the value that it should as a result of this change in \(X\). We then inspect our types to see if \(Y\) would change from \(0\) to \(1\) in this situation. It is this counterfactual that isolates the causal significance of the path that runs through \(M\). It is only if \(Y\) would \emph{not} change to \(1\) in this situation that we have identified a causal-type for which the \(M\)-mediated path matters.

Assuming a positive effect of \(X\) on \(M\) (\(\theta^M=\theta^M_{01}\)), we thus need to apply three queries to \(\theta^Y\):\footnote{Using standard potential outcomes notation, we can express the overall query, conditioning on a positive effect of \(X\) on \(M\), via the inequality \(Y(1, M(1)) - Y(0, M(0)) > Y(1, M(0)) - Y(0, M(0))\). The three specific queries formulated below simply correspond to the three unique elements of this expression. We can also readily map the path query that we are defining here---does the positive effect of \(X\) on \(Y\) depend on \(X\)'s effect on \(M\)---onto a query posed in terms of indirect effects. For instance, in our binary setup, conditioning our path query on a positive causal effect of \(X\) on \(Y\), a positive effect of \(X\) on \(M\), and an imagined change from \(X=0\) to \(X=1\) generates precisely the same result (identifies the same \(\theta^Y\) types) as asking which \(\theta^Y\) types are consistent with a positive indirect effect of \(X\) on \(Y\), conditioning on a positive total effect and \(X=1\).}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Is \(X=1\) a counterfactual cause of \(Y=1\)? Establishing this positive effect of \(X\) involves two queries:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \item
    Where \(X=0\), does \(Y=0\)? As we are assuming \(X\) has a positive effect on \(M\), if \(X=0\) then \(M=0\) as well. We thus look down the \(X=0, M=0\) column and eliminate those types in which we do not observe \(Y=0\). This eliminates types \(9\) through \(16\).
  \item
    Where \(X=1\), does \(Y=1\)? Again, given \(X\)'s assumed positive effect on \(M\), \(M=1\) under this condition. Looking down the \(X=1, M=1\) column, we eliminate those types where we do not see \(Y=1\). We retain only types \(2, 4, 6,\) and \(8\).
  \end{enumerate}
\item
  Is \(X\)'s effect on \(M\) necessary for \(X\)'s positive effect on \(Y\)? That is, do we see \(Y=1\) \emph{only} if \(M\) takes on the value that \(X=1\) generates (\(M=1\))? To determine this, we inspect the \emph{counterfactual} condition in which \(X=1\) yet \(M=0\), and we ask: does \(Y=0\)? Of the four remaining types, only \(2\) and \(6\) pass this test.
\end{enumerate}

Under these and only these two values of \(\theta^Y\)---\(\theta_{00}^{01}\) and \(\theta_{00}^{11}\)---we will see a positive effect of \(X\) on \(Y\) for which the \(M\)-mediated path is causally necessary as long as \(X\) also has a positive effect on \(M\). These two \(\theta^Y\) values are also different from one another in an interesting way. For type \(\theta_{00}^{11}\), \(X\)'s effect on \(Y\) runs strictly through \(M\): if \(M\) were to change from \(0\) to \(1\) \emph{without} \(X\) changing, \(Y\) would still change from \(0\) to \(1\). \(X\) is causally important for \(Y\) \emph{only} insofar as it affects \(M\). In a case of type \(\theta_{00}^{11}\), then, anything else that similarly affects \(M\) would generate the same effect on \(Y\) as \(X\) does. In type \(\theta_{00}^{01}\), however, both \(X\)'s change to \(1\) \emph{and} the resulting change in \(M\) are necessary to generate \(Y\)'s change to \(1\); \(X\)'s causal effect thus requires both the mediated and the unmediated pathway. Andhere \(X\) itself matters in the counterfactual sense; for a case of type \(\theta_{00}^{01}\), some other cause of \(M\) would \emph{not} generate the same effect on \(Y\).

We can undertake the same exercise for sequence (2), in which \(X\) first has a negative effect on \(M\), or \(\theta^M=\theta^M_{10}\). Here we adjust the three queries for \(\theta^Y\) to take account of this negative effect. Thus, we adjust query 1a so that we are looking for \(Y=0\) when \(X=0\) and \(M=1\). In query 1b, we look for \(Y=1\) when \(X=1\) and \(M=0\). And for query 2, we want types in which \(Y\) fails to shift to \(1\) when \(X\) shifts to \(1\) but \(M\) stays at \(1\). Types \(\theta_{01}^{00}\) and \(\theta_{11}^{00}\) pass these three tests.

In sum, we can define a query about causal paths as a query about the value of \(\theta\) terms on the causal graph. For the graph in Figure \ref{fig:DAGpaths}, asking whether \(X\)'s effect runs via the \(M\)-mediated path is asking whether one of four combinations of \(\theta^M\) and \(\theta^Y\) hold in case:

\begin{itemize}
\tightlist
\item
  \(\theta^M=\theta^M_{01}\) and (\(\theta^Y=\theta_{00}^{01}\) or \(\theta_{00}^{11}\))
\item
  \(\theta^M=\theta^M_{01}\) and (\(\theta^Y=\theta_{01}^{00}\) or \(\theta_{11}^{00}\))
\end{itemize}

It is worth noting how different this formulation of the task of identifying causal pathways is from widespread understandings of process tracing. Scholars commonly characterize process tracing as a method in which we determine whether a mechanism was operating by establishing whether the events lying along that path occurred. As a causal-model framework makes clear, finding out that \(M=1\) (or \(M=0\), for that matter) does not establish what was going on causally. Observing this intervening step does not by itself tell us what value \(M\) \emph{would} have taken on if \(X\) had taken on a different value, or whether this would have changed \(Y\)'s value. We need instead to conceive of the problem of identifying pathways as one of figuring out the \emph{counterfactual} response patterns of the variables along the causal chain. As we will demonstrate later in the book, explicitly characterizing those response patterns as nodes in a causal model helps us think systematically about empirical strategies for drawing the relevant inferences.

--\textgreater{}

--\textgreater{}

--\textgreater{}

--\textgreater{}

--\textgreater{}

\hypertarget{bayeschapter}{%
\chapter{Bayesian Answers}\label{bayeschapter}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We run through the logic of Bayesian updating and show how it is used for answering queries of interest. We illustrate with applications to correlational and process tracing inferences.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Bayesian methods are just sets of procedures to figure out how to update beliefs in light of new information.

We begin with a prior belief about the probability that a hypothesis is true. New data then allow us to form a posterior belief about the probability of the hypothesis. Bayesian inference takes into account the consistency of the evidence with a hypothesis, the uniqueness of the evidence to that hypothesis, and background knowledge about the problem.

In the next section we review the basic idea of Bayesian updating. The following section applies it to the problem of updating on causal estimands given a causal model and data.

\hypertarget{bayes-basics}{%
\section{Bayes Basics}\label{bayes-basics}}

For simple problems, Bayesian inference accords well with our intuitions. Once problems get slightly more complex however, our intuitions often fail us.

\hypertarget{simple-instances}{%
\subsection{Simple instances}\label{simple-instances}}

Say I draw a card from a deck. The chances it is a Jack of Spades is just 1 in 52. If I tell you that the card is indeed a spade and asked you now what are the chances it is a Jack of Spades, you should guess 1 in 13. If I told you it was a heart you should guess there is no chance it is a Jack of Spades. If I said it was a face card and a spade you should say 1 in 3.

All those answers are applications of Bayes' rule. In each case the answer is derived by assessing what is possible, given the new information, and then assessing how likely the outcome of interest among the states that are possible. In all the cases you calculate:

\[\text{Probability Jack of Spades | Information} = \frac{\text{Is Jack of Spades Consistent with Information?}}{\text{How many cards are consistent with Information?}} \]

The same logic goes through when things are not quite so black and white.

Now consider two slightly trickier examples.

\textbf{Interpreting Your Test Results}. Say that you take a test to see whether you suffer from a disease that affects 1 in 100 people. The test is good in the sense that if you have the disease it will say you have it with a 99\% probability. If you do not have it, then with a 99\% probability, it will say that you do not have it. The test result says that you have the disease. What are the chances you have it? You might think the answer is 99\%, but that would be to mix up the probability of the result given the disease with the probability of the disease given the result. In fact the right answer is 50\%, which you can think of as the share of people that have the disease among all those that test positive. For example if there were 10,000 people, then 100 would have the disease and 99 of these would test positive. But 9,900 would not have the disease and 99 of these would test positive. So the people with the disease that test positive are half of the total number testing positive.

As an equation this might be written:

\[\text{Probability You have the Disease | Test} = \frac{\text{How many people have the disease and test positive?}}{\text{How many people test positive?}} \]

\textbf{Two-Child Problem} Consider last an old puzzle found described \citet{gardner1961second}. \emph{Mr Smith has two children, \(A\) and \(B\). At least one of them is a boy. What are the chances they are both boys?}
To be explicit about the puzzle, we will assume that the information that one child is a boy is given as a truthful answer to the question ``is at least one of the children a boy?'' Assuming that there is a 50\% probability that a given child is a boy, people often assume the answer is 50\%. But surprisingly the answer is 1 in 3. The information provided rules out the possibility that both children are girls and so the right answer is found by readjusting the probability that two children are boys based on this information. As an equation:

\[\text{Probability both are boys | Not both girls} = \frac{\text{Probability  both boys}}{\text{Probability they are not both girls}} = \frac{\text{1 in 4}}{\text{3 in 4}}\]

\hypertarget{bayes-rule-for-discrete-hypotheses}{%
\subsection{Bayes' Rule for Discrete Hypotheses}\label{bayes-rule-for-discrete-hypotheses}}

Formally, all of these equations are applications of Bayes' rule which is a simple and powerful formula for deriving updated beliefs from new data.

The formula is given as:
\begin{eqnarray}
\Pr(H|\mathcal{D})&=&\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\Pr(\mathcal{D})}\\
                  &=&\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\sum_{H'}\Pr(\mathcal{D}|H')\Pr(H'))}
\end{eqnarray}

where \(H\) represents a hypothesis and \(\mathcal{D}\) represents a particular realization of new data (e.g., a particular piece of evidence that we might observe).

Looking at the formula we see that the posterior belief derives from three considerations. First, the likelihood: how likely are we to have observed these data if the hypothesis were true, \(\Pr(\mathcal{D}|H\))? Second, how likely were we to have observed these data regardless of whether the hypothesis is true or false, \(\Pr(\mathcal{D})\)? These first two questions, then, capture how consistent the data are with our hypothesis and how specific the data are to our hypothesis. As shown in the equation above the second question can usefully be reposed as one about all the different ways (alternative Hypotheses, \(H'\)) that could give rise to the data.

Note, that contrary to some claims, the denominator does not require a listing of all possible hypotheses, just an exhaustive collection of hypotheses. For example we might have the notion of the probability that the accused's fingerprints would be on the door if she were or were guilty without having to decompose the ``not guilty'' into a set of hypotheses regarding who else might be guilty.

Our posterior belief is further conditioned by the strength of our prior level of confidence in the hypothesis, \(\Pr(H)\). The greater the prior likelihood that our hypothesis is true, the greater the chance that new data consistent with the hypothesis has \emph{in fact} been generated by a state of the world implied by the hypothesis.

\hypertarget{the-dirichlet-family-and-bayes-rule-for-continuous-parameters}{%
\subsection{The Dirichlet family and Bayes' Rule for Continuous Parameters}\label{the-dirichlet-family-and-bayes-rule-for-continuous-parameters}}

This basic formula extends in a simple way to collections of continuous variables. For example, say we are interested in the value of some parameter vector \(\theta\) (as a vector, \(\theta\) can contain many quantities we are uncertain about), we can calculate this, given a prior probability distribution over possible values of \(\theta\), \(p\), and given data \(D\) as:

\[p(\theta|\mathcal{D})=\frac{p(\mathcal{D}|\theta)p(\theta)}{\int_{\theta'}p(\mathcal{D|\theta'})p(\theta')d\theta}\]

Bayes rule requires the ability to express a prior distribution but it does not require that the prior have any particular properties other than being probability distributions.

In practice however when we are dealing with continuous parameters, it can be useful to make use of ``off the shelf'' distributions.

In practice we will often be interested in forming beliefs about the share of units that are of a particular type. For this type of question we will make quite heavy use of ``Dirichlet'' distributions -- a family of distributions that capture beliefs about shares.

Consider for example the share of people in a population that voted---this is a quantity between 0 and 1. Two people might may both believe that the turnout was around 50\% but may differ in how certain they are about this claim. One might claim to have no information and to believe that any turnout rate between 0 and 100\% is equally likely, giving an expected turnout of 50\%; another might be completely confident that the number if 50\% and entertain no other possibilities.

We can capture such beliefs quite well by using the Beta distribution---a special case of the Dirichlet. The Beta is a distribution over the \([0,1]\) that is governed by two parameters , \(\alpha\) and \(\beta\). In the case in which both \(\alpha\) and \(\beta\) are 1, the distribution is uniform -- all values are seen as equally likely. As \(\alpha\) rises large outcomes are seen as more likely and as \(\beta\) rises, lower outcomes are seen as more likely. If both rise proportionately the expected outcome does not change but the distribution becomes tighter.

An attractive feature of the Beta distribution is that if one has a prior Beta(\(\alpha\), \(\beta\)) over the probability of some event (e.g.~that a coin comes up heads), and then one observes a positive case, the Bayesian posterior distribution is also a Beta with with parameters \(\alpha+1, \beta\). Thus in a sense if people start with uniform priors and build up knowledge on seeing outcomes, their posterior beliefs should be Beta distributions.

Figure \ref{fig:Betas} shows a set of such distributions, starting with one that has greater variance than uniform (this corresponds to the non informative ``Jeffrey's prior''), then uniform, then for a case in which multiple negative and positive outcomes are seen, in equal number, and finally a set of priors with mean of 3/4.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/Betas-1.pdf}
\caption{\label{fig:Betas}Beta distributions}
\end{figure}

Dirichlet distributions generalize the Beta to the situation in which there are beliefs not just over a proportion, or a probability, but over collections of probabilities. For example if four outcomes are possible and each is likely to occur with probability \(\theta_k\), \(k=1,2,3,4\) then beliefs about these probabilities are distributions over the a three dimensional unit simplex---that is, all 4 element vectors of probabilities that add up to 1. The distribution has as many parameters as there are outcomes and these are traditionally recorded in a vector, \(\alpha\). Similar to the Beta distribution, an uninformative prior (Jeffrey's prior) has \(\alpha\) parameters of \((.5,.5,.5, \dots)\) and a uniform (``flat'') distribution has \(\alpha = (1,1,1,,\dots)\).

As with the Beta distribution, the Dirichlet updates in a simple way. If you have a Dirichlet prior with parameter \(\alpha = (\alpha_1, \alpha_2, \dots)\) and you observe outcome \(1\), for example, then then posterior distribution is also Dirichlet with parameter vector \(\alpha' = (\alpha_1+1, \alpha_2,\dots)\).

\hypertarget{moments}{%
\subsection{Moments}\label{moments}}

In what follows we often refer to the ``posterior mean'' or the ``posterior variance.'' These are simply summary statistics of the posterior distribution and can be calculated easily once the posterior is known. For example the posterior mean of a parameter \(\theta_1\)---just one in a collection of parameters stored in \(\theta\)---is simply \(\overline{\theta}_1 | \mathcal{D} = \int \theta_1 p(\theta | \mathcal{D}) d\theta\). Note importantly that this is calculated using the posterior over the entire vector \(\theta\), there is no notion of updating parameter \(\theta_1\) on its own. Similarly the posterior variance is \(\int (\theta_1 - (\overline{\theta}_1 | \mathcal{D})^2 p(\theta | \mathcal{D}) d\theta\).

\hypertarget{bayes-estimation-in-practice}{%
\subsection{Bayes estimation in practice}\label{bayes-estimation-in-practice}}

Although the principle of Bayesian inference is quite simple, in practice calculating posteriors for continuous parameters is computationally complex.

In principle with continuous parameters there is an infinity of possible parameter values. Analytic solutions are not, in general, easy to come by and so in practice researchers use some form of sampling.

Imagine for instance you were interested in forming a posterior on the share intending to vote democrat, given polling data. (This is not truly continuous, but with large elections it might as well be).

One approach is to coarsen the parameter space---we calculate the probability of observing the polling data given possible values \(\theta = 0, \theta = .1, \theta = .2, \dots, \theta = 1\), and, apply Bayes rule to form a posterior for each of these these possibilities. The downside of the this approach is that it for a decent level of precision it becomes computationally expensive with large parameter spaces and parameter spaces get large quickly. For instance if you are interested in vote shares you might find .4, .5, and .6 too coarse and want posteriors for 0.51 or even 0.505; this would require calculations for 200 parameter values. If you had two parameters that you wanted to slice up each into 200 possible values, you would then have 40,000 parameter pairs to worry about. What's more, \emph{most} of those calculations would not be very informative if the real uncertainty all lies in some small (though possibly unknown) range -- such as between 40\% and 60\%.

An alternative approach is to use variants of Markov Chain Monte Carlo sampling. Under these approaches parameter vectors are sampled and their likelihood is evaluated. If they have high likelihood then new parameter vectors near them are draw with a high probability. Based on the likelihood associated with these new draws, new draws are made. The result is a chain of draws that build up to approximate the posterior distribution. The output from these procedures is not a set of probabilities for each possible parameter vector but rather a a set of draws of parameter vectors from the posterior distribution.

Many algorithms have been developed to achieve these tasks efficiently; in all of our applications we rely on the \texttt{stan} procedures which involve\ldots{}.

\hypertarget{bayes-applied}{%
\section{Bayes applied}\label{bayes-applied}}

\hypertarget{bayesian-inference-on-queries}{%
\subsection{Bayesian Inference on Queries}\label{bayesian-inference-on-queries}}

In Chapter 2 we described estimands of interest as queries over the values of root nodes in directed acyclic graphs.

Once queries are defined in terms of the values of roots then formation of beliefs, given data \(W\), about estimands follows immediately from application of Bayes rule.

Let \(Q(u)\) define the value of the query in context \(u\). The updated beliefs about the query are given by the distribution:

\[P(q | W) = \int_{u:Q(u) = q} P(u|W)du =  \int_{u:Q(u) = q} \frac{P(W|u)P(u)}{\int_{u'}P(W|u')P(u')du'}du\]

This expression gathers together all the contexts that produce a given value of \(Q\) and assesses how likely these are, collectively, given the data.\footnote{Learning about roots from observed data is sometimes termed \emph{abduction}; see \citet{pearl2009causality}, p 206.} For an abstract representation of the relations between assumptions, queries, data, and conclusions, see Figure 1 in \citet{pearl2012causal}.

Return now to Mr Smith's puzzle. The two ``roots'' are the sexes of the two children, child \(A\) and child \(B\). The query here is \(Q\): ``Are both boys?'' which can be written in terms of the roots. The statement ``\(Q=1\)'' is equivalent to the statement (\(A\) is a boy \& \(B\) is a boy). Thus it takes the value \(q=1\) in just one context. Statement \(q=0\) is the statement (``\(A\) is a boy \& \(B\) is a girl'' or ``\(A\) is a girl \& \(B\) is a boy'' or ``\(A\) is a girl \& \(B\) is a girl''). Thus \(q=0\) in three contexts. If we assume that each of the two children is equally likely to be a boy or a girl with independent probabilities, then each of the four contexts is equally likely.
The result can then be figured out as \(P(Q=1) = \frac{1\times \frac{1}{4}}{1\times \frac{1}{4} + 1\times \frac{1}{4}+1\times \frac{1}{4}+0\times \frac{1}{4}} = \frac{1}{3}\). This answer requires summing over only one context. \(P(Q=0)\) is of course the complement of this, but using the Bayes formula one can see that it can be found by summing over the posterior probability of three contexts in which the statement \(Q=0\) is true.

We will often want to think about our causal queries being collections of states of the world --- i.e., of unit causal types. Returning to our discussion of queries in Chapter \ref{questions}, suppose we start with the model \(X \rightarrow M \rightarrow Y\), and our query is whether \(X\) has a positive effect on \(Y\). This is a query that is satisfied by four sets of unit types: those in which \(X\) has a positive effect on \(M\) and \(M\) has a positive effect on \(Y\), with \(X\) being either 0 or 1; and those in which \(X\) has a negative effect on \(M\) and \(M\) has a negative effect on \(Y\), with \(X\) being either 0 or 1. Our inferences on the query will thus involve gathering these different unit types, and their associated posterior probabilities, together.

One interesting feature of Bayesian updating is that we update more strongly in favor of the hypothesis for which the evidence is least damaging to the most-likely ways in which the hypothesis could be true. Suppose our prior belief was that it was much more unlikely that \(M\) had a negative effect on \(Y\), than that \(M\) had a positive effect on \(Y\). This makes one of the ways in which \(X\) could have a positive effect on \(Y\) (the chain of negative effects) much less likely than the other way in which \(X\) could have a positive effect on \(Y\) (the chain of positive effects). This means that evidence, say, against a chain of negative effects and evidence against a chain of positive effects will not be equally consequential for our query: in particular, we will update more strongly against the query if we find evidence against a chain of positive effects than if we find evidence against a chain of negative effects. Evidence against a chain of positive effects speaks against the \emph{most} likely way in which the query could be true, whereas evidence against a chain of negative effects speaks against a way the query could be true that we did not think was very likely to begin with.

\hypertarget{simple-bayesian-process-tracing}{%
\subsection{Simple Bayesian Process Tracing}\label{simple-bayesian-process-tracing}}

Process tracing in its most basic form seeks to use within case evidence to draw inferences about the case. For example, with a focus on whether \(X\) caused \(Y\) , data on a ``clue'', \(K\), is used to make inference about whether or not the outcome in that case was generated by the case's treatment status. We refer to the within-case evidence gathered during process tracing as \emph{clues} in order to underline their probabilistic relationship to the causal relationship of interest. Readers familiar with the framework in \citet{collier2004sources} can usefully think of our ``clues'' as akin to causal process observations, although we highlight that there is no requirement that the clues be generated by the causal process.

To make inferences, the analyst looks for clues that will be observed with some probability if the case is of a given type and that will \emph{not} be observed with some probability if the case is \emph{not} of that type.

It is relatively straightforward to express the logic of process tracing in Bayesian terms, a step that will aid the integration of qualitative with quantitative causal inferences. As noted by others (e.g.~\citet{BennettBayes}, \citet{beachpedersen2013process}, \citet{rohlfing2012case}), there is an evident connection between the use of evidence in process tracing and Bayesian inference. .

To illustrate, suppose we are interested in regime collapse. We already have \(X,Y\) data on one authoritarian regime: we know that it suffered economic crisis (\(X=1\)) and collapsed (\(Y=1\)). We want to know what caused the collapse. To make progress we will try to draw inferences given a ``clue.'' Beliefs about the probabilities of observing clues for cases with different causal effects derive from theories of, or evidence about, the causal process connecting \(X\) and \(Y\). Suppose we theorize that the mechanism through which economic crisis generates collapse runs via diminished regime capacity to reward its supporters during an economic downturn. A possible clue to the operation of a causal effect, then, might be the observation of diminishing rents flowing to regime supporters shortly after the crisis. If we believe the theory, then this is a clue that we might believe to be highly probable for cases of type \(b\) that have experienced economic crisis (where the crisis in fact caused the collapse) but of low probability for cases of type \(d\) that have experienced crisis (where the collapse occurred for other reasons).

To make use of Bayes rule we need to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  define our parameters, which are the key quantities of interest
\item
  provide prior beliefs about the parameters of interest
\item
  define a likelihood function
\item
  provide the probability of the data
\item
  plug these into Bayes' rule to calculate a posterior on the parameters of interest
\end{enumerate}

We discuss each of these in turn.

\textbf{Parameters.} The inferential challenge is to determine whether the regime collapsed \emph{because} of the crisis (\(b\) type) or whether it would have collapsed even without it (\(d\) type). We do so using further information from the case---one or more clues. We use the variable \(K\) to register the outcome of the search for a clue, with \(K\)=1 indicating that a specific clue is searched for and found, and \(K\)=0 indicating that the clue is searched for and not found.

Let \(j\in \{a,b,c,d\}\) refer to the type of an individual case. Our hypothesis, in this initial setup, consists simply of a belief about \(j\) for the case under examination: specifically whether the case is a \(b\) type (\(j=b)\). The parameter of interest is the causal type.

\textbf{Prior.} We then assign a prior degree of confidence to the hypothesis (\(p = Pr(H)\)). This is, here, our prior belief that an authoritarian regime that has experienced economic crisis is a \(b\).

\textbf{Likelihood.} The likelihood, \(\Pr(K=1|H)\) is the probability of observing the clue, when we look for it in our case, if the hypothesis is true---i.e., here, if the case is a \(b\) type. The key feature of a clue is that the probability of observing the clue is believed to depend on the case's causal type. In order to calculate the probability of the data we will in fact need two such probabilities: we let \(\phi_b\) denote the probability of observing the clue for a case of \(b\) type (\(\Pr(K=1|j=b)\)), and \(\phi_d\) the probability of observing the clue for a case of \(d\) type (\(\Pr(K=1|j=d)\)). The key idea in many accounts of process tracing is that the \emph{differences} between these probabilities provides clues with `'probative value,'' that is, the ability to generate learning about causal types. The likelihood, \(\Pr(K=1|H)\), is simply \(\phi_b\).

\textbf{Probability of the data.} This is the probability of observing the clue when we look for it in a case, \emph{regardless} of its type, \((\Pr(K=1))\). More specifically, it is the probability of the clue in a treated case with a positive outcome. As such a case can only be a \(b\) or a \(d\) type, this probability can be calculated simply from \(\phi_b\) and \(\phi_d\), together with our beliefs about how likely an \(X=1, Y=1\) case is to be a \(b\) or a \(d\) type.
This probability aligns (inversely) with Van Evera's concept of `'uniqueness.''

\textbf{Inference.} We can now apply Bayes' rule to describe the learning that results from process tracing. If we observe the clue when we look for it in the case, then our \emph{posterior} belief in the hypothesis that the case is of type \emph{b} is:

\begin{eqnarray*}
\Pr(j = b |K=1, X=Y=1)=  \frac{\phi_b p }{\phi_b p+\phi_d (1-p)}
\end{eqnarray*}

In this exposition we did not make use of a causal model in a meaningful way---we simply need the priors and the clue probabilities.

In fact, however, these numbers can be derived from a causal model. To illustrate, imagine a simple causal model in which the \(X, Y\) relationship is completely mediated by \(K\). In particular, suppose, from background knowledge of the conditional distribution of outcomes given their causes, we have that:

\begin{itemize}
\tightlist
\item
  \(\Pr(K=1 | X=0) = 0\), \(\Pr(K=1 | X=1) = .5\)
\item
  \(\Pr(Y=1 | K=0) = .5\), \(\Pr(Y=1 | K=1) = 1\)
\end{itemize}

This data is consistent with a world in which half \(b\) and \(c\) types in the first step and half \(b\) and \(d\) types in the second step. Assume that the case at hand is sampled from this world.

Then we can calculate that the prior probability, \(p\), that \(X\) caused \(Y\) given \(X=Y=1\) is \(p = \frac13\).\footnote{Given \(X=1\), \(Y=1\) is consistent with \(b\) types at both stages, which arises with probability .25, or with a \(d\) type in the second stage, which arises with probability .5. The conditional probability is therefore \(.25/.75 = 1/3\).} We can also calculate the probability that \(K=1\) for a treated \(b\) and \(d\) case respectively as \(\phi_b=1\) and \(\phi_d=0.5\) (convince yourself of these numbers!). We then get:

\begin{eqnarray*}
\Pr(j = b |K=1, X=Y=1)&=&\frac{1\times \frac13}{1 \times \frac13 + 0.5 \times \frac23}=0.5
\end{eqnarray*}

We thus shift our beliefs from a prior of \(\frac13\) to a posterior of \(\frac12\). In contrast had we \emph{not} observed the clue our posterior would have been 0.

As should be clear from the above, the inferential leverage in process tracing comes from differences in the probability of observing \(K=1\) for different causal types. Thus, the logic described here generalizes Van Evera's familiar typology of tests by conceiving of the certainty and uniqueness of clues as lying along a continuum.

Van Evera's four tests (``smoking gun,'' ``hoop,'' ``straw in the wind,'' and ``doubly decisive'') represent, in this sense, special cases---particular regions that lie on the boundaries of a ``probative-value space.'' To illustrate the idea, we represent the range of combinations of possible probabilities for \(\phi_b\) and \(\phi_d\) as a square in Figure \ref{CluesInferences1} and mark the spaces inhabited by Van Evera's tests. As can be seen, the type of test involved depends on both the relative \emph{and} absolute magnitudes of \(\phi_b\) and \(\phi_d\). The probative value of a test depends on the difference between them. Thus, a clue acts as a smoking gun for proposition ``\(b\)'' (the proposition that the case is a \(b\) type) if it is highly unlikely to be observed if proposition \(b\) is false, and more likely to be observed if the proposition is true (bottom left, above diagonal). A clue acts as a ``hoop'' test if it is highly likely to be found if \(b\) is true, even if it still quite likely to be found if it is false. Doubly decisive tests arise when a clue is very likely if \(b\) and very unlikely if not. It is, however, also easy to imagine clues with probative qualities lying in the large space amidst these extremes.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/unnamed-chunk-20-1.pdf}
\caption{\label{fig:unnamed-chunk-20}\label{CluesInferences1} A mapping from the probability of observing a clue if the proposition that a case is a \(b\) type is true (\(\phi_b\)) or false (\(\phi_d\)) to a generalization of the tests described in Van-Evera (1997).}
\end{figure}

In this illustration we note that we draw both the priors and the probative value from a causal model. If we altered the model---for example if we had a stronger first stage and so a larger value for \(\Pr(K=1|X=0)\)---this would alter both our prior, \(p\), and our calculations of \(\phi_d\). An implication of this is that, although one might be tempted to think of the priors and the probative values as independent quantities, and contemplate how inferences change as priors change (as we did for example in Appendix FLAG REF), keeping probative value fixed, that kind of thought experiment may assume values that are justified by an underlying model.

\hypertarget{three-principles-of-bayesian-updating}{%
\section{Three principles of Bayesian updating}\label{three-principles-of-bayesian-updating}}

FLAG: REDO THESE THREE EXAMPLES WITHOUT PHI

\hypertarget{AppPriors}{%
\subsection{Priors matter}\label{AppPriors}}

The amount of learning that results from a given piece of new data depends strongly on prior beliefs. We saw this already with the example of interpreting our test results above. Figure \ref{CluesInferences2} illustrates the point for process tracing inferences.

In each subgraph of Figure \ref{CluesInferences2} , we show how much learning occurs under different scenarios. The horizontal axis indicates the level of prior confidence in the hypothesis and the curve indicates the posterior belief that arises if we do (or do not) observe the clue. As can be seen, the amount of learning that occurs---the shift in beliefs from prior to posterior---depends a good deal on what prior we start out with. For a smoking gun test, the amount of learning is highest for values roughly in the 0.2 to 0.4 range---and then declines as we have more and more prior confidence in our hypothesis. For a hoop test, the amount of learning when the clue is \emph{not} observed is greatest for hypotheses in which we have middling-high confidence (around 0.6 to 0.8), and minimal for hypotheses in which we have a very high or a very low level of confidence.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/CluesInferences2-1.pdf}
\caption{\label{fig:CluesInferences2}Figure shows how the learning from different types of tests depends on priors regarding the proposition. A smoking gun test has the greatest impact on beliefs when priors are middling low and the clue is observed; a `'hoop test'' has the greatest effect when priors are middling high and the clue is not observed.}
\end{figure}

The implication here is that our inferences with respect to a hypothesis must be based not just on the search for a clue predicted by the hypothesis but also on the \emph{plausibility} of the hypothesis, based on other things we know. Suppose, for instance, that we fail to observe evidence that we are 90 percent sure we \emph{should} observe if a hypothesized causal effect has occurred: a strong hoop test is failed. But suppose that the existing literature has given us a very high level of confidence that the hypothesis \emph{is} right. This high prior confidence, sometimes referred to as a ``base rate,'' is equivalent to believing that the causal effect exists in a very high proportion of cases. Thus, while any given case with a causal effect has only a 0.1 chance of not generating the clue, the high base rate means that the vast majority of cases that we observe without the clue will nonetheless be cases with causal effects. Thus, the failure of even a strong hoop test, involving a highly certain prediction, should only marginally reduce our confidence in a hypothesis that we strongly expect to be true.

A similar line of reasoning applies to smoking gun tests involving hypotheses that prior evidence suggests are very unlikely to be true. Innocent people may be very unlikely to be seen holding smoking guns after a murder. But if a very high proportion of people observed are known to be innocent, then a very high proportion of those holding smoking guns will in fact be innocent---and a smoking-gun clue will be far from decisive.

We emphasize two respects in which these implications depart from common intuitions. First, we cannot make \emph{general} statements about how decisive different categories of test, in Van Evera's framework, will be. It is commonly stated that hoop tests are devastating to a theory when they are failed, while smoking gun tests provide powerful evidence in favor of a hypothesis. But, in fact the amount learned depends not just on features of the clues but also on prior beliefs.

Second, although scholars frequently treat evidence that goes against the grain of the existing literature as especially enlightening, in the Bayesian framework the contribution of such evidence may sometimes be modest, precisely because received wisdom carries weight. Thus, although the discovery of \emph{disconfirming} evidence---an observation thought to be strongly inconsistent with the hypothesis---for a hypothesis commonly believed to be true is more informative (has a larger impact on beliefs) than \emph{confirming} evidence, this does not mean that we learn more than we would have if the prior were weaker. \% But it is not true as a general proposition that we learn more the bigger the ``surprise'' a piece of evidence is.
\%The effect of disconfirming evidence on a hypothesis about which we are highly confident will be \emph{smaller} than it would be for a hypothesis about which we are only somewhat confident.
When it comes to very strong hypotheses, the ``discovery'' of disconfirming evidence is very likely to be a false negative; likewise, the discovery of supporting evidence for a very implausible hypothesis is very likely to be a false positive. The Bayesian approach takes account of these features naturally.\footnote{We note, however, that one common intuition---that little is learned from disconfirming evidence on a low-plausibility hypothesis or from confirming evidence on a high-plausibility one---\emph{is} correct.}

\hypertarget{simultaneous-joint-updating}{%
\subsection{Simultaneous, joint updating}\label{simultaneous-joint-updating}}

When we update we often update over multiple quantities. When we see a smoking gun, for instance, we might update our beliefs that the butler did it, but we might also update our beliefs about how likely we are to see smoking guns -- maybe they are not so rare as we thought!

Intuitively you might think of this updating as happening sequentially -- first of all you update over the general proposition, then you update over the particular claim. But in fact you update over both quantities at once.

Here we elaborate on the intuition of fully Bayesian process tracing, in which updating occurs over both causal type (\(j\)) and beliefs about the probabilities with which clues are observed for each type (\(\phi\) values). The illustration in the text makes clear how updating over type occurs, given beliefs about \(\phi\) values. But how does updating over \(\phi\) occur?

Suppose that we observe a case with values \(X=1, Y=1\). We begin by defining a prior probability distribution over each parameter. Suppose that we establish a prior categorical distribution reflecting uncertainty over whether the case is a \(b\) type (e.g., setting a probability of 0.5 that it is a \(b\) and 0.5 that is a \(d\) type). We also start with priors on \(\phi_b\) and \(\phi_d\). For concreteness, suppose that we are certain that the clue is unlikely for a \(d\) type (\(\phi_d=.1\)), but we are very uncertain about \(\phi_b\); in particular, we have a uniform prior distribution over \([0,1]\) for \(\phi_b\). Note that, even though we are very uncertain about \(\phi_b\), the clue still has probative value, arising from the fact that the expected value of \(\phi_b\) is higher than that of \(\phi_d\).

Suppose that we then look for the clue in the case and observe it. This observation shifts posterior weight away from a belief that the case is a \(b\). See Figure \ref{fig:correlation} for an illustration. Yet it \emph{simultaneously} shifts weight toward a higher value for \(\phi_b\) and a lower value for \(\phi_d\). The reason is that the observed clue has a relatively high likelihood \emph{both} for combinations of parameter values in which the case is a \(d\) and \(\phi_b\) is low \emph{and} for combinations in which the case is a \(b\) and \(\phi_b\) is \emph{high} (or, equivalently, in this example, where \(\phi_d\) is low). The marginal posterior distribution of \(\phi_b\) will thus be shifted upward relative to its prior marginal distribution. The joint posterior distribution will also reflect a dependency between the probability that the case is a \(b\) vs.~a \(d\), on the one hand, and \(\phi_b\) and \(\phi_d\) on the other.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/unnamed-chunk-21-1.pdf}
\caption{\label{fig:unnamed-chunk-21}\label{fig:correlation} Joint posteriors distribution on whether a case is a \(b\) or \(d\) and on the probability of seeing a clue for a \(b\) type (\(\phi_b\)).}
\end{figure}

\hypertarget{posteriors-are-independent-of-the-ordering-of-data}{%
\subsection{Posteriors are independent of the ordering of data}\label{posteriors-are-independent-of-the-ordering-of-data}}

We often think of learning as a process in which we start off with some set of beliefs---our priors, we gather data, \(D_1\), and update our beliefs, forming a posterior; we then observe new data and we update again, forming a new posterior, having treated the previous posterior as a new prior. In such cases it might seem natural that it matters which data we saw first and which later.

For instance EXAMPLE

In fact though, Bayesian updating is deaf to ordering. If we learn first that the card is a face card and second that it is black, our posteriors that it is a Jack of Spades go from 1 in 52 to 1 in 12 to 1 in 6. If we learn first that the card is black and second that it is a face card, our posteriors that it is a Jack of Spades go from 1 in 52 to 1 in 26 to 1 in 6. We end up in the same places in both cases. And we would ave had the same conclusion if we learned in one go that the card is a black face card.

The math of this is easy enough. Our posterior given two sets of data \(D_1, D_2\) can be written:

\[p(\theta | D_1, D_2) = \frac{p(\theta, D_1, D_2)}{p(D_1, D_2)} = \frac{p(\theta, D_1 | D_2)p(D_2)}{p(D_1 | D_2)p(D_2)}= \frac{p(\theta, D_1 | D_2)}{p(D_1 | D_2)}\]

or, equivalently:

\[p(\theta | D_1, D_2) = \frac{p(\theta, D_1, D_2)}{p(D_1, D_2)} = \frac{p(\theta, D_2 | D_1)p(D_1)}{p(D_2 | D_1)p(D_1)}= \frac{p(\theta, D_2 | D_1)}{p(D_2 | D_1)}\]

In other words our posteriors given both \(D_1\) and \(D_2\) can be thought of as the result of updating on \(D_2\) given we already know \(D_1\) or the result of updating on \(D_1\) given we already know \(D_2\).

This fact will be useful in applications. In practice we might assume that we have beliefs based on background data \(D_1\), for example regarding general relations between \(X\) and \(Y\) and a flat prior, and we then update again with new data on \(K\). Rather than updating twice, the fact that updating is invariant to order means that we can assume a flat prior and update once given data on \(X\), \(Y\), and \(K\).

\hypertarget{part-model-based-causal-inference}{%
\part{Model-Based Causal Inference}\label{part-model-based-causal-inference}}

\hypertarget{pt}{%
\chapter{Process Tracing with Causal Models}\label{pt}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We connect the literature on causal models to qualitative inference strategies used in process tracing. We provide a procedure for inference on case level queries from causal models. In addition we extract a set of implications for process tracing. We show how a key result from the causal models literature provides a condition for when clues may be (or certainly will not be) informative.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

FLAG: Change causal type to unit type throughout (though note I've introduced this already in the \textbf{``unit causal type''} paragraph.)

\hypertarget{process-tracing-and-causal-models}{%
\section{Process tracing and causal models}\label{process-tracing-and-causal-models}}

This chapter demonstrates how we can use causal models to conduct confirmatory process tracing: that is, to draw causal inferences about a single case from case-level data.

\hypertarget{the-intuition}{%
\subsection{The intuition}\label{the-intuition}}

We first walk through the basic intuition and then provide a more formal account.

When we undertake process tracing, we seek to answer a causal query about a given case.
The key insight driving our approach is that \textbf{the inference about a causal estimand for a case is a claim about what causal types are both likely ex ante (given prior knowledge) and consistent with the data}.\footnote{This differs from the task for mixed methods that we will address in Chapter 8 as these concern claims about the distribution of causal types in populations.}

The estimand of interest can be a statement about any number of case-level causal features, including a case-level causal effect, the pathway through which an effect operates, an actual cause, or causal attribution. We will use observations from the case itself to address this query. We do so via a procedure in which we first encode prior knowledge in the form of a causal model, use data to learn about features of the model, and then take what we have learned about the model and map it into our query.

Given a causal model, we form posteriors over estimands as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Specify all causal types}. A causal type, recall, specifies the values that a unit is expected to take, absent any interventions, but also the values it would take given some interventions on some variables. Examples of types might be:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Type 1: (\(X=1\)) \emph{and} (\(Y=1\) if \(X=1\), \(Y=0\) if \(X=0\)).
\item
  Type 2: (\(X=0\)) \emph{and} (\(Y=1\) if \(X=1\), \(Y=0\) if \(X=0\)).
\item
  Type 3: (\(X=1\)) \emph{and} (\(Y=1\) if \(X=1\), \(Y=1\) if \(X=0\)).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Specify priors over causal types.} Report how likely you think it is that a given unit is of a particular causal type. In the simplest case one might place 0 weight on some causal types (that might be ruled out by theory, for example) and equal weight on the others.
\item
  \textbf{Specify the estimand in terms of causal types.} For instance the estimand ``\(Y\) responds positively to \(X\)'' can be thought of as a collection of causal types: Q=\{Type 1, Type 2\}.\footnote{More generally an estimand might be a function of the distribution of causal types.}
\item
  \textbf{Specify the set of causal types that are consistent with the data.} For instance if we observe \(X=1, Y=1\) we might specify the data-consistent set as \{Type 1, Type 3.\}.
\item
  \textbf{Update.} Updating is done then by adding up the prior probabilities on all causal types that are consistent with both the data and the estimand, and dividing this by the sum of prior probabilities on all causal types that are consistent with the data (whether or not they are consistent with the estimand).
\end{enumerate}

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/ptvenn-1.pdf}
\caption{\label{fig:ptvenn}Logic of simple updating on arbitrary estimands.}
\end{figure}

This process is represented graphically with Figure \ref{fig:ptvenn}, where we can think of probabilities as proportionate to areas. Our causal model defines the causal type space. We then proceed by a process of elimination. Only some of the causal types in the model are consistent with prior knowledge. Only some are consistent with the data that we observe. Finally, any query itself maps onto a subset of the possible causal types. The causal types that remain in contention once we have observed the evidence are those at the intersection of consistency with priors and consistency with the data. \(A\) represents those types that are \emph{also} consistent with a given answer to the query (say, \(X\) has a positive effect on \(Y\)).

Thus, our belief about the query before we have seen the data is the probability of all causal types consistent with our priors and with the query (\(A + B\)) as a proportion of all types consistent with our priors. Once we have seen the data, we have reduced the permissible types to \(A + C\). Our posterior belief on the query is, then, the probabilities of those remaining types that are consistent with the query as a share of the probabilities of \emph{all} remaining types, or \(A/(A+C)\).

What we are doing here is straightforward: assessing causal possibilities for their compatibility with both the evidence at hand and our prior knowledge of how the world works. The formalization that we will present ensures that prior knowledge and evidence are all recorded explicitly while forcing logical consistency on the inferences that emerge from them.

\hypertarget{a-formalization-of-the-general-approach}{%
\subsection{A formalization of the general approach}\label{a-formalization-of-the-general-approach}}

More formally, the general approach to inference draws on the components we outlined in chapters 2 to 4: graphical causal models (DAGs), nodal and causal types, and priors. We now show how these elements formally interact with data to generate causal inferences. We continue to focus on a situation with binary variables, though suggest later in the chapter how this can be extended. Though we walk through the procedure for simple models, the approach outlined here can be applies to \emph{any} causal model with binary variables and to any estimands defined over the model.

The process tracing procedure operates as follows:

\textbf{A DAG}. We begin with a DAG, or graphical causal model. As we know, a DAG identifies a set of variables and describes the parent-child relations between them, indicating for each variable which other variables are its direct (possible) causes. These relationship, in turn, tell us which (non-descendant) variables a given variable is \emph{not} independent of given the other variables in the model.

\textbf{Nodal types}. Once we have specified a DAG, we have defined the full set of possible nodal types: the types defining the value that a variable will take on given the values of its parents, which we have denoted with \(\theta\) values. At each node, the range and number of nodal types is defined by the number of parents that that node has and the number of values the variables can take on. For instance, assuming all variables to be binary, if \(Y\) has parents \(X\) and \(W\) (so \(k=2\)), then there are \(2^{\left(2^2\right)}=16\)) possible causal types for the \(Y\) node. There are \(2^2\) possible combinations of values that two binary causal variables can take on----\((X=0,W=0), (X=0,W=1), (X=1,W=0), (X=1,W=1)\)---which implies four possible causal conditions over which \(Y\)'s possible responses must be defined. For instance, as we have seen, with two causal variables, we can have \(\theta^Y_{0000}\), where \(Y\) is always 0; \(\theta^Y_{0001}\), where \(Y\) is 0 unless both \(X\) and \(W\) are 1; and so on.\footnote{These nodal types can require many indices--\(2^k\) for a node with \(k\) parents---and the rule we follow is that the \(i\)th subscript indicates the value the node takes when parent \(j \in {1, 2, ..., k}\) take values \(\mod(floor((i-1)/(2^{j-1})), 2)\) For instance for \texttt{Y0111} the first index means that Y takes the value 0 where both parents are 0, in all other cases it takes value 1.} To get the total number of nodal types, we simply raise \(2\) (since \(Y\) is binary) to the number of causal conditions (4), giving the number of possible patterns of \(Y\) values that could be generated across these four conditions (16). (The full set of nodal types for two causal variables in a binary setup is given in \ref{tab:PO16}.)\footnote{More generally, let us say that any node \(j\) can take on \(r_j\) possible values and has parents belonging to set \(PA_j\) and that each parent, \(i \in PA_j\), can take on \(r_i\) values. Then the number of nodal types for node \(j\) is equal to \(r_j^{\prod_{i \in PA_j}r_i}\). Informally, the exponent in this expression simply multiplies by one another the number of values that each of \(j\)'s parents can take on. This product tells us the number of causal conditions across which \(j\)'s responses must be defined. We then raise the number of values that \(j\) can take on to the power of the number of causal conditions. With all variables binary, this expression translates to \(2^{\left(2^k\right)}\) nodal types for a node with \(k\) parents.}

All variables in a model have nodal types defining the value they take on given the value of their parents, including those variables without substantive parents. Suppose that \(X\) and \(W\), in this model, have no substantively defined parents. We nonetheless define a nodal type for each of them, which simply captures their exogenous assignment to some value. With \(X\) binary, for instance, there are two nodal types, \(\theta^X_{0}\), where \(X\) is set to \(0\), and \(\theta^X_{1}\), where \(X\) is set to \(1\).

\textbf{Unit causal types}. We will want to be able to conceive not just of types for individual nodes but of the full collection of nodal types across all nodes in a model. We refer to a unit's full set of nodal types as its \emph{unit causal type} --- or, more simply, unit type --- which we represent as \(\theta\). A unit type is simply a listing that contains one nodal type for each node in the model. For instance, with a model with variable \(X\), \(W\), and \(Y\), each unit has a \emph{causal} type composed of its \emph{nodal} types on each of the three nodes.\footnote{A model in which each node \(j\) has \(k_j\) parents has \(\prod_j2^{\left(2^{k_j}\right)}\) causal types that uniquely determine what data will be observed for a type under all possible interventions on its exogenous nodes.} Thus, one causal type in this model could be \(\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_1, \theta^Y = \theta^Y_{1101})\). Another could be \(\theta = (\theta^X = \theta^X_0, \theta^W = \theta^W_1, \theta^Y = \theta^Y_{0001})\). And so on.

We show the mapping between nodal and causal types, for a simply \(X \rightarrow Y\) model, in Table \ref{tab:nodalcausalmatrix}. The column headings represent the \(8\) permissible causal types, each expressed simply as a concatenated strings of nodal types. The row headings represent the nodal types. In each interior cell, a \(1\) or \(0\) indicates whether or not a given nodal type is a component of a given causal type. As can be seen, each causal type has two nodal types that are its components since there are two nodes in this model. Each \(X\)-nodal type is part of four causal types since it can be combined with four different \(Y\)-nodal types, while each \(Y\)-nodal type is part of two causal types since it can be combined with two \(X\)-nodal types.

\begin{longtable}[]{@{}rcccccccc@{}}
\caption{\label{tab:nodalcausalmatrix}. A mapping between nodal types and causal types for a simple \(X \rightarrow Y\) model.}\tabularnewline
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedleft
\textbf{Causal Types \(\rightarrow\)}\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{11}\)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedleft
\textbf{Causal Types \(\rightarrow\)}\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0\).\(\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1\).\(\theta^Y_{11}\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\textbf{Nodal types \(\downarrow\)}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\theta^X_0\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\theta^X_1\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\textbf{Priors}: Our background beliefs about a causal domain will usually consist of more than just beliefs about which variables have causal connections; they will also typically contain beliefs about what \emph{kinds} of effects operate between variables. That is, they will contain beliefs about which types are possible or, more generally, are more or less common in the world. We express these beliefs over causal effects as either restrictions on nodal types or as probability distributions over the nodal types.

In general, when doing process tracing in this framework, we think of a given case of interest -- the one we are studying and seek to learn about -- as being drawn at random from a population. Thus, our prior beliefs about a \emph{single} case -- before we do the process tracing -- are really beliefs about that population. So, for instance, our prior belief about the probability that inequality has a positive effect on democratization in Mexico in 1999 is our belief about how commonly inequality has a positive effect on democratization in the population of cases that are ``like'' Mexico in 1999.\footnote{The reference population for a case is defined based on whatever we already know about the case. Thus, for instance, if we already know that the case has \(Y=1\) before we begin process tracing, then the relevant population for the formation of prior beliefs is all cases in which \(Y=1\).}

We let \(\lambda^j\) denote our belief about the population distribution of nodal types at node \(j\). A \(\lambda^j\) is simply a vector of proportions, one for each possible nodal type, with the proportions adding up to \(1\). So, for instance, \(\lambda^Y\) for our current example would be a vector with four values, each of which expresses a proportion for one of the four nodal types at \(Y\). So we might have \(\lambda^Y_{01}=0.1\), \(\lambda^Y_{11}=0.05\), and so on -- with the \(\lambda^Y\) values summing to \(1\) because these values are defined over the full set of possible nodal types for \(Y\).

We can, in turn, use these population parameters -- these beliefs about nodal-type proportions in the population -- to create prior probabilities over the \emph{causal} type for the case at hand. Since causal types are merely combinations of nodal types, and our case has been drawn at random from the population, we can take a set of posited proprtions of nodal types in the population and readily calculate the probability that our case is of any given causal type. To do so, we need to join together \(\lambda\)'s across the nodes in a model.

Let us first see how this works in a situation in which we assume that the nodal types are independent of one another. We can think of this as a situation in which there is no confounding that is not captured in the graph -- no variable missing from the model that is a common ancestor of multiple nodes in the model. Here, our beliefs over causal types are simply the product of our beliefs over the component nodal types (since the joint probability of independent events is simply the product of their individual probabilities). For instance, one causal type might be ``a unit in which \(X=1\) and in which \(Y=1\) no matter what value \(X\) takes.'' In this case the probability that a case is of this causal type might be written \(\Pr(\theta^X = \theta^X_1)\Pr(\theta^Y = \theta^Y_{11}) = \lambda^X_1\lambda^Y_{11}\).

The simplest way in which we can express beliefs about the differential probabilities of different causal possibilities is by \emph{eliminating} nodal types that we do not believe to be possible---setting their parameter values to \(0\). Suppose, for instance, that we are examining the effect of ethnic diversity on civil war in a case. We might not know whether ethnic diversity causes civil war in this case, but we might have sufficient background knowledge to believe that ethnic diversity never has a \emph{negative} effect on civil war: it never prevents a civil war from happening that would have happened in the absence of ethnic diversity. We would thus want to set the parameter value for a negative causal effect to \(0\). If we then know nothing about the relative frequencies of the three remaining nodal types for \(Y\), we may (following the principle of indifference), frequency of positive effects, null effects with civil war destined to happen, and null effects with civil war never going to happen, assigning a weight of \(\frac{1}{3}\) to each of them.

In a situation of unobserved confounding, our beliefs over causal types are still well defined, though they are no longer the simple product of beliefs over nodal types. Let us imagine for instance, in a simple \(X \rightarrow Y\) model, that we believe that some unobserved factor both makes cases more likely to have \(X = 1\) and makes it more likely that \(X\) has a positive effect on \(Y\). This is the same as saying that the probability that \(\theta^X = \theta^X_1\) is positively correlated with the probability that \(\theta^Y = \theta^Y_{01}\). Now, our probability that \emph{both} \(X=1\) and \(X\) has a positive effect must be calculated using the joint probability formula, \(\Pr(A, B) = \Pr(A)\Pr(B|A)\).\footnote{In words, the probability of \(A\) and \(B\) occurring is equal to the probability of \(A\) occurring times the probability of \(B\) occurring \emph{given} that \(A\) occurs.} Thus, \(\Pr(\theta^Y = \theta^Y_{01}, \theta^X = \theta^X_1) = \Pr(\theta^Y = \theta^Y_{01})\Pr(\theta^X = \theta^X_1 | \theta^Y = \theta^Y_{01})\). To form priors over causal types in this situation, we need to posit beliefs about a set of more complex, conditional proportions for \(X\)'s type. Specifically, we need to posit, \emph{for those cases} with a positive effect of \(X\) on \(Y\), what proportion are ``assigned'' to \(X=1\); and, separately, what proportion are assigned to \(X=1\) among those cases \emph{without} a positive effect of \(X\) on \(Y\).

These conditional proportions may, of course, be difficult for the researcher to form beliefs about. Forming a belief about them amounts to saying that we do not know what generates confounding, but we know the correlations it generates in the data. We may wonder how often we will be in that epistemological position. An alternative way to parse the problem, then, is to \emph{model} the confounding by including the confounder (say, \(Z\)) as a new node in the graph. In the above example, \(Z\) would point into both \(X\) and \(Y\). We would then posit population proportions for a set of nodal types for \(X\) -- representing \(X\)'s possible responses to \(Z\) -- and for \(Y\) -- representing \(Y\)'s possible responses to both \(X\) and \(Z\). We may find it easier to reason and form beliefs about these more complex nodal types than about the conditional proportions involved in unobserved confounding. The two approaches work out to be analytically equivalent given equivalent underlying beliefs, so the choice between them will be a matter of researcher preference.\footnote{As we will see later in the book, another approach is to gather data on additional cases. When analyzing multiple cases, we can set up our priors to allow for the possibility of unobserved confounding and then, potentially, learn about that confounding from the data. This is not possible under our procedure for single-case process tracing, where we treat the population parameters as given and fixed.}

Importantly, in process tracing, we are focused on drawing case-level inferences and, as such, we treat the population-level parameters as given and fixed. In general, these parameters derive from our beliefs about how the world works, and those beliefs will typically be uncertain. The key point, however, is that in process tracing, the population parameters serve as an \emph{input} into the analysis, conditioning our inferences from the evidence; but we do not \emph{update} on these population-level beliefs once we see the data from a single case. Importantly, as we show later in the book, we \emph{do} update on population-level inferences in the more general setup that we introduce in Chapter \ref{mixing} for analyzing mixed data in multiple cases. We also show in Chapter \ref{evaluation} how we can test the sensitivity of conclusions to the values at which we set population parameters. Interestingly, as we also show, process-tracing inferences, including uncertainty about conclusions, are unaffected by the level of uncertainty we might have about population parameters; we thus do not specify this uncertainty for the purposes of process tracing.

The relationship between causal types, nodal types, and the correlation among nodal types is captured in what we call a \emph{parameter matrix.} We show a parameter matrix for a simple \(X \rightarrow Y\) model with no unobserved confounding in Table \ref{tab:parammatrix}. Here each column label (except the last) represents the probability that a case is of a given causal type. Each row label represents a population-level parameter: a belief about the proportions of different nodal types in the population. We indicate a set of possible parameter values in the final column.

\begin{longtable}[]{@{}cccccccccc@{}}
\caption{\label{tab:parammmatrix}. A mapping between nodal types and causal types for a simple \(X \rightarrow Y\) model (with no unobserved confounding).}\tabularnewline
\toprule
\begin{minipage}[b]{0.10\columnwidth}\centering
\textbf{Causal types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Parameter values (population proportions)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.10\columnwidth}\centering
\textbf{Causal types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_0,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
\(\theta^X_1,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Parameter values (population proportions)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.10\columnwidth}\centering
\textbf{Population parameters} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\centering
\(\lambda^X_1\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.5\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\centering
\(\lambda^X_0\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.5\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\centering
\(\lambda^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.2\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\centering
\(\lambda^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.2\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\centering
\(\lambda^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.4\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\centering
\(\lambda^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.2\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

To start with the first two rows, these represent the population proportions of each of \(X\)'s nodal types. For instance, \(\lambda^X_{0}\) is our belief about the proportion of cases in the population that are of nodal type \(\theta^X_{0}\). The first row, \(\lambda^X_{1}\), represents our belief about the inverse: the proportion of cases in the population of type \(\theta^X_{1}\). We posit beliefs about these parameters in the final column, indicating that we think that half of cases in the population are ``assigned'' to \(X=0\) and half to \(X=1\). Note that, since there are only two possible nodal types for \(X\), and their proportions must sum to 1, there is actually just one degree of freedom here: once we've specified one of these parameter values, the other is defined as well.

The last four rows represent the proportion of cases in the population with different \(Y\)-nodal types: in order, the proportion in which \(X\) has no effect on \(Y\), with \(Y\) fixed at \(0\); the proportion in which \(X\) has a negative effect; the proportion in which \(X\) has a positive effect; and the proportion in which \(X\) has no effect, with \(Y\) fixed at \(1\). Again, in the last column, we provide possible values for these proportions, the four of which must also sum to \(1\). Here we are stating that positive \(X \rightarrow Y\) effects are twice as common in the population as the other three nodal types, which we set at equal prevalence.

The interior cells indicate whether a given population parameter enters into the prior probability of a given causal type. Thus, for instance, to calculate the prior probability of the causal type \(\theta^X_1, \theta^Y_{10}\), we need to multiply the two parameters values corresponding to the \(1\)'s in this causal type's column: \(\lambda^X_1\) by \(\lambda^Y_{10}\). Given the parameter values we have assigned for this example, then, the prior on this causal type is simply \(0.5 \times 0.2 = 0.1\).

The prior probability that a case is of a given causal type thus comes directly from our beliefs about how nodal types are distributed in the population. All we know before we study a case is whatever we know about cases ``like'' it in general. It is then these causal-type probabilities -- which represent probabilities that a \emph{given case} is of a particular causal type -- that we will update on once we see the data for this case.

We show the somewhat more complex situation of unobserved confounding in Table \ref{tab:parammatrixconf}. It is the first four rows that allow for unobserved confounding---the correlations across types. In a potential outcomes framework, we could think of these rows as capturing differential ``assignment propensities'' for \(X\). Here, we allow for different probabilities of \(X\)'s type being \(\theta^X_1\) depending on what \(Y\)'s type is. Thus, \(\lambda^X_0 | \theta^Y= \theta^Y_{01}\) is the proportion of \(\theta^X_0\) types among cases with \(\theta^Y_{01}\) type: put differently, it is the probability of \(X\) being assigned to \(0\) when \(X\) has a positive effect on \(Y\). The second row represents the inverse proportion: the proportion of a \(\theta^X_1\) types among \(\theta^Y_{01}\) types. The next two rows then capture the proportions of the \(X\)-types among all \emph{other} \(Y\)-types (i.e., among those cases for which \(X\) does \emph{not} have a positive effect on \(Y\)).

Unobserved confounding in this setup takes the form of a difference in the proportions of a given \(X\) type among different \(Y\) types. Thus, if \(\lambda^X_1, | \theta^Y_{01}\) is not the same as \(\lambda^X_1 | \theta^Y \neq \theta^Y_{01}\), we have unobserved confounding. Imagine, for instance, if we are studying the effect of faster economic growth (\(X\)) on democratization (\(Y\)), and we believe that there is some unobserved factor that both makes some countries' economies grow more quickly and also makes economic growth more likely to have a positive effect on democratization. This belief amounts to a belief that the probability of a case being assigned to \(X=1\) is higher if \(Y\)'s nodal type is \(\theta^Y_{01}\) than if it is not. In other words, in terms of the rows in Table \ref{tab:parammatrix}, we believe here that \(\lambda^X_1 | \theta^Y=\theta^Y_{01}\) is greater than \(\lambda^X_1 | \theta^Y \neq \theta^Y_{01}\). To illustrate, we provide parameter values along these lines in the final column.

Again, however, a researcher might prefer to specify the confounder (say, \(Z\)) as a node in the model. The rows in the parameter matrix would then be a set of population parameters defined as proportions of \emph{un}conditional nodal types, with four \(X\)-types representing possible responses to \(Z\), and 16 \(Y\) types, representing \(Y\)'s possible responses to \(X\) and \(Z\).

\begin{longtable}[]{@{}rcccccccc@{}}
\caption{\label{tab:parammmatrixconf}. A mapping between nodal types and causal types for a simple \(X \rightarrow Y\) model \emph{with} unobserved confounding.}\tabularnewline
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedleft
\textbf{Causal Types \(\rightarrow\)}\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{11}\)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.12\columnwidth}\raggedleft
\textbf{Causal Types \(\rightarrow\)}\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_0,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering
\(\theta^X_1,\theta^Y_{11}\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\textbf{Population parameters \(\downarrow\)}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^X_0 | \theta^Y= \theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^X_1 | \theta^Y=\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^X_0| \theta^Y \neq \theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^X_1 | \theta^Y \neq \theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.12\columnwidth}\raggedleft
\(\lambda^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering
1\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

One special kind of prior that we might wish to set is to disallow a particular (conditional) type altogether. For instance, if studying the effect of we may believe that

\textbf{Possible data types.} A \emph{data type} is a particular pattern of data that we could potentially observe for a given case. More specifically, a data type is a set of values, one for each node in a model. For instance, in our \(X, W, Y\) setup, \(X=1, W=0, Y=0\) would be one data type.

Importantly, each possible causal type \emph{maps into a single data type.} One intuitive way to think about why this is the case is that a causal type tells us (a) the values to which all exogenous variables in a model are assigned and (b) how all endogenous variables respond to their parents. Given these two components, only one set of node values is possible. For example, causal type \(\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_0, \theta^Y = \theta^Y_{0100})\) imples data \(X=1, W=0, Y=1\). There is no other set of data that can be generated by this causal type.

Equally importantly, however, \emph{the mapping from causal types to data types is not one-to-one.} More than one causal type can generate the same case-level data pattern. For instance, the causal type \(\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_0, \theta^Y = \theta^Y_{1101})\) will \emph{also} generate the data type, \(X=1, W=0, Y=1\). Thus, observing this data type leaves us with ambiguity about the causal type by which it was generated.

A full mapping between causal types and data types can be summarized by an ``ambiguity matrix.'' In Table \ref{tab:ambigmatrix}, we provide an example of such a matrix, derived directly from the parameter matrix in Table \ref{tab:parammatrix}. Here, the rows represent causal types and the columns (except for the last) represent data types. The notation for data types is straightforward, with for instance \(X0Y0\) meaning that \(X=0, Y=0\) has been observed. In the interior cells, the \(1\)'s and \(0\)'s indicate whether or not a given data type could arise from a given causal type. We can readily see here that each causal type can generate only one data type.

We can also see the ambiguity of the data, however, since each data type can be generated by two causal types. For instance, if we observe \(X=1, Y=1\), we know that the case is either of causal type \(\theta^X_1,\theta^Y_{01}\) or of causal type \(\theta^X_1,\theta^Y_{11}\) -- but do not know which.

\begin{longtable}[]{@{}cccccc@{}}
\caption{\label{tab:ambigmatrix}. An ambiguity matrix, mapping from data types to causal types for a simple \(X \rightarrow Y\) model.}\tabularnewline
\toprule
\textbf{Data types} \(\rightarrow\) & X0Y0 & X1Y0 & X0Y1 & X1Y1 & Priors on causal types\tabularnewline
\midrule
\endfirsthead
\toprule
\textbf{Data types} \(\rightarrow\) & X0Y0 & X1Y0 & X0Y1 & X1Y1 & Priors on causal types\tabularnewline
\midrule
\endhead
\textbf{Causal types} \(\downarrow\) & & & & &\tabularnewline
\(\theta^X_0,\theta^Y_{00}\) & 1 & 0 & 0 & 0 & 0.1\tabularnewline
\(\theta^X_1,\theta^Y_{00}\) & 0 & 1 & 0 & 0 & 0.1\tabularnewline
\(\theta^X_0,\theta^Y_{10}\) & 0 & 0 & 1 & 0 & 0.1\tabularnewline
\(\theta^X_1,\theta^Y_{10}\) & 0 & 1 & 0 & 0 & 0.1\tabularnewline
\(\theta^X_0,\theta^Y_{01}\) & 1 & 0 & 0 & 0 & 0.2\tabularnewline
\(\theta^X_1,\theta^Y_{01}\) & 0 & 0 & 0 & 1 & 0.2\tabularnewline
\(\theta^X_0,\theta^Y_{11}\) & 0 & 0 & 1 & 0 & 0.1\tabularnewline
\(\theta^X_1,\theta^Y_{11}\) & 0 & 0 & 0 & 1 & 0.1\tabularnewline
\bottomrule
\end{longtable}

In the last column, we provide prior probabilities for each of the causal types. These have been calculated directly from the parameter matrix (Table \ref{tab:parammatrix}). To see how the calculation works, start with a causal type in the parameter matrix -- say, \(\theta^X_0,\theta^Y_{01}\). We go down that causal type's column and select the rows with \(1\)'s, representing the parameters for the included nodal types, \(\lambda^X_0\) and \(\lambda^Y_{01}\). As we want the joint probability of these two nodal types (and a parameter matrix is constructed such that the rows represent independent events),\footnote{That is, when there is unobserved confounding, we express conditional proportions, making all of the proportions conditionally independent of one another.} we simply multiply together the values for these included parameters: \(0.5 \times 0.4 = 0.2\). As noted, our prior belief about whether the case at hand is of a given causal type is a straightforward function of our beliefs about how prevalent each of the component nodal types is in the population.

As models get more complex, the numbers of causal and data types simply multiply. In Table \ref{tab:ambigmatrixmed}, we show the ambiguity matrix for a simple mediation model (\(X \rightarrow M \rightarrow Y\)). Here, the causal types are combinations of three nodal types, one for each variable in the model. Similarly, the data types have three elements, one for each variable. We now have 8 data types and 32 causal types.

\begin{longtable}[]{@{}cccccccccc@{}}
\caption{\label{tab:ambigmatrixmed}. An ambiguity matrix, mapping from data types to causal types for a simpe mediation model, \(X \rightarrow M \rightarrow Y\).}\tabularnewline
\toprule
\begin{minipage}[b]{0.23\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M0Y1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M0Y1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\centering
Priors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.23\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M0Y1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M0Y1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X0M1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\centering
X1M1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\centering
Priors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\centering
\textbf{Causal types} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{00},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{10},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{01},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{11},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{00},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{10},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{01},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{11},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{00},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{10},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{01},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.08\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.08\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{11},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{00},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{10},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{01},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.04\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_0,\theta^M_{11},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
0.02\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Again, the ambiguities arising from data patterns are apparent. For instance, if we observe \(X=1, M=0, Y=0\), we see that there are four causal types that could have generated this pattern. To unpack the situation a bit, these data tell us that \(\theta^X = \theta^X_1\). But they do not tell us whether \(M\)'s type is such that \(X\) has a negative effect on \(M\) (\(\theta^M_{10}\)) or \(X\) has no effect with \(M\) fixed at \(0\) (\(\theta^M_{00}\)). Similarly, we do not know whether \(M\) has a positive effect on \(Y\) (\(\theta^Y_{01}\)) or no effect with \(Y\) fixed at \(0\) (\(\theta^Y_{00}\)). This leaves four combinations of nodal types---four causal types---that are consistent with the data.

Our priors here derive from a set of parameter values, much like in the previous example, in which the \(X\) types are equally common (0.5 each); a positive effect of \(X\) on \(M\) is twice as common (0.4) as the other \(M\) types (all set to 0.2); and a positive effect of \(M\) on \(Y\) is twice as common (0.4) as all other \(Y\) types (all at 0.2). We can then easily see why we thus get priors on some causal types are higher than those on others: for instance, the two causal types with priors of 0.08 both have two positive effects (at the \(X \rightarrow Y\) and \(M \rightarrow Y\) stages) while the causal types with priors of 0.02 include no positive effects at either stage.

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-26}Ambiguity matrix for X -> M -> Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows.}
\centering
\begin{tabular}{l|r|r|r|r|r|r|r|r|r}
\hline
  & X0M0Y0 & X1M0Y0 & X0M1Y0 & X1M1Y0 & X0M0Y1 & X1M0Y1 & X0M1Y1 & X1M1Y1 & prior\\
\hline
X0M00Y00 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X1M00Y00 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X0M10Y00 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X1M10Y00 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X0M01Y00 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.04\\
\hline
X1M01Y00 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0.04\\
\hline
X0M11Y00 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X1M11Y00 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X0M00Y10 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0.02\\
\hline
X1M00Y10 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.02\\
\hline
X0M10Y10 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X1M10Y10 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.02\\
\hline
X0M01Y10 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0.04\\
\hline
X1M01Y10 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0.04\\
\hline
X0M11Y10 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X1M11Y10 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0.02\\
\hline
X0M00Y01 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.04\\
\hline
X1M00Y01 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0.04\\
\hline
X0M10Y01 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0.04\\
\hline
X1M10Y01 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0.04\\
\hline
X0M01Y01 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.08\\
\hline
X1M01Y01 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0.08\\
\hline
X0M11Y01 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0.04\\
\hline
X1M11Y01 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0.04\\
\hline
X0M00Y11 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0.02\\
\hline
X1M00Y11 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.02\\
\hline
X0M10Y11 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0.02\\
\hline
X1M10Y11 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.02\\
\hline
X0M01Y11 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0.04\\
\hline
X1M01Y11 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0.04\\
\hline
X0M11Y11 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0.02\\
\hline
X1M11Y11 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0.02\\
\hline
\end{tabular}
\end{table}

\textbf{Updating on types given the data.} Once we observe actual data in a case, we can then update on the probabilities assigned to each causal type. The logic is simple. When we observe a set of data from a case, we place \(0\) probability on all causal types that could not have produced these data; we then scale up the probabilities on all causal types that could have.

We can see how this works within an ambiguity matrix. Let's return to the ambiguity matrix in Table \ref{tab:ambigmatrix}. We start out with a set of probability weights on all rows (causal types). Now, suppose that we observe the data \(X=1, Y=1\), i.e., data type \(X1Y1\). We then look down the \(X1Y1\) column, and we know that all rows with a \(0\) in them represent causal types that \emph{could not have} generated these data. These causal types are thus excluded. What is left are two rows: \(\theta^X_1, \theta^Y_{01}\) and \(\theta^X_1, \theta^Y_{11}\). Returning now to the probabilities, we put 0 weight on all of the excluded rows; and then we scale up the remaining probabilities so that they sum to 1 (preserving the ratio between them). The priors of 0.2 and 0.1 in the retained rows scale up to \(\frac{2}{3}\) and \(\frac{1}{3}\), which become our \emph{posterior} probabilities on the causal types. We display an updated ambiguity matrix, with excluded data types and causal types removed, in Table \ref{tab:ambigupdate}.

Before we see any data on the case at hand, then, we believe (based on our beliefs about the population to which the case belongs) that there is a 0.2 probability that the case is one in which \(X\) is assigned to \(1\) and has a positive effect on \(Y\); and 0.1 probability that it's a case in which \(X\) gets assigned to \(1\) and has no effect on \(Y\) with \(Y\) fixed at \(1\). Seeing the \(X=1, Y=1\) data, we now believe that there is a 0.667 probability that the case is of the former type, and a 0.333 probability that it is of the latter type.

\begin{longtable}[]{@{}cccl@{}}
\caption{\label{tab:ambigupdate}. An updated version of the ambiguity matrix in Table \ref{tab:ambigmatrix}, after observing \(X=1, Y=1\) in a case.}\tabularnewline
\toprule
\begin{minipage}[b]{0.31\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.28\columnwidth}\raggedright
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.31\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.28\columnwidth}\raggedright
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.31\columnwidth}\centering
\textbf{Causal types} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\centering
\(\theta^X_1,\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
0.2\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\raggedright
0.6667\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\centering
\(\theta^X_1,\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
0.1\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\raggedright
0.3333\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We can also see how this works for our \(X \rightarrow M \rightarrow Y\) model, and the ambiguity matrix in Table \ref{tab:ambigmatrixmed}. If we observe the data \(X=1, M=0, Y=0\), for instance, this exercise would yield the updated ambiguity matrix in Table \ref@(tab:ambigmedupdate). Here we have eliminated all rows (causal types) with a \(0\) in the relevant data-type column (\(X1M0Y0\)) and formed the posteriors by scaling up the priors in the retained rows.

\begin{longtable}[]{@{}cccc@{}}
\caption{\label{tab:ambigmedupdate}. An updated version of the ambiguity matrix in Table \ref{tab:ambigmatrixmed}, after observing \(X=1, M=0, Y=0\) in a case.}\tabularnewline
\toprule
\begin{minipage}[b]{0.36\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.36\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.36\columnwidth}\centering
\textbf{Causal types} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.1667\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.1667\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.3333\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.3333\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

A notable feature of the logic of single-case process tracing is that the relative probabilities on the retained causal types never change. If we start out believing that causal type \(A\) is twice as likely as causal type \(B\), and both \(A\) and \(B\) are retained once we see the data, then \(A\) will be twice as likely as \(B\) in our posteriors. All updating occurs by \emph{eliminating} causal types from consideration and zeroing in on those that remain.

\begin{tabular}{l|r|r|r}
\hline
type & X1M0Y0 & prior & posterior\\
\hline
X1M00Y00 & 1 & 0.02 & 0.1667\\
\hline
X1M10Y00 & 1 & 0.02 & 0.1667\\
\hline
X1M00Y01 & 1 & 0.04 & 0.3333\\
\hline
X1M10Y01 & 1 & 0.04 & 0.3333\\
\hline
\end{tabular}

A similar logic applies if partial data are observed: that is, if we do not collect data for all nodes in the model. The one difference is that, now, rather than reducing to one column we entertain the possibility of any data \emph{type} consistent with the \emph{observed data}. In general, more than one data type will be consistent with partial data. For instance, suppose that we observe \(X=1, Y=0\) but do not observe \(M\)'s value. These are data that are consistent with both the data type \(X1M0Y0\) and the data type \(X1M1Y0\) (since the unobserved \(M\) could be either \(0\) or \(1\)). We thus retain both of these data-type columns as well as all causal types consistent with \emph{either} of these data types. This gives the updated ambiguity matrix in Table \ref{tab:ambigmedupdatepartial}. We note that, with these partial data, we are not able to update as strongly. For instance, for the causal type \(\theta^X_1,\theta^M_{00},\theta^Y_{00}\), instead of updating to a posterior probability of 0.1667, we update to a posterior of only 0.0833 -- because there is a larger set of causal types with which these partial data are consistent.

\begin{longtable}[]{@{}ccccc@{}}
\caption{\label{tab:ambigmedupdatepartial}. An updated version of the ambiguity matrix in Table \ref{tab:ambigmatrixmed}, after observing partial data in case: \(X=1, Y=0\), with \(M\) unobserved.}\tabularnewline
\toprule
\begin{minipage}[b]{0.32\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.32\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\centering
\textbf{Causal types} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0833\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0833\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.1667\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{00}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0833\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.1667\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0833\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.1667\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.1667\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{tabular}{l|r|r|r|r}
\hline
type & X1M0Y0 & X1M1Y0 & prior & posterior\\
\hline
X1M00Y00 & 1 & 0 & 0.02 & 0.0833\\
\hline
X1M10Y00 & 1 & 0 & 0.02 & 0.0833\\
\hline
X1M01Y00 & 0 & 1 & 0.04 & 0.1667\\
\hline
X1M11Y00 & 0 & 1 & 0.02 & 0.0833\\
\hline
X1M01Y10 & 0 & 1 & 0.04 & 0.1667\\
\hline
X1M11Y10 & 0 & 1 & 0.02 & 0.0833\\
\hline
X1M00Y01 & 1 & 0 & 0.04 & 0.1667\\
\hline
X1M10Y01 & 1 & 0 & 0.04 & 0.1667\\
\hline
\end{tabular}

\textbf{Updating on estimands.} We now have a posterior probability for each causal type for the case at hand. The causal question we are interested in answering, our estimand, may not be about causal types \emph{per se.} It is about an estimand that can be expressed as a \emph{combination} of causal types.

For instance, suppose we are working with the model \(X \rightarrow M \rightarrow Y\); and that our question is, ``Did \(X=1\) cause \(Y=1\)?''. This question is asking both:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Does \(X=1\) in this case?
\item
  Does \(X\) have a positive effect on \(Y\) in this case?
\end{enumerate}

The causal types that qualify are those, and only those, in which the answer to both is ``yes.''

Meeting condition (1) requires that \(\theta^X=\theta^X_1\).

Meeting condition (2) requires that \(\theta^M\) and \(\theta^Y\) are such that \(X\) has an effect on \(M\) that yields a positive effect of \(X\) on \(Y\). This could occur via a positive \(X \rightarrow M\) effect linked to a positive \(M \rightarrow Y\) effect or via a negative \(X \rightarrow M\) effect linked to a negative \(M \rightarrow Y\) effect.

Thus, the qualifying causal types in this model are:

\begin{itemize}
\tightlist
\item
  \(\theta^X_1, \theta^M_{01}, \theta^Y_{01}\)
\item
  \(\theta^X_1, \theta^M_{10}, \theta^Y_{10}\)
\end{itemize}

Our \emph{prior} on the estimand---what we believe before we collect data on the case at hand---is given simply by summing up the prior probabilities on each of the causal types that correspond to the estimand. Note that we must calculate the prior from the full ambiguity matrix, before excluding types for inconsistency with the data. Returning to the full ambiguity matrix in Table \ref{tab:ambigmatrixmed}, we see that the priors on these two types (given the population parameters assumed there) are 0.08 and 0.02, respectively, giving a prior for the estimand of 0.1.

The posterior on any estimand is, likewise, given by summing up the posterior probabilities on each of the causal types that correspond to the estimand, drawing of course from the updated ambiguity matrix. For instance, if we observe the data \(X=1, M=1, Y=1\), we update to the ambiguity matrix in Table \ref{tab:ambigmedupdate2}. Our posterior on the estimand, ``Did \(X=1\) cause \(Y=1\)?'' is the sum of the posteriors on the above two causal types. Since \(\theta^X_1, \theta^M_{10}, \theta^Y_{10}\) is excluded by the data, this just leaves the posterior on \(\theta^X_1, \theta^M_{01}, \theta^Y_{01}\), 0.4444, which is the posterior belief on our estimand.

If we observe only the partial data, \(X=1, Y=1\), then we update to the ambiguity matrix in Table \ref{tab:ambigmedupdatepartial2}. Now both corresponding causal types are included, and we sum their posteriors to get the posterior on the estimand: \(0.0769 + 0.3077 = 0.3846\).

FLAG: Briefly discuss other estimand(s) one could do, though don't show in detail here. Will do pathways in Chap. 7.

\begin{tabular}{l|r|r|r}
\hline
type & X1M1Y1 & prior & posterior\\
\hline
X1M01Y01 & 1 & 0.08 & 0.4444\\
\hline
X1M11Y01 & 1 & 0.04 & 0.2222\\
\hline
X1M01Y11 & 1 & 0.04 & 0.2222\\
\hline
X1M11Y11 & 1 & 0.02 & 0.1111\\
\hline
\end{tabular}

\begin{longtable}[]{@{}cccc@{}}
\caption{\label{tab:ambigmedupdate2}. An updated version of the ambiguity matrix in Table \ref{tab:ambigmatrixmed}, after observing \(X=1, M=1, Y=1\) in a case.}\tabularnewline
\toprule
\begin{minipage}[b]{0.36\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
X1M1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.36\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
X1M1Y1\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.36\columnwidth}\centering
\textbf{Causal types} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.08\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.4444\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.2222\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.2222\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.36\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\centering
0.1111\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}ccccc@{}}
\caption{\label{tab:ambigmedupdatepartial2}. An updated version of the ambiguity matrix in Table \ref{tab:ambigmatrixmed}, after observing partial data in case: \(X=1, Y=0\), with \(M\) unobserved.}\tabularnewline
\toprule
\begin{minipage}[b]{0.32\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.32\columnwidth}\centering
\textbf{Data types} \(\rightarrow\)\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M0Y0\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\centering
X1M1Y0\strut
\end{minipage} & \begin{minipage}[b]{0.19\columnwidth}\centering
Priors on causal types\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
Posteriors on causal types\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\centering
\textbf{Causal types} \(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0769\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{10}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0769\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.08\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.3077\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{01}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.1538\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{00},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0769\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{10},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0769\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{01},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.04\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.1538\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\centering
\(\theta^X_1,\theta^M_{11},\theta^Y_{11}\)\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\centering
0\strut
\end{minipage} & \begin{minipage}[t]{0.19\columnwidth}\centering
0.02\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
0.0769\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

For more complex models and estimands, it can be more difficult to eyeball the corresponding causal types. In practice, therefore, we use a function in the \texttt{gbiqq} package to do this for us.

\begin{verbatim}
X0.M10.Y10, X1.M10.Y10, X0.M01.Y10, X1.M01.Y10, X0.M10.Y01, X1.M10.Y01, X0.M01.Y01, X1.M01.Y01
\end{verbatim}

This completes the abstract representation of the process tracing procedure. We now build up the intuition by walking through the procedure for simple mediation and moderation models.

\hypertarget{illustration-with-code}{%
\subsection{Illustration with code}\label{illustration-with-code}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XMY <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> M -> Y"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{       }\KeywordTok{set_parameters}\NormalTok{ (}\KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.4}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.4}\NormalTok{, }\FloatTok{.2}\NormalTok{))}

\KeywordTok{query_model}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ XMY, }
              \DataTypeTok{queries =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{PC =} \StringTok{"Y[X=1] > Y[X=0]"}\NormalTok{), }
              \DataTypeTok{given =} \KeywordTok{list}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{, }\StringTok{"X==1 & Y==1 & M==0"}\NormalTok{, }\StringTok{"X==1 & Y==1 & M==1"}\NormalTok{),}
              \DataTypeTok{using =} \StringTok{"parameters"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r}
\hline
Query & Given & Using & mean\\
\hline
PC & - & parameters & 0.200\\
\hline
PC & X==1 \& Y==1 & parameters & 0.385\\
\hline
PC & X==1 \& Y==1 \& M==0 & parameters & 0.250\\
\hline
PC & X==1 \& Y==1 \& M==1 & parameters & 0.444\\
\hline
\end{tabular}

\hypertarget{five-principles}{%
\section{Five principles}\label{five-principles}}

\hypertarget{classic-qualitative-tests-are-special-cases-of-updating-on-a-model}{%
\subsection{Classic qualitative tests are special cases of updating on a model}\label{classic-qualitative-tests-are-special-cases-of-updating-on-a-model}}

The approach we have described here updates on the model given data on all variables, and from the model makes inferences to estimands.

This procedure appears different to the approach described, for example, in \citet{collier2004sources} and in Chapter 5, in which one seeks specific evidence that is directly informative about causal propositions: ``clues'' that are arise with different probabilities if one proposition or another is true. In fact however the approaches are deeply connected. This ``probative value of clues'' approach can indeed be \emph{justified} by reference to more fully elaborated models of the world.

To see this we can write down the probability of observing \(K=1\) conditional on causal type \(X\), using the \(\phi\) notation from \citet{humphreys2015mixing} and introduced in Chapter 5. Here \(\phi_{jx}\) refers to the probability of observing a clue in a case of type \(j\) when \(X=x\). Starting with our prior distribution over the lower-level causal types (the \(\lambda\)'s), we can derive, for an \(X=1\) case, the probability of seeing the clue if the case is of type \(b\) (positive effect) or of type \(d\) (no effect, \(Y\) always \(1\)):

\begin{equation}
\begin{split}
\phi_{b1} & = \frac{\lambda_{01}^{K}\lambda_{01}^{Y}}{\lambda_{01}^{K}\lambda_{01}^{Y}+\lambda_{10}^{K}\lambda_{10}^{Y}}\\ 
\phi_{d1} & = \frac{\lambda_{11}^{Y}(\lambda_{01}^{K}+\lambda_{11}^{K})+\lambda_{11}^{K}\lambda_{01}^{Y}}{\lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y}}
\end{split}
\label{eqn:phisfromlambdas}
\end{equation}

These quantities allow for easy mapping between our prior beliefs about our causal query---as expressed in the lower level model---and the classic process-tracing tests in \citet{Van-Evera:1997}. Figure \ref{fig:phis} illustrates. In each panel, we manipulate a prior for one or more of the lower-level causal effects, keeping all other priors flat, and we see how probative value changes. As the curves for \(\phi_b\) and \(\phi_d\) diverge, probative value is increasing since there is an increasing difference between the probability of seeing the clue if \(X\) has a positive effect on \(Y\) and the probability of seeing the clue if \(X\) has no effect.

In the left panel, we see that as we place a lower prior probability on \(K\)'s being negatively affected by \(X\),\footnote{For a given value of \(\lambda^K_{01}\), we hold the other \(\lambda^K\) values equal by assigning a value of \((1-\lambda^K_{01})/3\) to each.} seeking \(K=1\) increasingly takes on the quality of a hoop test for \(X\)'s having a positive effect on \(Y\). The clue, that is, increasingly becomes something we must see if \(X\) positively affects \(Y\), with the clue remaining moderately probable if there is no effect. Why? The less likely we believe it is that \(K=0\) was caused by \(X=1\), the less consistent the observation of \(K=0\) is with \(X\) having a positive causal effect on \(Y\) via \(K\) (since, to have such an effect, if \(X=1\) and \(K=0\), would precisely have to mean that \(X=1\) \emph{caused} \(K=0\)).

In the second graph, we simultaneously change the prior probabilities of zero effects at both stages in the sequence: of \(K\) and \(Y\) being \(1\) regardless of the values of \(X\) and \(K\), respectively.\footnote{For a given value of \(\lambda^K_{11}\), we hold the other \(\lambda^K\)'s equal by assigning a value of \((1-\lambda^K_{11})/3\) to each; likewise for \(\lambda^Y_{11}\) and the other \(\lambda^Y\) values.} We see here that, as the probabilities of zero effects jointly diminish, seeking \(K=1\) increasingly becomes a smoking-gun test for a positive effect of \(X\) on \(Y\): the probability of seeing the clue if the case is a \(d\) type diminishes. The reason is that, as zero effects at the lower level become less likely, it becomes increasingly unlikely that \(K=1\) could have occurred without a positive effect of \(X\) on \(K\), and that \(Y=1\) could have occurred (given that we have seen \(K=1\)) without a posiitve effect of \(K\) on \(Y\).

\begin{figure}

{\centering \includegraphics[width=.85\textwidth]{ii_files/figure-latex/phis-1} 

}

\caption{The probability of observing $K$ given causal type for different beliefs on lower-level causal effects. In the left figure, priors on all lower-level causal effects are flat except for the probability that $X$ has a negative effect on $K$. If we believe that it is unlikely that $X$ has a negative effect on $K$, $K$ becomes a `hoop' test for the proposition that a case is of type $b$. The righthand figure considers simultaneous changes in $\lambda_{11}^K$ and  $\lambda_{11}^Y$---the probabilities that $K=1$ regardless of $X$, and that $Y=1$  regardless of $K$, with flat distributions on all other lower-level effects. With $\lambda_{11}^K$, $\lambda_{11}^Y$ both close to 0, $K$ becomes a 'smoking gun' test for the proposition that $X$ has a positive effect on $Y$ ($b$ type).}\label{fig:phis}
\end{figure}

\hypertarget{conditional-independence-alone-does-not-provide-probative-value}{%
\subsection{Conditional independence alone does not provide probative value}\label{conditional-independence-alone-does-not-provide-probative-value}}

\hypertarget{uncertainty-does-not-alter-inference-for-single-case-causal-inference}{%
\subsection{Uncertainty does not alter inference for single case causal inference}\label{uncertainty-does-not-alter-inference-for-single-case-causal-inference}}

In the procedure described for process tracing in this chapter (and different to what we introduce in Chapter 8) we assume that \(\lambda\) is known and we do not place uncertainty around it.

This might appear somewhat heroic, but in fact for single case inference it is without loss of generality. The expected inferences we would make for any estimand accounting for priors is the same as the inferences we if we use the expectation only.

To see this, let \(\pi_j\) denote the probability of observing causal type \(j\) and \(p(D)\) te probability of observing data realization \(D\). Say that \(j \in D\) if type \(j\) produces data type \(D\) and say \(j \in E\) if casual type \(j\) is an element of the estimand set of interest. For instance in an \(X \rightarrow Y\) model, if we observe \(X=Y=1\) then \(D\) consists of causal types \(D={(\theta^X_1, \theta^Y_{01}), (\theta^X_1, \theta^Y_{11})})\) and the estimand set for ``\(X\) has a positive effect on \(Y\)'' consists of \(E={(\theta^X_1, \theta^Y_{01}), (\theta^X_0, \theta^Y_{01})})\).

The posterior on an estimand \(E\) given data \(D\) given prior over \(\pi\), \(p(\pi)\) is:

\[\Pr(E | D) = \int_\pi  \frac{\sum_{j \in E \cap D}\pi_j}{\sum_{j \in D}\pi_j} f(\pi)d\pi\]

However, since for any \(\pi\), \(\sum_{j \in D}\pi_j = p(D)\) we have:

\[\Pr(E | D) = \int_\pi  \sum_{j \in E \cap D}\pi_j f(\pi)d\pi/p(D) = \sum_{j \in  E \cap D} \overline{\pi}_j/p(D)\]

\hypertarget{probative-value-requires-d-connection}{%
\subsection{\texorpdfstring{Probative value requires \(d-\)connection}{Probative value requires d-connection}}\label{probative-value-requires-d-connection}}

As we have argued, causal estimands can be expressed as the values of exogenous nodes in a causal graph. Case-level causal effects and causal paths can be defined in terms of response-type nodes; average effects and notable causes in terms of population-level parameter nodes (e.g., \(\pi\) or \(\lambda\) terms); and questions about actual causes in terms of exogenous conditions that yield particular endogenous values (conditioning on which makes some variable a counterfactual cause).

We thus define causal inference more generally as \emph{the assessment of the value of one or more unobserved (possibly unobservable) exogenous nodes on a causal graph, given observable data.} To think through the steps in this process, it is useful to distinguish among three different features of the world, as represented in our causal model: there are the things we want to learn about; the things we have already observed; and the things we could observe. As notation going forward, let:

\begin{itemize}
\tightlist
\item
  \(\mathcal Q\) denote the exogenous variables that define our \emph{query}; we generally assume that \(\mathcal Q\) cannot be directly observed so that its values must be inferred
\item
  \(\mathcal W\) denote a set of previously observed nodes in the causal model, and
\item
  \(\mathcal K\) denote a set of additional variables---clues---that we have not yet observed but could observe.
\end{itemize}

Now suppose that we seek to design a research project to investigate a causal question. How should the study be designed? Given that there are some features of the world that we have already observed, which additional clues should we seek to collect to shed new light on our question? In terms of the above notation, what we need to figure out is whether a given \(\mathcal K\) might be informative about---might provide additional leverage on---\(\mathcal Q\) given the prior observation of \(\mathcal W\).

To ask whether one variable (or set of variables) is informative about another is to ask whether the two (sets of) variables are, on average, \emph{correlated} with one another, given whatever we already know. Likewise, if two variables' distributions are fully \emph{independent} of one another (conditional on what else we have observed), then knowing the value of one variable can provide no new information about the value of the other.

Thus, asking whether a set of clues, \(\mathcal K\), is informative about \(\mathcal Q\) given the prior observation of \(\mathcal W\), is equivalent to asking whether \(\mathcal K\) and \(\mathcal Q\) are conditionally independent given \(\mathcal W\). That is, \(\mathcal K\) can be informative about \(\mathcal Q\) given \(\mathcal W\) only if \(\mathcal K\) and \(\mathcal Q\) are \emph{not} conditionally independent of one another given \(\mathcal W\).

As we have shown, as long as we have built \(\mathcal Q\), \(\mathcal K\), and \(\mathcal W\) into our causal model of the phenomenon of interest, we can answer this kind of question by inspecting the structure of the model's DAG. In particular, what we need to go looking for are relationships of \emph{\(d\)-separation}. The following proposition, with only the names of the variable sets altered, is from \citet{pearl2009causality} (Proposition 1.2.4):

\textbf{Proposition 1:} If sets \(\mathcal Q\) and \(\mathcal K\) are \(d\)-separated by \(\mathcal W\) in a DAG, \(\mathcal G\), then \(\mathcal Q\) is independent of \(\mathcal K\) conditional on \(\mathcal W\) in every distribution compatible with \(\mathcal G\). Conversely, if \(\mathcal Q\) and \(\mathcal K\) are \emph{not} \(d\)-separated by \(\mathcal W\) in DAG \(\mathcal W\), then \(\mathcal Q\) and \(\mathcal K\) are dependent conditional on \(\mathcal W\) in at least one distribution compatible with \(\mathcal G\).

We begin with a causal graph and a set of nodes on the graph (\(W\)) that we have already observed. Given what we have already observed, \emph{a collection of clue nodes, \(\mathcal K\), will be uninformative about the query nodes, \(\mathcal Q\), if \(\mathcal K\) is \(d\)-separated from \(\mathcal Q\) by \(\mathcal W\) on the graph.} When \(\mathcal W\) \(d\)-separates \(\mathcal K\) from \(\mathcal Q\), this means that what we have already observed already captures all information that the clues might yield about our query. On the other hand, if \(\mathcal K\) and \(\mathcal Q\) are \(d\)-connected (i.e., not \(d\)-separated) by \(W\), then \(K\) is \emph{possibly} informative about \(Q\).\(K\) is not \(d\)-separated from \(\mathcal Q\) by \(\mathcal W\).\footnote{This proposition is almost coextensive with the definition of a DAG. A DAG is a particular kind of dependency model (``graphoid'') that is a summary of a collection of ``independency statements'', \((I)\), over distinct subsets of \(\mathcal V\) (Pearl and Verma 1987), where \(I(\mathcal Q,\mathcal W,\mathcal K)\) means ``we learn nothing about \(\mathcal Q\) from \(\mathcal K\) if we already know \(\mathcal W\)''. More formally:
  \[I(\mathcal K, \mathcal W,\mathcal Q) \leftrightarrow P(\mathcal K,\mathcal Q|\mathcal W)=P(\mathcal K|\mathcal W)P(\mathcal Q|\mathcal W)\]
  A Directed Acyclic Graph Dependency model is one where the set of independencies correspond exactly to the relations that satisfy \(d\)-separation (Pearl and Verma 1987, p376). Thus on DAG \(\mathcal G\), \(I(\mathcal K,\mathcal W,\mathcal Q)_\mathcal G\) implies that \(\mathcal K\) and \(\mathcal Q\) are \(d\)-separated by \(\mathcal W\).} Note, moreover, that under quite general conditions (referred to in the literature as the \emph{faithfulness} of a probability distribution) then there are at least \emph{some} values of \(\mathcal W\) for which \(\mathcal K\) \emph{will} be informative about \(\mathcal Q\).\footnote{Put differently, there will not be any conditional independencies that are \emph{not} captured in the DAG.}

Let us examine Proposition 1 in practice. We begin with the simplest case possible, and then move on to more complex models.

The very simplest probabilistic causal graph has \(X\) influencing \(Y\), with \(X\) determined by a coin flip. Assuming that there is some causal heterogeneity---that is, it is unknown in any particular case whether \(X\) causes \(Y\)---we also include a response-type variable, \(Q\), pointing into \(Y\), as shown in Figure \ref{fig:d-sepsimple}. Here, \(Q^Y\) determines the value of \(Y\) that will be generated by \(X\). Asking about the causal effect of \(X\) in a case thus means learning the value of \(Q^Y\) in that case. As will be recalled, in a binary setup with one causal variable, a response-type variable can take on one of four values, \(q^Y_{00}\), \(q^Y_{10}\), \(q^Y_{01}\) and \(q^Y_{11}\),\footnote{As a reminder, we read \(q^Y_{ij}\) (when \(X\) is binary) as meaning that \(Y\) will take on value \(i\) when \(X=0\) and value \(j\) when \(X=1\).} corresponding to the four possible causal types in this setting.

\begin{figure}

{\centering \includegraphics[width=.5\textwidth]{ii_files/figure-latex/unnamed-chunk-33-1} 

}

\caption{\label{fig:d-sepsimple} A simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's response type for $Y$.}\label{fig:unnamed-chunk-33}
\end{figure}

Let us assume that we have observed nothing yet in this case and then ask what clue(s) might be informative about \(Q^Y\), the node of interest. The other two nodes in the graph are \(X\) and \(Y\): these are thus the possible clues that we might go looking for in our effort to learn about \(Q^Y\) (i.e., they are the possible members of \(\mathcal K\)).

First, can we learn about \(Q^Y\) by observing \(X\)? We can answer this question by asking whether \(X\) is \(d\)-connected to \(Q^Y\) on the graph given what we have already observed (which is nothing). We can see visually that there is no active path from \(X\) to \(Q^Y\): the only path between \(X\) and \(Q\) is blocked by colliding arrow heads. Thus, \(X\) and \(Q^Y\) are \(d\)-separated, meaning that \(X\) will not be informative about \(Q^Y\): observing the value that a causal variable takes on in a case---having seen nothing else in the case---tells us nothing whatsoever about that variable's effect on the outcome. If we want to know whether a case is of a type in which the presence of natural resources would cause civil war, observing only that the case has natural resources does not help answer the question.

--\textgreater{}

What, then, if we instead were to observe only \(Y\)? Is \(Y\) \(d\)-connected to \(Q\) given what we have already observed (which, again, is nothing)? It is: the arrow from \(Q^Y\) to \(Y\) is an active path. Observing only the \emph{outcome} in a case does tell us something about causal effects. Returning to the natural resources and civil war example, observing only that a country has had a civil is informative about the case's causal type (the value of \(Q^Y\)). In particular, it rules out the possibility that this is a case in which nothing could cause a civil war: that is, it excludes \(q^Y_{00}\) (i.e., \(c\)-type) as a possible value of \(Q^Y\).

Suppose now, having observed \(Y\), that we were to consider also observing \(X\). Would we learn anything further about \(Q^Y\) from doing so? We have already seen that observing \(X\) alone yields no information about \(Q^Y\) because the two nodes are unconditionally \(d\)-separated, the path between them blocked by the colliding arrowheads at \(Y\). However, as we have seen, observing a collider variable (or one of its descendants) \emph{unblocks} the flow of information, generating relations of conditional dependence across the colliding arrowheads. Here, \(X\) and \(Q^Y\) are \(d\)-connected by \(Y\): thus, if we have \emph{already} observed \(Y\), then observing \(X\) does confer additional information about \(Q^Y\). Knowing only that a country has natural resources tells us nothing about those resources' effect on civil war in that country. But if we already know that the country has a civil war, then learning that the country has natural resources helps narrow down the case's possible response types. Having already used the observation of \(Y=1\) to rule out the possibility that \(Q^Y=q^Y_{00}\), observing \(X=1\) \emph{together with} \(Y=1\) allows us to additionally rule out the possibility that natural resources \emph{prevent} civil war, i.e., that \(Q^Y=q^Y_{01}\).\footnote{That is, we can rule out that the case is an \(a\) type, or one with a negative causal effect.}

Finally, what if we observe \(X\) first and are considering whether to seek information about \(Y\)? Would doing so be informative? \(X\) does not \(d-\)separate \(Q^Y\) from \(Y\); thus, observing \(Y\) will be informative about \(Q^Y\). In fact, observing \(Y\) if we have already seen \(X\) is \emph{more} informative than observing \(Y\) alone. The reasoning follows the logic of collision discussed just above. If we observe \(Y\) having already seen \(X\), not only do we reap the information about \(Q^Y\) provided by \(Y\)'s correlation with \(Q^Y\); we simultaneously open up the path between \(X\) and \(Q^Y\), learning additionally from the conditional dependence between \(X\) and \(Q^Y\) given \(Y\).

We put Proposition 1 to work in a slightly more complex set of models in Figure \ref{fig:34graphs}. Here we investigate the informativeness of a clue that is neither \(X\) nor \(Y\). Each graph in Figure \ref{fig:34graphs} has four variables: \(X\); \(Y\); a possible clue, \(K\); and a response-type variable, \(Q\). We draw all 34 possible graphs with variables \(X\), \(Y\), \(K\), and \(Q\) for causal models in which (a) all variables are connected to at least one other variable, (b) \(X\) causes \(Y\) either directly or indirectly, and (c) \(Q\) is a direct cause of \(Y\) but is not caused by any other variable in the model and is thus exogenous. The title of each panel reports \(K\)'s conditional informativeness using principles of \(d\)-separation: it tells us when \(K\) is possibly informative about \(Q\) depending on whether \(X\), \(Y\), both or none are observed.\footnote{Note the ``possibly'' can be dropped under the assumption that the underlying probability model is ``stable'' (Pearl 2009, section 2.9.1) and with the interpretation that \(K\) is informative about \(Q\) for some, but not necessarily all, values of \(W\).}

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/unnamed-chunk-35-1.pdf}
\caption{\label{fig:unnamed-chunk-35}\label{fig:34graphs} All connected directed acyclic graphs over \(X,Y,K,Q\), in which \(Q\) is an exogenous variable that directly causes \(Y\), and \(X\) is a direct or indirect cause of \(Y\). The title of each graph indicates the conditions under which \(K\) can be informative about (i.e., is not \(d\)-separated from) \(Q\), given the prior observation of \(X\), \(Y\), both, or neither (\ldots{}).}
\end{figure}

\hypertarget{probative-value}{%
\subsection{Probative value}\label{probative-value}}

The results show us not just what kinds of variables can be informative about a case's response-type but also what combinations of observations yield leverage on case-level causal effects. A number of features the graphs are worth highlighting:

\begin{itemize}
\item
  \textbf{Clues at many stages.} Process tracing has focused a great deal on observations that lie ``along the path'' between suspected causes and outcomes. What we see in Figure \ref{fig:34graphs}, however, is that observations at many different locations in a causal model can be informative about causal effects. We see here that \(K\) can be informative when it is pre-treatment (causally prior to \(X\)---e.g.~panel (3)), post-treatment but pre-outcome (that is, ``between'' \(X\) and \(Y\) as, e.g., in panel (20)), an auxiliary effect of \(X\) that itself has no effect on \(Y\) (e.g., in panel (19)), post-outcome (after \(Y\)---e.g., in panel (15)), or a joint effect of both the suspected cause and the outcome (e.g., panel (31)).
\item
  \textbf{Mediator Clues}. While clues that lie in between \(X\) and \(Y\) may be informative, they can only be informative under certain conditions. For instance, when a clue serves \emph{only} as a mediator in our model (i.e., its only linkages are being caused by \(X\) and being affected by \(Y\)) and \(Q\) only affects \(Y\), as in panels (20) and (21), the clue is only informative about \(Q\) if we have also observed the outcome, \(Y\). Of course, this condition may commonly be met---qualitative researchers usually engage in retrospective research and learn the outcome of the cases they are studying early on---but it is nonetheless worth noting why it matters: in this setup, \(K\) is unconditionally \(d\)-separated from \(Q\) by the collision at \(Y\); it is only by observing \(Y\) (the collider) that the path between \(K\) and \(Q\) becomes unblocked. (As we saw above, the very same is true for observing \(X\); it is only when we know \(Y\) that \(X\) is informative about \(Q\).)
\end{itemize}

In short, observations along causal paths are more helpful in identifying causal effects to the extent that we have measured the outcome. Importantly, this is not the same as saying that mediator clues are \emph{only} informative about causal effects where we have observed the outcome. Observing \(Y\) is necessary for the mediator to be informative about a \(Q\) term that is connected only to \(Y\). Observing a mediator without the outcome, however, could still be informative about the overall effect of \(X\) on \(Y\) by providing leverage on how the mediator responds to \(X\), which is itself informative about \(X\)'s effect on \(Y\) via the mediator.\footnote{In other words, the clue would then be providing leverage on a response-type variable pointing into the mediator itself.} Moreover, observing the mediator could be informative without the observation of \(Y\) if, for instance, \(Q\) also points into \(K\) itself or into a cause of \(K\). As we discuss below, the clue then is informative as a ``symptom'' of the case's response type, generating learning that does not hinge on observing the outcome.

\begin{itemize}
\tightlist
\item
  \textbf{Symptoms as clues.} Some clues may themselves be affected by \(Q\): that is to say, they may be symptoms of the same conditions that determine causal effects in a case. For instance, in our illustrative model involving government survival, government sensitivity functions as a response-type variable for the effect of a free press (\(X\)) on government removal (\(Y\)): a free press only generates government removal when the government is non-sensitive to public opinion. Sensitivity to public opinion thus represents our query variable, \(Q\), if we seek to learn whether a free press causes government removal in a case. While it may not be possible to observe or otherwise measure the government's sensitivity, there may be \emph{consequences} of government sensitivity that are observable: for instance, whether government officials regularly consult with civil-society actors on policy issues. While consultations would not be part of the causal chain generating the free press's effect, observing consultations (or the lack of them) would be informative about that effect because consultations are a symptom of the same conditions that enable the effect.
\end{itemize}

We see that \(K\) is a child or descendant of \(Q\) in several of the graphs in Figure \ref{fig:34graphs}: \(Q\) directly causes \(K\) in panels (7) through (14), (17), (18), (25)-(30), (33), and (34); \(Q\) causes (K) only indirectly through \(X\) in panels (22) through (24); \(Q\) causes (K) only indirectly through \(Y\) in panels (15), (16), and (31); and \(Q\) causes \(K\) only indirectly through \(X\) and through \(Y\) in panel (32). We can then use the principle of \(d\)-separation to figure out when the symptom clue is potentially informative, given what we have already observed. It is easy to see that \(K\) is potentially informative, no matter what we have already observed, if \(K\) is directly affected by \(Q\); there is nothing we could observe that would block the \(Q \rightarrow K\) path. Thus, \(Q\)'s ``symptom'' can, in this setup, contain information about type above and beyond that contained in the \(X\) and \(Y\) values. However, where \(Q\) affects \(K\) only through some other variable, observing that other variable renders \(K\) uninformative by blocking the \(Q\)-to-\(K\) path. For instance, where \(Q\) affects \(K\) indirectly through \(X\), once we observe \(X\), we already have all the information about \(Q\) that would be contained in \(K\).

\begin{itemize}
\tightlist
\item
  \textbf{Surrogates as clues.} Clues may be consequences of the outcome, as in graphs (15) and (16). If \(K\) is a consequence \emph{only} of \(Y\), then it will contain no new information about \(Q\) where \(Y\) is already known. However, in situations where the outcome has not been observed, \(K\) can act as a ``surrogate'' for the outcome and thus yield leverage on \(Q\) (\citet{frangakis2002principal}). A researcher might, for instance, seek to understand causal effects on an outcome that is difficult to directly observe: consider, for instance, studies that seek to explain ideational change. Ideas themselves, the \(Y\) in such studies, are not directly observable. However, their consequences---such as statements by actors or policy decisions---will be observable and can thus serve as informative surrogates for the outcome of interest.
\end{itemize}

Clues may similarly serve as surrogates of a cause, as in graphs (19) and (22). Here \(X\) causes \(K\), but \(K\) plays no role in the causal process generating \(Y\). \(K\) is of no help if we can directly measure \(X\) since the latter \(d\)-separates \(K\) from \(Q\). But if an explanatory variable cannot be directly measured---consider, e.g., ideas or preferences as causes---then its consequences, including those that have no relationship to the outcome of interest, can provide leverage on the case-level causal effect.

Clues can also be a consequence of both our suspected cause and the outcome of interest, thus serving as what we might call ``double surrogates,'' as in panels (31) and (32). Here \(X\) is a direct cause of \(Y\), and \(K\) is a joint product of \(X\) and \(Y\). A double surrogate can be informative as long as we have not already observed both \(X\) and \(Y\). Where data on either \(X\) or \(Y\) are missing, there is an open path between \(K\) and \(Q\). If we have already observed both, however, then there is nothing left to be learned from \(K\).

\begin{itemize}
\tightlist
\item
  \textbf{Instruments as clues.} Clues that are causally prior to an explanatory variable, and have no other effect on the outcome, can sometimes be informative. Consider, for instance, graph (3). Here \(K\) is the only cause of \(X\). It can thus serve as a proxy. If we have seen \(X\), then \(X\) blocks the path between \(K\) and \(Q\), and so \(K\) is unhelpful. \(K\) can be informative, though, if we have \emph{not} observed \(X\). Note that informativeness here still requires that we observe \(Y\). Since \(Y\) is a collider for \(Q\) and the \(K \rightarrow X \rightarrow\) chain, we need to observe \(Y\) in order to \(d\)-connect \(K\) to \(Q\).
\end{itemize}

A rather different setup appears in graph (5), where both \(K\) and \(Q\) cause \(X\). Now the conditions for \(K\)'s informativeness are broader. Observing \(X\) still makes \(K\) uninformative as a proxy for \(X\) itself. However, because \(X\) is a collider for \(K\) and \(Q\), observing \(X\) \emph{opens up} a path from \(K\) to \(Q\), rendering a dependency between them. Still, we have to observe at least one of \(X\) or \(Y\) for the instrument to be informative here. This is because both of \(K\)'s paths to \(Q\) run through a collision that we need to unblock by observing the collider. For one path, the collider is \(X\); for the other path, the collider is \(Y\).\footnote{As a simple example one might imagine a system in which \(X = K\) if \(q \in {a,b}\) and \(X = 1-K\) if \(q \in {c,d}\). Then if we observe, say, \(X=Y=K=1\), we can infer that \(q = b\). Another way to think about what is happening in graph (5) is that \(K\) is providing information about the \emph{assignment process}. In this graph, the causal effect (\(Y\)'s potential outcomes, determined by \(Q\)) is also a partial determinant of the assignment of cases to values on \(X\). In terms of cross-case correlational inference, then, we would think of this as a situation of confounding. Observing another cause of \(X\), then, allows us to more fully characterize the process of assignment.}

Other patterns involving instrumentation are also imaginable, though not graphed here. For example, we might have a causal structure that combines instrumentation and surrogacy. Suppose that \(X\) is affected by \(Q\) and by an unobservable variable \(\theta_X\); and that \(\theta_X\) has an observable consequence, \(K\). Then \(K\), though not a cause of \(X\), is a ``surrogate instrument'' \citep{hernan2006instruments} as it is a descendant of an unobserved instrument, \(U\), and thus allows us to extract inferences similar to those that we could draw from a true instrument.

\begin{itemize}
\tightlist
\item
  \textbf{Confounders as clues.} In several of the graphs, \(K\) is a confounder in that it is a direct cause of both \(X\) and \(Y\) (panels (4), (6), (12), and (14)). Let us focus on graph (4), which isolates \(K\)'s role as a confounder. Here \(K\) can be informative via two possible paths. First, if \(X\) is not observed but \(Y\) is, then \(K\) is \(d\)-connected to \(Q\) along the path \(K \rightarrow X \rightarrow Y \leftarrow Q\). \(K\) is in this sense serving as a proxy for \(X\), with its path to \(Q\) opened up by the observation of the collider, \(Y\). Second, with \(Y\) observed, \(K\) can provide information on \(Q\) via the more direct collision, \(K \rightarrow Y \leftarrow Q\). If \(X\) \emph{is} observed, then the first path is blocked, but the second still remains active. As with any pre-outcome variable, for a confounder clue to provide purchase on \(Y\)'s response type, \(Y\) itself must be observed.
\end{itemize}

In a sense, then, the role of confounders as clues in case-level inference is the mirror image of the role of confounders as covariates in cross-case correlational inference. In a correlational inferential framework, controlling for a variable in \(K\)'s position in graph (5) renders the \(X, Y\) correlation (which we assume to be observed) informative about \(X\)'s average causal effect. When we use confounders as evidence in within-case inference, it is our observations of other variables that determine how informative the confounder \emph{itself} will be about \(X\)'s causal effect.

It is important to be precise about the kinds of claims that one can make from graphs like those in Figure \{fig:34graphs\}. The graphs in this figure allow us to identify informativeness about an unobserved node \(Q\) that is a parent of \(Y\). This setup does not, however, capture all ways in which clues can be informative about the causal effect of \(X\) on \(Y\) or about other causal estimands of interest. For instance, as noted above, even if a clue is uninformative about a \(Q\) node pointing into \(Y\), it may still help establish whether \(X\) causes \(Y\): the statement that \(X\) causes \(Y\) will for some graphs be a statement about a \emph{collection} of nodes that form the set of query variables \(\mathcal Q\). This is the case, for instance, in any graph of the form \(X \rightarrow M \rightarrow Y\), where we are interested not just in \(Y\)'s response to \(M\) (the mediator) but also in \(M\)'s response to \(X\). Of interest, thus, are not just a \(Q^Y\) response-type node pointing into \(Y\) but also a \(Q^M\) response-type node that is a parent of \(M\). Observations that provide leverage on either \(Q\) term will thus aid an inference about the overall causal effect. A clue \(K\) that is \(d-\)separated from \(Q^Y\) may nevertheless be informative about \(X\)'s effect on \(Y\) if it is not \(d-\)separated from \(Q^M\); this opens up a broader range of variables as informative clues.

Additionally, as our discussion in Chapter 2 makes clear, estimands other than the case-level causal effect---such as average causal effects, actual causes, and causal paths---involve particular features of context: particular sets of exogenous nodes as members of our query set, \(\mathcal Q\). Thus, even for the same causal model, informativeness will be defined differently for each causal question that we seek to address. The broader point is that we can identify what kinds of observations may address our estimand if we can place that estimand on a causal graph and then assess the graph for relationships of \(d\)-separation and -connection.

Further, we emphasize that a DAG can only tell us when a clue \emph{may} be informative (conditional some prior observation): \(d-\)connectedness is necessary but not sufficient for informativeness. This fact derives directly from the rules for drawing a causal graph: the absence of an arrow between two variables implies that they are \emph{not} directly causally related, while the presence of an arrow does not imply that they always are. As we saw in our analysis of the government-removal example in Chapter 2, whether variables connected to one another by arrows in the original DAG were in fact linked by a causal effect depended on the context. Likewise, whether a clue \(K\) is in fact informative may depend on particular values of \(\mathcal W\)---the variables that have already been observed. As a simple example, let \(q = k_1w + (1-w)k_2\), where \(W\) is a variable that we have already observed and \(K_1\) and \(K_2\) are clues that we might choose to observe next. Here, if \(w=1\) then learning \(K_1\) will be informative about \(Q\), and learning \(K_2\) will not; but if \(w=0\), then \(K_1\) will be uninformative (and \(K_2\) informative).

In general, then, graphical analysis alone can help us exclude unhelpful research designs, given our prior observations and a fairly minimal set of prior beliefs about causal linkages. This is no small feat. But identifying those empirical strategies that will yield the greatest leverage requires engaging more deeply with our causal model, as we explore next.

\hypertarget{ptapp}{%
\chapter{Application: Process Tracing with a Causal Model}\label{ptapp}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We apply the causal-model-based approach to process tracing to a major substantive issue in comparative politics: the relationship between inequality and democratization. We illustrate how one can convert theories in this domain into relatively simple causal models. Drawing on data from \citet{haggard2012inequality}, we use qualitative restrictions on causal types together with flat priors to draw inferences about the probabilty with which inequality facilitated or hindered democratization.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this chapter, we demonstrate how causal-model-based process-tracing works using real data. We undertake this illustration on a substantive issue that has been of central interest to students of comparative politics for decades: the causes of democratization. As the literature and range of arguments about democratization are vast, we focus on just a piece of the debate---specifically on causal claims about the relationship between economic inequality and democratization, with particular attention to the work of \citet{boix2003democracy}, \citet{acemoglu2005economic}, and \citet{haggard2012inequality}. In this chapter, we demonstrate process tracing with causal models, while in a later chapter we demonstrate the integration of process-tracing with correlational analysis. Our focus in this chapter is on using process tracing to assess \emph{the case-level causal effect of inequality on democracy.}

\hypertarget{inequality-and-democratization-the-debate}{%
\section{Inequality and Democratization: The Debate}\label{inequality-and-democratization-the-debate}}

Sociologists, economists, and political scientists have long theorized and empirically examined the relationship between inequality and democracy (e.g., \citet{dahl1973polyarchy}, \citet{bollen1985political}, \citet{acemoglu2005economic}, \citet{boix2003democracy}, \citet{ansell2014inequality}). In recent years, the work of \citet{boix2003democracy}, \citet{acemoglu2005economic}, and \citet{ansell2014inequality} represent major theoretical advances in specifying when and how inequality might generate transitions to democracy (as well as its persistence, which we bracket here). The first and third of these books also provide large-n cross-national and historical tests of their theories' key correlational predictions. \citet{haggard2012inequality}, moreover, derive causal process observations from a large number of ``Third Wave'' cases of democratization in order to examine these theories' claims about the centrality of distributional issues to regime change. We provide a very condensed summary of the core logic of \citet{boix2003democracy} and \citet{acemoglu2005economic} before seeking to translate that logic into a causal model for the purposes of process tracing, using a transformed version of Haggard and Kaufman's causal-process data.

We briefly summarize the core logics of and differences among these three sets of arguments here, bracketing many of their moving parts to focus on the basic theorized relationship between inequality and democracy. Both Boix's and Acemoglu and Robinson's theories operate within a Meltzer-Richard (\citet{meltzer1981rational}) framework in which, in a democracy, the median voter sets the level of taxation-and-transfer and, since mean income is higher than median income, benefit from and vote for a positive tax rate, implying redistribution from rich to poor. The poorer the median voter, the more redistribution she will prefer. Democracy, with its poorer median voter, thus implies greater redistribution than (rightwing) authoritarianism---a better material position from the poor at the expense of the rich elite. Thus, in each of these approaches, struggles over political regimes are conflicts over the distribution of material resources.

In Boix's model, the poor generally prefer democracy for its material benefits. When they mobilize to demand regime change, the rich face a choice as to whether to repress or concede, and they are more likely to repress as inequality is higher since, all else equal, they have more to lose from democracy. Thus, with the poor always preferring democracy over rightwing authoritarianism, inequality reduces the prospects for democratization.

In Acemoglu and Robinson's model, inequality simultaneously affects the expected net gains to democracy for both rich and poor. At low levels of inequality, democracy is relatively unthreatening to the elite, as in Boix, but likewise of little benefit to the poor. Since regime change is costly, the poor do not mobilize for democracy when inequality is low, and democratization does not occur. At high levels of inequality, democracy is of great benefit to the poor but has high expected costs for the elite; thus, democratization does not occur because the elite repress popular demands for regime change. In Acemoglu and Robinson's model, democracy emerges only when inequality is at middling levels: high enough for the poor to demand it and low enough for the rich to be willing to concede it.

Ansell and Samuels, finally, extend the distributive politics of regime change in two key ways. First, they allow for a two-sector economy, with a governing elite comprising the landed aristocracy and an urban industrial elite excluded from political power under authoritarian institutions. Total inequality in the economy is a function of inequality in the landed sector, inequality in the industrial sector, and the relative size of each. Second, authoritarian (landed) elites can tax the industrial bourgeoisie, thus giving the industrial elite an incentive to seek constraints on autocratic rule. Third, in Ansell and Samuels' model, rising industrial inequality means a rising industrial elite, generating a larger gap between them and industrial workers, though the industrial masses are richer than the peasantry. A number of results follow, of which we highlight just a couple. Rising land inequality reduces the likelihood of bourgeois rebellion by giving the landed elite greater repressive capacities and increasing their expected losses under democracy. As industrial inequality rises, however, the industrial elite have more to lose to confiscatory taxation and thus greater incentive to push for partial democracy (in which they have the ability to constrain the government, though the poor remain politically excluded) as well as greater resources with which to mobilize and achieve it. Full democracy, brought on by joint mass and bourgeois rebellion, is most likely as the industrial sector grows in relative size, giving the urban masses more to lose to autocratic expropriation and more resources with which to mobilize and rebel.

These three theoretical frameworks thus posit rather differing relationships between inequality and democracy. Taking these theoretical logics as forms of background knowledge, we would consider it possible that inequality reduces the likelihood of democracy or that it increases the likelihood of democracy. Yet one feature that all three theories have in common is a claim that distributional grievances drive demands for regime change. Moreover, in both Boix and Acemoglu and Robinson, less economically advantaged groups are, all else equal, more likely to demand democracy the worse their relative economic position. Ansell and Samuels' model, on the other hand, suggests that relative deprivation may cut both ways: while poorer groups may have more to gain from redistribution under democracy, better-off groups have more to fear from confiscatory taxation under autocracy. In all three frameworks, \emph{mobilization} by groups with material grievances is critical to transitions to democracy: elites do not voluntarily cede power.

In their qualitative analysis of ``Third Wave'' democratizations, Haggard and Kaufman point to additional factors, aside from inequality, that may generate transitions. Drawing on previous work on 20th century democratic transitions (e.g., \citet{huntington1993third}, \citet{linz1996problems}), they pay particular attention to international pressures to democratize and to elite defections.

\hypertarget{a-structural-causal-model}{%
\section{A Structural Causal Model}\label{a-structural-causal-model}}

We now need to express this background knowledge in the form of a structural causal model. Suppose that we are interested in the case-level causal effect of inequality on democratization of a previously autocratic political system. Suppose further, to simplify the illustration, that we conceptualize both variables in binary terms: inequality is either high or low, and democratization either occurs or does not occur. This means that we want to know, for a given case of interest, whether high inequality (as opposed to low inequality) causes democracy to emerge, prevents democracy from emerging, or has no effect (i.e., with democratization either occurring or not occurring independent of inequality). We can represent this query in the simple, high-level causal model shown in Figure \ref{fig:dagdemochigh}. Here, the question, ``What is the causal effect of high inequality on democratization in this case?'' is equivalent to asking what the value of \(\theta^D\) is in the case, where the possible values are \(\theta_{00}^D, \theta_{01}^D, \theta_{10}^D\), and \(\theta_{11}^D\). We assume here that the case's response type, \(\theta^D\), is not itself observable, and thus we are in the position of having to make inferences about it.

Drawing on the grammar of causal graphs discussed in Chapter 5, we can already identify possibilities for learning about \(\theta^D\) from the other nodes represented in this high-level graph. Merely observing the level of inequality in a case will tell us nothing since \(I\) is not \(d-\)connected to \(\theta^D\) if we have observed nothing else. On the other hand, only observing the outcome---regime type---in a case \emph{can} give us information about \(\theta^D\) since \(D\) \emph{is} \(d-\)connected to \(\theta^D\). For instance, if we observe \(D=1\) (that a case democratized), then we can immediately rule out \(\theta_{00}^D\) as a value of \(\theta^D\) since this type does not permit democratization to occur. Further, conditional on observing \(D\), \(I\) is now \(d-\)connected to \(\theta^D\): in other words, having observed the outcome, we can additionally learn about the case's type from observing the status of the causal variable. For example, if \(D=1\), then observing \(I=1\) allows us additionally to rule out the value \(\theta_{10}^D\) (a negative causal effect).

Now, observing just \(I\) and \(D\) alone will always leave two response types in contention. For instance, seeing \(I=D=1\) (the case had high inequality and democratized) would leave us unsure whether high inequality caused the democratization in this case (\(\theta^D=\theta_{01}^D\)) or the democratization would have happened anyway (\(\theta^D=\theta_{11}^D\)). This is a limitation of \(X, Y\) data that we refer to in \citet{humphreys2015mixing} as the ``fundamental problem of type ambiguity.'' Note that this does not mean that we will be left indifferent between the two remaining types. Learning from \(X, Y\) data alone---narrowing the types down to two---can be quite significant, depending on our priors over the distribution of types. For example, if we previously believed that a \(\theta_{00}^D\) type (cases in which democracy will never occur, regardless of inequality) was much more likely than a \(\theta_{11}^D\) type (democracy will always occur, regardless of inequality) and that positive and negative effects of inequality were about equally likely, then ruling out the \(\theta_{00}^D\) and \(\theta_{10}^D\) values for a case will shift us toward the belief that inequality caused democratization in the case.\footnote{This is because we are ruling out both a negative effect and the type of null effect that we had considered the most likely, leaving a null effect that we consider relatively unlikely.}

\begin{figure}

{\centering \includegraphics{ii_files/figure-latex/dagdemochigh-1} 

}

\caption{Simple democracy, inequality model}\label{fig:dagdemochigh}
\end{figure}

Nonetheless, we can increase the prospects for learning by \emph{theorizing} the relationship between inequality and democratization. Given causal logics and empirical findings in the existing literature, we can say more than is contained in Figure \ref{fig:dagdemochigh} about the possible structure of the causal linkages between inequality and democratization. And we can embed this prior knowledge of the possible causal relations in this domain in a lower-level model that is consistent with the high-level model that most simply represents our query. If we were to seek to fully capture them, the models developed by Boix, Acemoglu and Robinson, and Ansell and Samuels would, each individually, suggest causal graphs with a large number of nodes and edges connecting them. Representing all variables and relationships jointly contained in these three models would take an extremely complex graph. Yet there is no need to go down to the lowest possible level---to generate the \emph{most} detailed graph---in order to increase our empirical leverage on the problem.

We represent in Figure \ref{fig:lowdem} one possible lower-level model consistent with our high-level model. Drawing on causal logics in the existing literature, we unpack the nodes in the high-level model in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We interpose a mediator between inequality and democratization: mobilization (\(M\)) by economically disadvantaged groups expressing material grievances. \(M\) is a function of both \(I\) and of its own response-type variable, \(\theta^M\), which defines its response to \(I\). In inserting this mediator, we have extracted \(\theta^M\) from \(\theta^D\), pulling out that part of \(D\)'s response to \(I\) that depends on \(M\)'s response to \(I\).
\item
  We specify a second influence on democratization, international pressure (\(P\)). Like \(\theta^M\), \(P\) has also been extracted from \(\theta^D\); it represents that part of \(D\)'s response to \(I\) that is conditioned by international pressures.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=.7\textwidth]{ii_files/figure-latex/unnamed-chunk-37-1} 

}

\caption{\label{fig:lowdem} A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type.}\label{fig:unnamed-chunk-37}
\end{figure}

In representing the causal dependencies in this graph, we allow for inequality to have (in the language of mediation analysis) both an ``indirect'' effect on democratization via mobilization and a ``direct'' effect. The arrow running directly from \(I\) to \(D\) allows for effects of inequality on democratization beyond any effects running via mobilization of the poor, including effects that might run in the opposite direction. (For instance, it is possible that inequality has a positive effect on democratization via mobilization but a negative effect via any number of processes that are not explicitly specified in the model.) The graph also implies that there is no confounding: since there is no arrow running from another variable in the graph to \(I\), \(I\) is assigned as-if randomly.

The lower-level graph thus has two exogenous, response-type nodes that will be relevant to assessing causal effects: \(\theta^M\) and \(\theta^{D_{lower}}\). \(\theta^M\), capturing \(I\)'s effect on \(M\), ranges across the usual four values for a single-cause, binary setup: \(\theta_{00}^M, \theta_{01}^M, \theta_{10}^M\), and \(\theta_{11}^M\).

\(\theta^{D_{lower}}\) is considerably more complicated, however, because this node represents \(D\)'s response to three causal variables: \(I\), \(M\), and \(P\). One way to put this is that the values of \(\theta^{D_{lower}}\) indicate how inequality's direct effect will depend on mobilization (and vice versa), conditional on whether or not there is international pressure. We need more complex notation than that introduced in Chapter 5 in order to represent the possible response types here.

In Chapter 5, we needed four subscripts on \(\theta\) to represent the potential outcomes for the four combinations of values that two binary variables can take on. With a third binary variable, we now require eight.

\begin{longtable}[]{@{}ccc@{}}
\caption{\label{tab:interpretation} Interpreting the eight subscripts on \(\theta^D\) given 3 parents.}\tabularnewline
\toprule
subscript position & Interpretation & \(\theta^D_{01011010}\)\tabularnewline
\midrule
\endfirsthead
\toprule
subscript position & Interpretation & \(\theta^D_{01011010}\)\tabularnewline
\midrule
\endhead
1 & Value of D when: I = 0 \& P = 0 \& M = 0 & D = 0\tabularnewline
2 & Value of D when: I = 1 \& P = 0 \& M = 0 & D = 1\tabularnewline
3 & Value of D when: I = 0 \& P = 1 \& M = 0 & D = 0\tabularnewline
4 & Value of D when: I = 1 \& P = 1 \& M = 0 & D = 1\tabularnewline
5 & Value of D when: I = 0 \& P = 0 \& M = 1 & D = 1\tabularnewline
6 & Value of D when: I = 1 \& P = 0 \& M = 1 & D = 0\tabularnewline
7 & Value of D when: I = 0 \& P = 1 \& M = 1 & D = 1\tabularnewline
8 & Value of D when: I = 1 \& P = 1 \& M = 1 & D = 0\tabularnewline
\bottomrule
\end{longtable}

Thus, to illustrate, \(\theta^D_{01011010}\) represents a type in which inequality always (regardless of pressure) has a positive direct effect on democratization when \(M=0\) (subscripts 2 and 4) and always has a negative direct effect on democratization when \(M=1\) (subscripts 6 and 8). Also notice that pressure itself never has an effect (compare, for instance, subscript 1 and 3); and that mobilization has a positive effect when inequality is low (subscripts 5 and 7) but a negative effect when inequality is high (subscripts 6 and 8).

The result is \(2^8=256\) possible response types for \(D\). With 4 response types for \(M\), we thus have 1024 possible combinations of causal effects between named variables in the lower-level graph. How do these lower-level response types map onto the higher-level response types that are of interest? In other words, which combinations of lower-level types represent a positive, negative, or zero causal effect of inequality on democratization?

To define a causal effect of \(I\) in this setup, we need to define the ``joint effect'' of two variables as being the effect of changing both variables simultaneously (in the same direction, unless otherwise specified). Thus, the joint effect of \(I\) and \(M\) on \(D\) is positive if changing both \(I\) and \(M\) from \(0\) to \(1\) changes \(D\) from \(0\) to \(1\). We can likewise refer to the joint effect of an increase in one variable and a decrease in another. Given this definition, a positive causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Linked positive mediated effects.} \(I\) has a positive effect on \(M\); and \(I\) and \(M\) have a \emph{joint} positive effect on \(D\) when \(P\) takes on whatever value it takes on in the case.
\item
  \textbf{Linked negative mediated effects} \(I\) has a negative effect on \(M\); and \(I\) and \(M\) have a \emph{joint} negative effect on \(D\) when \(P\) takes on whatever value it takes on in the case.
\item
  \textbf{Positive direct effect} \(I\) has no effect on \(M\) and \(I\) has a positive effect on \(D\) at whatever value \(M\) is fixed at and whatever value \(P\) takes on in the case.
\end{enumerate}

If we start out with a case in which inequality is high and democratization has not occurred (or inequality is low and democratization \emph{has} occurred), we will be interested in the possibility of a negative causal effect. A negative causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{Positive, then negative mediated effects} \(I\) has a positive effect on \(M\); and \(I\) and \(M\) have a \emph{joint} negative effect on \(D\) when \(P\) takes on whatever value it takes on in the case.
\item
  \textbf{Negative, then joint negative mediated effects} \(I\) has a negative effect on \(M\); and jointly increasing \(I\) while decreasing \(M\) generates a decrease in \(D\) when \(P\) takes on whatever value it takes on in the case.
\item
  \textbf{Negative direct effects} \(I\) has no effect on \(M\) and \(I\) has a negative effect on \(D\) at whatever value \(M\) is fixed at and whatever value \(P\) takes on in the case.
\end{enumerate}

Finally, all other response patterns yield \emph{no} effect of inequality on democratization.

Thus, for a case in which \(I=D=1\), our query amounts to assessing the probability that \(\theta^M\) and \(\theta^D_{lower}\) jointly take on values falling into conditions 1, 2, or 3. And for a case in which \(I \neq D\), where we entertain the possibility of a negative effect, our query is an assessment of the probability of conditions 4, 5, and 6.

\hypertarget{forming-priors}{%
\subsection{Forming Priors}\label{forming-priors}}

We now need to express prior beliefs about the probability distribution from which values of \(\theta^M\) and \(\theta^D_{lower}\) are drawn. We place structure on this problem by drawing a set of beliefs about the likelihood or monotonicity of effects and interactions among variables from the theories in Boix, Acemoglu and Robinson, and Ansell and Samuels. As a heuristic device, we weight more heavily those propositions that are more widely shared across the three works than those that are consistent with only one of the frameworks. We intend this part of the exercise to be merely illustrative of how one might go about forming priors from an existing base of knowledge; there are undoubtedly other ways in which one could do so from the inequality and democracy literature.

Specifically, the belief that we embed in our priors about \(\theta^M\) is:

\begin{itemize}
\tightlist
\item
  \textbf{Monotonicity of \(I\)'s effect on \(M\)}: In Acemoglu and Robinson, inequality should generally increase the chances of---and, in Boix, should never prevent---mobilization by the poor. Only in Ansell and Samuels' model does inequality have a partial downward effect on the poor's demand for democracy insofar as improved material welfare for the poor increases the chances of autocratic expropriation; and this effect is countervailed by the greater redistributive gains that the poor will enjoy under democracy as inequality rises.\footnote{In addition, as the industrial bourgeoisie become richer, which increases the Gini, this group faces a greater risk of autocratic expropriation. If we consider the rising bourgeosie's mobilization to be mobilization by a materially disadvantaged group, then this constitutes an additional positive effect of inequality on mobilization.} Consistent with the weight of prior theory on this effect, in our initial run of the analysis, we rule out negative effects of \(I\) on \(M\). We are indifferent in our priors between positive and null effects and between the two types of null effects (mobilization always occurring or never occurring, regardless of the level of inequality). We thus set our prior on \(\theta^M\) as: \(p(\theta^M=\theta^M_{10})=0.0\), \(p(\theta^M=\theta^M_{00})=0.25\), \(p(\theta^M=\theta^M_{11})=0.25\), and \(p(\theta^M=\theta^M_{01})=0.5\). We relax this monotonicity assumption, to account for the Ansell and Samuels logic, in a second run of the analysis.
\end{itemize}

For our prior on democracy's responses to inequality, mobilization, and international pressure (\(\theta^D_{lower}\)), we extract the following beliefs from the literature:

\begin{itemize}
\item
  \textbf{Monotonicity of direct \(I\) effect: no positive effect}: In none of the three theories does inequality promote democratization via a pathway \emph{other than} via the poor's rising demand for it. In all three theories, inequality has a distinct negative effect on democratization via an increase in the elite's expected losses under democracy and thus its willingness to repress. In Ansell and Samuels, the distribution of resources also affects the probability of success of rebellion; thus higher inequality also reduces the prospects for democratization by strengthening the elite's hold on power. We thus set a zero prior probability on all types in which \(I\)'s direct effect on \(D\) is positive for any value of \(P\).
\item
  \textbf{Monotonicity of \(M\)'s effect: no negative effect}: In none of the three theories does mobilization reduce the prospects of democratization. We thus set a zero probability on all types in which \(M\)'s effect on \(D\) is negative at any value of \(I\) or \(P\).
\item
  \textbf{Monotonicity of \(P\)'s effect: no negative effect}: While international pressures are only discussed in Haggard and Kaufman's study, none of the studies considers the possibility that international pressures to democratize might prevent democratization that would otherwise have occurred. We thus set a zero probability on all types in which \(P\)'s effect is negative at any value of \(I\) or \(M\).
\end{itemize}

In all, this reduces the number of nodal types for \(D\) from 256 to just 20.

For all remaining, allowable types, we set flat priors.

In Table \ref{tab:apptypes2} we show, for the remaining 20 allowable types, how international pressure moderates the effects of inequality (direct) and mobilization. We use the following notation to characterize conditioning effects of pressure on the effect of another variable, \(X\):

\begin{itemize}
\item
  \(N\): \(P\) has no moderating effect
\item
  \(O_X\): \(P=1\) creates an ``opportunity'' for \(X\) to have an effect that it does not have at \(P=0\); at \(P=1\) and \(X=0\), \(D\) takes on the value it does when \(X=0\) and \(X\) has an effect, but does not take on this value when \(P=0\) and \(X=0\)
\item
  \(C_X\): \(P=1\) is a causal ``complement'' to \(X\), allowing \(X\) to have an effect it did not have at \(P=0\); at \(P=1\) and \(X=1\), \(D\) takes on the value it does when \(X=1\) and \(X\) has an effect, but does not take on this value when \(P=0\) and \(X=1\)
\item
  \(S_X\): \(P=1\) ``substitutes'' for \(X\), generating the outcome that \(X=1\) was necessary to generate at \(P=0\); at \(P=1\) and \(X=0\), \(D\) takes on the value it does when \(X=1\) and \(X\) has an effect, but does not take on this value when \(P=0\) and \(X=0\)
\item
  \(E_X\): \(P\) ``eliminates'' \(X\)'s effect, preventing \(X=1\) from generating the outcome it generates when \(P=0\); at \(P=1\) and \(X=1\), \(D\) does not take on the value it does when \(X=1\) and \(X\) has an effect, but does take on this value when \(P=0\) and \(X=1\)
\end{itemize}

\emph{FLAG: Types are given in gbiqq ordering but with this notation we need to check if interpretation is still consistent with cell contents (suspect not as P,M ordering may be reversed)}

\begin{longtable}[]{@{}ccc@{}}
\caption{\label{tab:apptypes2} Table of allowable types. The table characterizes the nature of the interaction, using the notation explained in the main text. We impose flat priors across all non-excluded types.}\tabularnewline
\toprule
Nodal Type \(\theta^D\) & Nature of interaction & Prior probability\tabularnewline
\midrule
\endfirsthead
\toprule
Nodal Type \(\theta^D\) & Nature of interaction & Prior probability\tabularnewline
\midrule
\endhead
\(\theta_{00000000}\) & \(N\) & 0.05\tabularnewline
\(\theta_{00000010}\) & \(C_M, O_I\) & 0.05\tabularnewline
\(\theta_{00100010}\) & \(N\) & 0.05\tabularnewline
\(\theta_{00001010}\) & \(O_I\) & 0.05\tabularnewline
\(\theta_{00101010}\) & \(S_M, O_I\) & 0.05\tabularnewline
\(\theta_{10101010}\) & \(N\) & 0.05\tabularnewline
\(\theta_{00000011}\) & \(C_M\) & 0.05\tabularnewline
\(\theta_{00100011}\) & \(C_M, E_I\) & 0.05\tabularnewline
\(\theta_{00110011}\) & \(N\) & 0.05\tabularnewline
\(\theta_{00001011}\) & \(C_M, S_M, O_I\) & 0.05\tabularnewline
\(\theta_{00101011}\) & \(S_M, E_I, O_I\) & 0.05\tabularnewline
\(\theta_{10101011}\) & \(C_M, E_I, O_I\) & 0.05\tabularnewline
\(\theta_{00111011}\) & \(S_M, O_I\) & 0.05\tabularnewline
\(\theta_{10111011}\) & \(N\) & 0.05\tabularnewline
\(\theta_{00001111}\) & \(N\) & 0.05\tabularnewline
\(\theta_{00101111}\) & \(S_M, E_I\) & 0.05\tabularnewline
\(\theta_{10101111}\) & \(E_I\) & 0.05\tabularnewline
\(\theta_{00111111}\) & \(S_M\) & 0.05\tabularnewline
\(\theta_{10111111}\) & \(S_M, E_I\) & 0.05\tabularnewline
\(\theta_{11111111}\) & \(N\) & 0.05\tabularnewline
\bottomrule
\end{longtable}

Since \(P\) conditions the effect of \(I\), we must also establish a prior on the distribution of \(P\). In this analysis, we set the prior probability of \(P=1\) to 0.5, implying that before seeing the data we think that international pressures to democratize are present half the time.

\hypertarget{results}{%
\section{Results}\label{results}}

We can now choose nodes other than \(I\) or \(D\) to observe from the lower-level model. Recall that our query is about the joint values of \(\theta^M\) and \(\theta^{D_{lower}}\). By the logic \(d-\)separation, we can immediately see that both \(M\) and \(P\) may be informative about these nodes when \(D\) has already been observed. Conditional on \(D\), both \(M\) and \(P\) are \(d-\)connected to both \(\theta^M\) and \(\theta^{D_{lower}}\). Let us see what learn, then, if we search for either mobilization of the lower classes or international pressure or both, and find either clue either present or absent.

We consider four distinct situations, corresponding to four possible combinations of inequality and democratization values that we might be starting with. In each situation, the nature of the query changes. Where we start with a case with low inequality and no democratization, asking if inequality caused the outcome is to ask if the lack of inequality caused the lack of democratization. Where we have high inequality and no democratization, we want to know if democratization was prevented by high inequality (as high inequality does in Boix's account). For cases in which democratization occurred, we want to know whether the lack or presence of inequality (whichever was the case) generated the democratization.

Inference is done by applying Bayes rule to the observed data given the priors. Different ``causal types'' are consistent or inconsistent with possible data observations. Conversely the observation of data lets us shift weight towards causal types that are consistent with the data and away from those that are not. A case that displays \(D=1\) when \(P=M=I=0\) cannot be of type \(\theta_{00000000}\), thus observation of \(P=M=I=0, D=1\) would force a shift in weight away from this (and other) nodal types and onto nodal type \(\theta_{11111111}\) (and other compatible types) .

\hypertarget{pathways}{%
\section{Pathways}\label{pathways}}

FLAG: Do pathways as estimands

FLAG: ADD ESTIMAND: DOES INEQUALITY REDUCE THE EFFECT OF MOBILIZATION.

FLAG: ADD analysis with unobserved confounding.

\hypertarget{cases-with-incomplete-data}{%
\subsection{Cases with incomplete data}\label{cases-with-incomplete-data}}

We consider first causal relations for cases that did not democratize. These cases are not included in \citet{haggard2012inequality}.

The results for cases that did not democratize (at the time in question) are presented in tables \ref{tab:Tapp1} and Table \ref{tab:Tapp2}. Each table shows, for one kind of case, our posterior beliefs on the causal responsibility of \(I\) for the outcome for different search strategies.

\hypertarget{i0-d0-non-democracy-with-low-inequality}{%
\subsubsection{\texorpdfstring{\(I=0, D=0\): Non democracy with low inequality}{I=0, D=0: Non democracy with low inequality}}\label{i0-d0-non-democracy-with-low-inequality}}

To begin with \(I=0, D=0\) cases, did the lack of inequality cause the lack of democratization (as, for instance, at the lefthand end of the Acemoglu and Robinson inverted \(U\)-curve)?

\begin{table}[t]

\caption{\label{tab:Tapp1}No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.}
\centering
\begin{tabular}{l|r|r|r}
\hline
  & P & M & posterior\\
\hline
I0P0M0D0 & 0 & 0 & 0.107\\
\hline
I0P1M0D0 & 1 & 0 & 0.250\\
\hline
I0P0M1D0 & 0 & 1 & 0.000\\
\hline
I0P1M1D0 & 1 & 1 & 0.000\\
\hline
I0M0D0 & NA & 0 & 0.150\\
\hline
I0M1D0 & NA & 1 & 0.000\\
\hline
I0P0D0 & 0 & NA & 0.088\\
\hline
I0P1D0 & 1 & NA & 0.231\\
\hline
I0D0 & NA & NA & 0.128\\
\hline
\end{tabular}
\end{table}

We start out, based on the \(I\) and \(D\) values and our model, believing that there is a 0.107 chance that low inequality prevented democratization. We then see that our beliefs shift most dramatically if we go looking for mobilization and find that it was present. The reason is that any positive effect of \(I\) on \(D\) has to run through the pathway mediated by \(M\) because we have excluded a positive direct effect of \(I\) on \(D\) in our priors. Moreover, since we do not allow \(I\) to have a negative effect on \(M\), observing \(M=1\) when \(I=0\) must mean that \(I\) has no effect on \(M\) on this case, and thus \(I\) cannot have a positive effect on \(D\) (regardless also of what we find if we look for \(P\)). If we do \emph{not} observe mobilization when we look for it, we now think it is somewhat more likely that \(I=0\) caused \(D=0\) since it is still possible that high inequality \emph{could} cause mobilization.

We also see that observing whether there is international pressure has a substantial effect on our beliefs. When we observe \(M=1\) (or don't look for \(M\) at all), the presence of international pressure increases the likelihood that low inequality prevented democratization. Intuitively, this is because international pressure, on average across types, has a positive effect on democratization; so pressure's presence creates a greater opportunity for low inequality to counteract international pressure's effect and prevent democratization from occurring that otherwise would have (if there had been high inequality and the resulting mobilization).

\hypertarget{i1-d0-non-democracy-with-high-inequality}{%
\subsubsection{\texorpdfstring{\(I=1, D=0\): Non democracy with high inequality}{I=1, D=0: Non democracy with high inequality}}\label{i1-d0-non-democracy-with-high-inequality}}

In cases with high inequality and no democratization, the question is whether high inequality prevented democratization via a negative effect, as theorized by Boix. That negative effect has to have operated via inequality's direct effect on democratization since our monotonicity restrictions allow only positive effects via mobilization. Here, the consequence of observing \(P\) is similar to what we see in the \(I=0, D=0\) case: seeing international pressure greatly increases our confidence that high inequality prevented democratization, while seeing no international pressure moderately reduces that confidence. There is, returning to the same intuition, more opportunity for high inequality to exert a negative effect on democratization when international pressures are present, pushing toward democratization.

\begin{table}[t]

\caption{\label{tab:Tapp2}Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.}
\centering
\begin{tabular}{l|r|r|r}
\hline
  & P & M & posterior\\
\hline
I1P0M0D0 & 0 & 0 & 0.263\\
\hline
I1P1M0D0 & 1 & 0 & 0.571\\
\hline
I1P0M1D0 & 0 & 1 & 0.393\\
\hline
I1P1M1D0 & 1 & 1 & 0.667\\
\hline
I1M0D0 & NA & 0 & 0.394\\
\hline
I1M1D0 & NA & 1 & 0.475\\
\hline
I1P0D0 & 0 & NA & 0.340\\
\hline
I1P1D0 & 1 & NA & 0.615\\
\hline
I1D0 & NA & NA & 0.438\\
\hline
\end{tabular}
\end{table}

Here, however, looking for \(M\) has more modest effect than it does in an \(I=0, D=0\) case. This is because we learn less about the indirect pathway from \(I\) to \(D\) by observing \(M\): as we have said, we already know from seeing high inequality and no democratization (and under our monotonicity assumptions) that any effect could not have run through the presence or absence of mobilization.

However, \(M\) provides some information because it, like \(P\), acts as \emph{moderator} for \(I\)'s direct effect on \(D\) (since \(M\) is also pointing into \(D\)). As we know, learning about moderators tells us something about (a) the rules governing a case's response to its context (i.e., its response type) and (b) the context it is in. Thus, in the first instance, observing \(M\) together with \(I\) and \(D\) helps us eliminate types inconsistent with these three data points. For instance, if we see \(M=0\), then we eliminate any type in which \(D\) is 0, regardless of \(P\)'s value, when \(M=0\) and \(I=1\). Second, we learn from observing \(M\) about the value of \(M\) under which \(D\) will be responding to \(I\). Now, because \(M\) is itself potentially affected by \(I\), the learning here is somewhat complicated. What we learn most directly from observing \(M\) is \emph{the effect of \(I\) on \(M\)} in this case. If we observe \(M=1\), then we know that \(I\) has no effect on \(M\) in this case; whereas if we observe \(M=0\), \(I\) might or might not have a positive effect on \(M\). Learning about this \(I \rightarrow M\) effect then allows us to form a belief about how likely \(M\) would be to be 0 or 1 if \(I\) changed from \(0\) to \(1\); that is, it allows us to learn about the context under which \(D\) would be responding to this change in \(I\) (would mobilization be occurring or not)? This belief, in turn, allows us to form a belief about how \(D\) will respond to \(I\) given our posterior beliefs across the possible types that the case is.

The net effect, assuming that we have not observed \(P\), is a small upward effect in our confidence that inequality mattered if we see no mobilization, and a small downward effect if we see mobilization. Interestingly, if we \emph{do} observe \(P\), the effect of observing \(M\) reverses: observing mobilization increases our confidence in inequality's effect, while observing no mobilization reduces it.

\hypertarget{inferences-for-cases-with-observed-democratization}{%
\subsection{Inferences for cases with observed democratization}\label{inferences-for-cases-with-observed-democratization}}

We now turm to cases in which democratization has occurred---the category of cases that Haggard and Kaufman examine.

For these cases we use data from \citet{haggard2012inequality} to show the inferences we would draw using this procedure and the actual observations made for a set of 8 cases.

Haggard and Kaufman consider only cases that democratized, so all cases in this table have the value \(D=1\). We show here how confident we would be that the level inequality caused democratization if (a) we observed only the cause and effect (\(I\) and \(D\)); (b) we additionally observed either the level of mobilization by disadvantaged classes or the level of international pressure; and (c) if we observed both, in addition to \(I\) and \(D\). Note that countries labels are marked in the ``full data'' cells in the lower right quadrant, but their corresponding partial data cells can be read by moving to the left column or the top row (or to the top left cell for the case with no clue data).

In coding countries' level of inequality, we rely on Haggard and Kaufman's codings using the Gini coefficient from the Texas Inequality dataset. In selecting cases of democratization, we use the codings in \citet{cheibub2010democracy}, one of two measures used by Haggard and Kaufman. Our codings of the \(M\) and \(P\) clues come from close readings of the country-specific transition accounts in \citet{haggard2012distributive}, the publicly shared qualitative dataset associated with \citet{haggard2012inequality}. We code \(M\) as \(1\) where the transition account refers to anti-government or anti-regime political mobilization by economically disadvantaged groups, and as \(0\) otherwise. For \(P\), we code \(P=0\) is international pressures to democratize are not mentioned in the transition account. The main estimates refer to analyses with only qualitative, monotonicity restrictions on our priors. We also show in square brackets the estimates if we allow for a negative effect of inequality on mobilization but believe it to be relatively unlikely.

\hypertarget{i0-d1-low-inequality-democracies}{%
\subsubsection{\texorpdfstring{\(I=0, D=1\): Low inequality democracies}{I=0, D=1: Low inequality democracies}}\label{i0-d1-low-inequality-democracies}}

In a case that had low inequality and democratized, did low inequality cause democratization, as Boix's thesis would suggest? Looking at the first set of cases in Table \ref{tab:HK8cases1}, did Mexico, Albania, Taiwan, and Nicaragua democratize because they had relatively low inequality? Based only on observing the level of inequality and the outcome of democratization, we would place a 0 probability on inequality having been a cause. What can we learn, then, from our two clues?

We are looking here for a negative effect of \(I\) on \(D\), which in our model can only run via a direct effect, not through mobilization. Thus, the learning from \(M\) is limited for the same reason as in an \(I=1, D=0\) case. And \(M\) is modestly informative as a moderator for the same reasons and in the same direction, with observing mobilization generally reducing our confidence in inequality's negative effect relative to observing no mobilization. In our four cases, if we observe the level of mobilization, our confidence that inequality mattered goes up slightly (to 0) in Mexico and Taiwan, where mobilization did not occur, and goes down slightly in Albania and Nicaragua (to 0) where mobilization did occur.

\begin{table}[t]

\caption{\label{tab:HK8cases1}Four cases with low inequality and  democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.}
\centering
\begin{tabular}{l|l|l|l|l|l|l}
\hline
Case & M: Mobilization? & P: Pressure? & No clues & M only & P only & M and P\\
\hline
Mexico (2000) & 0 & 0 & 0.438 & 0.475 & 0.615 & 0.667\\
\hline
Taiwan (1996) & 0 & 1 & 0.438 & 0.475 & 0.34 & 0.393\\
\hline
Albania (1991) & 1 & 0 & 0.438 & 0.394 & 0.615 & 0.571\\
\hline
Nicaragua (1984) & 1 & 1 & 0.438 & 0.394 & 0.34 & 0.263\\
\hline
\end{tabular}
\end{table}

Looking for the international pressure clue is, however, highly informative, though the effect runs in the opposite direction as in an \(I=1, D=0\) case. It is observing the absence of international pressure that makes us more confident in low inequality's effect. Since democratization \emph{did} occur, the presence of international pressure makes it less likely for low inequality to have generated the outcome since international pressure could have generated democratization by itself. Once we bring this second clue into the analysis, Mexico and Taiwan sharply part ways: seeing no international pressure in Mexico, we are now much more confident that inequality mattered for the Mexican transition (1); seeing international pressure in Taiwan, we are now substantially less confident that inequality mattered to the Taiwanese transition (0). Similarly, observing \(P\) sharply differentiates the Albanian and Nicaraguan cases: seeing no international pressure in the Albanian transition considerably boosts our confidence in inequality's causal role there (0), while observing international pressure in the Nicaraguan transition strongly undermines our belief in an inequality effect there (1).

\hypertarget{i1-d1-high-inequality-democracies}{%
\subsubsection{\texorpdfstring{\(I=1, D=1\): High inequality democracies}{I=1, D=1: High inequality democracies}}\label{i1-d1-high-inequality-democracies}}

Where we see both high inequality and democratization, the question is whether high inequality caused democratization via a positive effect. Considering the second set of cases in Table \ref{tab:HK8cases2}, did high inequality cause Mongolia, Sierra Leone, Paraguay, and Malawi to democratize?

Observing only the level of inequality and the democratization outcome, we would have fairly low confidence that inequality mattered, with a posterior on that effect of 1. Let us see what we can learn if we also observe the level of mobilization and international pressure.

As in an \(I=0, D=0\) case, \(M\) can now be highly informative since this positive effect has to run through mobilization. Here it is the observation of a lack of mobilization that is most telling: high inequality cannot have caused democratization, given our model, if inequality did not cause mobilization to occur. There is no point in looking for international pressure since doing so will have no effect on our beliefs. Thus, when we observe no mobilization by the lower classes in Mongolia and Paraguay, we can be certain (given our model) that high inequality did \emph{not} cause democratization in these cases. Moreover, this result does not change if we also go and look for international pressure: neither seeing pressure nor seeing its absence shifts our posterior away from 1.

\begin{table}[t]

\caption{\label{tab:HK8cases2}Four cases with high inequality and  democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.}
\centering
\begin{tabular}{l|l|l|l|l|l|l}
\hline
Case & M: Mobilization? & P: Pressure? & No clues & M only & P only & M and P\\
\hline
Mongolia (1990) & 0 & 0 & 0.128 & 0 & 0.231 & 0\\
\hline
Paraguay (1989) & 0 & 1 & 0.128 & 0 & 0.088 & 0\\
\hline
Sierra Leone (1996) & 1 & 0 & 0.128 & 0.15 & 0.231 & 0.25\\
\hline
Malawi (1994) & 1 & 1 & 0.128 & 0.15 & 0.088 & 0.107\\
\hline
\end{tabular}
\end{table}

If we do see mobilization, on the other hand---as in Sierra Leone and Malawi---we are slightly more confident that high inequality was the cause of democratization (1). Moreover, if we first see \(M=1\), then observing international pressure can add much more information; and it substantially differentiates our conclusions about the causes of Sierra Leone's and Malawi's transitions. Just as in an \emph{\$I=0, D=1} case, it is the absence of international pressure that leaves the most ``space'' for inequality to have generated the democratization outcome. When we see the absence of pressure in Sierra Leone, our confidence that high inequality was a cause of the transition increases to 0; seeing pressure present in Malawi reduces our confidence in inequality's effect to 1.

\hypertarget{model-definition-and-inference-in-code}{%
\section{Model definition and inference in code}\label{model-definition-and-inference-in-code}}

How is this model defined and used in practice?

Using the \texttt{gbiqq} package we can set this model up, along with restrictions, in a few lines a follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }
\StringTok{  }\KeywordTok{make_model}\NormalTok{(}\StringTok{"I -> M -> D <- P; I -> D"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\StringTok{"(M[I=1] < M[I=0])"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\StringTok{"(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can then calculate conditional inferences as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{conditional_inferences}\NormalTok{(model, }
                       \DataTypeTok{query =} \StringTok{"D[I=0]==0"}\NormalTok{,}
                       \DataTypeTok{given =} \StringTok{"D==1 & I==1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{concluding-thoughts}{%
\section{Concluding thoughts}\label{concluding-thoughts}}

Haggard and Kaufman set out to use causal process observations to test inequality-based theories of democratization against the experiences of ``Third Wave'' democratizations. Their principal test is to examine whether they see evidence of distributive conflict in the process of democratization, defined largely as the presence or absence of mobilization prior to the transition. They secondarily look for other possible causes, specifically international pressure and splits in the elite.

In interpreting the evidence, Haggard and Kaufman generally treat the absence of mobilization as evidence against inequality-based theories of democratization as a whole (p.~7). They also see the \emph{presence} of distributive mobilization in cases with high inequality and democratization as evidence against the causal role of inequality (p.~7). These inferences, however, seem only loosely connected to the logic of the causal theories under examination. Haggard and Kaufman express concern that inequality-oriented arguments point to ``cross-cutting effects'' (p.~1) of inequality, but do not systematically work through the implications of these multiple pathways for empirical strategy. Our analysis suggests that a systematic engagement with the underlying models can shift that interpretation considerably. Under the model we have formulated, where inequality is \emph{high}, the absence of mobilization in a country that democratized is indeed damning to the notion that inequality mattered. However, where inequality is \emph{low}---precisely the situation in which Boix's theory predicts that we will see democratization---things are more complicated. If we assume that inequality cannot prevent mobilization, then observing no mobilization does not work against the claim that inequality mattered for the transition; indeed, it slightly supports it, at least given what we think is a plausible model-representation of arguments in the literature. Observing the absence of inequality in such a case, however, can undercut an inequality-based explanation if (and only if) we believe it is possible that inequality might prevent mobilization that would otherwise have occurred. Further, in cases with high inequality and democratization, it is the \emph{absence} of mobilization by the lower classes that would least consistent with the claim that inequality mattered. Observing mobilization, in contrast, pushes in favor of an inequality-based explanation.

Moreover, it is striking that Haggard and Kaufman lean principally on a mediator clue, turning to evidence of international pressure and elite splits (moderators, or alternative causes) largely as secondary clues to identify ``ambiguous'' cases. As we have shown, under a plausible model given prior theory, it is the moderator clue that is likely to be much more informative.

Of course, the model that we have written down is only one possible interpretation of existing theoretical knowledge. It is very possible that Haggard and Kaufman and other scholars in this domain hold beliefs that diverge from those encoded in our working model. The larger point, however, is that our process tracing inferences will inevitably \emph{depend}---and could depend greatly---on our background knowledge of the domain under examination. Moreover, formalizing that knowledge as causal model can help ensure that we are taking that prior knowledge systematically into account---that the inferences we draw from new data are consistent with the knowledge that we bring to the table.

The analysis also has insights regarding case selection. Haggard and Kaufman justify their choice of only \(D=1\) cases as a strategy ``designed to test a particular theory and thus rests on identification of the causal mechanism leading to regime change'' (p.~4). Ultimately, however, the authors seem centrally concerned with assessing whether inequality, as opposed to something else, played a key causal role in generating the outcome. As the results above demonstrate, however, there is nothing special about the \(D=1\) cases in generating leverage on this question. The tables for \(D=0\) show that, given the model, the same clues can shift beliefs about as much for \(D=0\) as for \(D=1\) cases. We leave a more detailed discussion of this kind of issue in model-based case-selection for Chapter REFERENCE.

Finally we emphasize that all of the inference in this chapter depends on a model that is constrained by theoretical insights but not one that is trained by data. Although we are able to make many inferences using this model, given the characteristics of a case of interest, we have no empirical grounds to justify these inferences. In Chapter REFERENCE we show how this model can be trained with broader data from multiple cases and in Chapter REFERENCE we illustrate how the model itself can be put into question.

\hypertarget{mixing}{%
\chapter{Integrated inferences}\label{mixing}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We argue that mixed methods can be thought of as the analysis of single cases with vector valued variables. Reconceptualizing as large n is useful primarily for computation reasons and often comes with hidden independence assumptions. We illustrate the single case approach and provide a set of models for the many case approach.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The main goal of this chapter is to generalize the model developed in Chapter \ref{pt} to research situations in which we have data on multiple cases. In doing so we generalize the model in \citet{humphreys2015mixing} to one that in which rather than the probative value of clues being \emph{assumed}, they are derived from a causal structure.

We start however with a conceptual point: the exact structure introduced in Chapter 6 for single case analysis can be used \emph{as is} for multi-case analysis. To see this you should think of the the nodes as vector-valued, and the estimands as just a particular summary of the vector-valued case level causal effects. Thought of this way the conceptual work for mixed methods inference from models has been done already and our goal here is more technical---how to exploit assumptions on independence across cases to generate simpler theories of repeated phenomena.

\hypertarget{theres-only-ever-one-case}{%
\section{There's only ever one case}\label{theres-only-ever-one-case}}

Conceptualized correctly, there is no difference at all between the data types or the inference used in within-case and cross-case inference. The reason is not, as \citet{king1994designing} suggest, that all causal inference is fundamentally correlational, even in seemingly single case studies. Nor is the point that, looked at carefully, single ``case studies'' can be disaggregated into many cases. The intuition runs in the opposite direction: fundamentally, model-based inference always involves comparing \emph{a} pattern of data with the logic of the model. Looked at carefully, studies with multiple cases can be conceptualized of as single-case studies: the drawing of inferences from a single collection of clues.

The key insight is that, when we move from a causal model with one observation to a causal model with multiple observations, all that we are doing is replacing nodes with a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors).

To illustrate the idea that multi-case studies are really single-case studies with vector valued variables, consider the following situation. There are two units studied, drawn from some population, a binary treatment \(X\) is assigned independently with probability .5 to each case; an outcome \(Y\) along with clue variable \(K\) is observable. We suppose \(X\) can affect \(Y\) and in addition there is a background, unobserved, variable \(\theta\) (causal type) that takes on values in \(\{a,b,c,d\}\), that affects both \(K\) and \(Y\). We will assume that \(\theta\) is not independently assigned and that the two units are more likely to have the same values of \(\theta\) than different values. For simplicity, we will suppose that for any given case \(K=1\) whenever \(X\) causes \(Y\), and \(K=1\) with a 50\% probability otherwise. Thus, \(K\) is informative about a unit's causal type.

Note that we have described the problem at the unit level. We can redescribe it at the population level however as a situation in which a treatment vector \(X\) can take on one of four values, \((0,0), (0,1), (1,0), (1,1)\) with equal probability (or more strictly: as determined by \(\theta\)). \(\theta\) is also a vector with two elements that can take on one of 16 values \((a,a), (a,b),\dots (d,d)\) as determined by \(U_\theta\). In this case we will assume that the 16 possibilities are not equally likely, which captures the failure of independence in the unit level assignments. \(Y\) is a vector that reflects the elements of \(\theta\) and \(X\) in the obvious way (e.g \(X=(0,0), \theta=(a,b)\) generates outcomes \(Y=(1,0)\); though it is immediately obvious that representing nodes in vector forms allows for more general vector-level mappings to allow for SUTVA violations. \(K\) has the same domain as \(X\) and \(Y\), and element \(K[j]=1\) if \(\theta[j]=b\).

Note that to describe the estimand, the Sample Average Treatment Effect, we also need to consider operations and queries defined at the vector level. In practice we consider three operations, one in which both units have \(X\) forced to 0 and two in which one unit has \(X\) set to 0 and the other has \(X\) set to 1. Thus we are interested in the average effect of changing one unit to treatment while the other is held in control. Note also that before our estimands were binary---of the form: is it a \(b\) type?--and our answer was a probability; now our estimand is categorical and our answer is a distribution (what is the probability the SATE is 0, what is the probability the SATE is .5, etc\ldots{})

Represented in this way we can use the tools of Chapters 6 and 7 to fully examine this seemingly multi-case study. In the below we examine a situation in which we consider the value of observing \(K\) on one case --- in this set up this is equivalent to observing part of the vector \(K\) and making inferences on the full vector \(\theta\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gbiqq)}

\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X->Y"}\NormalTok{)}

\NormalTok{fit <-}\StringTok{ }\KeywordTok{fitted_model}\NormalTok{()}

\NormalTok{updated_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{gbiqq}\NormalTok{(model, }\DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X=}\DecValTok{1}\NormalTok{, }\DataTypeTok{Y=}\DecValTok{1}\NormalTok{), }
                                     \DataTypeTok{chains =} \DecValTok{10}\NormalTok{, }\DataTypeTok{iter =} \DecValTok{12000}\NormalTok{, }\DataTypeTok{stan_model =}\NormalTok{ fit)}

\NormalTok{updated_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{gbiqq}\NormalTok{(model, }\DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{Y=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)), }
                                     \DataTypeTok{chains =} \DecValTok{10}\NormalTok{, }\DataTypeTok{iter =} \DecValTok{12000}\NormalTok{, }\DataTypeTok{stan_model =}\NormalTok{ fit)}

\NormalTok{updated_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{gbiqq}\NormalTok{(model, }\DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X=}\DecValTok{1}\NormalTok{, }\DataTypeTok{Y=}\OtherTok{NA}\NormalTok{), }
                                     \DataTypeTok{chains =} \DecValTok{10}\NormalTok{, }\DataTypeTok{iter =} \DecValTok{12000}\NormalTok{, }\DataTypeTok{stan_model =}\NormalTok{ fit)}

\CommentTok{# query_model(query = "Y[X=1] - Y[X=0]", )}

\KeywordTok{query_model}\NormalTok{(updated_}\DecValTok{1}\NormalTok{, }
                        \DataTypeTok{query =} \KeywordTok{te}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{), }
                        \DataTypeTok{subsets =} \KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{), }
                        \DataTypeTok{using =} \KeywordTok{c}\NormalTok{(}\StringTok{"parameters"}\NormalTok{, }\StringTok{"posteriors"}\NormalTok{), }
                        \DataTypeTok{expand_grid =} \OtherTok{TRUE}\NormalTok{ )}

\KeywordTok{query_model}\NormalTok{(updated_}\DecValTok{2}\NormalTok{, }
                        \DataTypeTok{query =} \KeywordTok{te}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{), }
                        \DataTypeTok{subsets =} \KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{), }
                        \DataTypeTok{using =} \KeywordTok{c}\NormalTok{(}\StringTok{"parameters"}\NormalTok{, }\StringTok{"posteriors"}\NormalTok{), }
                        \DataTypeTok{expand_grid =} \OtherTok{TRUE}\NormalTok{ )}

\KeywordTok{query_model}\NormalTok{(updated_}\DecValTok{3}\NormalTok{, }
                        \DataTypeTok{query =} \KeywordTok{te}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{), }
                        \DataTypeTok{subsets =} \KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{), }
                        \DataTypeTok{using =} \KeywordTok{c}\NormalTok{(}\StringTok{"parameters"}\NormalTok{, }\StringTok{"posteriors"}\NormalTok{), }
                        \DataTypeTok{expand_grid =} \OtherTok{TRUE}\NormalTok{ )}


\CommentTok{# IMPORTANT TO DISTINGUISH BETWEEN:}

\CommentTok{# what is the chance that X caused Y given X=1, Y=1 and}
\CommentTok{# what is the chance that X causes Y given you have seen a case in which X=1, Y=1 and}
\CommentTok{# Only data that your query has not conditioned on is informative (is dta conditionnally independent of teh query)}
\end{Highlighting}
\end{Shaded}

\hypertarget{general-procedure}{%
\section{General procedure}\label{general-procedure}}

In practice however thinking of nodes as capturing the outcomes on all units leads to enormous complexity. For example an exogeneous variable \(X\) which takes on values of 0 or 1 at random for 10 units has \(2^{10}\) types in this conceptualization, rather than just two when thought of at the case level.

We reduce complexity however by thinking of models as operating on units and learning about models by observing \emph{multiple} realizations of processes covered by the model, rather than just one. Thinking about it this way is not free however as it requires invoking some kind of independence assumptions --- that outcomes in two units do not depend on each other. If we cannot stand by that assumption, then we have to build independence failures into our models.

With multiple cases we\ldots{}

\textbf{A DAG}. As for process tracing, we begin with a graphical causal model specifying possible causal linkages between nodes.

\textbf{Nodal types}. Just as in process tracing, the DAG and variable ranges define the set of possible nodal types in the model---the possible ways in which each variable is assigned (if exogenous) or determined by its parents (if endogenous).

\textbf{Causal types}. And, again, a full set of nodal types gives rise to a full set of causal types, encompassing all possible combinations of nodal types across all nodes in the model.

\textbf{Priors}. The first difference between single- and multiple-case inference lies in how we set priors on causal types. In process tracing, we set parameter values for each nodal type (or conditional nodal type, for unobserved confounding). Our parameters---e.g., \(\lambda^X_0\), \(\lambda^Y_{01}\)---represent our beliefs about the proportions of these types in the population. When we only observe a single data type---data on a single case---we do not have sufficient information to learn about the distribution of types in the population. And so we treat these population-level beliefs as fixed parameters, rather than priors that we update on. (What we update on, in process tracing, is our priors on whether a \emph{given case} is of a particular type or set of types.) Likewise, uncertainty about those population-level parameters has no effect on our inferences for a single case.

When we get to observe data on multiple cases, however, we have the opportunity to learn \emph{both} about the cases at hand \emph{and} about the population. Moreover, our level of uncertainty about population-level parameters will shape our inferences. We thus want our parameters (the \(\lambda\)'s) to be drawn from a prior \emph{distribution} --- a distribution that expresses our uncertainty and over which we can update once we see the data.

While different distributions may be appropriate to the task in general, uncertainty over proportions (of cases, events, etc.) falling into a set of discrete categories is described by a Dirichlet distribution, as discussed in Chapter \ref{bayeschapter}. As will be recalled, the parameters of a Dirichlet distribution (the \(\alpha\)'s) can be thought of as conveying both the relative expected proportions in each category and our degree of uncertainty.

To first examine a situation with no observed confounding, we need to specify a prior distribution for each set of nodal types. For a simple \(X \rightarrow Y\) model, we have two parameter sets: one for \(X\)'s types and one for \(Y\)'s types. For \(X\)'s types, we specify \(\alpha^X_0\) and \(\alpha^X_1\), corresponding to the nodal types \(\theta^X_0\) and \(\theta^X_1\), respectively. For \(Y\)'s types, we specify \(\alpha^Y_{00}\), \(\alpha^Y_{10}\), \(\alpha^Y_{01}\), and \(\alpha^Y_{11}\), corresponding to the nodal types \(\theta^Y_{00}\), etc. So, for instance, setting \(\alpha^Y_{00}=1\), \(\alpha^Y_{10}=1\), \(\alpha^Y_{01}=1\), and \(\alpha^Y_{11}=1\) yields a uniform distribution in which all share allocations of types in the population are equally likely. Setting \(\alpha^Y_{00}=3\), \(\alpha^Y_{10}=3\), \(\alpha^Y_{01}=3\), and \(\alpha^Y_{11}=3\) puts more weight on share allocations in which the shares are relatively equal. Setting \(\alpha^Y_{00}=5\), \(\alpha^Y_{10}=5\), \(\alpha^Y_{01}=10\), and \(\alpha^Y_{11}=5\) puts greater weight positive causal effects than the other three types. And we can express greater certainty about that weighting by setting higher absolute alpha values, such as \(\alpha^Y_{00}=50\), \(\alpha^Y_{10}=50\), \(\alpha^Y_{01}=100\), \(\alpha^Y_{11}=5\).

\textbf{NEED A DISCUSSION OF RESTRICTIONS APPROACH TO SETTING PRIORS}

A specified parameter set for node \(j\), then, allows for a range of possible allocations of nodal types in the population, or \(\lambda^j\)'s while making some \(\lambda^j\)'s more likely than others. A set of \(\alpha^j\)'s, in other words, gives us a prior distribution over nodal types at node \(j\).

Thus, under the parameter set for \(Y\) (\(\alpha^Y_{00}=3\), \(\alpha^Y_{10}=3\), \(\alpha^Y_{01}=3\), \(\alpha^Y_{11}=3\)), a relatively equal allocation like \(\lambda^Y_{00}=0.26\), \(\lambda^Y_{10}=0.24\), \(\lambda^Y_{01}=0.2\), \(\lambda^Y_{11}=0.3\) will have a higher probability than a highly skewed allocation like \(\lambda^Y_{00}=0.1\), \(\lambda^Y_{10}=0.1\), \(\lambda^Y_{01}=0.7\), \(\lambda^Y_{11}=0.1\). Likewise, \(\alpha^Y_{00}=5\), \(\alpha^Y_{10}=5\), \(\alpha^Y_{01}=10\), \(\alpha^Y_{11}=5\) puts more prior weight on any \(\lambda^Y\) with relatively more positive effects and an even distributon among other types than on a \(\lambda^Y\) with an even spread across all types or than one skewed towards negative effects. Meanwhile, the tighter distribution given by (\(\alpha^Y_{00}=50\), \(\alpha^Y_{10}=50\), \(\alpha^Y_{01}=100\), \(\alpha^Y_{11}=5\)) implies even steeper differences in those probabilities.

For a model with any number of nodes, we can then imagine a draw of one \(\lambda^j\) from its prior distribution for each node, giving a full \(\lambda\) vector. Any particular \(\lambda\) vector, in turn, implies a probability distribution over \emph{causal} types (\(\theta\)). With the help of a parameter matrix (mapping from parameters to causal types), we can then, just as with process tracing, calculate the prior probability that a case is of any particular causal type, given the parameter (\(\lambda\)) values we have drawn. Implicitly, then, our prior distribution over \(\lambda\) gives rise in turn to a prior distribution over the causal type shares in the population.

Where there is unobserved confounding, we now need parameter sets corresponding to the correlated types, as with the setup for process tracing. Thus, if we believe the likelihood of \(X=1\) is correlated with whether or not \(X\) has a positive effect on \(Y\), we will need two parameter sets (rather than one) for \(X\): one for \(X\)'s value when \(\theta^Y = \theta^Y_{01}\) and one for \(X\)'s value when \(\theta^Y \neq \theta^Y_{01}\). For each of these parameter sets, we specify two \(\alpha\) parameters representing our beliefs about \(X\)'s assignment. We can draw \(\lambda\) values for these conditional nodal types from the resulting Dirichlet distributions, as above, and can then calculate causal type probabilities in the usual way.

\textbf{Event probabilities}. We now need to build a likelihood function that can map from beliefs about the world to data: i.e., that can tell us how likely we are to see a given data pattern---across multiple cases---under a given distribution of causal types in the population. The first step in building the likelihood function is to calculate event probabilities: the probability of observing a case of a particular data type given a particular population-level distribution of causal type shares (that is, given a \(\lambda\) draw). We assume, for now, that we deploy the same data strategy for each case, collecting data on all nodes.

We denote an event probability for a given data pattern for variables \(X, Y, \dots\) as \(w_{x, y, \dots}\). For instance, the probability of observing \(X=0, Y=1\) in a case (given \(\lambda\)) is \(w_{01}\). An ambiguity matrix, just as for process tracing, tells us which causal types are consistent with a particular data type, as observed for a single case. To calculate the probability of the data given a distribution of causal types, we simply add together the probabilities of all of the causal types with which it is consistent.

See, for instance, the parameter matrix and the ambiguity matrix in Tables \ref{tab:parammmatrixmix} and \ref{tab:ambigmatrixmix}. We have indicated a single draw of \(\lambda\) values (population type shares) in the parameter matrix, and these have been used to calculate the priors on causal types provided in the ambiguity matrix. Let's now calculate the event probability for each data type. Starting with \(X=0, Y=0\), we can read off the ambiguity matrix that the consistent causal types are (\(\theta^X_0, \theta^Y_{00}\)) and (\(\theta^X_0, \theta^Y_{01}\)). The event probability, \(w_{00}\), is then given by adding together the probabilities of these two causal types, \(0.1 + 0.2 = 0.3\). All four event probabilities, for the four data types, are then calculated in the same way:

\begin{itemize}
\tightlist
\item
  \(w_{00} = 0.1 + 0.2 = 0.3\)
\item
  \(w_{10} = 0.1 + 0.1 = 0.2\)
\item
  \(w_{01} = 0.1 + 0.2 = 0.2\)
\item
  \(w_{11} = 0.2 + 0.1 = 0.3\)
\end{itemize}

As any case must be of one and only one data type, the full set of event probabilities for a single \(\lambda\) draw must naturally sum to \(1\).

\begin{table}[t]

\caption{\label{tab:parammmatrixmix}A parameter matrix for a simple $X 
ightarrow Y$ model (with no unobserved confounding), indicating a single draw of $\lambda$ values from the prior distribution.}
\centering
\begin{tabular}{l|r|r|r|r|r|r|r|r|l|r|l|l|l|r|r}
\hline
  & X0.Y00 & X1.Y00 & X0.Y10 & X1.Y10 & X0.Y01 & X1.Y01 & X0.Y11 & X1.Y11 & Shares.param\_names & Shares.param\_value & Shares.param\_set & Shares.node & Shares.nodal\_type & Shares.gen & Shares.priors\\
\hline
X.0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & X.0 & 0.4 & X & X & 0 & 1 & 1\\
\hline
X.1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & X.1 & 0.6 & X & X & 1 & 1 & 1\\
\hline
Y.00 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & Y.00 & 0.3 & Y & Y & 00 & 2 & 1\\
\hline
Y.10 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & Y.10 & 0.2 & Y & Y & 10 & 2 & 1\\
\hline
Y.01 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & Y.01 & 0.2 & Y & Y & 01 & 2 & 1\\
\hline
Y.11 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & Y.11 & 0.3 & Y & Y & 11 & 2 & 1\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:ambigmatrixmix}An ambiguity matrix for a simple $X 
ightarrow Y$ model (with no unobserved confounding), showing the priors over causal types arising from a single draw of $\lambda$ from its prior distribution.}
\centering
\begin{tabular}{l|r|r|r|r|r}
\hline
  & X0Y0 & X1Y0 & X0Y1 & X1Y1 & prior\\
\hline
X0Y00 & 1 & 0 & 0 & 0 & 0.12\\
\hline
X1Y00 & 0 & 1 & 0 & 0 & 0.18\\
\hline
X0Y10 & 0 & 0 & 1 & 0 & 0.08\\
\hline
X1Y10 & 0 & 1 & 0 & 0 & 0.12\\
\hline
X0Y01 & 1 & 0 & 0 & 0 & 0.08\\
\hline
X1Y01 & 0 & 0 & 0 & 1 & 0.12\\
\hline
X0Y11 & 0 & 0 & 1 & 0 & 0.12\\
\hline
X1Y11 & 0 & 0 & 0 & 1 & 0.18\\
\hline
\end{tabular}
\end{table}

For a case in which only partial data are observed, we follow the same basic logic as with partial process-tracing data. We retain all columns (data types) in the ambiguity matrix that are consistent with the partial data. So, for instance, if we observe only \(Y=1\), we would retain both the \(X=0, Y=1\) column and the \(X=1, Y=1\) column. We then calculate the event probability by summing causal-type probabilities for all causal types that could have produced these partial data --- i.e., all those with a \(1\) in \emph{either} column.

\textbf{Likelihood}. Now that we know the probability of observing each data pattern in a \emph{single} case given \(\lambda\), we can use these event probabilities to aggregate up to the likelihood of observing a data pattern across multiple cases (given \(\lambda\)). With discrete variables, we can think of a given multiple-case data pattern simply as a set of counts: for, say, \(X, Y\) data, we will observe a certain number of \(X=0, Y=0\) cases (\(n_{00}\)), a certain number of \(X=1, Y=0\) cases (\(n_{10}\)), a certain number of \(X=0, Y=1\) cases (\(n_{01}\)), and a certain number of \(X=1, Y=1\) cases (\(n_{11}\)). A data pattern, given a particular set of variables observed (a search strategy), thus has a multinomial distribution. The likelihood of a data pattern under a given search strategy, in turn, takes the form of a multinomial distribution conditional on the number of cases observed and the event probabilities for each data type, given a \(\lambda\) draw.

Let us assume now that we have a 3-node model, with \(X, Y\), and \(M\) all binary. Let \(n_{XYK}\) denote an 8-element vector recording the number of cases in a sample displaying each possible combination of \(X,Y,K\) data, thus: \(n_{XYM}=(n_{000},n_{001},n_{100},\dots ,n_{111})\). The elements of \(n_{XYK}\) sum to \(n\), the total number of cases studied. Likewise, let the event probabilities for data types given \(\lambda\) be registered in a vector, \(w_{XYK}=(w_{000},w_{001},w_{100},\dots ,w_{111})\). The likelihood of a data pattern, \(\mathcal D\) is then:

\[
\Pr(\mathcal{D}|\lambda) = 
  \text{Multinom}\left(n_{XYK}|n, w_{XYK}\right)  \\
\]
In other words, the likelihood of observing a particular data pattern given \(\lambda\) is given by the corresponding value of the multinomial distribution given the event probabilities.

What if we have a mixture of search strategies? Suppose, for instance, that we have collected \(X,Y\) data on a set of cases, and that we have additionally collected data on \(M\) for a random subset of these. We can think of this as conducting quantitative analysis on a large sample and conducting in-depth process tracing on a subsample. We then can summarize our data in two vectors, the 8-element \(n_{XYM}\) vector for the cases with process tracing, and a 4-element vector \(n_{XY*} = (n_{00*},n_{10*},n_{01*},n_{11*}\) for the partial data on those cases with no process tracing. Likewise, we now have two sets of event probabilities: one for the cases with complete data, \(w_{XYM}\), and a 4-element vector for those with partial data, \(w_{XY*}\). Let \(n\) denote the total number of cases examined, and \(k\) the number for which we have data on \(K\).

Now, assuming that each observed case represents an independent, random draw from the population, we can form the likelihood function as a product of multinomial distributions:

\[
\Pr(\mathcal{D}|\theta) = 
  \text{Multinom}\left(n_{XY*}|n-k, w_{XY*}\right) \times \text{Multinom}\left(n_{XYK}|k, w_{XYK}\right)  \\
\]

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

Say a data strategy seeks data on \(X\) and \(Y\) in 2 cases and seeks data on \(K\) if ever \(X=Y=1\).

The probability of each data type is as given in table below:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.24\columnwidth}\raggedright
type:\strut
\end{minipage} & \begin{minipage}[b]{0.70\columnwidth}\raggedright
prob:\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\(X0Y0\)\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
\(\lambda^X_0(\lambda^Y_{00}+\lambda^Y_{01}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\(X0Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
\(\lambda^X_0(\lambda^Y_{11}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\(X1Y0\)\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
\(\lambda^X_1(\lambda^Y_{00}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\(X1M0Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
\(\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{11}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\(X1M1Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
\(\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01}))\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The two observations can be thought of as a multinomial draw from these five event types.

Alternatively they can also be thought of as the product of a draw from a strategy in which a set of units is drawn with observations on \(X,Y\) only and another set is drawn with observations on \(X, M,Y\).

In the single multinomial view we have the probability of seeing data with \(X=Y=0\) in one case and \(X=1, M=0, Y=1\) in another is:

\begin{itemize}
\tightlist
\item
  \(2P(X=0, Y=0)P(X=1, M=0, Y=1)\)
\end{itemize}

In the conditional strategy view we have

\begin{itemize}
\tightlist
\item
  \(2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\)
\end{itemize}

In the two strategy view we have

\begin{itemize}
\tightlist
\item
  \(P(X=0, Y=0)P(X=1, M=0, Y=1)\)
\end{itemize}

which is the same up to a constant.

Say rather than conditioning \(X=Y=1\) to examine \(M\) one of the two cases were chosen at random to observe \(M\) and it just so happend to be be a case with \(X=Y=1\):

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.11\columnwidth}\raggedright
type:\strut
\end{minipage} & \begin{minipage}[b]{0.84\columnwidth}\raggedright
prob:\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X0Y0\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(.5\lambda^X_0(\lambda^Y_{00}+\lambda^Y_{01}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X0Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(.5\lambda^X_0(\lambda^Y_{11}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X1Y0\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(.5\lambda^X_1(\lambda^Y_{00}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X1Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(.5\lambda^X_1(\lambda^Y_{11}+\lambda^Y_{01}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X0M0Y0\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(.5\lambda^X_0(\lambda^M_{00}+\lambda^M_{01}))(\lambda^Y_{00}+\lambda^Y_{01}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X0M1Y0\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(.5\lambda^X_0(\lambda^M_{11}+\lambda^M_{10}))(\lambda^Y_{00}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\ldots{}\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X1M0Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{11}+\lambda^Y_{10}))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\(X1M1Y1\)\strut
\end{minipage} & \begin{minipage}[t]{0.84\columnwidth}\raggedright
\(\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01}))\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In the single multinomial view we have the probability of seeing data with \(X=Y=0\) in one case and \(X=1, M=0, Y=1\) in another is now:

\begin{itemize}
\tightlist
\item
  \(2P(X=0, Y=0)P(X=1, M=0, Y=1)\)
\end{itemize}

In the conditional strategy view we have

\begin{itemize}
\tightlist
\item
  \(2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\)
\end{itemize}

In the two strategy view we have

\begin{itemize}
\tightlist
\item
  \(P(X=0, Y=0)P(X=1, M=0, Y=1)\)
\end{itemize}

which is the same up to a constant.

\hypertarget{illustration}{%
\section{Illustration}\label{illustration}}

Consider a generalization of the models introduced in Chapter 6 in which a treatment \(X\) is a cause of both \(K\) and \(Y\), and outcome \(Y\) is a product of both \(X\) and \(K\). Though \(K\) is both a mediator and a moderator for the effect of \(X\). There are now 16 nodal types for \(Y\), 4 for \(K\) and 2 for \(X\), yielding 32 causal types.

To allow for the possibility of non-random selection of \(X\) we will assume that the assignment probability for \(X\) depends on \(U^Y\). This is a feature shared also in the baseline model when we specify \(\pi\) as a function of types \(a\),\(b\),\(c\),\(d\).

Our piors requires specifying:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A distribution over the 15-dimensional simplex representing possible values of \(\lambda^Y\)--which in turn determine types \(u^Y\).
\item
  A distribution over the 3-dimensional vector representing possible values of \(\lambda^K\), which in turn determine types \(u^K\).
\end{enumerate}

The model is restricted in various ways. We assume now confounding in the assignemnt of \(X\). Less obviously we implicitly assume that \(K\) is independent of \(\theta^Y\) conditional on \(X\).

With these elements in hand, however, all we need now is to provide a mapping from these fundamental parameters to the parameters used in the baseline model to form the likelihood.

The key transformation is the identification of causal types resulting from the 64 combinations of \(\lambda^Y\) and \(\lambda^K\). These are shown below.

TABLE TO SHOW CAUSAL TYPES

Consider the following matrices of values for \(u_Y\) and \(u_K\), where \(\lambda_{pq}^{rs}\) is the probability that \(u^Y = t_{pq}^{rs}\), meaning that \(Y\) would take the value \(p\) when \(X=0, K=0\), \(q\) when \(X=0, K=1\), \(r\) when \(X=1, K=0\), and \(s\) when \(X=1, K=1\). Similarly \(\lambda_{w}^{z}\) is the probability that \(u^K\) takes value \(t_{w}^{z}\) meaning that \(K\) takes the value \(w\) when \(X=0\) and \(z\) when \(X=1\).

TABLE TO SHOW CONDITIONAL PROBABILITIES OF K GIVEN X=1 AND TYPE

These types are the \emph{transformed parameters}; the probability of a type is just the sum of the probabilities of the fundamental types that compose it, formed by taking the product of the \(\lambda^Y\) and \(\lambda^K\) values marked in the rows and columns of table \ref{tab:types}.

Similarly \(\phi_{tx}\) can be constructed as the probability of observing \(K\) conditional on this type (again, sums of products of probabilities associated with cells in table \ref{tab:types}). For instance, using the row and column indices in exponents (GIVE FULL LABELS) from table \ref{tab:types}:

\[\phi_{b1}=\frac{\lambda_K^2(\lambda_Y^2+\lambda_Y^4+\lambda_Y^6+\lambda_Y^8)+\lambda_K^4(\lambda_Y^2+\lambda_Y^4+\lambda_Y^{10}+\lambda_Y^{12})}{
\lambda_K^1(\lambda_Y^3+\lambda_Y^4+\lambda_Y^7+\lambda_Y^8)+\lambda_K^2(\lambda_Y^2+\lambda_Y^4+\lambda_Y^6+\lambda_Y^8)+\lambda_K^3(\lambda_Y^3+\lambda_Y^4+\lambda_Y^11+\lambda_Y^{12})+\lambda_K^4(\lambda_Y^2+\lambda_Y^4+\lambda_Y^{10}+\lambda_Y^{12})}\]

With these transformed parameters in hand, the likelihood is exactly the same as that specified in the baseline model.

\hypertarget{illustrated-inferences}{%
\section{Illustrated inferences}\label{illustrated-inferences}}

\hypertarget{xy-model}{%
\subsection{XY model}\label{xy-model}}

Consider the simple model in which \(X\) causes \(Y\) without confounding.

Assuming flat priors on types, what inferences do we draw from different sorts of (small) datasets. Do we learn more about effects from two cases that are the same, two cases that differ on X and Y only or two cases that differ on both.

The results are given in table \ref{tab:XYresultstable}.

\begin{table}[t]

\caption{\label{tab:XYresultstable}Inferences for different data observations in a simple X->Y model}
\centering
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
data & a & b & c & d & ate & bd & bc\\
\hline
00 & 0.20 & 0.30 & 0.31 & 0.20 & 0.09 & 1.51 & 0.96\\
\hline
01 & 0.30 & 0.20 & 0.19 & 0.31 & -0.10 & 0.65 & 1.04\\
\hline
11 & 0.20 & 0.30 & 0.20 & 0.31 & 0.09 & 0.97 & 1.52\\
\hline
01, 01 & 0.34 & 0.16 & 0.17 & 0.34 & -0.17 & 0.49 & 0.98\\
\hline
11, 11 & 0.17 & 0.34 & 0.17 & 0.33 & 0.17 & 1.03 & 2.02\\
\hline
01, 11 & 0.23 & 0.23 & 0.16 & 0.37 & 0.00 & 0.62 & 1.41\\
\hline
10, 11 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 & 0.98 & 1.00\\
\hline
00, 11 & 0.17 & 0.37 & 0.23 & 0.23 & 0.20 & 1.58 & 1.64\\
\hline
11, 11, 11 & 0.14 & 0.36 & 0.14 & 0.36 & 0.22 & 1.01 & 2.65\\
\hline
\end{tabular}
\end{table}

We note a number of features:

\begin{itemize}
\tightlist
\item
  \(X=1, Y=1\) data does not discriminate between \(\theta^Y_{01}\) and \(\theta^Y_{11}\) and so while more of this data puts greater weight on both \(\lambda^Y_{01}\) and \(\lambda^Y_{11}\), it does nothing to discriminate between them.
\item
  Similarly \(X=0, Y=0\) data does not discriminate between \(\theta^Y_{01}\) and \(\theta^Y_{00}\) though it puts greater weight (uniformly) on \(\lambda^Y_{01}\) and \(\lambda^Y_{00}\),
\item
  For this reason, greatest weight is placed on \(\theta^Y_{01}\) when data on both \(X=Y=0\) and \(X=Y=1\) cases are found.
\item
  The fractions suggest a common formula:
\end{itemize}

\[\lambda^Y|n_{xy} \sim Dirichlet\left(1+\frac{n_{01} + n_{10}}2, 1+\frac{n_{00} + n_{11}}2, 1+\frac{n_{00} + n_{10}}2, 1+\frac{n_{01} + n_{11}}2\right)\]

Posterior mean on ATE is then \(\frac{n_{00} + n_{11} - n_{01} - n_{10}}n\).

\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
data & a & b & c & d & ate & bd & bc\\
\hline
111 & 0.12 & 0.13 & 0.32 & 0.43 & 0.01 & 0.31 & 0.41\\
\hline
111, 111 & 0.11 & 0.14 & 0.28 & 0.48 & 0.03 & 0.29 & 0.50\\
\hline
000, 111 & 0.12 & 0.16 & 0.36 & 0.36 & 0.04 & 0.45 & 0.44\\
\hline
111, 111, 111 & 0.10 & 0.15 & 0.25 & 0.50 & 0.05 & 0.29 & 0.60\\
\hline
\end{tabular}

\hypertarget{considerations}{%
\section{Considerations}\label{considerations}}

\hypertarget{the-identification-problem}{%
\subsection{The identification problem}\label{the-identification-problem}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X1 -> M1 -> Y <- M2 <- X2"}\NormalTok{)}

\CommentTok{# restrict such that *only* M1 OR M2 could cause Y -- can we create a DD test? / achieve identification}
\end{Highlighting}
\end{Shaded}

\hypertarget{continuous-data}{%
\subsection{Continuous data}\label{continuous-data}}

We can similarly shift from binary to continuous variable values through an expansion of the causal types. Suppose that \(Y\) can take on \(m\) possible values. With \(k\) explanatory variables, each taking on \(r\) possible values, we then have \(m^{r^k}\) causal types and, correspondingly, very many more elements in \(\phi\). Naturally, in such situations, researchers might want to reduce complexity by placing structure onto the possible patterns of causal effects and clue probabilities, such as assuming a monotonic function linking effect sizes and clue probabilities.

\hypertarget{measurement-error}{%
\subsection{Measurement error}\label{measurement-error}}

We have assumed no measurement error; in applications there could be considerable interest in measurement error. On one hand clue information may contain information about possible mismeasurement on \(X\) and \(Y\); on the other hand there might interest in whether measured clues adequately capture those features of a causal process that is thought to be measureable.

The probability of different types of measurement error can be included among the set of parameters of interest, with likelihood functions adjusted accordingly. Suppose, for instance, that with probability \(\epsilon\) a \(Y=0\) case is recorded as a \(Y=1\) case (and vice versa). Then the event probability of observing an \(X=1\),\(Y=1\) case, for example, is \(\epsilon \lambda_a \pi_a + (1-\epsilon) \lambda_b \pi_b + \epsilon \lambda_c \pi_c + (1-\epsilon) \lambda_d \pi_d\). \%If instead there were measurement error on \(X\) but not on \(Y\), then the event probability would be: \(\epsilon \lambda_a (1-\pi_a) + (1-\epsilon) \lambda_b \pi_b + \epsilon \lambda_d (1-\pi_d) + (1-\epsilon) \lambda_d \pi_d\).
Similar expressions can be derived for measurement error on \(X\) or \(K\). Specifying the problem in this way allows us both to take account of measurement error and learn about it.

\hypertarget{spillovers}{%
\subsection{Spillovers}\label{spillovers}}

Spillovers may also be addressed through an appropriate definition of causal types. For example a unit \(i\) that is affected either by receiving treatment or via the treatment of a neighbor, \(j\), might have potential outcomes \(Y_i(X_i,X_j)=\max(X_i,X_j)\) while another type that is not influenced by neighbor treatment status has \(Y_i(X_i,X_j)=\max(X_i)\). With such a set-up, relevant clue information might discriminate between units affected by spillovers and those unaffected.

\hypertarget{clustering-and-other-violations-of-independence}{%
\subsection{Clustering and other violations of independence}\label{clustering-and-other-violations-of-independence}}

\hypertarget{parameteric-models}{%
\subsection{Parameteric models}\label{parameteric-models}}

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

ADD REFERENCE TO TABLE 1 OF FOR MIXED DATA ``Ability and Achievement'' Otis Duncan

\hypertarget{mixingapp}{%
\chapter{Mixed-Method Application: Inequality and Democracy Revisited}\label{mixingapp}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In chapter 7 we drew inferences from a `theory based' democracy and inequality model on data. Here we train the model on data before making case level inferences, allowing for the possibility of confounding in the assignment of inequality. In this case we update our beliefs over the population parameters and not just over the case level parameters. Thus we simultaneously learn about our theory and use our theory to learn about cases. The data informed inferences are, on the whole, weaker than the theory based inferences of Chapter 7.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

FLAG: Change causal types to unit types everywhere.

\hypertarget{a-trained-model}{%
\section{A trained model}\label{a-trained-model}}

We now apply these ideas on mixed method research to our model of democracy and inequality. The key difference to the exercise in in Chapter 7 is that whereas there we took the model as given, and sought to draw inferences to case given the model, the model now becomes an object that we both learn from and learn about. In essence we use the data on many cases to update our beliefs about the general model and then use this "trained' model to then make inferences about cases.

Along with this change in goals come some changes in the structure of the model:

\begin{itemize}
\tightlist
\item
  Instead of positing a belief about the causal type of a given case, \(\theta\), we now need to posit a belief over the \emph{distribution} of nodal types---that is, over \(\lambda\). Concretely, whereas in the simple process tracing model we \emph{specified} that inequality has a positive effect on mobilization for some share of units, we now specify a distribution over the proportion of units for which inequality could have a positive effect, ranging anywhere from 0 to 1. This means that we can now update on these population-level distributions as the model confronts data.
\end{itemize}

The same applies to beliefs about confounding. Since \(\lambda\) may include not beliefs about the \emph{joint} distribution of nodal types, we can set priors on or allow for the possibility of confounding in our model. In the application below, we focus on potential confounding in the relationship between inequality and mobilization---the possibility that inequality may be more or less likely in places where inequality would induce mobilization. Here we do not set a prior belief about the direction or magnitude of such confounding, but we set up the parameter matrix to allow for the possibility of confounding. This allows us, in turn, to learn about confounding from the data.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Model definition in code}

We begin with the same basic model as we used in Chapter \ref{ptapp}, with inequality (\(I\)) potentially affecting democratization (\(D\)) both through a direct pathway and through an indirect pathway mediated by mobilization (\(M\)). International pressure (\(P\)) is also a ``parent'' of democratization.

Further, we impose the same set of qualitative restrictions, ruling out a negative effect of inequality on mobilization, a direct positive effect of inequality on democratization, a negative effect of mobilization on democracy, and a negative effect of pressure on democratization. Note that this setup allows for inequality to have a positive (through mobilization) effect on democratization, a negative (direct) effect on democratization, or of course no effect at all.

Finally, we allow for confounding. The theoretical intuition we want to instantiate in the model is that the level of inequality could be endogenous to inequality's effect on mobilization. In particular, in places where mobilization would pose a mobilizational threat, governments may work harder to reduce inequality. To allow for this possibility, we need to create distinct elements of \(\lambda\) representing the conditional distribution of \(I\)'s nodal types given \(M\)'s: one parameter for \(\theta^I\)'s distribution when \(M\)'s nodal type is \(\theta^M_{01}\), and another parameter for \(\theta^I\)'s distribution when \(M\)'s nodal type is something else.

We can use \texttt{gbiqq} to define the model, with these restrictions and this confounding possibility, as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"I -> M -> D <- P; I -> D"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{#Specify the DAG}
\StringTok{  }
\StringTok{         }\KeywordTok{set_restrictions}\NormalTok{(}\KeywordTok{c}\NormalTok{( }
           \StringTok{"(M[I=1] < M[I=0])"}\NormalTok{, }
           \StringTok{"(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"}\NormalTok{))  }\OperatorTok{%>%}\StringTok{ }\CommentTok{#Exclude a set of negative-effect and positive-effect nodal types.}

\StringTok{          }\KeywordTok{set_confound}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{I =} \StringTok{"(M[I=1]  > M[I=0])"}\NormalTok{))  }\CommentTok{# Allow I to have a distinct conditional distribution when M's nodal type is \textbackslash{}theta_01. }
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This model, with confounding, is represented graphically as in Figure \ref{fig:pimdgraph}. The possibility of confounding is represented with the bidirected edge, connecting \(I\) and \(M\).

\includegraphics{ii_files/figure-latex/pimdgraph-1.pdf}

\hypertarget{data}{%
\section{Data}\label{data}}

As in Chapter \ref{ptapp}, wewill confront the model with data drawn from coding narratives in the Supplementary Material for \citet{haggard2012inequality}. However, rather than implementing the analysis case-by-case, we now derive leverage from the joint distribution of the data to train the model---that is, to update its parameters, allowing us to derive estimates for a range of population- and case-level estimands.

Here is a snapshot of the data:

Note that this is not a rectangular dataset in that Haggard and Kaufman's collection of clues was conditional on the outcome, \(D=1\): they gathered qualitative data on the presence of international pressure and the presence of mass-mobilization \emph{only} for those cases that democratized. This is not an uncommon case-selection principle. The analyst often reasons that more can be learned about how an outcome arises by focusing in on cases where the outcome of interest has in fact occurred. (We assess this case-selection intuition, in the context of model-based inferences, in Chapter \ref{caseselection}.)

\begin{tabular}{l|r|r|r|r}
\hline
Case & P & I & M & D\\
\hline
Afghanistan & NA & 1 & NA & 0\\
\hline
Albania & 0 & 0 & 1 & 1\\
\hline
Algeria & NA & 0 & NA & 0\\
\hline
Angola & NA & 1 & NA & 0\\
\hline
Argentina & 0 & 0 & 1 & 1\\
\hline
Bangladesh & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

The raw correlations between variables is shown in Table \ref{pimdcorr}. Some correlations are missing because, as mentioned, data on some variables were only gathered conditional on the values of others. For those quantities where we do see correlations, they are not especially strong. There is, in particular, a weak overall relationship between inequality and democratization --- though, of course, this is consistent with inequality having heterogeneous effects across the sample. The strongest correlation in the data is between \(P\) and \(M\), which are assumed to be uncorrelated in the model, though this correlation is also quite weak.

\begin{verbatim}
## Warning in cor(data[, -1], use =
## "pairwise.complete.obs"): the standard deviation is
## zero
\end{verbatim}

\begin{tabular}{l|r|r|r|r}
\hline
  & P & I & M & D\\
\hline
P & 1.000 & 0.157 & -0.177 & NA\\
\hline
I & 0.157 & 1.000 & 0.114 & -0.154\\
\hline
M & -0.177 & 0.114 & 1.000 & NA\\
\hline
D & NA & -0.154 & NA & 1.000\\
\hline
\end{tabular}

\hypertarget{inference}{%
\section{Inference}\label{inference}}

With data and model in hand, we can now update our model to get posteriors on the distribution of all admissible causal types. In practice this is done by constructing a \texttt{stan} model that maps from a set of parameters to a distribution on causal types, which in turn provide a likelihood function for observable data. (Using the \texttt{gbiqq} package the posterior is calculated by \texttt{gbiqq(model,\ data)}.)

The parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{param_mat <-}\StringTok{ }\KeywordTok{get_parameter_matrix}\NormalTok{(model)}
\KeywordTok{dim}\NormalTok{(param_mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  29 240
\end{verbatim}

FLAG: Need to talk a bit through what's happening here.

\hypertarget{did-inequality-cause-democracy}{%
\subsection{\texorpdfstring{Did inequality \emph{cause} democracy?}{Did inequality cause democracy?}}\label{did-inequality-cause-democracy}}

We have used the data to update on \(\lambda\): our beliefs about the distributions of nodal types, including about their joint distributions (i.e., confounding). We can now ask \texttt{gbiqq} to use the updated parameters to calculate any estimand of interest.

One set of questions we can ask of the updated model is about the probability that high inequality causes democratization. We can pose this question at different levels of conditioning. For instance, we can ask:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{For all cases}. For what proportion of cases in the population does inequality have a positive effect on democratization?
\item
  \textbf{For all cases displaying a given causal state and outcome}. Looking specifically at those cases that in fact had high inequality and democratized, for what proportion was the high inequality a cause of democratization?
\item
  \textbf{For cases displaying a given causal state and outcome, and with additional clues present or absent.} What if we have also collected clues on mediating or moderating nodes? For instance, for what proportion of high-inequality, democratizing cases \emph{with} mobilization did inequality cause the outcome? For what proportion \emph{without} mobilization? Likewise for the presence or absence of international pressure?
\end{enumerate}

We ask \texttt{gbiqq} now to query \(\lambda\)'s posterior distribution to generate posterior distributions for each of these quantities. We can define our queries quite simply in terms of the causal types that correspond to the effect of interest and then take the conditional probability of these. We present the code and results of these operations below and in Figure \ref{mixedhist}.

FLAG: Let's graph the priors for each of these queries next to the posteriors.

\includegraphics{ii_files/figure-latex/mixedhist-1.pdf}

FLAG: Not sure how much it is worth saying here substantively about the results given how strongly they are driven by the restrictions.

We can see that the share of cases overall in which inequality causes democratization is estimated to be very low, with a good deal of confidence. The proportion is considerably higher for those cases that in fact experienced high inequality and democratization. The proportion of positive causal effects is believed to be even higher for those in which mobilization occurred, especially where an alternative causes---international pressure---was absent, though our uncertainty about this share is also very high. We also see that the absence of mobilization tells us for certain that democratization was not caused by inequality. Interestingly, however, this result derives from the model restrictions, rather than from the data: under the restrictions we imposed, a positive effect of inequality could operate \emph{only} through mobilization.

\hypertarget{did-inequality-prevent-democracy}{%
\subsection{\texorpdfstring{Did inequality \emph{prevent} democracy?}{Did inequality prevent democracy?}}\label{did-inequality-prevent-democracy}}

We can undertake the same exercise for a negative causal effect, generating estimates a comparable set of estimands, as shown in Figure \ref{mixedhist2}.

\includegraphics{ii_files/figure-latex/mixedhist2-1.pdf}

We see that inequality appears, overall, more commonly to prevent democratization than to cause it. We are, moreover, most confident that inequality played a preventative role in those cases in which there was mobilization and international pressure---both of which \emph{could} have generated democratization---but still no democratization occurred.

Importantly, in the mixed-data setting, we can think of our estimands in both population-level and case-level terms. We have initialy posed these question as population-level queries. The results in Figures \ref{mixedhist} and \ref{mixedhist2} can be understood as our estimates of the \emph{share} of cases in the population, with a given set of characteristics, for which a particular causal effect holds. Yet these distributions, by the very same token, represent our beliefs about the probability that \(I\) had a positive causal effect in an individual case. Thus, for instance, graph \(PC2\) gives us the posterior on the proportiuon of \(I=D=M=1\) cases in the population for which \(I=1\) caused \(D=1\). But it also tells us, for an individual case randomly selected from the population of \(I=D=M=1\) cases, the probability that high inequality caused democratization in the case. The key difference from the pure process-tracing setup is that here our case-level inferences are informed by \emph{data} from the population, rather than population-level beliefs serving only as priors.

\hypertarget{prior-posterior-comparison-for-multiple-estimands}{%
\section{Prior / posterior comparison for multiple estimands}\label{prior-posterior-comparison-for-multiple-estimands}}

Estimands can be calculated for both the prior and posterior distributions.

Inequality causes democratization:

\begin{table}[t]

\caption{\label{tab:IcausesDem}Prior}
\centering
\begin{tabular}{}
\hline

\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:IcausesDem}Posterior}
\centering
\begin{tabular}{}
\hline

\hline
\end{tabular}
\end{table}

Inequality prevents democratization:

\begin{table}[t]

\caption{\label{tab:IpreventsDem}Prior}
\centering
\begin{tabular}{}
\hline

\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:IpreventsDem}Posterior}
\centering
\begin{tabular}{}
\hline

\hline
\end{tabular}
\end{table}

FLAG: Multiple estimands:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Other links in the causal network: What is the proportion of cases in which \(M\) has a positive effect on \(D\)? The proportion in which \(P\) has a positive effect on \(D\)?
\item
  Confounding: What is the difference in probability of \(I=1\) for cases in which \(M\) has a positive effect on \(D\) vs.~cases in which it does not?
\end{enumerate}

FLAG: Relax qualitative restrictions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  As in Chap 7, allow for negative effects of \(I\) on \(M\), with low prior probability but a good deal of uncertainty.
\item
  Then we can
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Examine sensitivity to restrictions by comparing results with those from the more restricted model
\item
  Examine PATHWAYS as an estimand. So most effects seem to be negative, but are they direct effects (consistent with Boix) or indirect effects running through reduced mobilization (consistent with Ansell and Samuels)?
\end{enumerate}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{part-design-choices}{%
\part{Design Choices}\label{part-design-choices}}

\hypertarget{elements-of-design}{%
\chapter{Elements of Design}\label{elements-of-design}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

A fully specified causal model includes the information needed to assess the properties of a research design that seeks to learn from or learn about the model. We talk through how to go from defining causal models to ``declaring'' research designs and use this framework in later chapters to inform decisions about details of design choices.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

So far we have described a way to think about causal models, a way to specify causal estimands, and a Bayesian approach to inference, given models and estimands. Together with a strategy for data gathering these elements are enough to fully characterize a research design. If in addition we provide criteria for evaluating a design we have enough to be able to simulate the behavior of a research design and assess whether a design is up to the task fo answering the questions we want to answer.

Once we have a method to assess the performance of a given design we can can start asking what kind of design is optimal, given some beliefs about the world (see \citet{blair2016declaring} for more on this general approach to design declaration and diagnosis). In the next chapters we use this approach to assess a set of design choices including choices regarding the clues about which data is sought, the types of cases for which data is sought, and the number of cases for which different types of data is sought.

In the remainder of this chapter we discuss a simple evaluative criterion for a design and give examples for design declaration for a simple single case process tracing design and a mixed methods design.

\hypertarget{model-inquiry-data-strategy-answer-strategy}{%
\section{Model, inquiry, data strategy, answer strategy}\label{model-inquiry-data-strategy-answer-strategy}}

We use the MIDA approach (model, inquiry, data strategy, answer strategy) approach to declare a simple process tracing design with an arbitrary model.

\begin{itemize}
\item
  \textbf{Model.} We will define a model as introduced in Chapter 2.
\item
  \textbf{Inquiry} As discussed in Chapter 4, an inquiry is a question asked of a model. This is typically a question about the distribution of a variable in some controlled or natural condition, or some summary of such distributions. We refer to the quantity being targeted by a query as the estimand.
\item
  \textbf{Data strategy.} The data strategy describes how data will be gathered. In typical \texttt{DeclareDesign} applications this includes both randomization and data gathering (sampling) strategies. We focus on data gathering (though unrestricted randomization schemes are easily accommodated in the workhorse model). A sampling strategy might indicate a sequence of conditional data gathering schemes, for instance: gather data on \(X\) and \(Y\) for 100 cases, then gather data on \(M\) for all cases in which \(X=Y\).
\item
  \textbf{Answer strategy}. The answer strategy combines observed data with a causal model to generate an updated model from which inferences can be drawn.
\end{itemize}

Importantly, the model used in the answer strategy does not need to be the same model as assumed at the model step since we could imagine analysts coming to the data with quite different models in mind. Of course any model used in the answer strategy should generally involve the same variables as in the model itself.

A design is a concatenation of these four components or ``steps.''

The concatenation of steps lets us examine instances of \texttt{runs} of a design. A single instance would involve:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a single draw of a true parameter vector from the distribution given in the model definition
\item
  a calculation of the value of an estimand given this true parameter draw
\item
  the generation of a dataset given the model implied by step 1 and the data strategy
\item
  the generation of an answer to the inquiry generated from the realized data from 3. and the answer strategy
\end{enumerate}

With the observation of multiple instances we get to assess the distribution of our answers --- and our uncertainty around these -- over repeated draws, and each time we get to see how well the answer we get maps onto the assumed truth in that draw.

\hypertarget{defining-a-model}{%
\subsection{Defining a model}\label{defining-a-model}}

\hypertarget{five-questions}{%
\subsubsection{Five questions}\label{five-questions}}

To define a model we declare the set of variables we are interested in and the relations of independence between them. In particular we answer the following five questions:

\textbf{1. What are the nodes?} As discussed in Chapter 2 we have some liberty in selecting which nodes we care about to explain a phenomenon. In defining the nodes (variables) we generally also define the ranges of the variables---indicating, for example whether they are binary, categorical, or continuous. In defining the edges we identify the set of parents of any node.

\textbf{2. Where are the arrows?} Can you justify conditional independence claims?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Where are there plausible threats of \textbf{unobserved confounding}?
\item
  What are the functional relations? What are the parameters? In the general case a model might specify functional relations linking relating parents to children. In the binary world a finite set of parameters can be used to characterize all possible functional relations. What restrictions? Of course restrictions are never required: rather than imposing a qualitative restriction it is possible to allow effects in all directions and let the data determine what patterns appear likely or not. It is important to note however that in general monotonicity claims cannot be inferred from data with full confidence.\footnote{For instance in an \(X \rightarrow Y\) model, define \(\overline{y}_j = E(Y(X=j))\). Observation of \(\overline{y}_0\) and \(\overline{y}_1\) is consistent with a world in which there are positive effects for share \(\overline{y}_1 - \overline{y}_0\) of units and negative effects for none; or one with positive effects for \(\overline{y}_1\) and negative effects for share \(\overline{y}_0\) (provided \(\overline{y}_1 + \overline{y}_0 <1\)), or anything in between.}
\item
  What are your \textbf{priors}? When defined as part of the model we think of these priors as being the priors from the vantage point of someone assessing a design and they need not be the same as the priors used in the analysis. There are different approaches to generating priors. One might be to try to generate priors that reflect the state of the literature. A second might be to more formally develop priors on the basis of prior data --- that is, start with a model with uninformative priors and update the model using \emph{past} evidence. A third approach is to gather data from target readers---for example to gather data from policy makers or disciplinary experts.
\end{enumerate}

\hypertarget{example}{%
\subsubsection{Example}\label{example}}

To illustrsate this process of model construction, refer back to our discussion of the inequality and democracy model in section \ref{inequalitytheory}. There we discussed a theory that unpacked a higher level model but that did not place any restrictions on functional forms --- that is, on causal types. We might imagine such restrictions being justified by theory, however.
Drawing on \citet{boix2003democracy}, for instance, we might theorize that inequality can have a negative effect on democratization by giving the elite more to lose from majority rule, making autocrats less willing to hand over power. Inequality's positive effect, we might further posit, derives from the fact that it gives the poor more to gain from the redistribution that democratization would enable (\citet{acemoglu2005economic}). However, this positive effect can only unfold to the extent that the masses are able to mobilize, and the capacity to mobilize will hinge on ethnic homogeneity. Ethnic homogeneity thus defines the causal possibilities in regard to \(I\)'s effect on \(D\). First, homogeneity is necessary for a positive effect of inequality. Second, by enabling mobilization around distributional demands, ethnic homogeneity rules out a net negative effect of inequality (as inequality's mobilizing effects will balance out elite fears of expropriation). Third, by making mass mobilization easier in general, ethnic homogeneity makes possible mobilization and democratization \emph{without} inequality. Under ethnic heterogeneity, on the other hand, inequality can have a negative effect, or it can no effect at all with autocracy entrenched.

Put differently, under ethnic homogeneity, inequality's effect can only correspond to a \(b\) type or a \(d\) type, while under heterogeneity the effect can only be of type \(a\) or type \(c\). \(E\) thus allows us to partition the range of causal possibilities that model (a) had lumped together under \(\theta^D\). Now we can capture this logic with a functional equation in which \(\theta^D\) now takes on just two possible values (0 or 1), rather than four:

\begin{equation}
D=IE(1-\theta^D)+\theta^DE+(1-E)(1-I)\theta^D
\end{equation}

We can work through the arithmetic to observe \(E\)'s causal-partitioning effect. Whether \(E\) is 0 or 1 determines whether we are in a world of posiitve effects (\(b\) types) and democratization regardless (\(d\) types), or a world of negative effects (\(a\) types) and autocracy regardless (\(c\) types). Note that the righthand side is a sum of three expressions. We can think of \(E\) as a ``switch'' that turns these expressions ``on'' or ``off.'' When \(E=1\), the third expression goes to 0, leaving only the first two in play ``on.'' Now, \(\theta^D\) determines whether \(I\) has a positive effect (when \(\theta^D=0\)) or no effect with \(D\) fixed at \(1\) (when \(\theta^D=1\)). Conversely, when \(E=0\), the first two expressions both go to \(0\), and \(\theta^D\) determines whether \(I\) will have a negative effect (when \(\theta^D=1\)) or no effect with \(D\) stuck at 0 (when \(\theta^D=0\)).

\hypertarget{evaluating-a-design}{%
\section{Evaluating a design}\label{evaluating-a-design}}

\hypertarget{expected-error-and-expected-posterior-variance}{%
\subsection{Expected error and expected posterior variance}\label{expected-error-and-expected-posterior-variance}}

How do you know if you have a good design?

One metric is to assess the \emph{expected (squared) error} resulting from posterior beliefs. In other words: how wrong are you likely to be if you base your best guess on your posterior mean?

Another way to think of the gains is as the \emph{expected posterior variance}: how certain do you expect you will be after you make use of this new information?

In fact these two quantities are equivalent (see for example \citet{scharf1991statistical}) -- at least if you assess expectations using the same priors as you use for forming posteriors.

To see this, imagine a situation in which there is an unknown parameter \(q\) and we have a data strategy that produces a distribution over data \(k\), given \(q\).

Let \(p(q,k)\) denote the joint prior distribution over \(q\) and \(k\) with marginal distributions \(p(k)\) and \(p(q)\).

For any \(k\) there is posterior estimate \(q_k\) and a posterior variance \(v_k\), both estimated using Bayes rule.

The expected squared error is then:

\[ESE := \int_q\int_k \left({q}_k-q\right)^2p(k, q)dkdq \]

This takes the error one might get with repsect to any true value of the parameter (\(q\)), given the data one might see given \(q\) and the inferences one might draw.

The expected posterior variance can be written:

\[EV := \int_k v_k p(k)dk\]

This takes the posterior variance, given some data, over all the possible data one might see given marginal distribution \(p(k)\).

We want to show that these are equivalent.

We take advantage of the fact that \(p(q,k) = p(k)p(q|k) = p(q)p(k|q)\) and that \(p(q|k)\) gives the posterior distribution of \(q\) given \(k\). We then have:

\[
\begin{eqnarray}
ESE &=& \int_q\int_k \left({q}_k-q\right)^2p(q,k)dkdq \\
    &=& \int_k\int_q \left({q}_k-q\right)^2p(q,k)dq dk \\
    &=& \int_k\int_q \left({q}_k-q\right)^2p(k)p(q|k)dq dk \\
    &=& \int_k\int_q \left({q}_k-q\right)^2p(q|k)dq p(k)dk \\
    &=& \int_k\left[\int_q \left({q}_k-q\right)^2p(q|k)dq\right]p(k)dk \\
    &=& \int_k v_k p(k)dk \\
    & = & EV
\end{eqnarray}
\]

Note that the key move is in recognizing that \(p(q |k)\) is in fact the posterior distribution on \(q\) given \(k\). In using this we assume that the same distribution is used for assessing error and for conducing analysis---that is we take the researcher's prior to be the relevant one for assessing error.

\hypertarget{expected-variance-almost-always-goes-down}{%
\subsection{Expected variance (almost) always goes down}\label{expected-variance-almost-always-goes-down}}

Moreover, it is easy to see that whenever inferences are sensitive to \(K\), the expected variance of the posterior will be lower than the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of \(K\), given what is already known from observation of \(W\).\footnote{See \citet{raiffa1961applied}. A similar expression can be given for the expected posterior variance from learning \(K\) in addition to \(W\) when \(W\) is not yet known. See, for example, Proposition 3 in \citet{geweke2014analysis}.}\\
\[Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))\]

However although \emph{expected} posterior variance goes down, it is still always possible that posterior variance rises. The increase in uncertainty does not however mean you haven't been learning. Rather you have learned that things aren't as simple as you thought.

\hypertarget{illustration-1}{%
\subsection{Illustration}\label{illustration-1}}

For illustration say that it is known that \(X=1, Y=1\) and that, given this information (playing the role of \(W\)), the posterior probability that a unit is of type \(b\)---for whom \(Y\) would be 0 were \(X=0\) (and not type \(d\), for which \(Y\) would be 1 regardless) is \(p\). Say then that that under some theory we have \(\phi_b := \Pr(K=1 | Y(0)=0, Y(1)=1, X=1)\), \(\phi_d := \Pr(K=1 | Y(0)=1, Y(1)=1, X=1)\).

Then what is the value added of this theory? Define \(Q\) here as the query regarding whether the unit is a \(b\) type. Then the prior variance, \(Var(Q|W)\), is simply \(p(1-p)^2 +(1-p)p^2 = p(1-p)\).

To calculate \(E_{K|W}(Var(Q|K,W))\), note that the posterior if \(K\) is observed is \(\frac{\phi_bp}{\phi_bp+\phi_d(1-p)}\). Let us call this \(\hat{q}_K\), and the belief when \(K\) is not observed \(\hat{q}_{\overline{K}}\).
In that case the \emph{expected error} is:

\[\text{Expected Error} = p\phi_b\left(1-\hat{q}_K\right)^2+(1-p)\phi_d\hat{q}_K^2+p(1-\phi_b)\left(1-\hat{q}_{\overline{K}}\right)^2+(1-p)(1-\phi_d)\hat{q}_{\overline{K}}^2\]

where the four terms are the errors when \(K\) is seen for a \(b\) type, when \(K\) is seen for a \(d\) type, when \(K\) is not seen for a \(b\) type, and when \(K\) is not see for a \(d\) type.

Defining \(\rho_K = (p\phi_b+(1-p)\phi_d)\) as the probability of observing \(K\) given the prior, we can write the posterior variance as:

\[\text{Expected Posterior Variance} = \rho_K\hat{q}_K(1-\hat{q}_K)+(1-\rho_K)\hat{q}_{\overline{K}}(1-\hat{q}_{\overline{K}})\]

With a little manipulation, both of these expressions simplify to:

\[\text{Expected Posterior Variance} =p(1-p)\left(\frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} + \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\right)\]

The gains are then:

\[\text{Gains} =1- \frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} - \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\]

Note that we still learn even if our posterior variance increases. For example say \(p = 1/5\), \(\phi_d = 1/3\), \(\phi_b = 2/3\) and we observe \(K=1\). Then our prior variace is \(p(1-p) = 4/15\). Our posterior is \(1/3\) and our posterior variance is 2/9, an increase. Even still although we are more uncertain we are wiser since we attribute a squared error to the guesses made by our former selves now of \((1/3)(1-1/5)^2 + (2/3)(0 - 1/5)^2 = 6/25\).

\hypertarget{other-loss-functions}{%
\subsection{Other loss functions}\label{other-loss-functions}}

Other loss functions could be used, including functions that take account of the costs of collecting additional data,\footnote{Further, one might call into question the value of a theory if the gains in precision depend upon data that are practically impossible to gather.} or to the risks associated with false diagnoses.\footnote{For instance, in \citet{heckerman1991toward}, an objective function is generated using expected utility gains from diagnoses generated based on new information over diagnoses based on what is believed already. In their treatment \citep[Equation 6]{heckerman1991toward}, the expected value of new information \(K\), given existing information \(W\) is: \(\sum{K}P(K|W)( EU(d(Q,W,K)|W, K) - EU(d(Q, W)|W, K))\) where \(EU\) is expected utility and \(d\) is the optimal inference (diagnosis) given available data. Note that the diagnosis can take account of \(K\) when it is observed, but the expected utility depends on \(K\) whether or not it is observed, as \(K\) carries information about the state of interest.}

\hypertarget{illustration-of-design-decaration-in-code}{%
\section{Illustration of Design Decaration in code}\label{illustration-of-design-decaration-in-code}}

In the \texttt{gbiqq} package there is a single function that lets you declare a full design in one go by letting you supply arguments to declare a model, an inquiry, a data strategy, and an answer strategy

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DeclareDesign)}

\NormalTok{n <-}\StringTok{ }\DecValTok{10}

\NormalTok{analysis_model  <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> Y"}\NormalTok{)  }\OperatorTok{%>%}\StringTok{ }\KeywordTok{set_priors}\NormalTok{(}\DataTypeTok{distribution =} \StringTok{"jeffreys"}\NormalTok{)}

\NormalTok{reference_model <-}\StringTok{ }\NormalTok{analysis_model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{set_priors}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{7}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{my_design <-}
\StringTok{  }
\StringTok{  }\KeywordTok{gbiqq_designer}\NormalTok{(}
    \DataTypeTok{reference_model =}\NormalTok{ reference_model,}
    \DataTypeTok{analysis_model  =}\NormalTok{ analysis_model,}
    \DataTypeTok{n               =}\NormalTok{ n,}
    \DataTypeTok{query           =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{PC =} \StringTok{"Y[X=1] > Y[X=0]"}\NormalTok{),}
    \DataTypeTok{given           =} \StringTok{"X==1 & Y==1"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Once declared like this you can use a design to draw likely data, run analyses, and run diagnostics.

Sample data can be generated with the \texttt{DeclareDesign} function \texttt{draw\_data}:

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-53}Sample Data (shown in compact form)}
\centering
\begin{tabular}{l|l|r}
\hline
event & strategy & count\\
\hline
X0Y0 & XY & 1\\
\hline
X1Y0 & XY & 2\\
\hline
X0Y1 & XY & 0\\
\hline
X1Y1 & XY & 7\\
\hline
\end{tabular}
\end{table}

We can look at sample estimands using \texttt{draw\_estimands}:

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-54}Sample Estimands}
\centering
\begin{tabular}{l|r}
\hline
estimand\_label & estimand\\
\hline
PC & 0.431\\
\hline
\end{tabular}
\end{table}

We can look at sample estimates using \texttt{draw\_estimates}:

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-55}Sample Estimates}
\centering
\begin{tabular}{l|l|l|r|r|l}
\hline
estimand\_label & Given & Using & estimate & sd & estimator\_label\\
\hline
PC & - & posteriors & 0.422 & 0.227 & est\_PC\\
\hline
\end{tabular}
\end{table}

Diagnosis is then implemented using \texttt{diagnose\_design}. The function \texttt{diagnose\_design} simulates the design many times and in each simulation gathers the estimand as well as the estimate and other statistics, and uses these to generate diagnosands -- such as mean squared error or expected posterior variance. Moreoever once a design is declared it is easy to sets of neighboring designs.

Here's an example in which for the diagnsis we compare the performance of our declared design to those of designs of different sizes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiple_designs <-}\StringTok{ }\KeywordTok{redesign}\NormalTok{(my_design, }\DataTypeTok{n =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{200}\NormalTok{))}

\KeywordTok{diagnose_design}\NormalTok{(multiple_designs, }\DataTypeTok{sims =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|l|l|l}
\hline
n & MSE & Posterior Var & Mean Estimate & SD Estimate & Mean Estimand & Bias\\
\hline
10 & 0.08 & 0.05 & 0.44 & 0.16 & 0.68 & -0.24\\
\hline
 & (0.01) & (0.00) & (0.02) & (0.01) & (0.01) & (0.01)\\
\hline
40 & 0.05 & 0.04 & 0.54 & 0.18 & 0.70 & -0.16\\
\hline
 & (0.01) & (0.00) & (0.02) & (0.01) & (0.01) & (0.02)\\
\hline
200 & 0.02 & 0.02 & 0.63 & 0.18 & 0.69 & -0.06\\
\hline
 & (0.00) & (0.00) & (0.02) & (0.01) & (0.01) & (0.01)\\
\hline
\end{tabular}

In this case the reference model is different from the analysis model. This might be the case for a resaercher with beliefs about causal effects contemplating how their analysis performed if they commited to undertaking an analysis using flat priors. Or it might reflect the evaluation of one researcher regarding the analyses of another. In any event the fact that these are out of line means that the MSE and expected posterior variance do not converge. Both however decline with larger designs.

In the chapters that follow we use essentially this procedure, though, for efficiency, rather than repeatedly iterating the full design we use a procedure in which we store the inferences that we would make given different possible data, figure out the probability that we would observe data of a given type, and then calculate the expected value of different diagnosands given a refernece model and an analysis model.

\hypertarget{clue}{%
\chapter{Clue Selection as a Decision Problem}\label{clue}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We draw out the implications of the causal model approach for clue selection strategies. We introduce a tool for generating an optimal decision tree for clue selection given.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Consider now the problem of determining what qualitative data to gather on a case. Evidently it makes sense to gather information on clues that have large probative value, but whether or not clues have probative value can depend on what clues have already been collected: Finding out that the butler had no motive may be informative for the claim that he is innocent, but it may not be useful if you already know he had no opportunity.

To motivate our thinking about clue-selection, consider again our running example with the free press and government removal. We can use this toy example to see, intuitively, how researchers may have a choice among observations that could be informative, and how the informativeness of an observation can depend on what is already known. In Figure \ref{fig:running}, we showed how one can use the structural equations to provide a set of conditional causal graphs that let one see easily what caused what at different values of the exogenous nodes \(S\) and \(X\). Each of these plots graphs a particular context. We can thus readily see which collection of exogenous nodes constitutes---gives the answer to---a given query, or estimand. Turning things around, we can also see, given a query, which nodes are informative about the probability that the query is true.\footnote{With larger graphs, continuous variables, and more stochastic components, it may not be feasible to graph every possible context; but the strategy for inference remains the same.}

FLAG: Where is the running example figure currently? Need to reference it.

FLAG: I'm having trouble getting the logical progression here. Seems somewhat broken in the sense that it's incomplete. Conceptually, we might think of informativeness situations as taking four possible forms of interest: a clue is always informative; never informative; informative only conditional on something else (obviously, what the something else is can vary); conditional only in the absence of something else. We seem to be covering always informative and conditionally uninformative. Seems odd not to also show always uninformative and informative only conditional on something else. But not sure if we can do this with this example.

For example, suppose that one can see that \(Y=0\) but does not know the causal effect of \(X\) on \(Y\). This is equivalent to saying that we know that we are in panel \(A\), \(B\), or \(C\) but do not know which of these we are in. Would it be helpful to collect the clue \(S\) if one has no other information? Defining the query in terms of root nodes, the question becomes \(S \stackrel{?}{=} 1\), or \(P(S=1|X=0,Y=0)\); the difference between the contexts in the two panels is that \(S=0\) when, and only when, \(X=0\) causes \(Y=0\). Given the structural equation for \(S\), \(P(S|X=0,Y=0) = P(S|X=0)\), and given independence of \(X\) and \(S\), \(P(S=1|X=0)= \pi^S\) (the simple assignment propensity). Figuring out \(S\) \emph{fully} answers the query.\footnote{Graphically what is important is that \(S\) is informative not because it is \(d-\)connected with \(Y\), but because it is \(d-\)connected to the query variable---here, simply, to itself.}

We can also see instances in this example of how existing data can make clues \emph{uninformative}. Say one wanted to know if \(X\) causes \(C\) in a case. As we can see from inspection of the panels, this query is equivalent to asking whether \(S=1\) (as \(X\) causes \(C\) only in those two panels (\(B\) and \(D\)) where \(S=1\)). Data on \(R\) is unconditionally informative about this query as \(R\) is not \(d-\)separated from \(S\). For example, \(R=1\) implies \(S=0\). However, if \(C\) and \(X\) are already known, then \(R\) is no longer informative because \(C\) and \(X\) together \emph{d}-separate \(R\) from \(S\).\footnote{We can come to the same conclusion by reasoning with the graphs: if \(X=0\) and \(C=1\), we know we are in subfigure \(A\) or \(B\), and \(X\) causes \(C\) only in panel \(B\). However, \(R\) is of no help to us in distinguishing between the two contexts as it takes the same value in both graphs.}

The running example also lets us demonstrate how informative clues can be found in many different places in a graph.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Informative spouses} Spouses---parents of the same child---can inform on one another. As we have seen in other examples, when an outcome has multiple causes, knowing the value of one of those causes helps assess the effect(s) of the other(s). For example, here, \(S\) and \(X\) are both parents of \(C\); \(S\) is thus informative for assessing whether \(X\) causes \(C\). Indeed this query, written in terms of roots, is simply \(P(S)\): \(X\) causes \(C\) if and only if \(S=1\). Likewise, \(S\) causes \(C\) (negatively) if and only if \(X=1\).
\item
  \textbf{Pre-treatment clues.} Did the absence of media reports on corruption (\(R=0\)) cause government survival (\(Y=0\))? Look to the pre-treatment clue, \(X\): \(X=0\) is a smoking gun establishing that the absence of a report produced government survival. Or, substantively, if there were a free press, then a missing report would never be a cause of survival since it would occur only in the absence of corruption, which would itself be sufficient for survival. More broadly, this example illustrates how knowledge of selection into treatment can be informative about treatment effects.
\item
  \textbf{Post-outcome clues.} Suppose we observe the presence of a free press (\(X=1\)) and want to know if it caused a lack of corruption (\(C=0\)), but cannot observe the level of corruption directly. Observing \(Y\)---which occurs after the outcome---is informative here: if \(X=1\), then \(X\) causes \(C\) (negatively) if and only if \(Y=0\). When an outcome is not observed, a consequence of that outcome can be informative about its value and, thus, about the effect of an observed suspected cause.
\item
  \textbf{Mediators as clues}: We see a politically sensitive government (\(S=1\)) and its survival (\(S=0\)). Did the government survive because of its sensitivity to public opinion? Here, the mediation clue \(C\) is helpful: a lack of corruption, \(C=0\), is evidence of \(S\)'s negative effect on \(Y\).
\end{enumerate}

And, of course, different clues can be informative in different ways for different types of estimand.

Needed then is a systematic way for identifying what clues to look for for answering a given type of causal quesiton, given what we already know---and perhaps, in what order to look for them.

\hypertarget{a-strategic-approach}{%
\section{A strategic approach}\label{a-strategic-approach}}

The representation of inference problems as one of querying a Bayesian model points to a relatively simple method for answering this question, at least for small problems. Consider, first, a situation in which one has access to data \(W\) and wants to know the expected probative value of all possible collections of data one could gather.

This can be done as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define the model.
\item
  Define a query on the model.
\item
  Define a data strategy: a set of clues for which one might search (e.g., observe the value of \(C\)).
\item
  Given prior data, figure out the probability of different realizations of the new data, and for each possible realization calculate the posterior variance. Then calculate the \emph{expected} posterior variance for the data strategy by taking an average of these variances, with weights given by the probability of observing the clue realization in question.
\item
  Repeat steps 3-4 for different data strategies.
\end{enumerate}

This procedure returns the expected posterior variances associated with different data strategies.

A still more sophisticated strategy would, for multiple clues, take sequence into account: it would tell us which clues to search for later in the process given the realization of clues sought earlier. The path-dependence of clue selection arises from the possibility that the informativeness of a clue may depend on the value of other nodes in the model. A given clue \(K_2\), for instance, may be informative if another clue \(K_1\) has the value of 1 but not if it has the value 0.

We provide tools for both of these approaches and illustrate them below for both the running example and the democracy application.

\hypertarget{clue-selection-with-a-simple-example}{%
\section{Clue selection with a simple example}\label{clue-selection-with-a-simple-example}}

Let's return to the running example and assess the informativeness of different clue strategies for different estimands. Whereas we have in previous chapters specified fully deterministic functional equations for this model, we amend the model here by allowing for uncertainty over the nodal types for \(C\) and \(R\). At \(C\), we allow for the possibilities that corruption is always present and that corruption is always present except when there is both a free press (\(X=1\)) and sensitivity to public opinion (\(S=1\)). Thus, we permit both \(\theta^C_{1111}\) and \(\theta^C_{1110}\). At \(R\), we allow for both \(\theta^R_{0001}\) and \(\theta^R_{0000}\): the possibility that there is reporting on corruption if and only if there is corruption and a free press, and the possibility that there is never reporting on corruption.

To summarize the intuition, then, governments will fall only if there is both corruption and reporting on corruption. We are uncertain whether corruption is ever-present or not, but if it is ever absent, it can only be because there exists both a free press and a government that cares about public opinion. We are also uncertain whether or not media reporting on corruption is always absent; but if it is ever present, it is only because there is both corruption and a free press. One implication is that governments that are sensitive to public opinion will never fall because they will always eschew corruption when a free press --- the only mechanism that can generate reporting on corruption --- is present. In turn, the presence of a free press can only matter for government survival if governments are \emph{not} sensitive and thus do not strategically adjust their behavior in response to the risk of reporting.

This model is formally defined in \texttt{gbiqq} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }
\StringTok{  }
\StringTok{  }\KeywordTok{make_model}\NormalTok{(}\StringTok{"S -> C -> Y <- R <- X; X -> C -> R"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{C =} \KeywordTok{c}\NormalTok{(}\StringTok{"1110"}\NormalTok{, }\StringTok{"1111"}\NormalTok{), }
                                 \DataTypeTok{R =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0000"}\NormalTok{), }
                                 \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{)), }
                   \DataTypeTok{keep =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Suppose that our query is whether \(X\) has a positive effect on \(Y\). Using this model we can ask how likely different data realizations are and what we would infer about our query from each possible data realization, given existing data. We illustrate for a situation in which we already know that \(Y=0\).

Application of the function \texttt{conditional\_inferences} produces a matrix with the results. We reproduce these as Table \ref{tab:showstrats5xx}. The first five columns of Table \ref{tab:showstrats5xx} define the data realizations. The matrix includes all combinations of possible realized values for all available clue strategies given that we have already observed \(Y=0\). A ``0'' or ``1'' represents the observed value for a node that we have chosen to observe while \(NA\) under a node indicates that that node is not observed under the given strategy. Thus, for instance, in the first row, we are collecting no clues beyond our prior data on \(X\) and \(Y\). In the second row, we have additionally sought clue \(S\) and observed a value of \(0\), while in the third row we have chosen the same strategy but observed \(S=1\).

In the sixth column, we see the inference we would make from each data-realization, the posterior probability that \(X\) has a positive effect on \(Y\), given that \(Y=0\). The final column indicates the probability of each data-realization, given the chosen strategy (\emph{not} conditioning on the prior observation, \(Y-0\)).

FLAG: Let's add posterior variance to Table \ref{tab:showstrats5xx}

\begin{table}[t]

\caption{\label{tab:showstrats5xx}Inferences given different data patterns. }
\centering
\begin{tabular}{r|r|r|r|r|r|r}
\hline
S & X & C & R & Y & posterior & prob\\
\hline
1 & 1 & 0 & 0 & 0 & 0.000 & 0.1250\\
\hline
0 & 0 & 1 & 0 & 0 & 0.500 & 0.2500\\
\hline
1 & 0 & 1 & 0 & 0 & 0.250 & 0.2500\\
\hline
0 & 1 & 1 & 0 & 0 & 0.000 & 0.1250\\
\hline
1 & 1 & 1 & 0 & 0 & 0.000 & 0.0625\\
\hline
NA & 1 & 0 & 0 & 0 & 0.000 & 0.1250\\
\hline
NA & 0 & 1 & 0 & 0 & 0.375 & 0.5000\\
\hline
NA & 1 & 1 & 0 & 0 & 0.000 & 0.1875\\
\hline
1 & NA & 0 & 0 & 0 & 0.000 & 0.1250\\
\hline
0 & NA & 1 & 0 & 0 & 0.333 & 0.3750\\
\hline
1 & NA & 1 & 0 & 0 & 0.200 & 0.3125\\
\hline
0 & 0 & NA & 0 & 0 & 0.500 & 0.2500\\
\hline
1 & 0 & NA & 0 & 0 & 0.250 & 0.2500\\
\hline
0 & 1 & NA & 0 & 0 & 0.000 & 0.1250\\
\hline
1 & 1 & NA & 0 & 0 & 0.000 & 0.1875\\
\hline
1 & 1 & 0 & NA & 0 & 0.000 & 0.1250\\
\hline
0 & 0 & 1 & NA & 0 & 0.500 & 0.2500\\
\hline
1 & 0 & 1 & NA & 0 & 0.250 & 0.2500\\
\hline
0 & 1 & 1 & NA & 0 & 0.000 & 0.1250\\
\hline
1 & 1 & 1 & NA & 0 & 0.000 & 0.0625\\
\hline
NA & NA & 0 & 0 & 0 & 0.000 & 0.1250\\
\hline
NA & NA & 1 & 0 & 0 & 0.273 & 0.6875\\
\hline
NA & 0 & NA & 0 & 0 & 0.375 & 0.5000\\
\hline
NA & 1 & NA & 0 & 0 & 0.000 & 0.3125\\
\hline
0 & NA & NA & 0 & 0 & 0.333 & 0.3750\\
\hline
1 & NA & NA & 0 & 0 & 0.143 & 0.4375\\
\hline
NA & 1 & 0 & NA & 0 & 0.000 & 0.1250\\
\hline
NA & 0 & 1 & NA & 0 & 0.375 & 0.5000\\
\hline
NA & 1 & 1 & NA & 0 & 0.000 & 0.1875\\
\hline
1 & NA & 0 & NA & 0 & 0.000 & 0.1250\\
\hline
0 & NA & 1 & NA & 0 & 0.333 & 0.3750\\
\hline
1 & NA & 1 & NA & 0 & 0.200 & 0.3125\\
\hline
0 & 0 & NA & NA & 0 & 0.500 & 0.2500\\
\hline
1 & 0 & NA & NA & 0 & 0.250 & 0.2500\\
\hline
0 & 1 & NA & NA & 0 & 0.000 & 0.1250\\
\hline
1 & 1 & NA & NA & 0 & 0.000 & 0.1875\\
\hline
NA & NA & NA & 0 & 0 & 0.231 & 0.8125\\
\hline
NA & NA & 0 & NA & 0 & 0.000 & 0.1250\\
\hline
NA & NA & 1 & NA & 0 & 0.273 & 0.6875\\
\hline
NA & 0 & NA & NA & 0 & 0.375 & 0.5000\\
\hline
NA & 1 & NA & NA & 0 & 0.000 & 0.3125\\
\hline
0 & NA & NA & NA & 0 & 0.333 & 0.3750\\
\hline
1 & NA & NA & NA & 0 & 0.143 & 0.4375\\
\hline
NA & NA & NA & NA & 0 & 0.231 & 0.8125\\
\hline
\end{tabular}
\end{table}

Each inference, under each data-realization, also has an associated posterior variance, or level of uncertainty. Given the probability of each data-realization, conditional on the clue strategy, it is easy to assess the \emph{expected} reduction in variance from a given clue strategy. We present these expected posterior variances for all possible clue strategies, given the prior observation of \(Y\), in Table \ref{tab:scxrylearning}.

\begin{tabular}{l|l|l|l|l}
\hline
Strategy & Given & Prior belief & Prior Uncertainty & Expected Posterior Uncertainty\\
\hline
X & Y==0 & 0.231 & 0.177639 & 0.144230769230769\\
\hline
S & Y==0 & 0.231 & 0.177639 & 0.168501769230769\\
\hline
C & Y==0 & 0.231 & 0.177639 & 0.167937\\
\hline
R & Y==0 & 0.231 & 0.177639 & 0.177639\\
\hline
X, R & Y==0 & 0.231 & 0.177639 & 0.144230769230769\\
\hline
X, S & Y==0 & 0.231 & 0.177639 & 0.134615384615385\\
\hline
X, C & Y==0 & 0.231 & 0.177639 & 0.144230769230769\\
\hline
C, R & Y==0 & 0.231 & 0.177639 & 0.167937\\
\hline
C, S & Y==0 & 0.231 & 0.177639 & 0.164051230769231\\
\hline
S, R & Y==0 & 0.231 & 0.177639 & 0.168501769230769\\
\hline
X, C, S & Y==0 & 0.231 & 0.177639 & 0.134615384615385\\
\hline
X, C, R & Y==0 & 0.231 & 0.177639 & 0.144230769230769\\
\hline
X, S, R & Y==0 & 0.231 & 0.177639 & 0.134615384615385\\
\hline
C, S, R & Y==0 & 0.231 & 0.177639 & 0.164051230769231\\
\hline
X, C, S, R & Y==0 & 0.231 & 0.177639 & 0.134615384615385\\
\hline
\end{tabular}

We operationalize higher levels of expected learning from a strategy as a greater expected reduction in variance upon observing the data. We can see a couple of patterns here.

\begin{itemize}
\item
  By far the biggest gains in learning come from observing \(X\). We can see this most readily by comparing the 1-clue strategies to one another. But in general, any strategy that includes observing \(X\) always does substantially better than the comparable strategy that excludes \(X\). The intuition here is fairly straightforward: if we want to know whether \(Y=0\) was caused by \(X=0\), and start out very uncertain about \(X\)'s value, we should expect to learn a good deal from figuring out whether \(X\) is in fact equal to \(0\).
\item
  There are also considerable gains from observing \(S\) or \(C\) by themselves. Consider, first, why observing \(S\) is informative. \(S\) is potentially informative because it tells us something about whether \(X\) can affect \(Y\) by affecting \(R\). Remember that a government is removed only if there is both corruption (\(C=1\)) and reporting on corruption (\(R=1\)). Moreover, there is only reporting on corruption (if ever) if \$C=1. Thus, for both of these reasons, \(X\) can only have a positive effect on government removal (by causing reporting on corruption) if \(C=1\): i.e., if there is corruption. And \(S\) tells us something about what \(C\)'s value is likely to be if \(X\) were set to 1.
\end{itemize}

Specifically, if we observe \(S=0\), then we know for sure that \(C=1\), regardless of \(X\), since \(C\) is always 1 when \(S=0\) under both permitted nodal types for \(C\). If \(S=1\), on the other hand, there's a lower chance that \(C\) would be equal to 1 if \(X\) were set to 1: in one of \(C\)'s permitted nodal types, there is always corruption; but in the other type, sensitive governments avoid corruption when there is a free press, so \(X\) moving to 1 would give us \(C=0\). We have put equal prior probabilities on these two nodal types. Thus, if we observe \(S=1\), we conclude that there is a lower probability that \(C\) will take on the value necessary for \(X\) to exert a positive effect on \(Y\) than if we observe \(S=0\).

Why, then, is \(C\) informative? If we observe \(C=0\), then we know that \(X\) must be equal to 1 since, under permitted nodal types for \(C\), there is an absence of corruption \emph{only} in the presence of a free press and sensitive governments. And if \(X=1\) with \(Y=0\), a positive effect is ruled out with certainty. If we observe \(C=1\), then there remains some possibility that \(X=0\) as well as some possibility \(C\) would remain at 1 if \(X\) were set to 1 (depending on \(C\)'s unknown nodal type), allowing \(X\) to yield a positive effect on \(Y\) through \(R\).

\begin{itemize}
\item
  There are no gains from observing \(R\) if \(Y=0\). We can see why by looking at our table of data possibilities consistent with \(Y=0\) (Table \ref{showstrats5xx}). As we can see, there is no possibility of observing anything other than \(R=0\) if we have already seen \(Y=0\). We can see why by thinking, jointly, about how \(Y\) is determined and how \(R\) is determined. \(Y\) can be 0 either because \(C=0\) or \(R=0\). So if \(R\) were equal to \(1\), this must mean that \(C\) was \(0\). However, a necessary condition for \(R\) to be 1, under \(R\)'s permitted nodal types, is \(C=1\) and \(X=1\). In other words, the condition under which \(R\) could be 1 is a condition under which \(Y\) would not be 0. Thus, if we already know \(Y=0\), we know \(R=0\), and there is no gain from actually looking for \(R\).
\item
  Once we decide to observe \(X\), then the next-most informative clue to add to our research design is \(S\): \(X, S\) has the lowest expected posterior variance of any of the 2-clue strategies. And, in fact, there are no gains to adding \(C\) to \(X\), relative to observing \(X\) by itself.
\end{itemize}

Let us develop the intuition underlying this result.

Imagine that we have already observed \(X\)'s value. If \(X=1\), then (given \(Y=0\)), a positive effect is immediately ruled out with certainty, rendering any further observations of no value. If we observe \(X=0\), however, then (under this causal model) we know for certain that \(C=1\), simply because \(C=1\) for both of \(C\)'s permitted nodal types when \(X=0\) (there is always corruption when there is no free press). Thus, there is nothing to be gained by observing \(C\). (We have already seen why there is nothing to be gained from observing \(R\).)

Why, we might still ask, are there possible gains to observing \(S\) even if we're going to observe \(X\)? \(S\) is informative because it tells us something about whether \(X\) can affect \(Y\) by affecting \(R\). The potential gains from observing \(S\) with \(X\) arise from the possibility that we may see \(X=0\) (since \(X=1\) woudl decide the matter by itself). If \(X=0\), then we still need to know whether \(Y\) \emph{would} be 1 if we changed \(X\) to 1. As discussed above, \emph{that} depends on whether \(C\) would be \(1\) if \(X\) were set to 1, and (as, again, explained above) \(S\) is informative on that matter.

\begin{itemize}
\item
  We see, further, in the table --- and it follows from the above logic --- that we cannot improve on an \(X, S\) strategy by gathering more data. Thus, if the search for information is costly, looking only for \(X\) and \(S\) dominates all 3- and 4-clue strategies.
\item
  Clues can be more informative jointly than separately, and the expected gains to observing one clue can depend on which other clues we plan to observe. To see this, notice that among the 1-clue strategies, observing \(C\) by itself is slightly \emph{more} informative than observing \(S\) by itself. However, if we are planning to observe \(X\), then the gains flip, and it is only \(S\) that offers additional useful information. As we have discussed, observing \(X\) makes observing \(C\) uninformative while \(S\) remains informative as a moderator of \(X\)'s effect.
\end{itemize}

We would add that the pattern here forms part of a broader point that we wish to emphasize in this chapter: while process tracing often focuses on examining steps along causal pathways, it will often be the case that we learn more from \emph{moderators}, like \(S\) in this model, than from mediators, like \(C\) and \(R\). We return to this point below.

\hypertarget{dependence-on-prior-beliefs}{%
\subsection{Dependence on prior beliefs}\label{dependence-on-prior-beliefs}}

As the foregoing discussion already suggests, optimal clue strategies can depend on our prior beliefs about causal relationships among the variables in the model. We illustrate this point here, examining how evaluation of clue strategies shift as we relax restrictions on nodal types and set informative priors over nodal types.

\textbf{Relaxing restrictions.} In the analysis above, we allowed for just two (of 16 possible) nodal types at both \(C\) and \(R\), effectively expressing strong beliefs about how \(C\)'s and \(R\)'s values are determined. But what if we are less certain than this?

Suppose that we are not sure that corruption can be prevented only throught a combination of a free press and government sensitivity. We think it possible that government sensitivity itself might be sufficient: that \(S\) might have a negative effect on \(C\) regardless of \(X\)'s value. (Perhaps, for instance, there are means other than via a free press through which the public might learn of government corruption.) We allow for this causal possibility by expanding the set of kept nodal types for \(C\) to include \(\theta^C_{1010}\) in defining the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Expand nodal types for C.}

\NormalTok{model <-}\StringTok{ }
\StringTok{  }
\StringTok{  }\KeywordTok{make_model}\NormalTok{(}\StringTok{"S -> C -> Y <- R <- X; X -> C -> R"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{C =} \KeywordTok{c}\NormalTok{(}\StringTok{"1110"}\NormalTok{, }\StringTok{"1111"}\NormalTok{, }\StringTok{"1010"}\NormalTok{), }
                                 \DataTypeTok{R =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0000"}\NormalTok{), }
                                 \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{)), }
                   \DataTypeTok{keep =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|l}
\hline
Strategy & Given & Prior belief & Prior Uncertainty & Expected Posterior Uncertainty\\
\hline
X & Y==0 & 0.2 & 0.16 & 0.1332666\\
\hline
S & Y==0 & 0.2 & 0.16 & 0.1454454\\
\hline
C & Y==0 & 0.2 & 0.16 & 0.1429428\\
\hline
R & Y==0 & 0.2 & 0.16 & 0.16\\
\hline
X, R & Y==0 & 0.2 & 0.16 & 0.1332666\\
\hline
X, S & Y==0 & 0.2 & 0.16 & 0.1167333\\
\hline
X, C & Y==0 & 0.2 & 0.16 & 0.12\\
\hline
C, R & Y==0 & 0.2 & 0.16 & 0.1429428\\
\hline
C, S & Y==0 & 0.2 & 0.16 & 0.13994995\\
\hline
S, R & Y==0 & 0.2 & 0.16 & 0.1454454\\
\hline
X, C, S & Y==0 & 0.2 & 0.16 & 0.1125\\
\hline
X, C, R & Y==0 & 0.2 & 0.16 & 0.12\\
\hline
X, S, R & Y==0 & 0.2 & 0.16 & 0.1167333\\
\hline
C, S, R & Y==0 & 0.2 & 0.16 & 0.13994995\\
\hline
X, C, S, R & Y==0 & 0.2 & 0.16 & 0.1125\\
\hline
\end{tabular}

The diagnosis of strategies under this adjusted set of beliefs, for the same query (whether \(X\) has a positive effect on \(Y\)) and prior data (\(Y=0\)) as before, are displayed in Table \ref{scxrylearning2}.

We see that, among 1-clue strategies, observing \(X\) is still the best choice. Among 2-clue strategies, however, things begin to look different. The best 2-clue strategy is also still \(X, S\). Where things change most significantly, however, is among 3-clue strategies: now, we can do even better by additionally observing \(C\). The reason is that, with greater uncertainty about its nodal types, \(C\)'s value is no longer known when \(X=0\): it is now possible that \(C=0\) when \(X=0\) since we think it possible that \(C\)'s nodal type is \(\theta^C_{1010}\). Since \(C\)'s value bears on whether \(X\) can have an effect via \(R\), we can thus in this situation potentially learn something by observing \(C\).

We can also see \(C\)'s enhanced informational value throughout the table. Among 1-clue strategies, observing \(C\) alone generates greater learning here than it does under the original setup. More strikingly, among 2-clue strategies we see that observing \(C\) can now generate learning even if we have \emph{already} observed \(X\) (whereas there was no gain from strategy \(X, C\) relative to \(X\) under the original model). While \(X, S\) is still a better strategy than \(X, C\), the change in diagnosis could matter if, for instance, we cannot observe \(S\) for some reason or if observing \(S\) is much more costly than observing \(C\).

Moreover, the expected variance reduction from observing \(S\) is also greater under the new model, for 1- and 2-clue strategies. To see the informal intuition here, note that \(S\) is potentially informative about \(C\)'s value as a parent of \(C\). And we now believe (with the added nodal type for \(C\)) that there may be an additional way in which \(S\) could matter for \(C\), and thus provide information about its value. Moreover, since the added nodal type has \(S\) exerting a negative effect on \(C\) regardless of \(X\)'s value, \(S\) can now be informative even if we have already observed \(X=0\).

Finally, we can see that nothing has changed in regard to \(R\), about whose nodal types we have retained the same beliefs. It is still uniformly unprofitable to observe \(R\) because we still know \(R\)'s value whenever \(X=0\).

This exercise also suggests a further interesting principle of clue-selection: that potential informativeness rests on uncertainty about what we will find.

\textbf{Changing priors.} We can also see what happens when, rather than permitting new nodal types, we have informative beliefs about the prevalence of permitted types. We can provide a simple demonstration by expressing stronger prior beliefs about \(S\)'s nodal type. Suppose we believe most governments to be sensitive to public opinion. This would imply that we should put greater prior weight on \(\theta^S_1\) than on \(\theta^S_0\). We can do this by setting a higher \(\alpha\) value corresponding to \(S=1\), and telling \texttt{gbiqq} to set all paramater values (the \(\lambda\)'s for each nodal type) to the means of the prior distributions:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Set priors for S with more weight on S=1. Should make S less informative -- the more informative value for S is expected to be less likely??}

\NormalTok{model_priors <-}\StringTok{ }
\StringTok{  }
\StringTok{  }\KeywordTok{make_model}\NormalTok{(}\StringTok{"S -> C -> Y <- R <- X; X -> C -> R"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{list}\NormalTok{(}
    \DataTypeTok{C =} \KeywordTok{c}\NormalTok{(}\StringTok{"1110"}\NormalTok{, }\StringTok{"1111"}\NormalTok{), }
    \DataTypeTok{R =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0000"}\NormalTok{), }
    \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{)), }
    \DataTypeTok{keep =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }
\StringTok{ }\KeywordTok{set_priors}\NormalTok{(}\DataTypeTok{node =} \StringTok{"S"}\NormalTok{, }\DataTypeTok{statement =} \StringTok{"S==1"}\NormalTok{, }\DataTypeTok{alphas =} \DecValTok{10}\NormalTok{) }\OperatorTok{%>%}
\StringTok{   }
\StringTok{ }\KeywordTok{set_parameters}\NormalTok{(}\DataTypeTok{type =} \StringTok{"prior_mean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These priors put roughly a 0.91 probability on \(S=1\).

\begin{tabular}{l|l|l|l|l}
\hline
Strategy & Given & Prior belief & Prior Uncertainty & Expected Posterior Uncertainty\\
\hline
X & Y==0 & 0.158 & 0.133036 & 0.114904263157895\\
\hline
S & Y==0 & 0.158 & 0.133036 & 0.130411\\
\hline
C & Y==0 & 0.158 & 0.133036 & 0.123939789473684\\
\hline
R & Y==0 & 0.158 & 0.133036 & 0.133036\\
\hline
X, R & Y==0 & 0.158 & 0.133036 & 0.114904263157895\\
\hline
X, S & Y==0 & 0.158 & 0.133036 & 0.111842105263158\\
\hline
X, C & Y==0 & 0.158 & 0.133036 & 0.114904263157895\\
\hline
C, R & Y==0 & 0.158 & 0.133036 & 0.123939789473684\\
\hline
C, S & Y==0 & 0.158 & 0.133036 & 0.122798236842105\\
\hline
S, R & Y==0 & 0.158 & 0.133036 & 0.130411\\
\hline
X, C, S & Y==0 & 0.158 & 0.133036 & 0.111842105263158\\
\hline
X, C, R & Y==0 & 0.158 & 0.133036 & 0.114904263157895\\
\hline
X, S, R & Y==0 & 0.158 & 0.133036 & 0.111842105263158\\
\hline
C, S, R & Y==0 & 0.158 & 0.133036 & 0.122798236842105\\
\hline
X, C, S, R & Y==0 & 0.158 & 0.133036 & 0.111842105263158\\
\hline
\end{tabular}

We see the results of this new set of diagnoses, with informative priors on \(S\)'s nodal types, in Table \ref{scxrylearning3}. Comparing with Table \ref{scxrylearning}, a number of features stand out. First is the much lower \emph{prior} variance under the new model: having a strong prior belief about \(S\)'s value gives us stronger prior beliefs about whether \(X\) could have caused \(Y\) since such an effect \emph{depends} on \(S\)'s value. A second striking difference is that searching for \(S\) is expected to be much less informative in this model. The reason is simple: we now have a strong prior belief about what we are likely to find when we search for \(S\). We \emph{could} be surprised, but we should not \emph{expect} to be. In the original model, in contrast, we were maximally uncertain about \(S\)'s value --- believing it had a 0.5 chance of being 1 --- and so there was much more to be gained by looking.

While not shown here, we get essentially the same result if we flip our priors and put much greater weight on \(S=0\), rather than on \(S=1\).

\hypertarget{clue-selection-for-the-democratization-model}{%
\section{Clue selection for the democratization model}\label{clue-selection-for-the-democratization-model}}

We now apply this approach to the model of democratization that we worked with in Chapters \ref(ptapp) and \ref(mixingapp).

We start by specifying the democratization model, with negative effects ruled for \(I \rightarrow M\), \(M \rightarrow D\), and \(P \rightarrow D\) and a positive direct effect ruled out for \(I \rightarrow D\).

Now, let us assume that we have already observed high inequality and the outcome of democratization in a case, and we want to know whether high inequality caused democratization. The decision we confront is what combination of the other variables --- mobilization or international pressure --- we should collect data on: we could observe nothing further; observe \(P\) only; observe \(M\) only; or observe both \(P\) and \(M\). In Table @ref(possible\_outcomespimd\_i1), we show all possible data realizations from all possible clue-selection strategies, the inferences we would draw from each realization, and the probability of that realization (not conditionining on \(I=D=1\).)

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-65}\label{possible_outcomespimd_i1} Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a positive effect on $D$) given that data realization.}
\centering
\begin{tabular}{l|r|r|r|r|r|r}
\hline
  & I & P & M & D & posterior & prob\\
\hline
I1P0M0D1 & 1 & 0 & 0 & 1 & 0.000 & 0.004\\
\hline
I1P1M0D1 & 1 & 1 & 0 & 1 & 0.000 & 0.025\\
\hline
I1P0M1D1 & 1 & 0 & 1 & 1 & 0.250 & 0.050\\
\hline
I1P1M1D1 & 1 & 1 & 1 & 1 & 0.107 & 0.117\\
\hline
I1M0D1 & 1 & NA & 0 & 1 & 0.000 & 0.029\\
\hline
I1M1D1 & 1 & NA & 1 & 1 & 0.150 & 0.167\\
\hline
I1P0D1 & 1 & 0 & NA & 1 & 0.231 & 0.054\\
\hline
I1P1D1 & 1 & 1 & NA & 1 & 0.088 & 0.142\\
\hline
I1D1 & 1 & NA & NA & 1 & 0.128 & 0.196\\
\hline
\end{tabular}
\end{table}

We show in Table @ref(pimdlearn\_i1d1) how we expect uncertainty to be reduced by different research designs. In this table, we show these reductions for the two kinds of cases in which democratization does occur. The first row displays the variance on our posterior belief about the effect of \(I\) on \(D\) before we observe anything at all. The next three rows show our expectations for looking for \(P\) only; looking for \(M\) only; and looking for both. The clearest message here is that, if we had to choose between clues, we should observe \(P\): given our model (including our priors on the types), we reduce our uncertainty more by learning about an alternative cause than by learning about a mediator.

We also see that the mediator is much more informative when the causal effect we are looking for is one that \emph{could} have operated via the mediator, as compared to when the mediator is informative only as a moderator of the cause's direct effects.

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-66}\label{pimdlearn_i1d1} Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a positive effect on $D$?) given different clue seeking  stratgies for cases in which we have observed high inequality and democratization.}
\centering
\begin{tabular}{l|l|r|r|r}
\hline
strategy & given & prior\_estimand & prior\_var & E\_post\_var\\
\hline
None & I==1 \& D==1 & 0.128 & 0.112 & 0.112\\
\hline
P & I==1 \& D==1 & 0.128 & 0.112 & 0.107\\
\hline
M & I==1 \& D==1 & 0.128 & 0.112 & 0.109\\
\hline
P and M & I==1 \& D==1 & 0.128 & 0.112 & 0.105\\
\hline
\end{tabular}
\end{table}

We turn next to considering those cases with low inequality that democratized, asking whether democratization occurred because of a \emph{negative} effect of inequality. The possible data realizations, resulting inferences, and data probabilities are shown in Table @ref(possible\_outcomespimd\_i0), while the expected learning estimates for each clue strategy are given in Table @ref(pimdlearn\_i0d1). The pattern here is similar, though somewhat starker: substantially greater gains to observing the moderator, \(P\), than the mediator \(M\). The gains to observing \(M\) here are very small indeed. We can already see from comparing the relevant rows in the data-possibilities table how little our posterior beliefs shift depending on \(M\)'s realized value. \(M\) is far less informative for assessing \(I\)'s causal effect for an \(I=0, D=1\) case than for a \(I=1, D=1\) case. The reason is that, in the former situation, we are looking for a positive effect while in the latter situation, we are looking for a negative effects; but only positive effects can operate through the mobilization pathway under the model restrictions. Thus, \(M\) is uninformative as a mediator of \(I\)'s effect in an \(I=0, D=1\) (though it is informative as a moderator for such a case, but less so).

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-67}\label{possible_outcomespimd_i0} Table shows possible data patterns for P and M given I = 0 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a negative effect on $D$) given that data realization.}
\centering
\begin{tabular}{l|r|r|r|r|r|r}
\hline
  & I & P & M & D & posterior & prob\\
\hline
I0P0M0D1 & 0 & 0 & 0 & 1 & 0.667 & 0.050\\
\hline
I0P1M0D1 & 0 & 1 & 0 & 1 & 0.393 & 0.117\\
\hline
I0P0M1D1 & 0 & 0 & 1 & 1 & 0.571 & 0.058\\
\hline
I0P1M1D1 & 0 & 1 & 1 & 1 & 0.263 & 0.079\\
\hline
I0M0D1 & 0 & NA & 0 & 1 & 0.475 & 0.167\\
\hline
I0M1D1 & 0 & NA & 1 & 1 & 0.394 & 0.138\\
\hline
I0P0D1 & 0 & 0 & NA & 1 & 0.615 & 0.108\\
\hline
I0P1D1 & 0 & 1 & NA & 1 & 0.340 & 0.196\\
\hline
I0D1 & 0 & NA & NA & 1 & 0.438 & 0.304\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-68}\label{pimdlearn_i0d1}Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a negative effect on $D$?) given different  clue seeking  stratgies for cases in which we have observed low inequality and democratization.}
\centering
\begin{tabular}{l|l|r|r|r}
\hline
strategy & given & prior\_estimand & prior\_var & E\_post\_var\\
\hline
None & I==0 \& D==1 & 0.438 & 0.246 & 0.246\\
\hline
P & I==0 \& D==1 & 0.438 & 0.246 & 0.229\\
\hline
M & I==0 \& D==1 & 0.438 & 0.246 & 0.245\\
\hline
P and M & I==0 \& D==1 & 0.438 & 0.246 & 0.225\\
\hline
\end{tabular}
\end{table}

Now, let us see what happens as we revise the model, making it less restrictive. We do this, first, by allowing for confounding between two nodes in the model, international pressure and democratization. In particular, we allow for the possibility that, in order to generate a perception of foreign-policy influence and success, other states may target democratization pressure on autocratic regimes that are likely to democratize in the presence of pressure. This includes regimes that will democratize \emph{only} if pressured as well as those that will democratize in the presence of pressure but where pressure itself was not a cause. We use the \texttt{set\_confound} function to define distinct parameters for \(P\)'s nodal type when this is the case. The confound condition here is extremely easy to define: it is simply all unit types in which \(D=1\) when \(P=1\) ((D{[}P=1{]} == 1)). This includes all unit types (combinations of nodal types) that generate democratization in the presence of international pressure.

Having set the confound condition, we can then express beliefs (parameter values) that \(P=1\) is more common relative to \(P=0\) when the condition is met than otherwise.\footnote{It will be recalled that, in single-case inference we must express beliefs about population-level shares of nodal types. This includes expressing beliefs about the parameters defining the confounding.} We keep all other parameter values flat across the nodal types that are not excluded.

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-70}\label{possible_outcomespimd_conf} Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a positive effect on $D$) given that data realization, with confounding involving $P$.}
\centering
\begin{tabular}{l|r|r|r|r|r|r}
\hline
  & I & P & M & D & posterior & prob\\
\hline
I1P0M0D1 & 1 & 0 & 0 & 1 & 0.000 & 0.0001\\
\hline
I1P1M0D1 & 1 & 1 & 0 & 1 & 0.000 & 0.0495\\
\hline
I1P0M1D1 & 1 & 0 & 1 & 1 & 0.250 & 0.0011\\
\hline
I1P1M1D1 & 1 & 1 & 1 & 1 & 0.107 & 0.2308\\
\hline
I1M0D1 & 1 & NA & 0 & 1 & 0.000 & 0.0495\\
\hline
I1M1D1 & 1 & NA & 1 & 1 & 0.108 & 0.2319\\
\hline
I1P0D1 & 1 & 0 & NA & 1 & 0.231 & 0.0012\\
\hline
I1P1D1 & 1 & 1 & NA & 1 & 0.088 & 0.2802\\
\hline
I1D1 & 1 & NA & NA & 1 & 0.089 & 0.2814\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-71}\label{pimdlearn_i1d1con}Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a positive effect on $D$?) given different  clue seeking  stratgies for cases in which we have observed high inequality and democratization, with confounding.}
\centering
\begin{tabular}{l|l|r|r|r}
\hline
strategy & given & prior\_estimand & prior\_var & E\_post\_var\\
\hline
None & I==1 \& D==1 & 0.089 & 0.0811 & 0.0811\\
\hline
P & I==1 \& D==1 & 0.089 & 0.0811 & 0.0807\\
\hline
M & I==1 \& D==1 & 0.089 & 0.0811 & 0.0794\\
\hline
P and M & I==1 \& D==1 & 0.089 & 0.0811 & 0.0791\\
\hline
\end{tabular}
\end{table}

We display the inferences we \emph{would} draw from different clue strategies and data realizations in Table @ref(possible\_outcomespimd\_conf); and we show the resulting diagnoses of clue strategies, for the situation in which we have already observed \(I=1\) and \(D=1\), in Table \ref@(pimdlearn\_i1d1con). As we can see from the latter table, the presence of a confound involving \(P\) reduces this clue's relative expected contribution to learning, making it now \emph{less} informative in expectation than \(M\).

Working backwards, we can readily identify the reason for this in Table @ref(possible\_outcomespimd\_conf). We see here that observing \(P=1\) moves our beliefs very little off of our prior of \(0.091\) because, given the confounding, we already strongly expect to see \(P=1\) in a case that democratized; actually observing \(P=1\) contains only a small amount of new information. Our beliefs over the query change a great deal if we observe \(P=0\)---since the absence of pressure makes it much more likely that democratization occurred because of high inequality. In fact, \(P\)'s value is far more consequential than \(M\)'s. However, we can also see from the last column of the table that the most impactful realization of \(P\)'s value is also extremely unlikely under the model (given \(I=D=1\)). Thus, \(P\) \emph{can} be highly informative under this model, but it is very unlikely to be.

We turn next to examining clue strategies for a different kind of query. So far we have concerned ourselves with queries about causal effects, but we now examine a query in which we care about the \emph{pathway} through which an effect occurs. We need to adjust the model to allow allow for multiple pathways since, under the restrictions we have been using so far, positive effects of inequality can run only indirectly and negative effects can only run directly. We now remove the restriction that excluded a negative effect of \(I\) on \(M\) and instead use \texttt{set\_priors} to state a belief that such a negative effect is less likely than \(M\)'s other nodal types.

To isolate how the query matters from how the restrictions matter, we first diagnose clue strategies for a causal-effect query under this new model: given \(I=0, D=1\), did \(I\) have a negative effect on \(D\)? Inferences conditional on data-realizations and clue strategies are displayed in Table @ref(possible\_outcomespimd\_priors\_effect) and expected posterior variances in Table @ref(tab:pimdlearn\_priors\_effect).

Starting with Table @(possible\_outcomespimd\_priors\_effect), a comparison with the parallel results for the model that excludes negative \(I \rightarrow M\) effects -- Table @ref(possible\_outcomespimd\_i0) -- is informative. Similar to what we saw in Chapter @ref(pt\_app), the inferences we draw from observing \(M\)'s values \emph{flip} in direction when we allow for negative effects of \(I\) on \(M\). When such effects were excluded, \(M\) was informative only as a moderator of \(I\)'s direct negative effect on \(D\). An observation of \(M=1\) counted as evidence against \(I=0\) being the cause of \(D=1\) since \(M=1\) could be the cause (given that \(M\) could have a positive effect on \(D\)); \(M=0\) counted, by a similar logic, as evidence in favor of \(I\)'s negative effect. Once we relax the restriction and allow negative effects of \(I\) on \(M\), \(M\) is additionally informative as a \emph{mediator} along a second pathway through which \(I\) can have a negative effect on \(M\). Now, observing \(M=0\) cuts in two directions: on the one hand, it rules out a negative effect of \(I\) running through \(M\) (informativeness as mediator); on the other hand, it makes it more likely that \(I=0\) caused \(D=1\) indirectly (informativeness as moderator). We can see that, in this situation, the information we get from \(M\) as a mediator overwhelms that which we get from \(M\) as a moderator since our posterior on \(I\)'s causal effect now moves \emph{downwards} if we observe \(M=0\). \(M=1\), of course, cuts both ways as well by a parallel logic, but with the net effect being an upward shift in our posterior on the causal effect.

So how informative overall is \(M\) as compared to \(P\)? In the model allowing negative \(I \rightarrow M\) effects, we are now learning in two ways from \(M\) rather than one; so our intuition might be that \(M\) has become more informative than it was in our original model. In Table @ref(tab:pimdlearn\_priors\_effect), we see that this is not at all the case! Much as in Table @ref(pimdlearn\_i0d1), for the same query and given data but under our original model, we still see very little---indeed, slightly less---reduction in expected posterior variance from the search for \(M\). \(M\) may be informative as both moderator and mediator in the new model, but what we learn from the mediation in effect \emph{undoes} some of the learning from the moderation by pushing our inferences in the opposing directions.

\textbackslash{}begin\{table\}{[}t{]}

\textbackslash{}caption\{(\#tab:table\_inferences\_priors\_effect)\label{possible_outcomespimd_priors_effect} Table shows possible data patterns for P and M given I = 0 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization for the query: does I have a negative effect on D?\}
\centering

\begin{tabular}{l|r|r|r|r|r|r}
\hline
  & I & P & M & D & posterior & prob\\
\hline
I0P0M0D1 & 0 & 0 & 0 & 1 & 0.667 & 0.043\\
\hline
I0P1M0D1 & 0 & 1 & 0 & 1 & 0.393 & 0.100\\
\hline
I0P0M1D1 & 0 & 0 & 1 & 1 & 0.690 & 0.075\\
\hline
I0P1M1D1 & 0 & 1 & 1 & 1 & 0.404 & 0.102\\
\hline
I0M0D1 & 0 & NA & 0 & 1 & 0.475 & 0.143\\
\hline
I0M1D1 & 0 & NA & 1 & 1 & 0.525 & 0.177\\
\hline
I0P0D1 & 0 & 0 & NA & 1 & 0.682 & 0.118\\
\hline
I0P1D1 & 0 & 1 & NA & 1 & 0.398 & 0.202\\
\hline
I0D1 & 0 & NA & NA & 1 & 0.503 & 0.320\\
\hline
\end{tabular}

\textbackslash{}end\{table\}

\textbackslash{}begin\{table\}{[}t{]}

\textbackslash{}caption\{(\#tab:pimdlearn\_priors\_effect)Prior estimand, prior variances and expected posterior variances for the query (does I have a negative effect on D that is mediated by M?) given different clue-seeking stratgies for cases in which we have observed low inequality and democratization.\}
\centering

\begin{tabular}{l|l|r|r|r}
\hline
strategy & given & prior\_estimand & prior\_var & E\_post\_var\\
\hline
None & I==0 \& D==1 & 0.503 & 0.25 & 0.2500\\
\hline
P & I==0 \& D==1 & 0.503 & 0.25 & 0.2312\\
\hline
M & I==0 \& D==1 & 0.503 & 0.25 & 0.2494\\
\hline
P and M & I==0 \& D==1 & 0.503 & 0.25 & 0.2313\\
\hline
\end{tabular}

\textbackslash{}end\{table\}

Now, let's see how things look when we are interested not in \(I\)'s causal effect on \(D\) but in whether it had an effect via particular pathway. Specifically, given \(I=0\) and \(D=1\), did \(I\) have a negative effect on \(D\) that was mediated by \(M\)? Following our discussion of mediation in Chapter \ref{questions}, we define the query as asking whether the following are true:

1 Does \(I\) have a negative effect on \(M\)? (in code, \(M[I=1] < M[I=0]\))
2 Does the change in \(M\) resulting from a change from \(I=1\) to \(I=0\) cause a change in \(D\) from \(0\) to \(1\)? (\((D[M=M[I=0]] > D[M=M[I=1]])\))
3 Does \(I\)'s effect on \(D\) \emph{depend} on \(I\)'s effect on \(M\)? In other words, would we still get the \(I \rightarrow D\) effect if \(M\) were fixed at the value that it takes on when \(I=1\)? (\((D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])\))

Inferences conditional on data strategies and realizations are displayed in Table @ref(possible\_outcomespimd\_path). We saw in Table @ref(possible\_outcomespimd\_priors\_effect) that seeing \(M=0\) slightly reduces our confidence that there was a negative effect of \(I\) on \(D\). However, we see now that observing \(M=0\) entirely \emph{eliminates} the possibility that this effect was mediated by \(M\). We also see that the data realization under which we update most strongly in favor of a negative causal effect--\(P=0, M=1\)--is also the realization most suportive of a belief in a mediated negative effect.

Turning to expected learning from alternative strategies, shown in Table @ref(pimdlearn\_path), we can see clearly -- by comparison to Table \ref@(tab:pimdlearn\_priors\_effect) -- how optimal clue strategies depend on the query of interest. Whereas \(M\) is only slight informative about \(I\)'s causal effect--with \(P\) the more informative clue--we expect to learn much more from \(M\) about the pathway-specific query, and \(M\) is far more informative than \(P\).

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-73}\label{possible_outcomespimd_path} Table shows possible data patterns for P and M given I = 0 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization for the pathway query: does I have a negative effect on D that is mediated by M?}
\centering
\begin{tabular}{l|r|r|r|r|r|r}
\hline
  & I & P & M & D & posterior & prob\\
\hline
I0P0M0D1 & 0 & 0 & 0 & 1 & 0.000 & 0.043\\
\hline
I0P1M0D1 & 0 & 1 & 0 & 1 & 0.000 & 0.100\\
\hline
I0P0M1D1 & 0 & 0 & 1 & 1 & 0.190 & 0.075\\
\hline
I0P1M1D1 & 0 & 1 & 1 & 1 & 0.088 & 0.102\\
\hline
I0M0D1 & 0 & NA & 0 & 1 & 0.000 & 0.143\\
\hline
I0M1D1 & 0 & NA & 1 & 1 & 0.131 & 0.177\\
\hline
I0P0D1 & 0 & 0 & NA & 1 & 0.121 & 0.118\\
\hline
I0P1D1 & 0 & 1 & NA & 1 & 0.044 & 0.202\\
\hline
I0D1 & 0 & NA & NA & 1 & 0.073 & 0.320\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-74}\label{pimdlearn_path}Prior estimand, prior variances and expected posterior variances for the query (does I have a negative effect on D that is mediated by M?) given different  clue-seeking  stratgies for cases in which we have observed low inequality and democratization.}
\centering
\begin{tabular}{l|l|r|r|r}
\hline
strategy & given & prior\_estimand & prior\_var & E\_post\_var\\
\hline
None & I==0 \& D==1 & 0.073 & 0.0677 & 0.0677\\
\hline
P & I==0 \& D==1 & 0.073 & 0.0677 & 0.0658\\
\hline
M & I==0 \& D==1 & 0.073 & 0.0677 & 0.0630\\
\hline
P and M & I==0 \& D==1 & 0.073 & 0.0677 & 0.0617\\
\hline
\end{tabular}
\end{table}

Now, what if we already know that \(I\) has a negative effect on \(D\) and we want to know via which pathway that effect operates---the direct or the indirect?

FLAG

FLAG: actual cause and notable cause ??

\hypertarget{dynamic-strategies}{%
\subsection{Dynamic Strategies}\label{dynamic-strategies}}

The clue-collection strategies described above assume that researchers identify the full set of clues to be gathered in advance and do not alter their strategies as they go along. However, the expected informativeness of a clue may depend on the values of other clues that we see first, implying that an optimal strategy will be dynamic, taking into account earlier observations in selecting later ones.

Given \(n\) nodes, a dynamic data collection strategy will be of the form:
\[\sigma = \{K_1, (K_2|K_1 = 1), (K_2|K_1 = 0), (K_3|K_1=1, K_2 =0)\dots\}\]

where each \(K_j\) is en element of the nodes on the graph, or is the empty set. Thus, we start with observing \(K_1\); then, whether we choose to observe \(K_2\) depends on the value of \(K_1\); whether we choose to observe \(K_3\) depends on the value of \(K_1\) and (if we observed it) \(K_2\); and so on. A strategy \emph{vector} specifies a series of conditional clue-search actions: it identifies the first clue sought and then which clues are sought conditional on the realization of all prior clues sought.

Each possible strategy has an associated expected reduction in variance. We can also build in an expected cost associated with each clue, allowing us to treat clue-selection as an optimization problem.

To illustrate with the running example, we imagine a situation in which it is known that \(Y=1\), and we are interested in whether \(Y=0\) because of \(S\) (though we don't know at the outset what the value of \(S\) is). We consider strategies in which we first seek information on one node and then, conditional on what we find, look or do not look for data on one other node. With five nodes, one already known, there are \(4 \times 4^2\) strategies of this form.

Suppose that we observe that \(Y=0\): the government was not replaced. We then want to know whether this is because the government was sophisticated (\(S=1\)). If we learn that the government was not sophisticated, then this answers the question in the negative. If we learn that the government was sophisticated then we can infer that this was the cause if we learn that there was a free press (or if we learn that there was no corruption).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make model}
\NormalTok{model <-}
\StringTok{    }\KeywordTok{make_model}\NormalTok{(}\StringTok{"S -> C -> Y <- R <- X; X -> C -> R"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{       }\KeywordTok{set_restrictions}\NormalTok{(}
       \DataTypeTok{labels =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{C =} \KeywordTok{c}\NormalTok{(}\StringTok{"1110"}\NormalTok{, }\StringTok{"1111"}\NormalTok{),}
                     \DataTypeTok{R =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0000"}\NormalTok{),}
                     \DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{)),}
       \DataTypeTok{keep =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ii_files/figure-latex/unnamed-chunk-76-1.pdf}

For each strategy we can then assess the expected variance reduction; in addition, if collecting different clues comes at different costs---but collection depends on past findings---then we can also calculate the expected costs of each strategy.

\begin{longtable}[]{@{}llllll@{}}
\caption{Illustration of three (of many) possible two step strategies.}\tabularnewline
\toprule
Strategy & Step 1 & Step 2 if 0 & Step 2 if 1 & Expected variance & Expected Cost\tabularnewline
\midrule
\endfirsthead
\toprule
Strategy & Step 1 & Step 2 if 0 & Step 2 if 1 & Expected variance & Expected Cost\tabularnewline
\midrule
\endhead
1 & S & None & None & 0.167 & 1\tabularnewline
2 & S & X & X & 0 & 2.5\tabularnewline
3 & S & None & X & 0 & 2\tabularnewline
& & & & &\tabularnewline
\bottomrule
\end{longtable}

Figure \ref{fig:tradeoffs} plots a collection of strategies based on two criteria---the variance reduction and the expected number of clues sought, which could be an indicator for cost. One can see a frontier of optimal strategies, depending on how these two desiderata trade-off against each other. For the figure, we imagined that \(X\) is the most costly to collect, followed by \(C\), then \(S\), then \(Y\), then \(R\). The cheapest strategy among those that minimize variance involves gathering \(C\) only. The lowest variance strategy that minimizes costs involves gathering \(Y\) only.

Here we implement the same exercise for the basic democracy model. We illustrate with a case where we know there is inequality and democratization and we want to know if inequality caused democratization. We will assume for the illustratation that mobilization is easy to observe but pressure is difficult.

\includegraphics{ii_files/figure-latex/unnamed-chunk-78-1.pdf}

We can see here that four strategies are non-dominated by any alternative. These are, in order of increasing cost:

1 Observe \(M\) first, then stop. This strategy has relatively high expected uncertainty but minimizes costs relative to any other strategy: we observe just one clue, and it's the cheaper one.
2 Observe \(P\) first, then stop. We'll learn more from this strategy than from Strategy 1, though at higher cost. Still there is no other strategy that allows us to reduce costs without increasing variqnce.
3 Observe \(P\) first; if \(P=0\), observe \(M\), otherwise stop. We, again, get uncertainty reduction here, relative to Strategy 2, but again at higher cost.
4 Observe \(M\) first; if \(M=0\), stop; if \(M=1\), observe \(P\). This strategy gets us the lowest expected posterior variance of any strategy. Moreover, it is not the highest-cost strategy, which would be to observe both clues no matter what. Once we've observed \(M=0\), we get nothing from the additional investment in \(P\) since \(M=0\) already tells us that \(I\) could not have had a positive effect on \(D\).

Note also that both Strategy 3 and Strategy 4 are \emph{conditional} two-clue strategies: they involve first seeking one clue and seeking a second clue only under one of the possible realizations of the first clue. But they have different outcomes. Perhaps most interestingly, we don't expect to learn the most by starting with the most probative clue. If we start with the more informative clue, \(P\), observing \(M\) only if \(P=0\), we expect to end up with \emph{more} uncertainty than if we start with the less informative clue, \(M\), and observe \(P\) only if \(M=1\).

\hypertarget{conclusion-2}{%
\section{Conclusion}\label{conclusion-2}}

Explicit statement of a causal model---including prior beliefs over roots---allows one to assess what will be inferred from all possible observations. This opens the way for simple strategies for assessing what data is most valuable, and in what order it should be gathered.

We are conscious that here we are pushing the basic logic to the limits. In practice researchers will often find it difficult to describe a model in advance and to place beliefs on nodes. Moreover the collection of new data could easily give rise to possibilities and logics that were not previously contemplated. Nothing here seeks to deny these facts; the claim here is a simpler one: insofar as one can specify a model before engaging in data gathering, the model provides a powerful tool to assess what data is most useful to gather.

\hypertarget{wide}{%
\chapter{Going wide and going deep}\label{wide}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Researchers often need to choose between collecting data on more cases or collecting more data within cases. We discuss the tradeoffs and communicate an intuition that clue data, even on a small number of cases, can be informative even when there is \(X, Y\) data on a very large number of cases, but only if it provides information that cannot be gathered from \(X,Y\) data, such as selection into treatment. Simulations suggest that going deep is especially valuable for observational research, situations with homogeneous treatment effects, and, of course, when there is strong probative value.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Full set of analyses

Graph full set

How does this approach guide researchers in making choices about research designs?

We address this question with a focus on characterizing the kind of learning that emerges from gathering different sorts of data---such , \emph{under different research conditions}. We report the results here of simulation-based experiments designed to tell us under what research conditions different mixes of methods can be expected to yield more accurate inferences. We also discuss, at a high level, the implications of the framework for strategies of qualitative case-selection.

\hypertarget{intuitions-does-a-sufficiently-large-n-always-trump-k}{%
\section{\texorpdfstring{Intuitions: Does a sufficiently large \(N\) always trump \(K\)?}{Intuitions: Does a sufficiently large N always trump K?}}\label{intuitions-does-a-sufficiently-large-n-always-trump-k}}

We begin by considering the learning that occurs upon observing outcomes from varying numbers of cases given different \(XY\) data ranging from small to quite large.

The goal here is to build up intuitions on how beliefs change given different observations and how this affects posterior variance. We address the question is a very controlled setting in which

\begin{itemize}
\tightlist
\item
  a researcher is confronted with balanced \(X,Y\) data that exhibits no correlation
\item
  the researcher can seek information on a highly informative (``doubly decisive'') clue, \(K\), on cases in the \(X=Y=1\) cell
\item
  though not known in advance, it turns out that each time the researcher finds evidence suggesting that the case in question is a \(b\) type (that is, indeed \(X\) causes \(Y\))
\item
  the selection probabilities are either unknown or known with near certainty
\end{itemize}

In this case, we can expect that seeing evidence of \(b\) types will shift the researcher to increase her beliefs on the average causal effect. But how strong will these shifts be and how does this depend on the amount of \(X, Y\) data available? Does the signal from the \(XY\) data drown out any signal from the \(K\) data?

Figure \ref{morn} reports answers to these questions. For the simulations we varied the size of the \(XY\) data from 5 observations in each cell to 5000. The key features of the simulations are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  When assignment propensities are unknown---as for example with observational data---the clue information shifts beliefs independent of how many \(XY\) cases there are. The key insight is that the clue provides information on assignment propensities which are informative about the share of each type in each cell and these shares determine treatment effects no matter how large or small the cells are.
\item
  When assignment propensities are known with large data there is a lot of learning over the distribution of types in a population (at least up to differences in types rather than the distribution of fundamental types). Clue information shifts beliefs about the types of the particular cases for which clue data is gathered but has almost no effect on estimates of the population estimand.
\item
  Not visible from the figure however: in the case with large \(N\) and known propensities, observation on many \(b\) types in the \(X=Y=1\) cell, while not changing estimates of \emph{average} treatment effects (\(\lambda_b- \lambda_s\)) does affect beliefs on \emph{heterogeneity}, because the data is more consistent with a world with many \(a\)s and \(b\)s than one with many \(c\)s and \(d\)s. For example if there were 10,000 data points in each \(X,Y\) and clue information on 20 cases in the \(X=Y=1\) cell suggest that these are all \(b\) types, then the conclusion would be that 95\% of the cases are \(a\) and \(b\) types, in equal proportion.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> Y <- K"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\StringTok{"(Y[X=0, K=1]==1) | (Y[X=0, K=0]==0)"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{set_parameters}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{.99}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.25}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We see that prior beliefs are for a 0 average effect which rises to approximately .5 for cases in which \(K=1\) is observed and falls to -.5 for cases in which \(K=0\) is observed.

\begin{tabular}{l|l|l|r|r}
\hline
Query & Given & Using & mean & sd\\
\hline
ATE & - & priors & -0.006 & 0.341\\
\hline
ATE & K==1 & priors & 0.498 & 0.222\\
\hline
ATE & K==0 & priors & -0.499 & 0.222\\
\hline
\end{tabular}

We see little difference in the prior on estimands. Despite this an important difference is that the model that allows for confounding also allows for updating on confounding, which the simple model does not.

\begin{tabular}{l|l|l|r|r}
\hline
Query & Given & Using & mean & sd\\
\hline
ATE & - & priors & -0.008 & 0.342\\
\hline
ATE & K==1 & priors & 0.502 & 0.224\\
\hline
ATE & K==0 & priors & -0.501 & 0.220\\
\hline
\end{tabular}

We now examine inferences given different data strategies:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
\NormalTok{  wd_}\DecValTok{1}\NormalTok{_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{wide_or_deep}\NormalTok{(model, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
  \KeywordTok{write_rds}\NormalTok{( wd_}\DecValTok{1}\NormalTok{_}\DecValTok{2}\NormalTok{, }\StringTok{"saved/wd_1_2.rds"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{wd_}\DecValTok{1}\NormalTok{_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/wd_1_2.rds"}\NormalTok{)}


\NormalTok{wd_sim <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model, n_K, n_fold) \{}
  
\NormalTok{    df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{Y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{K =} \OtherTok{NA}\NormalTok{) }\OperatorTok{%>%}
\StringTok{      }\KeywordTok{slice}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{row_number}\NormalTok{(), n_fold))}
\NormalTok{    df <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(df, }\DataTypeTok{K =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n_K), }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{n}\NormalTok{()}\OperatorTok{-}\NormalTok{n_K)))}
    
\NormalTok{    given <-}\StringTok{  }\KeywordTok{collapse_data}\NormalTok{(df, model)}

\NormalTok{    updated <-}\StringTok{ }\NormalTok{gbiqq}\OperatorTok{::}\KeywordTok{gbiqq}\NormalTok{(model, }\DataTypeTok{data =}\NormalTok{ df, }\DataTypeTok{stan_model =}\NormalTok{ fit)}
    \KeywordTok{query_model}\NormalTok{(updated, }
                  \DataTypeTok{queries =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ATE =} \StringTok{"Y[X=1] - Y[X=0]"}\NormalTok{), }
                  \DataTypeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
  
\NormalTok{\}}

\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
  \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{exists}\NormalTok{(}\StringTok{"fit"}\NormalTok{)) fit <-}\StringTok{ }\NormalTok{gbiqq}\OperatorTok{::}\KeywordTok{fitted_model}\NormalTok{()}
\NormalTok{  out <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(n_K) \{}\KeywordTok{sapply}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(k) }\KeywordTok{wd_sim}\NormalTok{(model, n_K, k))\})}
  \KeywordTok{write_rds}\NormalTok{(out, }\StringTok{"saved/wide_or_deep_XMY.rds"}\NormalTok{)}
\NormalTok{  \}}

\NormalTok{wd <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/wide_or_deep_XMY.rds"}\NormalTok{)}
\NormalTok{wd <-}\StringTok{ }\KeywordTok{t}\NormalTok{(wd[}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{14}\NormalTok{), ])}
\KeywordTok{rownames}\NormalTok{(wd) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Clues on 4 cases"}\NormalTok{, }\StringTok{"Clues on 8 cases"}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(wd) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"N=8"}\NormalTok{, }\StringTok{"N=40"}\NormalTok{, }\StringTok{"N=80"}\NormalTok{)}
\KeywordTok{kable}\NormalTok{((wd))}

\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
  \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{exists}\NormalTok{(}\StringTok{"fit"}\NormalTok{)) fit <-}\StringTok{ }\NormalTok{gbiqq}\OperatorTok{::}\KeywordTok{fitted_model}\NormalTok{()}
\NormalTok{  out <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(n_K) \{}\KeywordTok{sapply}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(k) }\KeywordTok{wd_sim}\NormalTok{(model_confound, n_K, k))\})}
  \KeywordTok{write_rds}\NormalTok{(out, }\StringTok{"saved/wide_or_deep_XMY2.rds"}\NormalTok{)}
\NormalTok{  \}}

\NormalTok{wd2 <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/wide_or_deep_XMY2.rds"}\NormalTok{)}
\NormalTok{wd2 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(wd2[}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{14}\NormalTok{), ])}
\KeywordTok{rownames}\NormalTok{(wd2) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Clues on 4 cases"}\NormalTok{, }\StringTok{"Clues on 8 cases"}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(wd2) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"N=8"}\NormalTok{, }\StringTok{"N=40"}\NormalTok{, }\StringTok{"N=80"}\NormalTok{)}
\KeywordTok{kable}\NormalTok{((wd2))}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluating-strategies}{%
\section{Evaluating strategies}\label{evaluating-strategies}}

As a metric of the returns from different research strategies we calculate the \emph{expected} inaccuracy in the estimation of the average treatment effect, as given by:

\[\mathcal{L}=\mathbb{E}_\theta(\mathbb{E}_{\mathcal{D}|\theta}(\tau(\theta)-\hat{\tau}(\mathcal{D}))^2) \]

where \(\tau(\theta)\) is the value of \(\lambda_b-\lambda_a\) (the average treatment effect) given \(\theta\), and \(\hat{\tau}(\mathcal{D})\) is the \emph{estimate} of this treatment effect (the mean posterior value) that is generated following some realization of data \(\mathcal{D}\). Thus, if some \(\theta\) characterized the true state of the world, then \(\mathbb{E}_{\mathcal{D}|\theta}(\tau^\theta-\hat{\tau})^2\) is the expected error in estimation of the causal effect given different realizations of the data, \(\mathcal{D}\), that could obtain in this state of the world. \(\mathcal{L}\) is then the expected value of these errors given prior beliefs over possible values of \(\theta\).

Note that, while we focus on errors on estimated average causal effects, similar exercises could assess how cross- and within-case observations distinctively contribute to other estimands \textbar{} including the causal explanations for individual cases and the validity of causal theories \textbar{} as well as to learning about inferential assumptions themselves (assignment and clue probabilities).
For all simulations, prior distributions are drawn with parameters as described in the Supplementary Materials (\S \ref{AppSimNotes}, Table \ref{tab:sims}). Priors on the type distribution are drawn from a Dirichlet distribution; priors for each of the \(\pi\) and \(\phi\) values are drawn independently from Beta distributions. We note that, while by construction priors on each parameter are independent, this will not generally be the case for posterior distributions. In most cases we simulate the prior distribution using 5200 draws of each parameter. For most experiments we then systematically vary the prior distribution for one parameter of the research situation between two extreme positions. We then calculate the expected posterior from each possible data realization and, in turn, the expected loss in estimates of treatment effects for a range of levels of investment in qualitative and quantitative evidence.

A few further features of the experiments below are worth noting. First, our illustrations focus on learning about population-level causal effects; however, the model can yield results about the benefits of alternative research designs for estimating a wide range of other quantities of interest, such as case-specific causal explanations or clue probabilities. Second, while we focus on the search for a \emph{single} clue in each case, the analysis can be extended to the case of an arbitrarily large set of clues. Third, in many of these experiments, the probative values are set at doubly decisive levels for all \(\phi\) parameters, and thus focus on the very optimistic case of maximally informative process tracing. Fourth, we illustrate tradeoffs at low levels of \(n\), but the model can be employed to make choices for arbitrarily large numbers of cases. Finally, we note that some results may be sensitive to the choice of priors. The results below should thus be understood as an illustration of the utility of the BIQQ framework for guiding research choices, rather than as a set of more general prescriptive design rules.

\hypertarget{varieties}{%
\section{Varieties of mixing}\label{varieties}}

What are the marginal gains from additional pieces of correlational and process-tracing evidence for the accuracy of causal estimates? Figure \ref{morn} displays results, plotting the errors associated with different mixes of correlational and process data.

\begin{itemize}
\tightlist
\item
  \textbf{Qualitative and quantitative data can act as partial substitutes for assessing causal effects}.
\item
  \textbf{The \emph{relative} marginal gains from going wider and going deeper vary with the study design}.
\item
  \textbf{Optimal strategies might involve going deep in a subsample of cases only.}
\end{itemize}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/m_or_n.pdf}
\caption{{Expected errors in the estimation of average treatment effects for designs in which $X, Y$, data is sought in $n$ studies (horizontal axis) and clue data is sought within $m$ of these. The shading of dots indicates the proportion of cases for which within-case data is sought (white = none; black = all). For small sample sizes ($n \in \{1,2,3,4\}$) we show results for all designs ($m \in \{1,2,\dots, n\})$. For larger sample sizes, we show only designs with clues sought in 0, half, and all cases.}}
\label{morn}
\end{figure}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
  & strategy & Query & Subset & estimand & estimates & MSE & post\_var\\
\hline
estimands\_database & Prior & Q 1 & All & 0.135 & 0.134 & 0.024 & 0.017\\
\hline
estimands\_database1 & A\_online & Q 1 & All & 0.135 & 0.141 & 0.018 & 0.016\\
\hline
estimands\_database2 & B\_offline & Q 1 & All & 0.135 & 0.136 & 0.024 & 0.017\\
\hline
estimands\_database3 & C\_X1Y1 & Q 1 & All & 0.135 & 0.136 & 0.020 & 0.016\\
\hline
estimands\_database4 & D\_random & Q 1 & All & 0.135 & 0.138 & 0.020 & 0.016\\
\hline
\end{tabular}

\hypertarget{probative-value-of-clues}{%
\section{Probative value of clues}\label{probative-value-of-clues}}

\hypertarget{effect-heterogeneity}{%
\section{Effect Heterogeneity}\label{effect-heterogeneity}}

\hypertarget{uncertainty-regarding-assignment-processes}{%
\section{Uncertainty Regarding Assignment Processes}\label{uncertainty-regarding-assignment-processes}}

\hypertarget{uncertainty-regarding-the-probative-value-of-clues}{%
\section{Uncertainty regarding the probative value of clues}\label{uncertainty-regarding-the-probative-value-of-clues}}

\hypertarget{caseselection}{%
\chapter{Case selection as a Decision Problem}\label{caseselection}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

With a causal model in hand, together with priors over parameters, you can assess in advance what conclusions you will draw from different observations and assess what kinds of observations are most worth seeking. We draw out the implications of this idea for case selection.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Full set of analyses

Graph full set

\begin{verbatim}
## Joining, by = c("Query", "given", "model", "N")
\end{verbatim}

\includegraphics{ii_files/figure-latex/unnamed-chunk-92-1.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-2.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-3.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-4.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-5.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-6.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-7.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-8.pdf}

\begin{verbatim}
## Warning: Removed 100 rows containing missing values
## (geom_point).
\end{verbatim}

\begin{verbatim}
## Warning: Removed 100 rows containing missing values
## (geom_errorbar).
\end{verbatim}

\includegraphics{ii_files/figure-latex/unnamed-chunk-92-9.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-10.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-11.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-12.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-13.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-14.pdf} \includegraphics{ii_files/figure-latex/unnamed-chunk-92-15.pdf}

\hypertarget{simple-walks-through}{%
\section{Simple walks through}\label{simple-walks-through}}

\hypertarget{learning-from-mediators-on-and-off-the-regression-line-for-an-x-m-y-model}{%
\subsection{Learning from mediators on and off the regression line for an X-\textgreater{}M-\textgreater{}Y model}\label{learning-from-mediators-on-and-off-the-regression-line-for-an-x-m-y-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# source("_packages_used.R")}

\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> M -> Y"}\NormalTok{)}
  
\NormalTok{gen_observed_data <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{n00=}\DecValTok{1}\NormalTok{, }\DataTypeTok{n01=}\DecValTok{1}\NormalTok{, }\DataTypeTok{n10=}\DecValTok{1}\NormalTok{, }\DataTypeTok{n11=}\DecValTok{1}\NormalTok{,}\DataTypeTok{times =} \DecValTok{1}\NormalTok{) \{}
  \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{Y =}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{M =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{ns =} \KeywordTok{c}\NormalTok{(n00, n01, n10, n11)}\OperatorTok{*}\NormalTok{times) }\OperatorTok{%>%}
\StringTok{    }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{uncount}\NormalTok{(ns) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{collapse_data}\NormalTok{(model, }\DataTypeTok{remove_family =} \OtherTok{TRUE}\NormalTok{)\}}


\CommentTok{# Data strategies}

\NormalTok{data_strats <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{N =} \DecValTok{4}\NormalTok{) \{}
  \ControlFlowTok{if}\NormalTok{(N}\OperatorTok{%%}\DecValTok{2} \OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{) }\KeywordTok{stop}\NormalTok{(}\StringTok{"even Ns only please"}\NormalTok{)}
  \KeywordTok{list}\NormalTok{(}
    \DataTypeTok{all_on =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\KeywordTok{c}\NormalTok{(N,N)}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DataTypeTok{withins =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{,}
                  \DataTypeTok{conditions =} \KeywordTok{list}\NormalTok{(}\StringTok{"X==0 & Y==0"}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{)),}
    \DataTypeTok{all_off =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\KeywordTok{c}\NormalTok{(N,N)}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DataTypeTok{withins =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{,}
                   \DataTypeTok{conditions =} \KeywordTok{list}\NormalTok{(}\StringTok{"X==0 & Y==1"}\NormalTok{, }\StringTok{"X==1 & Y==0"}\NormalTok{)),}
    \DataTypeTok{all_x1 =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\KeywordTok{c}\NormalTok{(N,N)}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DataTypeTok{withins =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{,}
                   \DataTypeTok{conditions =} \KeywordTok{list}\NormalTok{(}\StringTok{"X==1 & Y==0"}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{)),}
    \DataTypeTok{all_y1 =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\KeywordTok{c}\NormalTok{(N,N)}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DataTypeTok{withins =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{,}
                   \DataTypeTok{conditions =} \KeywordTok{list}\NormalTok{(}\StringTok{"X==0 & Y==1"}\NormalTok{, }\StringTok{"X==1 & Y==1"}\NormalTok{))}
\NormalTok{  )}
\NormalTok{\}}

\CommentTok{# Queries}

\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{exists}\NormalTok{(}\StringTok{"fit"}\NormalTok{)) fit <-}\StringTok{ }\KeywordTok{fitted_model}\NormalTok{()}

\CommentTok{# Diagnosis function }

\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
\NormalTok{test_corr <-}\StringTok{ }
\StringTok{  }\KeywordTok{diagnose_strategies}\NormalTok{(}
    \DataTypeTok{analysis_model =}\NormalTok{ model,}
    \DataTypeTok{data_strategies =} \KeywordTok{data_strats}\NormalTok{(}\DecValTok{2}\NormalTok{),}
    \DataTypeTok{given =} \KeywordTok{gen_observed_data}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{),}
    \DataTypeTok{queries =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ATE =} \StringTok{"Y[X=1]-Y[X=0]"}\NormalTok{, }
                   \DataTypeTok{a =} \StringTok{"Y[X=1]<Y[X=0]"}\NormalTok{, }
                   \DataTypeTok{b =} \StringTok{"Y[X=1]>Y[X=0]"}\NormalTok{, }
                   \DataTypeTok{c =} \StringTok{"(Y[X=1]==0) & (Y[X=0]==0)"}\NormalTok{, }
                   \DataTypeTok{d =} \StringTok{"(Y[X=1]==1) & (Y[X=0]==1)"}\NormalTok{,}
                   \DataTypeTok{a1 =} \StringTok{"M[X=1]<M[X=0]"}\NormalTok{, }
                   \DataTypeTok{b1 =} \StringTok{"M[X=1]>M[X=0]"}\NormalTok{, }
                   \DataTypeTok{c1 =} \StringTok{"(M[X=1]==0) & (M[X=0]==0)"}\NormalTok{, }
                   \DataTypeTok{d1 =} \StringTok{"(M[X=1]==1) & (M[X=0]==1)"}\NormalTok{,}
                   \DataTypeTok{a2 =} \StringTok{"Y[M=1]<Y[M=0]"}\NormalTok{, }
                   \DataTypeTok{b2 =} \StringTok{"Y[M=1]>Y[M=0]"}\NormalTok{, }
                   \DataTypeTok{c2 =} \StringTok{"(Y[M=1]==0) & (Y[M=0]==0)"}\NormalTok{, }
                   \DataTypeTok{d2 =} \StringTok{"(Y[M=1]==1) & (Y[M=0]==1)"}
\NormalTok{                   ),}
    \DataTypeTok{subsets =} \KeywordTok{list}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{,  }\OtherTok{TRUE}\NormalTok{,  }\OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{sims =} \DecValTok{8000}\NormalTok{,}
    \DataTypeTok{fit =}\NormalTok{ fit, }
    \DataTypeTok{refresh =} \DecValTok{1000}\NormalTok{,}
    \DataTypeTok{iter =} \DecValTok{8000}\NormalTok{)}

\KeywordTok{write_rds}\NormalTok{(test_corr, }\StringTok{"saved/test_corr.rds"}\NormalTok{)}

\NormalTok{test_flat <-}\StringTok{ }
\StringTok{  }\KeywordTok{diagnose_strategies}\NormalTok{(}
    \DataTypeTok{analysis_model =}\NormalTok{ model,}
    \DataTypeTok{data_strategies =} \KeywordTok{data_strats}\NormalTok{(}\DecValTok{2}\NormalTok{),}
    \DataTypeTok{given =} \KeywordTok{gen_observed_data}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{),}
    \DataTypeTok{queries =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ATE =} \StringTok{"Y[X=1]-Y[X=0]"}\NormalTok{, }
                   \DataTypeTok{a =} \StringTok{"Y[X=1]<Y[X=0]"}\NormalTok{, }
                   \DataTypeTok{b =} \StringTok{"Y[X=1]>Y[X=0]"}\NormalTok{, }
                   \DataTypeTok{c =} \StringTok{"(Y[X=1]==0) & (Y[X=0]==0)"}\NormalTok{, }
                   \DataTypeTok{d =} \StringTok{"(Y[X=1]==1) & (Y[X=0]==1)"}\NormalTok{,}
                   \DataTypeTok{a1 =} \StringTok{"M[X=1]<M[X=0]"}\NormalTok{, }
                   \DataTypeTok{b1 =} \StringTok{"M[X=1]>M[X=0]"}\NormalTok{, }
                   \DataTypeTok{c1 =} \StringTok{"(M[X=1]==0) & (M[X=0]==0)"}\NormalTok{, }
                   \DataTypeTok{d1 =} \StringTok{"(M[X=1]==1) & (M[X=0]==1)"}\NormalTok{,}
                   \DataTypeTok{a2 =} \StringTok{"Y[M=1]<Y[M=0]"}\NormalTok{, }
                   \DataTypeTok{b2 =} \StringTok{"Y[M=1]>Y[M=0]"}\NormalTok{, }
                   \DataTypeTok{c2 =} \StringTok{"(Y[M=1]==0) & (Y[M=0]==0)"}\NormalTok{, }
                   \DataTypeTok{d2 =} \StringTok{"(Y[M=1]==1) & (Y[M=0]==1)"}
\NormalTok{                   ),}
    \DataTypeTok{subsets =} \KeywordTok{list}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{,  }\OtherTok{TRUE}\NormalTok{,  }\OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{sims =} \DecValTok{8000}\NormalTok{, }\DataTypeTok{fit =}\NormalTok{ fit, }\DataTypeTok{refresh =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{iter =} \DecValTok{8000}\NormalTok{)}


\KeywordTok{write_rds}\NormalTok{(test_flat, }\StringTok{"saved/test_flat.rds"}\NormalTok{) }
\NormalTok{\}}

\NormalTok{test_corr <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/test_corr.rds"}\NormalTok{) }
\NormalTok{test_flat <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/test_flat.rds"}\NormalTok{) }

\CommentTok{# key thing to note is for $all_x1[[1]] a2 = b2}
\CommentTok{# and for $all_x1[[2]] a1 = b1 }
\CommentTok{# so in both cases shareas of a and b remains contstant}
\end{Highlighting}
\end{Shaded}

For flat prior data:

\begin{itemize}
\item
  if we examine two cases in different quadrants \emph{on the diagonal} (one \(X=Y=0\) case and one \(X=Y=1\) case) :

  \begin{itemize}
  \tightlist
  \item
    if we find that \(M\) is the same in the two cases then we increase our belief that \(X\) has no effect at all on \(Y\) and reduce our confidence that \(X\) had a positive or a negative effect on \(Y\). Since we are looking on the regression line however, our confidence that \(X\) had a \emph{positive} effect on \(Y\) is more strongly reduced, producing a posterior centered on a small negative effect.
  \item
    Conversely if we see that \(M\) is different in the two cases, then we have a correlation of the same sign between both \(X\) and \(M\) and between \(M\) and \(Y\) cases. We increase our confidence that \(X\) mattered for \(Y\) in general an din particular that it had a positive effect, resulting in a posterior centered on a small positive ATE.
  \end{itemize}
\item
  if we examine two cases in different quadrants \emph{off the diagonal} (one \(X=0, Y=1\) case and one \(X=1, Y=0\) case) :

  \begin{itemize}
  \tightlist
  \item
    if we find that \(M\) is the same in the two cases then we increase our belief that \(X\) has no effect at all on \(Y\) and reduce our confidence that \(X\) had a negative effect \(Y\) (producing a posterior centered on a small positive effect).
  \item
    Conversely if we see that \(M\) is different in the two cases, then we have a correlation (of different signs) between both \(X\) and \(M\) and between \(M\) and \(Y\) cases. We increase our confidence that \(X\) mattered for \(Y\) in general and in particular that it had a negative effect, resulting in a posterior centered on a small negative ATE.
  \end{itemize}
\item
  If we examine data conditioning on the value of \(X\) but with variation on \(Y\) (for instance \(X=1, Y=0\) and \(X=1, Y=1\))
  data patterns do not discriminate between X having a positive effect on Y an d X having a negative effect on Y. However variation in \(M\) is more consistent with \(M\) being responsive to \(X\) and thus the chances that \(X\) matters, positively or negatively, overall. In teh case with flat data this changes beliefs on positve and negative effects but not the difference between them. The ate then remains unchanged. FLAG work though intuition more
\item
  If we examine data conditioning on the value of \(Y\) but with variation on \(X\) (for instance \(X=0, Y=1\) and \(X=1, Y=1\))
  data patterns do not discriminate between X having a positive effect on Y an d X having a negative effect on Y. However variation in \(M\) is more consistent with \(M\) being responsive to \(X\) and thus the chances that \(X\) matters, positively or negatively, overall. In teh case with flat data this changes beliefs on positve and negative effects but not the difference between them. The ate then remains unchanged.
\end{itemize}

For correlated data:

Similar logics apply but the effects are stronger for evidence on the regression line. The reason is that given correlated data we believe there are more units with positive effects than negative effects. When we find evidence against causal relations on teh regression line that reduces our confidence for types with positive effects more than for types with negative effects and teh difference between teh shares with positive effects and negative eeffects smaller due to the fact that the beliefs in the shares with negative effects is not so strongly reduced. Wehen we find evidence for causal relations this magnifies teh difference between beliefs in shares positive and shares negatives; these effects are magnified however since our confidence for the positive effects change more than for the negative effects, since we are examining cases on the regression line. COnversely when examining cases of teh regression line, the two forces offset each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make sense given process tracing approach}

\NormalTok{long_data <-}\StringTok{  }\KeywordTok{expand_data}\NormalTok{(}\KeywordTok{gen_observed_data}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{), model)}

\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
\NormalTok{  updated <-}\StringTok{ }\KeywordTok{update_model}\NormalTok{(model, long_data, }\DataTypeTok{iter =} \DecValTok{14000}\NormalTok{, }\DataTypeTok{chains =} \DecValTok{12}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{set_parameters}\NormalTok{(}\DataTypeTok{param_type =} \StringTok{"posterior_mean"}\NormalTok{)}
  \KeywordTok{write_rds}\NormalTok{(updated, }\StringTok{"saved/ch13_longupdated.rds"}\NormalTok{) }
\NormalTok{\}}
\NormalTok{updated <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/ch13_longupdated.rds"}\NormalTok{) }

\KeywordTok{get_parameters}\NormalTok{(updated)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    X.0    X.1   M.00   M.01   M.11   Y.00   Y.01 
## 0.4995 0.5005 0.3148 0.3700 0.3152 0.2987 0.4028 
##   Y.11 
## 0.2984
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{conditional_inferences}\NormalTok{(updated, }\DataTypeTok{query =} \StringTok{"Y[X=1]!=Y[X=0]"}\NormalTok{, }\DataTypeTok{given =} \StringTok{"!is.na(X) & !is.na(Y) & !is.na(M)"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(X, Y, M)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   X M Y posterior    prob
## 1 0 0 0      0.31 0.24000
## 2 0 1 0      0.00 0.04704
## 3 0 0 1      0.00 0.10209
## 4 0 1 1      0.00 0.11041
## 5 1 0 0      0.00 0.11053
## 6 1 1 0      0.00 0.10245
## 7 1 0 1      0.00 0.04702
## 8 1 1 1      0.31 0.24047
\end{verbatim}

\hypertarget{learning-about-confounding}{%
\section{Learning about confounding}\label{learning-about-confounding}}

X -\textgreater{} M -\textgreater{} Y with confounding

M should be very informative for X-\textgreater{}Y relationship

\hypertarget{base-model-one-path}{%
\section{Base model, one path}\label{base-model-one-path}}

For on regression line cases, you can see cases where M correlates positively with X and Y correlates positively to Y; or cases where M is correlated with neither

For off regression line cases, you can see cases where M correlates negatively with X and positively with Y, or vice versa; and you can see cases where M is correlated with neither.

With no restrictions, you can't actually learn in a within-case sense because you have flat priors over X-\textgreater{}M and M-\textgreater{}Y effects. You're only able to learn from correlations.

A critical decision for scholars employing mixed methods is to determine which cases are most valuable for within-case analysis.

A host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of \(X\), \(Y\) data. Perhaps the most common strategy is to select cases in which \(X=1\) and \(Y=1\) and look to see whether in fact \(X\) caused \(Y\) in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases ``on the regression line'' or, for some purposes, cases ``off the regression line'' (e.g., \citet{Lieberman2005nested}). Some scholars suggest ensuring variation in \(X\) (most prominently, \citet{king1994designing}), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on \(X\) or \(Y\) (\citet{FL2008}, \citet{HerronQuinn}).

Which cases you should choose will likely depend on the purposes to which you want to put them.

A matching strategy for instance---selecting cases that are comparable on many features but that differ on \(X\)---replicates at a small scale the kind of inference done by matching estimators with large-\(n\) data. The strategy emphasize the inferences to be made from \(X,Y\) variation rather than inferences drawn specifically from within case information beyond what is available in the measurement of \(X\) and \(Y\). (Citations needed.)

Other treatments seek to use qualitative information to check assumptions made in \(X, Y\) analysis: for example, is the measurement of \(X\) and \(Y\) reliable in critical cases? (Citations needed) For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the \(X,Y\) inferences: for example influential cases that have unusually extreme values on \(X\) and \(Y\).\footnote{Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.} Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims.

A third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman's ``model-building'' mode of ``nested analysis'' (\citet{Lieberman2005nested}). Here it may be cases off the regression line that are of interest.

Weller and Barnes (CITE article) on case selection focus on (a) X/Y relations and (b) whether the cases are useful for hypothesis generation.

In what follows, we focus on a simpler goal: given existing \(X, Y\) data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of \(X\) on \(Y\)?

The basic insight of this chapter is simple enough: \emph{the optimal strategy for case selection for a model-based analysis can be determined by the model and the query}, just as we saw for the optimal clue-selection strategy in Chapter \ref{Clues}. Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as:

\begin{itemize}
\tightlist
\item
  go where the probative value is, and
\item
  sample from \(X\) and \(Y\) values in proportion to their occurrence in the population,
\item
  invest in collections of cases that provide complementary learning.
\end{itemize}

Beyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with \(X, Y, K\) data.

\hypertarget{explorations}{%
\section{Explorations}\label{explorations}}

Most closely related to our analysis in this chapter is the contribution of \citet{HerronQuinn}, who build on \citet{SeawrightGerring2008}. While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches. As we do, \citet{HerronQuinn} focus on a situation with binary \(X,Y\) data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, \(Z_i\) is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to ``treatment'', or an \(X=1\) value). \citet{HerronQuinn} assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly.

Our setup differs from that in \citet{HerronQuinn} in a few ways. \citet{HerronQuinn} paramaterize differently, though this difference is not important.\footnote{\citet{HerronQuinn} have a parameter \(\theta\) that governs the distribution of data over \(X\) and \(Y\) and then, conditional on \(X,Y\) values, a set of parameters \(\psi_{xy}\) that describe the probability of a case's being of a given causal type. We take both \(\theta\) and \(\psi_{xy}\) to derive from the fundamental distribution of causal types and assignment probabilities. Thus, for example, \(\psi_{00}\) from \citet{HerronQuinn} corresponds to \(\frac{(1-\pi_b)\lambda_b}{(1-\pi_b)\lambda_b + (1-\pi_c)\lambda_c}\) in our notation. The difference in paramaterization does have implications for interpretations of the priors. For example flat priors over \(\theta\) and \(\psi\) implies a tighter distribution that a uniform prior over the causal types. In fact \citet{HerronQuinn} use priors with greater variance than uniform in any event.} Perhaps the most important difference between our analysis and that in \citet{HerronQuinn} is that we connect the inference strategy to process-tracing approaches. Whereas \citet{HerronQuinn} assume that causal types can be read directly, we assume that these are inferred \emph{imperfectly} from clues. As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of \(X\). And, as in the baseline model, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases.

Here we assume that the case selection decision is made after observing the \(XY\) distribution and we explore a range of different possible contingency tables. In \citet{HerronQuinn} the distribution from which the contingency tables are drawn is fixed, though set to exhibit an expected observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large \(XY\) data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases.

Another important difference, is that in many of their analyses, \citet{HerronQuinn} take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has \emph{beliefs} about the true treatment effect that correspond to their priors, and for whom there is therefore no \emph{expected} bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher's beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)?

Finally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by \citet{SeawrightGerring2008}) and evaluating them, we define a strategy as the particular distribution over \(XY\) cells to be examined and proceed to examine \emph{every possible strategy} given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies---those strategies that perform similarly---emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies.

Despite these various differences, our results will agree in key ways with those in \citet{HerronQuinn}.

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

The procedure requires as inputs (i) a causal model, (ii) any data we have already observed, and (iii) the causal query we seek to answer.

The general intuition is that we can use the causal model and any previously observed data to estimate what observations we are more or less likely to make under a given case-selection strategy, and then figure out how far off from the (under the model) true estimand we can expect to be under the strategy, given whatever causal question we seek to answer.

Suppose that we want to estimate the average treatment effect of \(X\) on \(Y\) in a population and have initially observed \(X\) and \(Y\) data on a set of cases.

Suppose that we are now considering gathering process-tracing evidence for one of these cases to inform our estimate of the ATE. There are many different case-selection strategies we might pursue, and each of these can give rise to different possible data and thus to different possible conclusions. What should we do?

\textbf{DAG}. We start, as always, with a DAG representing our beliefs about which variables we believe to be direct causes of other variables. For the current illustration, suppose that we are operating with a simple mediation model, \(X \rightarrow M \rightarrow Y\).

\textbf{Given data.} If we have already observed something in a set of cases, we can use this information to condition our strategy for searching for further information. For instance, if we have observed \(X\)'s and \(Y\)'s value in a set of cases, we might select cases for process tracing based on their values of \(X\) and \(Y\). Further, what we have already observed in the cases may constrain what possible data we could end up with once we have collected the additional (process tracing) data.

\textbf{Priors}. As when conducting mixed-method inference, we can set qualitative restrictions and/or differential quantitative weights on the (possibly conditional) nodal types in the model. And we can indicate our uncertainty over the latter, by setting the \(\alpha\) parameters of the relevant Dirichlet distributions. For the current example, let us define restrictions at both the \(M\) and \(Y\) nodes, positing beliefs that \(X\) never has a negative effect on \(M\) and that \(M\) never has a negative effect on \(Y\). Let us further assume that we have flat priors over the remaining nodal types and posit similar assignment propensities for all types (no unobserved confounding).

\textbf{Query}. We define our query. This might, for instance, be the share of cases in the population in which \(X\) has a positive effect on \(Y\); or it might be \(X\)'s average effect on \(Y\). We can use the general procedure to identify case-selection strategies for any causal query that can be defined on a DAG. And, importantly, the optimal case-selection strategy may depend on the query. For instance, the best case-selection strategy for estimating the average causal effect of \(X\) on \(Y\) may not be the same as the best strategy for figuring out for what proportion of the population \(X\) has a positive effect on \(Y\).

\textbf{Define one or more strategies}. A strategy is defined, generically, as the search for data on a given set of \emph{nodes}, in a given \emph{number} of cases randomly selected \emph{conditional} on some information we already have about potential cases. Let us assume here that our strategy will involve uncovering \(M\)'s value in 1 case---but we are wondering how to choose this case. Consider four possible strategies, conditional on the \(X\) and \(Y\) values that we already know. We could do process tracing on a randomly selected \(X=1, Y=1\) case, a randomly selected \(X=0, Y=0\) case, the \(X=1, Y=0\) case, or the \(X=0, Y=1\) case. We itemize this set of possible strategies in the first column in Table \ref{tab:caseselect1}.

\textbf{Possible data}. For each strategy, there are multiple possible sets of data that we could end up observing. In particular, the data we could end up with will be the \(X,Y\) patterns we have already observed plus \emph{either} \(M=0\) \emph{or} \(M=1\) in the case that our strategy leads us to select for process tracing. We represent the data possibilities (showing just the possible \(M\) values) in the second column in Table \ref{tab:caseselect1}. Thus, for instance, for a strategy in which we choose a random \(X=1, Y=1\) case, we could end up observing the initial \(X,Y\) pattern plus \(M=0\) in one of the \(X=1, Y=1\) cases, or the initial \(X,Y\) pattern plus \(M=1\) in one of the \(X=1, Y=1\) cases.

\textbf{Probability of the data}. We now calculate a probability of each possible data realization, given the model and the data (the \(X\)'s and \(Y\)'s) that we have already observed. In practice, we do this in \texttt{gbiqq} via simulation. Starting with the model together with our priors, we update our beliefs about \(\lambda\) based on the initial \(X,Y\) data. This posterior now represents our \emph{prior} for the purposes of the process tracing; it represents what we believe about causal-type share allocations in the population, having seen the \(X,Y\) data only. We then use this posterior to draw a series of \(\lambda\) values.

Given that the ambiguity matrix gives us the mapping from causal types to data realizations, we can calculate for each \(lambda\) draw the probability of each data possibility given that particular \(\lambda\) and the strategy. We then average across repeated \(\lambda\) draws. (Since \(\lambda\)'s are being drawn from our prior, we are automatically weighting more heavily those \(\lambda\)'s that we believe to be most likely.) We show the data probabilities in the third column of Table \ref{tab:caseselect1}: one probability for each data-possibility given each strategy.

\textbf{Posterior variance on estimate given the data}. For each data possibility, we can then use \texttt{gibiqq} to ask what inference we would get from each data possibility, given whatever query we seek to answer. What we are in fact interested in for the purposes of case selection is the \emph{variance} of the posterior. We indicate in the fourth column of Table \ref{tab:caseselect1} the posterior variance for each possible data realization.

\textbf{Expected posterior variance under each strategy}. The quantity of ultimate interest is the posterior variance that we expect to end up with under each \emph{strategy}. Calculating this expectation is now elementary as we have both the posterior variance arising from each data possibility and the probability of each data possibility (given our prior beliefs and the data already observed). The expected posterior variance is simply an average of the posterior variances under each data possibility, weighted by the probability of each data possibility. The final column provides the expected posterior variances, one for each strategy.

\hypertarget{illustration-2}{%
\subsection{Illustration}\label{illustration-2}}

Consider a situation in which one has access to \(X,Y\) data on just six cases of the form:

\begin{tabular}{l|r}
\hline
event & count\\
\hline
X0Y0 & 2\\
\hline
X1Y0 & 1\\
\hline
X0Y1 & 1\\
\hline
X1Y1 & 2\\
\hline
\end{tabular}

We will consider three strategies:

\begin{itemize}
\tightlist
\item
  Strategy \(A\) chooses two cases on the regression line (one in the \(X=Y=0\) cell and one in the \(X=Y=1\) cell)
\item
  Strategy \(B\) chooses off the regression line
\item
  Strategy \(C\) selects on the dependent variable -- choosing cases with \(X=1, Y = 1\).
\end{itemize}

Different strategies yield different data types. Each one of these is likely to arise with a different probabilities and is associated with a different inference. This allows is to assess the \emph{expected} posterior variance associated with each strategy.

For each of these possible strategies we can assess the posterior that we would obtain.

The data possibilities, probabilities of each, and inferences given each, are shown in Table \ref{tab:chselillustration}.

\begin{table}[t]

\caption{\label{tab:chselillustration}Each column shows a possible distribution of data that can be generated from a given strategy. We calculate the probability of each data possibility, given the data seen so far, and the posterior variance associated with each one.}
\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
\hline
event & A1 & A2 & A3 & A4 & B1 & B2 & B3 & B4 & C1 & C2 & C3\\
\hline
X0M0Y0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
X0M0Y1 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
\hline
X0M1Y0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
X0M1Y1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0\\
\hline
X0Y0 & 1 & 1 & 1 & 1 & 2 & 2 & 2 & 2 & 2 & 2 & 2\\
\hline
X0Y1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\
\hline
X1M0Y0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0\\
\hline
X1M0Y1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 1 & 0\\
\hline
X1M1Y0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0\\
\hline
X1M1Y1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 2\\
\hline
X1Y0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\
\hline
X1Y1 & 1 & 1 & 1 & 1 & 2 & 2 & 2 & 2 & 0 & 0 & 0\\
\hline
Probability & 0.171 & 0.03 & 0.625 & 0.174 & 0.27 & 0.231 & 0.23 & 0.268 & 0.09 & 0.242 & 0.668\\
\hline
Posterior mean & 0.078 & 0.041 & 0.171 & 0.078 & 0.128 & 0.141 & 0.143 & 0.131 & 0.046 & 0.089 & 0.161\\
\hline
Posterior variance & 0.006 & 0.002 & 0.029 & 0.006 & 0.016 & 0.02 & 0.02 & 0.017 & 0.002 & 0.008 & 0.026\\
\hline
\end{tabular}
\end{table}

Each of the first two strategies generates one of four different data patterns. The third data strategy generates up to three data patterns. None of these data patterns overlap across strategies.
From the calculated probability of each data type, given the data seen so far, and the posterior variance given each data realization, the implied \emph{expected} variance is easily calculated. These are summarized below:

\begin{tabular}{l|r}
\hline
Strategy & Variance\\
\hline
Online & 0.015\\
\hline
Offline & 0.017\\
\hline
X=1, Y=1 & 0.015\\
\hline
\end{tabular}

In this example, we see that the researcher would expect to be better off---in the sense of having less posterior uncertainty---by focusing her process-tracing efforts where a greater share of the population of cases lies: on the regression line.

All of this can be done in code in a single line.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diagnose_strategies}\NormalTok{(}
    
   \DataTypeTok{analysis_model =}\NormalTok{ model,}
   
   \DataTypeTok{observed =}\NormalTok{ observed,}
   
   \DataTypeTok{data_strategies =} \KeywordTok{list}\NormalTok{(}
     \DataTypeTok{A_online  =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{within =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{, }
                       \DataTypeTok{conditions =} \KeywordTok{c}\NormalTok{(}\StringTok{"X==1 & Y==1"}\NormalTok{, }\StringTok{"X==0 & Y==0"}\NormalTok{)),}
     \DataTypeTok{B_offline =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{within =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{, }
                       \DataTypeTok{conditions =} \KeywordTok{c}\NormalTok{(}\StringTok{"X==1 & Y==0"}\NormalTok{, }\StringTok{"X==0 & Y==1"}\NormalTok{)),}
     \DataTypeTok{C_X1Y1    =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{N=}\DecValTok{2}\NormalTok{, }\DataTypeTok{within =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{vars =} \StringTok{"M"}\NormalTok{, }
                       \DataTypeTok{conditions =} \KeywordTok{c}\NormalTok{(}\StringTok{"X==1 & Y==1"}\NormalTok{))),}
   
   \DataTypeTok{queries =} \StringTok{"Y[X=1] - Y[X=0]"}
   
\NormalTok{   )}
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-99}The estimand and the expected estimate are the same under each data strategy. }
\centering
\begin{tabular}{l|l|r|r|r|r}
\hline
  & strategy & estimand & estimates & MSE & post\_var\\
\hline
estimands\_database & Prior & 0.1347 & 0.1340 & 0.0238 & 0.0166\\
\hline
estimands\_database1 & A\_online & 0.1347 & 0.1406 & 0.0180 & 0.0156\\
\hline
estimands\_database2 & B\_offline & 0.1347 & 0.1359 & 0.0236 & 0.0175\\
\hline
estimands\_database3 & C\_X1Y1 & 0.1347 & 0.1362 & 0.0197 & 0.0158\\
\hline
estimands\_database4 & D\_random & 0.1347 & 0.1377 & 0.0202 & 0.0163\\
\hline
\end{tabular}
\end{table}

On the relation between MSE and posterior variance:

\includegraphics[width=10,height=10]{ii_files/figure-latex/unnamed-chunk-100-1}

\hypertarget{results-from-experiments}{%
\section{Results from experiments}\label{results-from-experiments}}

We now apply the procedure systematically to a set of strategies for multiple models and given different types of pre-existing data.

The qualitative case-selection literature has identified a range of possible strategies for choosing cases for in-depth analysis. These include, for instance, selecting for variation on \(X\) (KKV), selecting for variation on \(Y\), selecting cases on the regression line (Seawright and Gerring, Lieberman), and selecting off the regression line (Seawright and Gerring, Seawright 2017, Lieberman).{[}These authors view selection off the regression line as best for arriving at inductive insight. We address this strategy primarily to show the contrast with the on-the-line strategy.{]} While we have not seen it advocated elsewhere, we might add to this list the strategy of selecting cases that are representative in their \(X,Y\) values of the larger set of cases from which we are selecting. (FLAG: insert proper citations). While it is difficult to clearly distinguish these strategies from each other with a small initial set of \(X, Y\) cases, we can do so readily if we start with a larger set of \(X,Y\) cases.

In Table \ref{caseselectlots}, we show the results of diagnoses of each of these five classes of strategies, assuming that we will be process-tracing 6 cases. In each diagnosis we start with 500 \(X, Y\) cases, with 100 \(X=Y=0\) cases, 200 \(X=Y=1\) cases, 130 \(X=0, Y=1\) cases, and 70 \(X=1, Y=0\) cases. The regression line here represents a positive association. We work with the same model and priors as above. Importantly, this means that the results we show here are not \emph{general} evaluations of these strategies, but contingent on this particular model and set of priors. And that is precisely our point: optimal case-selection will always hinge on our model, a claim that we demonstrate further below.

\hypertarget{different-strategies-for-different-estimands}{%
\section{Different strategies for different estimands}\label{different-strategies-for-different-estimands}}

\hypertarget{where-the-probative-value-is}{%
\section{Where the probative value is}\label{where-the-probative-value-is}}

For the general intuition, recall that the probative value of a process-tracing test hinges on the difference in clue likelihoods associated with the alternative hypotheses in play for a given case. Recall that for different values of \(X\) and \(Y\) cell, we want to use process tracing to help us distinguish between two specific types that are consistent with the \(X, Y\) pattern. Which types are in question varies across \(X,Y\) combinations. Table \ref{tab:FP} illustrates.

\begin{longtable}[]{@{}lcc@{}}
\caption{\label{tab:FP}. The ambiguity about types in each \(X, Y\) cell.}\tabularnewline
\toprule
\small & \(Y=0\) & \(Y=1\)\tabularnewline
\midrule
\endfirsthead
\toprule
\small & \(Y=0\) & \(Y=1\)\tabularnewline
\midrule
\endhead
\(X=0\) & \(b\) or \(c\) & \(a\) or \(d\)\tabularnewline
\(X=1\) & \(a\) or \(c\) & \(b\) or \(d\)\tabularnewline
\bottomrule
\end{longtable}

Thus, in the \(X=0, Y=0\) cell, what would be most useful is a clue that has high probative value in distinguishing between an untreated (\(X=0\)) \(b\) type and and an untreated \(c\) type. For a case in the \(X=1, Y=0\) cell, on the other hand, what matters is how well the clue can discriminate between treated (\(X=1\)) \(a\) and \(c\) types. In our notation, it is the difference in \(\phi_{jx}\) values for that indicates these cell-specific degrees of leverage.

To illustrate, consider a situation in which for a given clue we have \(\phi_{b1}\)=0.5\footnote{The probability of observing the clue for a \(b\) type (positive causal effect) case with \(X=1\).}; \(\phi_{d1}\)=0.5 \footnote{The probability of observing the clue for a \(d\) type (zero causal effect, \(Y\) fixed at 1) case with \(X=1\).}; \(\phi_{b0}\)=0.5\^{}\footnote{The probability of observing the clue for a \(b\) type (positive causal effect) case with \(X=0\).}; and \(\phi_{c0}\)=0.1\footnote{The probability of observing the clue for a \(c\) type (zero causal effect, \(Y\) fixed at 0) case with \(X=0\).}. In this situation, searching for the clue in \(X=Y=1\) cases will yield no leverage since the clue does not discriminate between the two types (\(b\) and \(d\)) that need to be distinguished given \(X=Y=1\). Here there is no additional learning about \(\lambda_b\) that can be gained from looking for the clue. In contrast, \(X=0, Y=0\) cases will be informative since the clue is much better at distinguishing between \(b\) and \(c\) types---the two types in contention for this kind of case. Thus, although process tracing here does not provide information on the prevalence of positive causal effects (\(b\) types) for an \(X=Y=1\) case, it does provide information when \(X=Y=0\).

While it is common practice for mixed-method researchers to perform their process tracing ``on the regression line,'' the BIQQ framework suggests that the gains to process tracing for different \(X\) and \(Y\) values in fact depend on the particular constellations of \(\phi\) values for the potentially available clues. More generally, the framework allows one to assess the expected gains from any given case-selection strategy \emph{ex ante} once priors have been specified.

\hypertarget{chapter-appendix-accounting-for-case-selection}{%
\section{Chapter Appendix: Accounting for case selection}\label{chapter-appendix-accounting-for-case-selection}}

\hypertarget{independent-case-selection-strategy}{%
\subsection{Independent case selection strategy}\label{independent-case-selection-strategy}}

We have focused on cases in which the researcher examines a fixed number of cases for clue information. An alternative strategy that produces a simpler likelihood is one in which each case is selected for within-case data gathering with some independent probability. The likelihood below introduces a case selection probability \(\kappa_{xy}\) that covers this case and allows for the possibility that selection probabilities are different for different \(X,Y\) combinations.

Thus we assume that \(X\), \(Y\) data is observed for all cases under study, but that \(K\) data may be sought for only a subset of these (we use the wildcard symbol '`\(*\)'' to denote that the value of the clue is unknown). We let \(n_{xyk}\) denote the number of cases of each type. Then, again assuming data is independently and identically distributed, the likelihood is:

\[\Pr(\mathcal{D}|\theta)= {\text{Multinomial}}(n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*} |
w_{000}, w_{001},w_{00*},w_{010}, w_{010},w_{01*}, w_{100}, w_{101},w_{10*},w_{110},w_{111} ,w_{11*})\]

where the event probabilities are now given by:

\[{\left( \begin{array}{c}
w_{000} \\ w_{001} \\  \vdots \\ w_{11*} \end{array} \right)=
\left( \begin{array}{c}
\lambda_b(1-\pi_b)\kappa_{00}(1-\phi_{b0}) + \lambda_c(1-\pi_c)\kappa_{00}(1-\phi_{c0})\\
\lambda_b(1-\pi_b)\kappa_{00}\phi_{b0} + \lambda_c(1-\pi_c)\kappa_{00}\phi_{c0}\\
\vdots \\
\lambda_b\pi_{b}(1-\kappa_{11}) + \lambda_d\pi_{d}(1-\kappa_{11})
\end{array} \right)}\]

Note we use a Greek symbol for the case selection probabilities to highlight that these may also be unknown and be an object of inquiry, entering into the vector of parameters, \(\theta\).

\subsubsection{Non-random $XY$ Sample Selection}\label{nonrandomcase}

While we have assumed in the canonical model that \(X,Y\) cases are selected at random, this need not be the case. Say instead that each case of type \(j\) is selected into the study with probability \(\rho_j\). In that case, assuming independent selection of cases for qualitative analysis, the likelihood function is now:

\[\Pr(\mathcal{D}|\theta) = {\text{Multinomial}}(n, w)\]
where: \[n = (n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*})\]

and the event probabilities, \(w\), are now, given by:

\[\left( \begin{array}{c}
w_{000} \\ w_{001} \\  \vdots \\ w_{11*}
\end{array} \right)=
\left( \begin{array}{c}
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_b)\kappa_{00}(1-\phi_{b0}) +
\frac{\rho_c \lambda_c}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_c)\kappa_{00}(1-\phi_{c0})\\
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_b)\kappa_{00}\phi_{b0}+
\frac{\rho_c \lambda_c}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_c)\kappa_{00}\phi_{c0}\\
\vdots \\
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}\pi_{b}(1-\kappa_{11})+
\frac{\rho_d \lambda_d}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_{11}}\pi_{d}(1-\kappa_{11})
\end{array} \right)\]

Note we have used a Greek symbol for the selection probabilities to highlight that these probabilities may be unknown and could enter into the set of parameters of interest, \(\theta\).

\hypertarget{conditional-random-case-selection}{%
\subsection{Conditional random case selection}\label{conditional-random-case-selection}}

Finally consider the likelihood for a design in which a researcher selects to search for clues as a function of the \(X,Y\) data. This is a somewhat harder case because the size of each \(X,Y\) group is stochastic. Let \(n_{xy} = n_{xy0}+n_{xy1}+n_{xy*}\) denote the number of cases with particular values on \(X\) and \(Y\), and let \(n_{XY}=(n_{00},n_{01},n_{10},n_{11})\) denote the collection of \(n_{xy}\) values.

Say now that conditional on the \(X,Y\) observations, a researcher sets a target of \(k_{xy}(n_{XY})\) cases for clue examination (note here that the number of clues sought for a particular \(X,Y\) combination can be allowed to depend on what is observed across all \(X\), \(Y\) combinations). Then the likelihood is:
\[\text{Multinomial}(n_{XY}|w_{XY})\prod_{x\in\{0,1\},y \in\{0,1\}}\text{Binom}(n_{xy1}|k_{xy}(n_{xy}), \psi_{xy1})\]

The multinomial part of this expression gives the probability of observing the particular \(X,Y\) combinations; the event probabilities for these depend on \(\lambda\) and \(\pi\) only \textbar{} for example \(w_{11} = \lambda_b \pi_b+\lambda_d \pi_d\). The subsequent binomials give the probability of observing the clue patterns conditional on searching for a given number of clues (\(k_{xy}(n_{xy})\)) and given an event probability \(\psi_{xy1}\) for seeing a clue given that the clue is sought for an \(x,y\) combination; thus for example:
\[ \psi_{111} = \frac{\lambda_b \pi_b}{\lambda_b \pi_b+\lambda_d \pi_d} \phi_{b1} + \frac{\lambda_d \pi_d}{\lambda_b \pi_b+\lambda_d \pi_d} \phi_{d1}\]

\hypertarget{appendix-stan-uncertainty}{%
\section{Appendix: Stan uncertainty}\label{appendix-stan-uncertainty}}

\hypertarget{part-models-in-question}{%
\part{Models in Question}\label{part-models-in-question}}

\hypertarget{justifying-models}{%
\chapter{Justifying models}\label{justifying-models}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

We outline strategies to reduce reliance on unfounded beliefs about the probative value of clues.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

The approach we have described to inference always involve updating beliefs given data. But to get off the ground researchers need to be able to state priors on all parameters. In many applications the problem of stating priors can be more fundamental than for many Bayesian applications for two reasons. First the beliefs are beliefs over the distribution of individual level effects and not just the beliefs over average effects. This puts us up against the fundamental problem of causal inference (Holland cite, Dawid cite FLAG). Second, the beliefs can do a lot of work---especially in small \(n\) applications. Indeed for the the process tracing described chapters 6 and 7 {[}FLAG ADD REFS{]} the inferences are little more than conditional applications of a model.

We see two broad responses to this problem.

One is \emph{emphasize the contingent nature of claims}. As we outlined in Chapter 4, some causal models might reasonably reflect actual beliefs about the world---for example one might, be convinced that a treatment was randomly assigned, that there is no interference, and that units are independently sampled from a distribution of types. All of these beliefs may be unwise, of course. But if held, then the simple \(X \rightarrow Y\) DAG in chapter 4 (FIGURE REF) is more of a representation of beliefs about the world than it is a model of the world, in the sense of a simplified representation.\footnote{Even in this simple case there are ways in which the representation is a model, not least the coding of events as a variable invovlves a form of modelling.} But as we noted in Chapter 4, for an even modestly more complex situation, it seems inevitable that the model being used is unquetionanly a model and hard to think of as a faithful summary of beliefs.

Recognizing in this way that we are generally dealing with models results in a useful reposing of the question: the question becomes not whether the assumptions are correct but whether the model is useful for some purpose \citep{clarke2012model}. That is the subject of Chapter 15.

Here we focus on more positive steps that might be taken to underpin a model. We highlight first how the type of approach used in Chapters 8 and 9 can be used to justify a process tracing model on the basis of a mixed methods model. These applications presuppose knowledge of a DAG however. In a sense, they simply push the question down a level. There are two further responses to this concern. One is to try to generate the DAG itself from data or a combination of data and theory. We discuss this approach here. Another is to assess the importance of DAG assumptions -- which we address in Chapter 15.

\hypertarget{bounds-on-probative-value}{%
\section{Bounds on probative value}\label{bounds-on-probative-value}}

Classic treatments of process tracing make use of Causal Process Observations --- observations that are taken to be indicative of a particular causal process in operation. We introduced in Chapter 5 (as well as in FLAG CITE humphreysjacobs) quantities such as \(\phi_{b}\)---the probability that \(K=1\) given \(X\) caused \(Y\) and \(X=Y=1\), or \(\phi_{d}\)-----the probability that \(K=1\) given \(X\) did not cause \(Y\) and \(X=Y=1\).

These accounts do not guide much guidance however regarding where these quantities come from --- given that causal types are unobservable how can one justify a belief about the probability of some observation \emph{given} a causal type. Is it even possible to justify such beliefs?

The grounded approach we described provides an answer to this puzzle. In short, knowledge of the structure of a causal model, together with data on exchangeable units, can be enough to place bounds on possible values of \(\phi_{b}, \phi_{d}\).

We illustrate the basic idea and then review some results in this area.

Imagine a fortunate situation in which (a) it is known that the true causal model has the form \(X \rightarrow M \rightarrow Y\) and (b) we have a lot of experimental data on the conditional distribution of \(M\) given \(X\) and of \(Y\) given \(M\) for exchangeable units (meaning that we can treat our unit of interest as if it were a draw from this set).

Let us define:

\begin{itemize}
\tightlist
\item
  \(\tau_1 = \Pr(M=1 | X=1) - \Pr(M=1 | X=0)\)
\item
  \(\rho_1 = \Pr(M=1 | X=1) - \Pr(M=0 | X=0)\)
\item
  \(\tau_2 = \Pr(Y=1 | M=1) - \Pr(Y=1 | M=0)\)
\item
  \(\rho_2 = \Pr(Y=1 | M=1) - \Pr(Y=0 | M=0)\)
\end{itemize}

These are all quantities that can be calculated from the data. The \(\tau\)s are average treatment effects and the \(\rho\)s are indicators for how common the \(Y=1\) outcome is.

We are interested in the probability of observing \(M=1\) given \(X=Y=1\):

\[\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + \lambda_{a}^K\lambda_{a}^Y}\]

Noting that \(\tau_j = \lambda_{b_j} - \lambda_{a_j}\):

\[\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + (\lambda_{b}^K-\tau_1)(\lambda_{b}^Y - \tau_2)}\]
which we can see is decreasing in \(\lambda_{b}^j\) (this may seem counterintuitive, but the reason is that with \(\tau^j\) fixed, lower \(\lambda_{b}^j\) also means lower \(\lambda_{a}^j\) which means less ambiguity about \emph{how} \(X\) affects \(Y\) (i.e.~through positive or negative effects on \(K\)).

The lowest permissible value of \(\lambda_{b_j}\) is \(\tau_j\), yielding \(\phi_{b1} = 1\).

The highest value obtainable by \(\lambda_{b_j}\) is when \(\lambda_{a_j} = \frac{1-\tau_j+\rho_j}2\) and so \(\lambda_{b_j} = \frac{1+\tau_j+\rho_j}2\).

In this case:
\[\phi_{b1} = \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2) + (1-\tau_1+\rho_1)(1-\tau_2+\rho_2)}= \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau}\]

And so:

\[\frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau_2} \leq \phi_{b1} \leq 1\]

These are the bounds on \(\phi_{b1}\). We can calculate bounds on \(\phi_{d1}\) in a similar way (though of course the bounds on \(\phi_{b1}\) and \(\phi_{d1}\) are not independent).

\[\phi_{d1} = \frac{\lambda_{b}^K\lambda_{d}^Y}{(\lambda_{a}^K + \lambda_{b}^K + \lambda_{c}^K)\lambda_{d}^Y+ \lambda_{c}^K\lambda_{a}^Y}\]

Figure \ref{fig:probval1} illustrates how ``smoking gun'' and ``hoop'' tests might each be justified with knowledge of \(\tau_j, \rho_j\).

\includegraphics{ii_files/figure-latex/unnamed-chunk-105-1.pdf}

For the smoking gun, \(\phi_{b1}\) is .5 because \(\lambda_a^j = \lambda_b^j\) so half of the upper level \(b\) types work through a positive effect on \(M\) and half through a negative effect on \(M\). \(\phi_{d1}\), on the other hand, is low here \(d\) types mostly arise because of \(c\) types in the first step and \(a\) types in the second, and hence most commonly with \(M=1\).

Whether the bounds map into useful probative value depends in part on whether causal effects are better identified in the first or the second stage. We can see this in Figure \ref{fig:probval2}.

The key difference between the panels is that \(\phi_d\) is constrained to be low in the first panel but not in the second.

For intuition note that a higher level \(d\) type will exhibit \(M=1\) if it is formed via \(db\), \(bd\),or \(dd\) and it will exhibit \(M=0\) if it is formed via \(ca\), \(cd\), \(ad\). The weak second stage makes it possible that there are no second stage d types, only a and b types. The stronger first stage makes it possible that there are no first stage \(c\) types. In that case the higher level d types are formed uniquely of \(db\) types -- which always exhibit \(M=1\) if \(X=1\).

This is not possible however for the data assume in the first panel. In the first panel the the higher value on \(\rho_2\) means that there must be at least .25 d types. And the weak first stage means that there must at least .5 a and c types combined. Thus there \emph{must} be a set of cases in which \(M\) is not observed even though we have an upper level d type.

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/probval2-1.pdf}
\caption{\label{fig:probval2}Probative value with different first and second stage relations}
\end{figure}

In short we emphasize that difficult as it might seem at first it is possible to put relatively tight bounds on probative value for causal types with access to experimental data on exchangeable units.

\hypertarget{the-possibility-of-identification-of-probative-value-from-experimental-data}{%
\section{The possibility of identification of probative value from experimental data}\label{the-possibility-of-identification-of-probative-value-from-experimental-data}}

While it is possible to calculate bounds on probative value, it can be simpler to calculate bounds on estimands directly. These bounds can be justified with reference to background data in the same ways as the bounds on probative value.

Following \citet{dawid2019bounding} we again imagine we had access to infinite experimental data on the effect of \(X\) on \(Y\) and we want to know for a case (exchangeable with any other in this population) with \(X=Y=1\), whether \(X=1\) caused \(Y=1\). Call this the ``probability of causation.''

Say we knew the marginal distributions:

\begin{itemize}
\tightlist
\item
  \(\Pr(Y=1|X=1) = .75\)
\item
  \(\Pr(Y=1|X=0) = .25\)
\end{itemize}

The we could represent this knowledge as Markovian transition matrix from \(X\) to \(Y\) like this:

\[P=\left( \begin{array}{cc} 0.50 & 0.50 \\ 0.25 & 0.75 \end{array}\right)\]

In this case, from results in \citet{dawid2017probability}, we can place bounds directly on the probability that \(X\) caused \(Y\), viz:

\[\frac13 \leq PC \leq \frac23 \]
For intuition note that \(P\) implies a causal effect of .25 and so the lowest value of \(\lambda_b\) consistent with \(P\) arises when \(\lambda_b = .25\) and \(\lambda_a = 0\), in which case \(\lambda_c = .25\) and \(\lambda_d = .5\). In this case \(\lambda_b/(\lambda_b+ \lambda_d)=\frac{1}{3}\). The highest consistent value of \(\lambda_b\) arises when \(\lambda_b = .5\) and \(\lambda_a = .25\), in which case \(\lambda_c = 0\) and \(\lambda_d = .25\). In this case \(\lambda_b/(\lambda_b+ \lambda_d)=\frac{2}{3}\).

Defining \(\tau\) and \(\rho\) as before, the more general formula for the case with \(\rho>0\) is:

\[\frac{2\tau}{1+\tau+\rho} \leq PC \leq \frac{1+\tau-|\rho|}{1+\tau+\rho} \]

Say now we have access to auxiliary data \(K\) and plan to make inferences based on \(K\).

We will suppose first that \(K\) is a mediator, as above, and second that \(K\) is a moderator.

\hypertarget{mediator}{%
\subsection{Mediator}\label{mediator}}

Say now that \emph{in addition} we know from experimental data, that \(K\) mediates the relationship between \(X\) and \(Y\); indeed we will assume that we have a case of complete mediation, such that, conditional on \(K\), \(Y\) does not depend on \(X\).

Say the transition matrices from \(X\) to \(K\) and \(K\) to \(Y\) are:

\[P^{xk}=\left( \begin{array}{cc} 1 & 0 \\ 1/2 & 1/2\end{array}\right), P^{ky}=\left( \begin{array}{cc} 1/2 & 1/2 \\ 0 & 1\end{array}\right)\]
Even without observing \(K\), this information is sufficient to place a prior on PC of \(p=\frac13\).

To see this, note that we can calculate:

\begin{itemize}
\tightlist
\item
  \(\lambda_a^K =0\), \(\lambda_b^K = \frac{1}{2}\), \(\lambda_c^K = \frac{1}{2}\), \(\lambda_d^K = 0\)
\item
  \(\lambda_a^Y =0\), \(\lambda_b^Y=\frac{1}{2}\), \(\lambda_c^Y=0\), \(\lambda_d^Y=\frac{1}{2}\)
\end{itemize}

and so:

\begin{itemize}
\tightlist
\item
  \(\lambda_b^u = \lambda_b^K\lambda_b^Y = \frac{1}4\)
\item
  \(\lambda_d^u = \lambda_d^Y\)
\item
  \(p = \frac{\lambda_b^u}{\lambda_b^u + \lambda_d^u} = \frac{1}3\).
\end{itemize}

whence:

\begin{itemize}
\tightlist
\item
  \(\phi_{b1} = 1\)
\item
  \(\phi_{d1} = \lambda_d^K + \lambda_b^K = \frac{1}{2}\)
\end{itemize}

More generally we can calculate the lower bound on the probability that \(X\) caused \(Y\) as the product of the lower bounds that \(X\) caused \(M\) and that \(M\) caused \(Y\), and similarly for the upper bound, using the same formula as before. Signing things so that \(\tau^j\geq 0\), \(j \in {1,2}\):

\[\frac{2\tau_1}{1+\tau_1+\rho_1}\frac{2\tau_2}{1+\tau_2+\rho_2}  \leq PC \leq \frac{1+\tau_1-|\rho_1|}{1+\tau_1+\rho_1}\frac{1+\tau_2-|\rho_2|}{1+\tau_2+\rho_2} \]

We have undertaken essentially the same operations as above except that now we are placing bounds on a substantive estimand of interest rather than first placing bounds on probative value of a clue and then turning to Bayes rule to place bounds on the estimand.

\hypertarget{moderator}{%
\subsection{Moderator}\label{moderator}}

Consider now a situation in which our case is drawn from a set of cases for which \(X\) and \(K\) were each randomly assigned. Say then that the transition matrices, conditional on \(K\) look as follows:

\[P^{K=0}=\left( \begin{array}{cc} 0 & 1 \\ 0.5 & 0.5 \end{array}\right), P^{K=1}=\left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right)\]
In this case we can now identify PC, even before observing \(K\). If \(K=0\), PC is 0---there are no cases with positive effects in this condition. If \(K=1\) PC = 1. We have a prior that \(K=1\) of .5 and after observing \(X=Y=1\) we raise this to \(2/3\). Thus our prior belief on \(PC\) --- before seeing \(K\)--- is \(2/3 * 1 + 1/3 * 0 = 2/3\).

How about \(\phi_{b1}\) and \(\phi_{d1}\)?

Here positive effects only arise when \(K=1\) and so \(\phi_{b1} = 1\). \(Y=1\) without being cause by \(X\) only if \(K=0\) and so \(\phi_{b0} = 0\). Thus we have a double decisive clue.

\hypertarget{case-level-bounds-from-mixed-data}{%
\subsection{Case level bounds from mixed data}\label{case-level-bounds-from-mixed-data}}

\hypertarget{learning-across-populations}{%
\section{Learning across populations}\label{learning-across-populations}}

Now consider strategies to learn about clues from observing patterns in different populations.

We first consider a situation in which we believe the same model holds in multiple sites but in which learning about the model requires combining data about different parts of the model from multiple studies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> Y <- Z -> K"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ii_files/figure-latex/unnamed-chunk-108-1.pdf}

We imagine we have access to three types of data;

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Study 1 is an experiment looking at the effects of \(X\) on \(Y\), ancillary data on \(K\) is collected but \(Z\) is not observed
\item
  Study 2 is a factorial study examining the joint effects of \(X\) and \(Z\) on \(Y\), \(K\) is not observed
\item
  Study 3 is an RCT looking at the relation between \(Z\) and \(K\). \(X\) and \(Y\) are not observed.
\end{enumerate}

Tables \ref{tab:frank1} - \ref{tab:frank3} show conditional inferences for the probability that \(X\) caused \(Y\) in \(X=Y=1\) cases conditional on \(K\) for each study, analyzed individually

\begin{table}[t]

\caption{\label{tab:frank1}Clue is uninformative in Study 1}
\centering
\begin{tabular}{l|r|r}
\hline
Subset & mean & sd\\
\hline
X == 1 \& Y == 1 \& K == 1 & 0.5 & 0.153\\
\hline
X == 1 \& Y == 1 \& K == 0 & 0.5 & 0.146\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:frank2}Clue is also uninformative in Study 2 (factorial)}
\centering
\begin{tabular}{l|r|r}
\hline
Subset & mean & sd\\
\hline
X == 1 \& Y == 1 \& K == 1 & 0.546 & 0.110\\
\hline
X == 1 \& Y == 1 \& K == 0 & 0.546 & 0.112\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:frank3}Clue is also uninformative in Study 3 (experiment studying $K$)}
\centering
\begin{tabular}{l|r|r}
\hline
Subset & mean & sd\\
\hline
X == 1 \& Y == 1 \& K == 1 & 0.5 & 0.154\\
\hline
X == 1 \& Y == 1 \& K == 0 & 0.5 & 0.152\\
\hline
\end{tabular}
\end{table}

In no case is \(K\) informative. In study 1 data on \(K\) is not available, in study 2 it is available but researchers do not know, quantitatively, how it relates to \(Z\). In the third study the \(Z,K\) relationship is well understood but the joint relation between \(Z,X\), and \(Y\) is not understood.

Table \ref{tab:frank4} shows the inferences when the data are combined with joint updating across all parameters.

\begin{table}[t]

\caption{\label{tab:frank4}Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$}
\centering
\begin{tabular}{l|r|r}
\hline
Subset & mean & sd\\
\hline
X == 1 \& Y == 1 \& K == 1 & 0.663 & 0.081\\
\hline
X == 1 \& Y == 1 \& K == 0 & 0.519 & 0.099\\
\hline
X == 1 \& Y == 1 \& K == 1 \& Z == 1 & 0.713 & 0.101\\
\hline
X == 1 \& Y == 1 \& K == 0 \& Z == 1 & 0.713 & 0.101\\
\hline
X == 1 \& Y == 1 \& K == 1 \& Z == 0 & 0.509 & 0.104\\
\hline
X == 1 \& Y == 1 \& K == 0 \& Z == 0 & 0.509 & 0.104\\
\hline
\end{tabular}
\end{table}

Here fuller understanding of the model lets researchers use information on \(Z\) to update on values for \(Z\) and in turn update on the likely effects of \(X\) on \(Y\). Rows 3-6 highlight that the updating works through inferences on \(Z\) and there are no gains when \(Z\) is known, as in Study 2.

In this example Studies 2 and 3 can be thought of as helper experiments for Study 1. Study 2 might be thought of as a mechnism study whereas Study 3 is more like a measurement study.

\hypertarget{different-models-for-different-sites}{%
\section{Different models for different sites}\label{different-models-for-different-sites}}

In the last example we assumed that the same model operated in the same wayat all sites. This is a strong assumption, though sometimes justifiable (for instance if sites were randomly allocated across studies).

If the same model does not operate at different sites it might still be possible to update in this way. For this, however, we need to be able to specify \emph{how} sites differ Consider a problem where the models partially differ across sites: for instance we believe that although treatment effects are different in two sites yet the mechanisms linking treatment to outcomes are the same. As a simple example we might imagine that \(X\) is differentially likely to produce \(M\) in two sites, but if it does the relation between \(M\) and \(Y\) is common across sites.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# In this model you are more likely to have an M=1 type regardless if Y = 1 regardless }
\CommentTok{# This produces a positive confound}

\NormalTok{model_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X->M->Y"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{            }\KeywordTok{set_confound}\NormalTok{(}\DataTypeTok{confound =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{M =} \StringTok{"(Y[M=1] ==1) & (Y[M=0]==1)"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{            }\KeywordTok{set_parameters}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.7}\NormalTok{, }
                             \FloatTok{.5}\NormalTok{, }\FloatTok{.5}\NormalTok{, }
                             \FloatTok{.7}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.1}\NormalTok{,}
                               \FloatTok{.2}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.4}\NormalTok{, }\FloatTok{.2}\NormalTok{))}

\KeywordTok{plot_dag}\NormalTok{(model_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ii_files/figure-latex/unnamed-chunk-109-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
  
\NormalTok{  df_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{simulate_data}\NormalTok{(model_}\DecValTok{1}\NormalTok{, }\DataTypeTok{n =} \DecValTok{20000}\NormalTok{, }\DataTypeTok{using =} \StringTok{"parameters"}\NormalTok{)}
  
\NormalTok{  posterior_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{gbiqq}\NormalTok{(model_}\DecValTok{1}\NormalTok{, df_}\DecValTok{1}\NormalTok{, }\DataTypeTok{stan_model =}\NormalTok{ fit)}
  
  
  
  \CommentTok{# In this model you are more likely to have an M=1 regardless if Y = 1 regardless }
  \CommentTok{# This produces a negative confound}
\NormalTok{  model_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X->M->Y"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{              }\KeywordTok{set_confound}\NormalTok{(}\DataTypeTok{confound =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{M =} \StringTok{"(Y[M=1] ==1) & (Y[M=0]==1)"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{              }\KeywordTok{set_parameters}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{7}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\DecValTok{0}\NormalTok{, }
                               \FloatTok{.5}\NormalTok{, }\FloatTok{.5}\NormalTok{, }
                                \DecValTok{0}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.7}\NormalTok{,}
                               \FloatTok{.2}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\FloatTok{.4}\NormalTok{, }\FloatTok{.2}\NormalTok{))}
\NormalTok{  df_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{simulate_data}\NormalTok{(model_}\DecValTok{2}\NormalTok{, }\DataTypeTok{n =} \DecValTok{20000}\NormalTok{, }\DataTypeTok{using =} \StringTok{"parameters"}\NormalTok{)}
  
\NormalTok{  posterior_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{gbiqq}\NormalTok{(model_}\DecValTok{2}\NormalTok{, df_}\DecValTok{2}\NormalTok{, }\DataTypeTok{stan_model =}\NormalTok{ fit)}
  
\NormalTok{  out1 <-}\StringTok{ }\KeywordTok{query_model}\NormalTok{(posterior_}\DecValTok{1}\NormalTok{, }\DataTypeTok{using=}\StringTok{"posteriors"}\NormalTok{, }\DataTypeTok{queries =} \KeywordTok{list}\NormalTok{(}\StringTok{`}\DataTypeTok{X on M}\StringTok{`}\NormalTok{ =}\StringTok{ "M[X=1] - M[X=0]"}\NormalTok{, }\StringTok{`}\DataTypeTok{M on Y}\StringTok{`}\NormalTok{ =}\StringTok{ "Y[M=1] - Y[M=0]"}\NormalTok{))  }
  
\NormalTok{  out2 <-}\StringTok{ }\KeywordTok{query_model}\NormalTok{(posterior_}\DecValTok{2}\NormalTok{, }\DataTypeTok{using=}\StringTok{"posteriors"}\NormalTok{, }\DataTypeTok{queries =} \KeywordTok{list}\NormalTok{(}\StringTok{`}\DataTypeTok{X on M}\StringTok{`}\NormalTok{ =}\StringTok{ "M[X=1] - M[X=0]"}\NormalTok{, }\StringTok{`}\DataTypeTok{M on Y}\StringTok{`}\NormalTok{ =}\StringTok{ "Y[M=1] - Y[M=0]"}\NormalTok{))  }
  
  \KeywordTok{write_rds}\NormalTok{(}\KeywordTok{list}\NormalTok{(posterior_}\DecValTok{1}\NormalTok{, posterior_}\DecValTok{2}\NormalTok{, out1, out2), }\StringTok{"saved/same_mechanism.rds"}\NormalTok{)}
  
\NormalTok{  \}}

\NormalTok{same_mechanism <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/same_mechanism.rds"}\NormalTok{)}

\KeywordTok{kable}\NormalTok{(same_mechanism[[}\DecValTok{3}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r}
\hline
Query & Subset & Using & mean & sd\\
\hline
X on M & All & posteriors & 0.204 & 0.006\\
\hline
M on Y & All & posteriors & 0.239 & 0.027\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kable}\NormalTok{(same_mechanism[[}\DecValTok{4}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r}
\hline
Query & Subset & Using & mean & sd\\
\hline
X on M & All & posteriors & 0.099 & 0.006\\
\hline
M on Y & All & posteriors & 0.189 & 0.068\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The marginal effect of X on M will be different in the two cases}
\CommentTok{# The effect of M on Y is the same however, though it is confounded}
\end{Highlighting}
\end{Shaded}

Under the model there is possibly a difference in the effect of \(X\) on \(Y\) in the

\hypertarget{observational-and-experimental}{%
\subsection{Observational and experimental}\label{observational-and-experimental}}

Let us imagine a second case in which one wants to update based on

\hypertarget{causal-discovery}{%
\section{Causal discovery}\label{causal-discovery}}

We start with a model with three variables, \(X,M,Y\) where \(X\) affects \(Y\) directly and indirectly through \(M\). We simulate data from this model -- assuming monotonicity but otherwise a flat distribution on types, and then try to recover the structure from this model.

In this case the data structure did not impose restrictions on the skeleton. The true graph can however be recovered with knowledge of the temporal ordering of variables.

Next we consider the model in which X causes Y through M but not directly. In this case we have a restriction --- specifically there is no arrow pointing directly from \(X\) to \(Y\). Again we impose monotonicity, draw data, and try to recover the model:

Again we have the correct skeleton and knowledge of timing is enough to recover the graph.

Finally we consider the model in which \(Y\) has two causes that do not influence each other. Again we impose monotonicity, draw data, and try to recover the model:

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/unnamed-chunk-114-1.pdf}
\caption{\label{fig:unnamed-chunk-114}DAGs from Data}
\end{figure}

\hypertarget{a-model-of-models}{%
\subsection{A model of models}\label{a-model-of-models}}

In the following mode there is an unknown, \(Q\), which determines the relevant causal model.
If \(Q=1\) then we have \(A \rightarrow B \rightarrow C2\); if \(Q=0\) then \(A \rightarrow B \leftarrow C1\). In this case the temporal order of \(C1\) and \(C2\) is observable, so there is not confusion there; what is not clear however is which is the important node to include in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"A -> B -> C2 <- C1 -> B <- Q -> C2"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# These restrictions capture the role of Q in turning parentage on or off }
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
    \StringTok{"(B[C1=1, Q=1] != B[C1=0, Q=1])"}\NormalTok{,}
    \StringTok{"(C2[B=1, Q=0] != C2[B=0, Q=0])"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\CommentTok{# These restrictions are for simplification: monotonicity and complementarity }
\StringTok{  }\KeywordTok{set_restrictions}\NormalTok{(}\KeywordTok{c}\NormalTok{(}
    \StringTok{"(C2[B=1] < C2[B=0])"}\NormalTok{,}
    \StringTok{"(C2[C1=1] < C2[C1=0])"}\NormalTok{,}
    \StringTok{"(B[A=1]  < B[A=0])"}\NormalTok{,}
    \StringTok{"(B[C1=1] < B[C1=0])"}\NormalTok{,}
    \StringTok{"((B[A=1, C1=1] - B[A=0, C1=1]) < (B[A=1, C1=0] - B[A=0, C1=0]))"}\NormalTok{))}
                   
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Statement: 
## [1] "A -> B -> C2 <- C1 -> B <- Q -> C2"
## 
## DAG: 
##   parent children
## 1      A        B
## 3     C1        B
## 4     C1       C2
## 5      Q        B
## 6      Q       C2
## 2      B       C2
## 
##  ------------------------------------------------------------------------------------------
## 
## Nodal types: 
## $A
## 0  1
## 
##   node position display interpretation
## 1    A       NA      A0          A = 0
## 2    A       NA      A1          A = 1
## 
## $C1
## 0  1
## 
##   node position display interpretation
## 1   C1       NA     C10         C1 = 0
## 2   C1       NA     C11         C1 = 1
## 
## $Q
## 0  1
## 
##   node position display interpretation
## 1    Q       NA      Q0          Q = 0
## 2    Q       NA      Q1          Q = 1
## 
## $B
## 00000000  00010000  01010000  00110000  11110000  00000101  00010101  01010101  00110101  01110101  11110101  00001111  00011111  01011111  00111111  11111111
## 
##   node position     display             interpretation
## 1    B        1 B[*]******* B | A = 0 & C1 = 0 & Q = 0
## 2    B        2 B*[*]****** B | A = 1 & C1 = 0 & Q = 0
## 3    B        3 B**[*]***** B | A = 0 & C1 = 1 & Q = 0
## 4    B        4 B***[*]**** B | A = 1 & C1 = 1 & Q = 0
## 5    B        5 B****[*]*** B | A = 0 & C1 = 0 & Q = 1
## 6    B        6 B*****[*]** B | A = 1 & C1 = 0 & Q = 1
## 7    B        7 B******[*]* B | A = 0 & C1 = 1 & Q = 1
## 8    B        8 B*******[*] B | A = 1 & C1 = 1 & Q = 1
## 
## $C2
## 00000000  01000100  11001100  00000001  00010001  01000101  01010101  11001101  11011101  00000011  00010011  00110011  01000111  01010111  01110111  110011112 nodal types omitted
## 
##   node position      display
## 1   C2        1 C2[*]*******
## 2   C2        2 C2*[*]******
## 3   C2        3 C2**[*]*****
## 4   C2        4 C2***[*]****
## 5   C2        5 C2****[*]***
## 6   C2        6 C2*****[*]**
## 7   C2        7 C2******[*]*
## 8   C2        8 C2*******[*]
##                interpretation
## 1 C2 | C1 = 0 & Q = 0 & B = 0
## 2 C2 | C1 = 1 & Q = 0 & B = 0
## 3 C2 | C1 = 0 & Q = 1 & B = 0
## 4 C2 | C1 = 1 & Q = 1 & B = 0
## 5 C2 | C1 = 0 & Q = 0 & B = 1
## 6 C2 | C1 = 1 & Q = 0 & B = 1
## 7 C2 | C1 = 0 & Q = 1 & B = 1
## 8 C2 | C1 = 1 & Q = 1 & B = 1
## 
## 
## Number of types by node
##  A C1  Q  B C2 
##  2  2  2 16 18 
## 
## Number of unit types:  2304
## 
##  ------------------------------------------------------------------------------------------
## 
## Restrictions: 
## A: 0 restricted types 
## C1: 0 restricted types 
## Q: 0 restricted types 
## B: 240 restricted types 
## C2: 238 restricted types
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ii_files/figure-latex/unnamed-chunk-115-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(do_diagnosis)\{}
\NormalTok{  model_Q0 <-}\StringTok{ }\KeywordTok{set_parameters}\NormalTok{(model, }\DataTypeTok{node =} \StringTok{"Q"}\NormalTok{, }\DataTypeTok{alphas =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{  model_Q1 <-}\StringTok{ }\KeywordTok{set_parameters}\NormalTok{(model, }\DataTypeTok{node =} \StringTok{"Q"}\NormalTok{, }\DataTypeTok{alphas =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  
\NormalTok{  data_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{make_data}\NormalTok{(model_Q0, }\DecValTok{100}\NormalTok{, }\DataTypeTok{vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C1"}\NormalTok{, }\StringTok{"C2"}\NormalTok{))}
\NormalTok{  data_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{make_data}\NormalTok{(model_Q1, }\DecValTok{100}\NormalTok{, }\DataTypeTok{vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C1"}\NormalTok{, }\StringTok{"C2"}\NormalTok{))}
  
\NormalTok{  model_Q0 <-}\StringTok{ }\KeywordTok{update_model}\NormalTok{(model, data_}\DecValTok{0}\NormalTok{)}
\NormalTok{  model_Q1 <-}\StringTok{ }\KeywordTok{update_model}\NormalTok{(model, data_}\DecValTok{1}\NormalTok{)}

\NormalTok{Qu0 <-}\StringTok{ }\KeywordTok{query_model}\NormalTok{(model_Q0, }\StringTok{"Q==1"}\NormalTok{, }\DataTypeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\NormalTok{Qu1 <-}\StringTok{ }\KeywordTok{query_model}\NormalTok{(model_Q1, }\StringTok{"Q==1"}\NormalTok{, }\DataTypeTok{using =} \StringTok{"posteriors"}\NormalTok{)}

  \KeywordTok{write_rds}\NormalTok{(}\KeywordTok{list}\NormalTok{(model_Q0, model_Q1, Qu0, Qu1), }\StringTok{"saved/ch14_Qornot.rds"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{Qornot <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"saved/ch14_Qornot.rds"}\NormalTok{)}

\KeywordTok{kable}\NormalTok{(Qornot[[}\DecValTok{3}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r}
\hline
Query & Given & Using & mean & sd\\
\hline
Q 1 & - & posteriors & 0.265 & 0.176\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kable}\NormalTok{(Qornot[[}\DecValTok{4}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r}
\hline
Query & Given & Using & mean & sd\\
\hline
Q 1 & - & posteriors & 0.579 & 0.18\\
\hline
\end{tabular}

{[}Ideally here however \(\lambda^Q\) is either 0 or 1 --- we want to know which world we are in{]}

\hypertarget{evaluation}{%
\chapter{Evaluating models}\label{evaluation}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these may be wrong.

\hypertarget{inferences-when-you-dont-buy-your-priors}{%
\section{Inferences when you don;t buy your priors}\label{inferences-when-you-dont-buy-your-priors}}

You need to provide priors in order to get Bayesian updating off the ground. But seeing your priors reported as hard cold numbers may make them feel like forced confessions. What if you just don't believe them?

Even in this case there are still two strategies for inference.

First you might take a bounding approach. MANSKI stuff.

Second, you might engage in qualitative inference. There is a literature on probabilistic causal models that assesses the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see \citet{parsons2001qualitative}. Under this kind of approach a researcher might willing to say that they think some probability \(p\) is not plausibly greater than .5, but unwilling to make a statement about their beliefs about where in the \(0\) to \(0.5\) range it lies. Such incomplete statements can be enough to rule our classes of conclusion.

\hypertarget{tools-for-evaluating-models}{%
\section{Tools for evaluating models}\label{tools-for-evaluating-models}}

\hypertarget{check-conditional-independencies}{%
\subsection{Check conditional independencies}\label{check-conditional-independencies}}

Claims of conditional independence - Show for whether there's a direct effect of X on Y

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reference_model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> M -> Y <- X"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{set_parameters}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\FloatTok{.7}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\KeywordTok{rep}\NormalTok{(.}\DecValTok{05}\NormalTok{, }\DecValTok{8}\NormalTok{), }\FloatTok{.25}\NormalTok{, }\KeywordTok{rep}\NormalTok{(.}\DecValTok{05}\NormalTok{, }\DecValTok{7}\NormalTok{)))}
\NormalTok{analyst_model   <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> M -> Y"}\NormalTok{)}

\NormalTok{data <-}\StringTok{ }\KeywordTok{simulate_data}\NormalTok{(reference_model, }\DataTypeTok{using =} \StringTok{"parameters"}\NormalTok{, }\DataTypeTok{n =} \DecValTok{1000}\NormalTok{)}
\CommentTok{# summary(lm(Y~X*M, data = data))}
\end{Highlighting}
\end{Shaded}

\hypertarget{check-confounding-assumptions}{%
\subsection{Check confounding assumptions}\label{check-confounding-assumptions}}

approach 2 -- say actual confound is q\textasciitilde{}=0; but model assumes q = 0. Draw data from priors, draw data; given data type (001, 100 etc) plot (a) the posterior distribution under no confounding nad (b) the distribution of estimands that gave rise to the data.

(Verma and Pearl, 1990) identify conditions under which we can check some independence assumptions.

Say we have model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{make_model}\NormalTok{(}\StringTok{"X -> M1 -> M2 -> Y "}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{set_confound}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{M1 =} \StringTok{"Y[M2=1]>Y[M2=0]"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Can we check that there is no direct path from \(X\) to \(Y\)?

Pearl (1995) gives conditions for assessing for discrete data whether \(Z\) has a direct effect on \(Y\). (Involves inequalities)

Evans (Graphical methods for inequality constraints in marginalized DAGs) generalizes the instrumental inequality.

\hypertarget{check-prior-dependence}{%
\subsection{Check prior dependence}\label{check-prior-dependence}}

\begin{itemize}
\tightlist
\item
  A graph showing how some conclusions changes as we relax one of the restrictions.
\end{itemize}

How much do our conclusions depend on qual restrictions?

\begin{itemize}
\tightlist
\item
  How do conclusions differ if we drop all restrictions
\end{itemize}

\hypertarget{check-fit}{%
\subsection{Check fit}\label{check-fit}}

Approaches using simulated data from the posterior predictive distribution are described in \citet{gabry2019visualization}

The graphs below compare (using tools in the \texttt{bayesplot}) package, show how typical the data we observe is for the model that correctly assumes a direct effect and the model that incorrectly excludes it.

How does the researcher's model do?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ppc_stat}\NormalTok{(data}\OperatorTok{$}\NormalTok{Y, replicates_ana)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value
## with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/unnamed-chunk-119-1.pdf}
\caption{\label{fig:unnamed-chunk-119}Posterior distribution of test statistics under researcher's model}
\end{figure}

How would we evaluate the less constrained (``true'') model?

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value
## with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{ii_files/figure-latex/unnamed-chunk-120-1.pdf}
\caption{\label{fig:unnamed-chunk-120}Distribution of \(Y\) under the (data updated) true model}
\end{figure}

Compare out of sample fit between model with posteriors and priors.

Reality checks -- posterior fits. Predict data from the posterior. Look at whether the data you see are similar to the data you actually have.

\hypertarget{assess-likelihoods-under-different-models}{%
\subsection{Assess likelihoods under different models}\label{assess-likelihoods-under-different-models}}

Bayesian information criterion (BIC)\footnote{\(BIC = \ln(n)k - 2\ln(\hat{L})\) where \(\hat{L}\) is the maximized likelhood, \(k\) is the number of parameters, and \(n\) the number of data points.}
BIC involves a penalty for more parameters

Compare likelihoods of the data under different models
Check look package for rstan

\hypertarget{evaluating-the-democracy-inequality-model}{%
\section{Evaluating the Democracy-Inequality model}\label{evaluating-the-democracy-inequality-model}}

\hypertarget{prior-check}{%
\subsection{Prior check}\label{prior-check}}

In a second iteration of the analysis, we show what happens if we loosen the monotonicity restriction on \(I\)'s effect on \(M\). Here we consider negative effects of \(I\) on \(M\) \emph{unlikely}, rather than impossible, and we consider null and positive effects somewhat likely. We refer to these priors as ``quantitative priors'' in the sense that they place a numerical value on beliefs rather than a logical restriction. Here, we set our prior on \(\theta^M\) as: \(p(\theta^M=\theta^M_{10})=0.1\), \(p(\theta^M=\theta^M_{00})=0.25\), \(p(\theta^M=\theta^M_{11})=0.25\), and \(p(\theta^M=\theta^M_{01})=0.4\). We show the results for the inferences given different findings in tables \ref{tab:HK8cases1quant} and \ref{tab:HK8cases2quant}. The mapping into expected posterior variance associated with each strategy is shown by the numbers in parentheses in Table \ref{CaseLearn}.

The results differ in various modest ways. However, the biggest difference we observe is in the degree to which the mobilization clue matters when we are looking for negative effects of inequality. As discussed, if we assumed monotonic positive effects of inequality on mobilization and monotonic positive effects of mobilization on inequality, then the mediator clue is uninformative about the indirect pathway since that pathway can only generate a positive effect. However, if we allow for the possibility of a negative effect of inequality on mobilization, we now make \(M\) informative as a mediator even when the effect of inequality that we are interested in is negative: it is now possible that inequality has a negative effect on democratization via a negative effect on mobilization, followed by a positive effect of mobilization on democratization. So now, observing whether mobilization occurred adds information about whether a negative effect could have occurred via the mobilization pathway.

Moreover, it is possible for the two effects of observing \(M\) on our beliefs to work in opposite ways. What we learn from observing \(M\) about the \(I \rightarrow M \rightarrow D\) pathway may push in a different direction from what we learn from observing \(M\) about the direct \(I \rightarrow D\) pathway. We see this dynamic at work in a case with low inequality and democratization. Where we are only learning about \(M\) as a moderator of \(I\)'s direct effect (monotonicity assumption in place), observing \(M=0\) shifts our beliefs in favor of \(I\)'s negative effect. But where we are learning about \(M\) as both mediator and moderator, observing \(M=0\) shifts our beliefs \emph{against} \(I\)'s negative effect. The reason for this latter result is straightforward: if \(I=0\) and we then see \(M=0\), then we have just learned that inequality's possible indirect negative effect, running via the mobilization pathway, has \emph{not} in fact occurred; and this has a considerable downward effect on our beliefs in an overall negative effect of inequality. This learning outweighs the small positive impact of observing \(M=0\) on our confidence that \(I\) had a direct negative effect on \(D\).

We see these differences most clearly in the cases of Albania (as compared to Mexico) and Nicaragua (as compared to Taiwan). Under priors fully constrained to monotonic causal effects, we saw that the mediator clue, \(M\), made only a small difference to our inferences. However, if we allow for a negative effect of \(I\) on \(M\), even while believing it to be unlikely, observing mobilization in Albania and Nicaragua makes us substantially more confident that inequality mattered, and differentiates our conclusions about these cases more sharply from our conclusions about Mexico and Taiwan, respectively.

\hypertarget{monotonic-restrictions}{%
\subsection{Monotonic restrictions}\label{monotonic-restrictions}}

Compare fit between model with and without monotonic restrictions

\hypertarget{final-words}{%
\chapter{Final Words}\label{final-words}}

The central idea of this book is that many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account of how and under what conditions causal relations play out.

The focus on design based inference that has grown in influence over the last decade has made it possible to dispense with such models. This is a remarkable achievement and has made it possible to put the testing of hypotheses on firm footing. But it has also come at the cost of a narrowing of questions to focus on variants of the average causal effect.

Building on pioneering work by scholars in computer science, statistics, and philosophy we have outlined an approach that uses Bayesian networks to shift in focus towards treating causal models as both tools and objects of inquiry. The models both guide inference---by providing guidance on what conclusions can one draw from a given case or set of cases given background evidence and a background model---and evolve as new data comes along. This approach holds out the promise of addressing a wide set of questions in an integrated way:

\begin{itemize}
\tightlist
\item
  \textbf{Case level questions}: does \(X\) explain \(Y\) in this case?
\item
  \textbf{Process questions}: did \(X\) matter for \(Y\) through this or that channel?
\item
  \textbf{Transportation questions}: what are the implications of this study for processes in other places?
\end{itemize}

Using causal models also provides a clear \emph{procedure} for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw. Even when quantities are not identified, they can be used to assess when conclusions are consistent with the data for different priors.

In exploring this approach we have rethought much of how we had been thinking about qualitative and quantitative inference. In starting to use these models and seeing their benefits we have have also developed a keener sense of the risks they entail. We outline these lessons and these risks next.

\hypertarget{general-lessons}{%
\section{General lessons}\label{general-lessons}}

We were motivated by an interest in showing that inferences from within case qualitative data could be combined with inferences from between case quantitative data. A canonical case might be gathering data on \(X\), \(Y\) data on many cases and \(M\) data on process for some. In fact however this distinction has no meaning in the formal set up and analysis of models. One could just as easily be interested in the effect of \(X\) on \(Y\) and have plentiful data on \(M\) but limited data on \(Y\) or \(X\). In this framework the qualitative and quantitative inference strategies are not just integrated, the distinction between them breaks down completely.

We started off thinking of beliefs about the values of estimands and beliefs about the informativeness of within case information as being essentially independent. This was a feature of the models we explored in \citet{humphreys2015mixing} and implicit in many accounts of process tracing: you articulate a belief about some hypothesis and you articulate a belief about how informative evidence will be about your hypothesis. When both of these beliefs are tehmselves derived from an integrated model however then the same conjectures that infrom your beliefs about the hypotheses also inform your beliefs about the informativeness of additional data, you just cannot think of them as independent from each other.

We started off thinking that in providing priors over causal relations you were directly stating beliefs about how the world works. In the simple case one might think that either \(X\) cuased \(Y\) or it did not and either \(M\) should be seen in the event that \(X\) caused \(Y\) or it shouldn't be. But these statements are in fact clearly model dependent. Beyond the model required to describe events in such crisp terms, the statements involve counterfactuals on counterfactuals---models of causal processes. Once a model involves assertions of conditional independence we are clearly in the business of dealing in simplifications and our priors become less statements of how we believe the world works to become somewhat statements about what set of models are least bad within a class of abstractions.

\hypertarget{worries-about-what-you-have-to-put-in}{%
\section{Worries about what you have to put in}\label{worries-about-what-you-have-to-put-in}}

Trading in models also brought into focus some of the limitations of DAGs in representing causal processes.

\textbf{Well defined nodes?} Do DAGs actually capture causal processes that qualitative researchers see -- qualitative researchers see that the domino 2 fell \emph{the moment it was hit} by domino 1. How do we express this in a DAG?

\textbf{Acyclic really?} The first assumption made in the construction of causal models in this book is the underlying DAG. One can specify a DAG without making any substantive claims about function forms or patterns of confounding. Yet even the DAG presents worries.

\textbf{Theoretically deeper models.}

\hypertarget{limits-on-what-you-can-get-out}{%
\section{Limits on what you can get out}\label{limits-on-what-you-can-get-out}}

\textbf{Complexity.} We have sought to use non-parametric models.. To maintain simplicity we have largely focused on models with binary nodes. At first blush this class of causal models appears very simple. In fact however we quickly learn that even with a small set of nodes produces a dizzying variety of causal types.

\textbf{Identification.} In modelling complete structures we see clearly how much easier it is to define problems than it is to solve them. Many of the quantities we care about are easily shown to not be identified by the causal models we employ. The causes of effect estimand is perhaps the most obvious of these.

\textbf{Limits of qualitative data under ignorable assignments.} You generally cannot conclude all that much about population quantities from only a small number of cases when causal effects are identified.

\hypertarget{a-world-of-models-practical-steps-forward-for-collective-cumulation}{%
\section{A world of models: Practical steps forward for collective cumulation}\label{a-world-of-models-practical-steps-forward-for-collective-cumulation}}

\hypertarget{part-appendices}{%
\part{Appendices}\label{part-appendices}}

\hypertarget{examplesappendix}{%
\chapter{\texorpdfstring{\texttt{gbiqq}}{gbiqq}}\label{examplesappendix}}

Examples of canonical models, together with a guide to the \texttt{gbiqq} package is provided at:

\url{https://macartan.github.io/causalmodels/}

\bibliography{bib.bib,packages.bib}


\end{document}
