--- 
title: "Integrated Inferences"
author: "Macartan Humphreys and Alan Jacobs"
date: "Draft!: `r Sys.Date()`"
documentclass: book
fontsize: 12pt
bibliography: [bib.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/ii
description: "Model based strategies for integrating qualitative and quantitative inferences."
cover-image: "dnieperriver.png"
site: bookdown::bookdown_site
header-includes:
 - \usepackage{amsmath}
 - \usepackage{mathspec}
 - \usepackage{color}
 - \usepackage{amssymb} 
 - \usepackage{amsfonts} 
 - \usepackage{dsfont}
---

```{r setup, include=FALSE}

options(
  htmltools.dir.version = FALSE, formatR.indent = 2,
  width = 55, digits = 4, warnPartialMatchAttr = FALSE, warnPartialMatchDollar = FALSE

)
```

# Preface {-}

```{r, include=FALSE}
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')

```

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("_packages_used.R")
```

*Quick Guide*

This book has four main parts:

* Part I introduces causal models and a Bayesian approach to learning about them and drawing inferences from them.

* Part II applies these tools to strategies of learning from process tracing and mixed methods research, with applications.

* Part III turn to design decisions, exploring strategies for assessing what kind of data is most useful for addressing different kinds of research questions given knowledge to date about a population or a case.

* Everything up to Part IV assumes that we have access to models we are happy with. In Part IV we turn to the difficult question of model justification and outline a range of strategies on can use to justify causal models. 

There is also  an accompanying `R` package currently hosted on github. This can be downloaded and installed as follows:

```{r, eval = FALSE}
install.packages("remotes")
remotes::install_github("macartan/gbiqq")
```

An  [appendix](#examplesappendix)  shows how to use the package for defining and learning from a set of canonical causal models. 


<!--chapter:end:index.Rmd-->

# Integrands  {#intro}

***

We introduce the project, describe the book's general approach, and explain how it differs from current approaches in the social sciences. We preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences from evidence.

***


```{r, include = FALSE}
source("_packages_used.R")
```


The engineer pressed the button, but the light didn't turn on. 

"Maybe the bulb is blown," she thought. 

She replaced the bulb, pressed the button and, sure enough, the light turned on.

"What just happened?" asked her philosopher friend.

"The light wouldn't turn on because the bulb was busted, but I replaced the bulb and fixed the problem." 

"Such hubris!" remarked her friend.  "If I understand you, you are saying that pressing the button *would have* caused a change in the light *if the bulb had not been busted*."

"That's right."

"But hold on a second. That's a causal claim about counterfactual events in counterfactual conditions that you couldn't have observed. I don't know where to begin. For one thing, you seem to be inferring from the fact that the light did not go on when you pressed the button the first time that pressing the button the first time had no effect at all. What a remarkable conclusion. Did it never occur to you that the light might have about to turn on anyway---and that your pressing that button at just that moment is what *stopped* the light going on?"

"What's more," the philosopher went on, "you seem also to be saying that pressing the button the second time *did* have an effect because you saw the light go on that second time. That's rather incredible. That light could be controlled by a different circuit that was timed to turn it on at just the moment that you pressed the button the second time. Did you think about that possibility?"
   
"On top of those two unsubstantiated causal claims," the philosopher continued, "you are *also* saying, I think, that the difference between what you *believe* to be a non-cause on the first pressing and a cause on the second pressing is itself due to the bulb. But, of course, countless other things could have changed! Maybe there was a power outage for a few minutes." 

"That hardly ever happens."

"Well, maybe the light only comes on the second time the button is pressed." 

"It's not that kind of button."

"So you say. But even if that's true, there are still so many other possible factors that could have mattered here---including things that neither of us can even imagine!"

The philosopher paused to ponder her friend's chutzpah.

"Come to think of it," the philosopher went on, "how do you even know the bulb was busted?"

"Because the light worked when I replaced the bulb."

"But that means," the philosopher responded, "that your measurement of the state of the bulb depends on your causal inference about the effects of the button. And we know where that leads. Really, my friend, you are lost."

"So do you want me to put the old bulb back in?"


## The Centrality of Causal Models

In the conversation between the philosopher and the engineer, the philosopher disputes what seems a simple inference. Some of her arguments suggest a skepticism bordering on paranoia and seem easily dismissed. Others seem closer to hitting a mark: perhaps there was nothing wrong with the bulb and the button was just the kind that has to be pressed twice. For an objection like this, we have to rely on the engineer's knowledge of how the button works. 

While the philosopher's skepticism guards against false inferences, it is also potentially paralyzing. 

Social scientists have been shifting between the poles staked out by the philosopher and the engineer for many years. The engineers bring background knowledge to bear on a question and deploy models of broad processes to make inferences about particular cases. The philosophers bring a skeptical lense and ask for justifications that depend as little as possible on imported knowledge.

This book is written for would-be engineers. It is a book about how we can mobilize our background knowledge about how the world works to learn more about the world. It is, more specifically, a study in how we can use causal models of the world to design and implement empirical strategies of causal inferences. But we will try to imagine throughout that the engineers have philosophers looking over their shoulders and will try to figure out ways to equip the engineers with plausible answers to the the philosophers' worries. 

By causal models, we mean systems of statements reflecting background knowledge of and uncertainty about possible causal linkages in a domain of interest. There are three closely related motivations for our move to examine the important role that models can play in empirical social inquiry. One is an interest in integrating qualitative knowledge with quantitative approaches, and a view---possibly a controversial one---that process tracing is a model-dependent endeavor.  A second is concern over the limits of design-based inference. A third and related motive is an interest in better connecting empirical strategies to theory. 

### Qualitative and mixed-method inference

Recent years have seen the elucidation of the inferential logic behind "process tracing" procedures used in qualitative political science and other disciplines. In our read, the logic provided in these accounts depends on a particular form of model-based inference.^[As we describe in @humphreys2015mixing, the term "qualitative research" means many different things to different scholars, and there are multiple approaches to mixing qualitative and quantitative methods. There we distinguish between approaches that suggest that qualitative and quantitative approaches address distinct, if complementary, questions; those that suggest that they involve distinct measurement strategies; and those that suggest that they employ distinct inferential logics. The approach that we employ in @humphreys2015mixing connects most with the third family of approaches. Most closely related, in political science, is work in  @GlynnQuinn2011, in which researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. In the present book, however, we move toward a position in which fundamental differences between qualitative and quantitative inference tend to dissolve, with all inference drawing on what might be considered a "qualitative" logic in which the researcher's task is to confront a pattern of evidence with a theoretical logic.] 

While process tracing as a method has been around for more than three decades (e.g., @george1985case), its logic has been most fully laid out by qualitative methodologists over the last 15 years (e.g., @bennett2014process, @george2005case, @brady2010rethinking, @Hall2003aligning, @mahoney2010after). Whereas @king1994designing sought to derive qualitative principles of causal inference within a correlational framework, qualitative methodologists writing in the wake of "KKV" have emphasized and clarified process-tracing's "within-case" inferential logic: in process tracing, explanatory hypotheses are tested based on observations of what happened within a case, rather than on covariation between causes and effects across cases. The process tracing literature has also advanced increasingly elaborate conceptualizations of the different kinds of probative value that within-case evidence can yield. 

For instance, qualitative methodologists have explicated the logic of different test types ("hoop", "smoking gun", etc.) involving varying degree of specificity and sensitivity (@collier2011understanding, @Mahony:Logic:2012).  A smoking-gun test is a test that seeks information that is only plausibly present if a hypothesis is true (thus, generating strong evidence for the hypothesis if passed), a hoop test seeks data that should certainly be present if a proposition is true (thus generating strong evidence against the hypothesis if failed), and a doubly decisive test is both smoking-gun and hoop (for an expanded typology, see also @rohlfing2013comparative). Other scholars have expressed the leverage provided by process-tracing evidence in Bayesian terms, moving from a set of discrete test types to a more continuous notion of probative value (@fairfield2017explicit, @BennettAppendix, @humphreys2015mixing).^[In @humphreys2015mixing, we use a fully Bayesian structure to generalize Van Evera's four test types in two ways: first, by allowing the probative values of clues to be continuous; and, second, by allowing for researcher uncertainty (and, in turn, updating) over these values. In the Bayesian formulation, use of process-tracing information is not formally used to conduct tests that are either "passed" or "failed", but rather to update beliefs about different propositions.]

<!-- ^[Note that these statements are statements about likelihood functions and do not require a specifically Bayesian mode of inference.] -->

Yet, conceptualizing the different ways in which probative value might operate leaves a fundamental question unanswered: what gives within-case evidence its probative value with respect to causal relations? We believe that, fundamentally, the answer lies in researcher beliefs that lies outside of the analysis in question. We enter a research situation with a model of how the world works, and we use this model to make inferences given observed patterns in the data---while at the same time updating those models based on the data. A key aim of this book is to demonstrate how models can --- and, in our view, must --- play in drawing case-level causal inferences.

As we will also argue, along with clarifying the logic of qualitative inference, causal models can also enable the systematic integration of qualitative and quantitative forms of evidence. Social scientists are increasingly pursuing mixed-method research designs. It is becoming increasingly common for scholars to pursue research strategies that combine quantitative with qualitative forms of evidence. A typical mixed-methods study includes the estimation of causal effects using data from many cases as well as a detailed examination of the processes taking place in a few. Prominent examples include Lieberman's study of racial and regional dynamics in tax policy (@lieberman2003race); Swank's analysis of globalization and the welfare state (@swank2002global); and Stokes' study of neoliberal reform in Latin America (@stokes2001mandates). Major recent methodological texts provide intellectual justification of this trend toward mixing, characterizing  small-$n$  and large-$n$ analysis as drawing on a single logic of inference and/or as serving complementary functions (King, Keohane, and Verba, 1994; Brady and Collier, 2004). The American Political Science Association now has an organized section devoted in part to the promotion of multi-method investigations, and the emphasis on multiple strategies of inference research is now embedded in guidelines from many research funding agencies (Creswell and Garrett, 2008). 

However, while scholars frequently point to the benefits of mixing correlational and process-based inquiry (e.g., @collier2010sources, p.~181), and have sometimes mapped out broad strategies of multi-method research design (@Lieberman2005nested, @SeawrightGerring2008), they have rarely provided specific guidance on how the integration of inferential leverage should unfold. In particular, the literature does has not supplied specific principles for aggregating findings---whether mutually reinforcing or contradictory---across different modes of analysis.A small number of exceptions stand out. In the approach suggested by @gordon2004quantitative, for instance, available expert (possibly imperfect) knowledge regarding the operative causal mechanisms for a small number of cases can be used to anchor the statistical estimation procedure in a large-N study. @WesternJackman1994 propose a Bayesian approach in which qualitative information shapes subjective priors which in turn affect inferences from quantitative data. Relatedly,  in @GlynnQuinn2011, researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. @seawrightbook presents an informal framework in which case studies are used to test the assumptions underlying statistical inferences, such as the assumption of no-confounding or the stable-unit treatment value assumption (SUTVA). 

Yet we still lack a comprehensive framework that allows us to enter qualitative and quantitative form of information into an integrated analysis for the purposes of answering the wide range of causal questions that are of interests to social scientists, including questions about case-level explanations and causal effects, average causal effects, and causal pathways. As we aim to demonstrate in this book, grounding inference in causal models provides a very natural way of combining information of the $X,Y$ variety with information about the causal processes connecting $X$ and $Y$. The approach can be readily addressed to both the  case-oriented questions that tend to be of interest to qualitative scholars and the population-oriented questions that tend to motivate quantitative inquiry. As will become clear, in fact, when we structure our inquiry in terms of causal models, the conceptual distinction between qualitative and quantitative inference becomes hard to sustain. Notably, this is not for the reason that "KKV"'s framework suggests, i.e., that all causal inference is fundamentally about correlating causes and effects. To the contrary, it is that in a causal-model-based inference, what matters for the informativeness of a piece of evidence is how that evidence is connected to our query, given how we think the world works. While the apparatus that we present is formal, the approach---in asking how pieces of evidence drawn from different parts of a process map on to a base of theoretical knowledge---is arguably most closely connected to process tracing in its core logic.

<!-- We see each of these three developments as being of great importance to the development of empirical social science. Collectively, they have generated far greater clarity and sophistication about, and diversified social scientists approaches to, the empirical assessment of causation. At the same time, we see each of these developments as, to date, limited in important ways.  -->


### The limits to design-based inference 

The engineer in our story tackles the problem of causal inference using models: theories of how the world works, generated from past experiences and applied to the situation at hand. The philosopher maintains a critical position, resisting models and the importation of beliefs not supported by evidence in the case at hand. 

The engineer's approach recalls the dominant orientation among social scientists until rather recently. At the turn of the current century, multivariate regression had become a nearly indispensable tool of quantitative social science, with a large family of statistical models serving as political scientists' and economists' analytic workhorses for the estimation of causal effects.  

Over the last two decades, however, the philosophers have raised a set of compelling concerns about the assumption-laden nature of standard regression analysis, while also clarifying how valid inferences can be made with limited resort to models in certain research situations. The result has been a growth in the use of design-based inference techniques that, in principle, allow for model-free estimation of causal effects (see @dunning2012natural, @GerGreKap04, @druckman2011experimentation, @palfrey2009laboratory among others). These include lab, survey, and field experiments and natural-experimental methods exploiting either true or "as-if" randomization by nature. With the turn to experimental and natural-experimental methods has come a broader conceptual shift, with a growing reliance on the "potential outcomes" framework as a model for thinking about causation (see @Rubin1974, @splawa1990application among others) and a reduced reliance on models of data-generating processes.

The ability to estimate average effects and to calculate $p$-values and standard errors without resort to models is an extraordinary development. In Fisher's terms, with these tools, randomization processes provide a "reasoned basis for inference," placing empirical claims on a powerful footing.

While acknowledging the strengths of these approaches, we also take seriously two points of concern. 

The first concern---raised by many in recent years (e.g., @thelen2015comparative)---is about design-based inference's scope of application. While experimentation and natural experiments represent powerful tools, the range of research situations in which model-free inference is possible is inevitably limited. For a wide range of causal conditions of interest to social scientists and to society, controlled experimentation is impossible, and true or "as-if" randomization is absent. Moreover, limiting our focus to those questions for, or situations in which, exogeneity can be established "by design" would represent a dramatic narrowing of social science's ken. It would be a recipe for, at best, learning more and more about less and less. To be clear, this is not an argument against experimentation or design based inference; yet it is an argument for why social science needs a broader set of tools.

The second concern is more subtle. The great advantage of design-based inference is that it liberates researchers from the need to rely on models to make claims about causal effects. The risk is that, in operating model-free, researchers end up learning about effect sizes but not about models. But models are what we want to learn about. Our goal as social scientists is to have a useful model for how the world works, not simply a collection of claims about the effects different causes have had in different times and places. It is through models that we derive an understanding of how things might work in contexts and for processes and variables that we have not yet studied. Thus, our interest in models is intrinsic, not instrumental. By taking models, as it were, out of the equation, we dramatically limit the potential for learning about the world. 

### Connecting theory and empirics

Theory and empirics have had a surprisingly uncomfortable relationship in political science. In a major recent intervention, for instance, @clarke2012model draw attention to and critique political scientists' extremely widespread reliance on the "hypothetico-deductive" (H-D) framework, in which a theory or model is elaborated, empirical predictions derived, and data sought to test these predictions and the model from which they derive.Clarke and Primo draw on decades of scholarship in the philosophy of science pointing to deep problems with the HD framework, including with the idea that the truth of a model logically derived from first principles can be *tested* against evidence. 

This book is also motivated by a concern with the relationship between theory and evidence in social inquiry. In particular, we are struck by the frequent lack of a clear link between theory, on the one hand, and empirical strategy and inference, on the other. We see this ambiguity as relatively common in both qualitative and quantitative work. We can perhaps illustrate it best, however, by reference to qualitative work, where the centrality of theory to inference has been most emphasized. In process tracing, theory is what justifies inferences. In their classic text on case study approaches, @george2005case describe process tracing as the search for evidence of "the causal process that a theory hypothesizes or implies" (6). Similarly, @Hall2003aligning conceptualizes the approach as testing for the causal-process-related observable implications of a theory, @mahoney2010after indicates that the events for which process tracers go looking are those posited by theory (128), and @gerring2006case describes theory as a source of predictions that the case-study analyst tests (116). Theory, in these accounts, is supposed to help us figure out where to look for discriminating evidence. 

What we do not yet have, however, is a systematic account of how researchers can derive within-case empirical predictions from theory and how exactly doing so provides leverage on a causal question. From what elements of a theory can scholars derive informative within-case observations? Given a set of possible things to be observed in a case, how can theory help us distinguish more from less informative observations? Of the many possible observations suggested by a theory, how can we determine which would add probative value to the evidence already at hand? How do the evidentiary requisites for drawing a causal inference, given a theory, depend on the particular causal question of interest---on whether, for instance, we are interested in identifying the cause of an outcome, estimating an average causal effect, or identifying the pathway through which an effect is generated? In short, how exactly can we ground causal inferences from within-case evidence in background knowledge about how the world works?

Most quantitative work in political science features a similarly weak integration between theory and research design. The modal inferential approach in quantitative work, both observational and experimental, involves looking for correlations between causes and outcomes, with minimal regard for intervening or surrounding causal relationships.^[One exception is structural equation modeling, which bears a close affinity to the approach that we present in this book, but has gained minimal traction in political science.] 

In this book, we seek to show how scholars can make much fuller and more explicit use of theoretical knowledge in designing their research projects and analyzing their observations. Like Clarke and Primo, we treat models not as maps of sort: maps, based on prior theoretical knowledge, about causal relations in a domain of interest. Also as in Clarke and Primo's approach, we do not write down a model in order to test its veracity. Rather, we show how we can systematically use causal models with particular characteristics to guide our empirical strategies and inform our inferences. Grounding our empirical strategy in a model allows us, in turn, to learn about the model itself as we encounter the data.  


##Key contributions

This book draws on methods developed in the study of Bayesian networks, a field pioneered by scholars in computer science, statistics, and philosophy. Bayesian networks, a form of causal model, have had limited traction to date in political science. Yet the literature on Bayesian networks and their graphical counterparts, directed acyclic graphs (DAGs), is a body of work that addresses very directly the kinds of problems that qualitative and quantitative scholars routinely grapple with.^[For application to quantitative analysis strategies in  political science, @glynn2007non give a clear introduction to how these methods  can be used to motivate strategies for conditioning and adjusting for causal inference; @garcia2015graphical demonstrate how these methods can be used to assess claims of external validity. With a focus on qualitative methods, @Waldner2015completeness uses causal diagrams to lay out a ''completeness standard'' for good process tracing. @weller2014finding employ  graphs to conceptualize the different possible pathways between causal and outcome variables among which qualitative researchers may want to distinguish. Generally, in discussions of qualitative methodology, graphs are used to capture core features of theoretical accounts, but  are not developed specifically to ensure a representation of the kind of independence relations implied by structural causal models (notably what is called in the literature the "Markov condition"). Moreover, efforts to tie these causal graphs to probative observations, as in @Waldner2015completeness, are generally limited to identifying steps in a causal chain that the researcher should seek to observe.] 

Drawing on this work, we show in the chapters that follow how a theory can be formalized as a causal model represented by a causal graph and a set of structural equations. Engaging in this modest degree of formalization, we seek to demonstrate, yields enormous benefits. It allows us, for a wide range of causal questions, to identify a.) a set of variables (or nodes) in the model, including unobservable factors, that represent the causal query and b.) a set of observable variables that are potentially informative about the nodes in the query. The payoffs of this approach are that it enables us to:

* make systematic use of theory to figure out what kinds of evidence have probative value for our causal queries and, in turn, to design maximally informative empirical strategies;

* draw inferences from the evidence in a manner disciplined by our theoretical knowledge of how the world works;

* integrate in this analysis information conventionally considered "qualitative" (e.g., evidence about causal mechanisms) with information conventionally considered "quantitative" (e.g., evidence about causal and outcome variables); and

* learn about the models that we bring to the table, which allows for the cumulation of knowledge through the updating of the theoretical knowledge base on which future inferences can draw.

As we will show, using causal models has substantial implications for common methodological advice and practice. To touch on just a few of these: Our elaboration and application of model-based process tracing shows that, given plausible causal models, process tracing's common focus on intervening causal chains may be much less productive than other empirical strategies, such as examining moderating conditions. Our examination of model-based case-selection indicates that for many common purposes there is nothing particularly especially informative about "on the regression line" cases or those in which the outcome occurred, and that case selection should often be driven by factors that have to date received little attention, such as the population distribution of cases and the probative value of the available evidence. And an analysis of clue-selection as a decision problem shows that the probative value of a piece evidence cannot be assessed in isolation, but hinges critically on what we have already observed.  

**MORE TO COME HERE ELABORATING IMPLICATIONS, COMPARING TO ADVICE AND PRACTICE IN LITERATURE**

We emphasize that the basic analytical apparatus that we employ in this book is not new, and the book's aim is not to generate fundamentally novel approaches to study causality in the world. Rather, we see the book's goals as being of three kinds. First, the book aims to import insight: to introduce political scientists to an approach that has received little attention in the discipline but that can be useful for addressing the sorts of causal causal questions with which political scientists are commonly preoccupied. As a model-based approach, it is a framework especially well suited to a field of inquiry in which exogeneity frequently cannot be assumed by design---that is, in which we often have no choice but to be engineers. Second, the book draws connections between the Bayesian networks approach and key concerns and challenges with which methodologists in our discipline routinely grapple. Working with causal models and DAGs most naturally connects to concerns about confounding and identification that have been central to much quantitative methodological development. Yet we also show how causal models can address issues central to process tracing, such as how to select cases for examination, how to think about the probative value of causal process observations, and how to structure our search for evidence, given finite resources. Third, the book provides a set of usable tools for implementing the approach. We provide intuition and software that researchers can use to make research design choices and draw inferences from the data.


## The Road Ahead

The book is divided into four main parts. 

The first part is about the basics. We start off by  describing the kinds of causal estimands of interest. The main goal here is to introduce the key ideas in the study of Bayesian nets and to argue for a focus of interest away from  average treatment effects as go-to estimands of interest and towards a focus on causal nets, or causal structures, as the key quantity of interest. The next chapter introduces key Bayesian ideas; what  Bayes' rule is and how to use it. The third chapter connects the study of Bayesian networks to theoretical claims. The key argument here is that nets should be thought of as theories which are themselves supportable by lower level networks (theories). Lower level theories are useful insofar as they provide leverage to learn about processes on higher level networks.

The second part applies these ideas to process tracing. Rather than conceptualizing process tracing as has been done in recent work as seeking process level data that is known to be informative about a causal claim, the approach suggested here is one in which the probative value of a clue is derived from its position in a causal network connecting variables of interest. Chapter 5 lays out the key logic of inference from clues and provides general criteria for assessing when it is and is not possible. Chapter 6 provides specific tools for assessing which collections of clues are most informative for a given estimand of interest and outlines a strategy for assessing which clues to gather when in a research process. Chapter 7 applies these tools to the problem of assessing the effects of economic inequality on democratization.

The third part moves to mixed data problems --- situations in which a researcher contains "quantitative" ($X,Y$) data on a set of cases and is considering gathering within case ("qualitative") data on some of these. In chapter 8 we argue that this situation is formally no different to the single case process tracing problem since a collection of cases can always be conceptualized as a single case with vector valued variables. The computational complexity is however greater in these cases and so in this chapter we describe a set of models that may be useful for addressing these problems. In this framework the problem of case selection is equivalent to the kind of problem of clue selection discussed in Chapter 6. For a canonical multicase model however we use simulation approaches to provide guidance for how cases should be selected. The broad conclusion here is that researchers should go where the probative value lies, and all else equal, should select cases approximately proportional to the size of $XY$ strata---whether or not these are "on the regression line." We conclude this part by revisiting the problem of inequality and democracy introduced in Chapter 7.

The fourth part steps back and puts the model-based approach into question. We have been advocating an embrace of models to aid inference. But the dangers of doing this are demonstrably large. The key problem is that with model-based inference, the inferences are only as good as the model. In the end, while we are supporting the efforts of engineers, we know that the philosopher is right. This final part  provides four responses to this (serious) concern. The first is that the dependence on models can sound more extreme than it is. Seemingly fixed parameters of models can themselves become quantities of interest in lower-level models, and there can be learning about these when higher-level models are studied. Thus models are both put to use and objects of interest. The second is that different types of conditional statements are possible; in particular as shown in work qualitative graphs.  The third response points to the sort of arguments that can be made to support models, most importantly the importation of knowledge from one study to another. The last argument, presented in the last substantive chapter, highlights the tools to *evaluate* models, using tools that are increasingly standard in Bayesian analysis. 

Here we go.




<!--chapter:end:01-intro.Rmd-->

# (PART) Foundations  {-}

# Causal Models {#models}

```{r, include = FALSE}
source("_packages_used.R")
```

***

We provide a lay language primer on the logic of causal models.

***


While social scientific methods can be addressed to many sorts of questions, matters of causation have long been central to theoretical and empirical work in political science, economics, sociology, and psychology. Causality is also the chief focus of this book. 

Causal knowledge, however, is not just the end goal of much empirical social science; it is also a key *input* into causal inference. Rarely do we arrive at causal inquiry fully agnostic about causal relations in the domain of interest. Moreover, our beliefs about how the world works---as we show later in this book---have profound implications for how the research process and inference should unfold. 

What we need is a language for expressing our prior causal knowledge such that we can full exploit it, drawing inferences and making research design decisions in ways that are logically consistent with our beliefs, and such that others can readily see and assess those underlying premises. Causal models provide such a language.

In this chapter we provide a basic introduction to causal models. Subsequent chapters in Part I layer on other foundational components of the book's framework, including a causal-model-based understanding of theory, the definition of common causal estimands within causal models, and the basics of Bayesian inference.

<!-- Causal questions, however, can come in many forms. Before we address how causal inferences can be drawn, we must first define what we mean by causation and unpack and define the different kinds of causal estimands in which we might be interested. -->

<!-- We start with a description of the causal estimands of interest. We first define a causal effect and then introduce causal graphs and estimands. We give an example of a simple causal model that we return to to illustrate key ideas throughout the paper. -->

## The counterfactual model

We begin with what we might think of as a meta-model, the counterfactual model of causation. The counterfactual model is the dominant approach to causal relations in the social sciences. At its core, a counterfactual understanding of causation captures a simple notion of causation as "difference-making."^[The approach is sometimes attributed to David Hume, whose writing contains ideas both about causality as regularity and causality as counterfactual. On the latter the key idea is "if the first object had not been, the second never had existed" [@hume2000enquiry, Section VIII]. More recently, the counterfactual view has been set forth by @splawa1990application and @lewis1973counterfactuals. See also  @lewis1986causation.] In the counterfactual view, to say that $X$ caused $Y$ is to say: *had* $X$ been different, $Y$ *would have been* different. Critically, the antecedent, "had $X$ been different,"  imagines a *controlled* change in $X$---an intervention that altered $X$'s value---rather than a naturally arising difference in $X$. The counterfactual claim, then, is not that $Y$ is different in those cases in which $X$ is different; it is, rather, that if one could have *made* $X$ different, $Y$ would have been different.

Turning to a substantive example, consider, for instance, the claim that India democratized ($Y$) because it had a relatively high level of economic equality ($X$) (drawing on the logic of @boix2003democracy). In the counterfactual view, this is equivalent to saying that, had India *not* had a high level of equality---where we imagine that we *made* equality in India lower---the country would not have democratized. High economic equality made a difference.

<!-- Causal effects are thus unmeasurable quantities. They are not differences between possible observations in the world, but differences between outcomes in the world and counterfactual outcomes that need to be inferred rather than measured.       -->

Along with this notion of causation as difference-making, we also want to allow for variability in how $X$ acts on the world. $X$ might sometimes make a difference, for some units of interest, yet sometimes not. High levels of equality might generate democratization in some countries or historical settings but not in others. Moroever, while equality might make democratization happen in some times in places, it might prevent that same outcome in others. In political science, we commonly employ the "potential outcomes" framework to describe the different kinds of counterfactual causal relations that might prevail between variables \citep{Rubin1974}. In this framework we characterize how a given unit responds to a causal variable by positing the outcomes that it *would* take on at different values of the causal variable.

It is quite natural to think about potential outcomes in the context of medical treatment. Consider a situation in which some individuals in a diseased population are observed to have received a drug ($X=0$) while others have not ($X=0$). Assume that, subsequently, a researcher observes which individuals become healthy ($Y=1$) and which do not ($Y=0$). Let us further stipulate that each individual belongs to one of four unobserved response ''types,'' defined by the potential effect of treatment on the individual:^[We implicitly invoke the assumption that the treatment or non-treatment of one patient has no effect on the outcomes of other patients. This is known as the stable unit treatment value assumption (SUTVA). See also \citet{HerronQuinn} for a similar classification of types.]

* **a**dverse: Those who would get better if and only if they do not receive the treatment
* **b**eneficial: Those who would get better if and only if they do receive the treatment
* **c**hronic: Those who will remain sick whether or not they receive treatment
* **d**estined: Those who will get better whether or not they receive treatment

We can express this same idea by specifying the set of "potential outcomes" associated with each type of patient, as illustrated in Table \@ref(tab:PO).

<!-- \begin{table}[h!]
\begin{tabular}{l|cccc} \small

& Type a & Type b & Type c & Type d \\

& **a**dverse effects & **b**eneficial Effects & **c**hronic cases & **d**estined cases \\
\hline
Not treated &    Healthy &       Sick &       Sick &    Healthy \\
Treated &       Sick &    Healthy &       Sick &    Healthy \\
\end{tabular}  
\caption{Potential Outcomes: What would happen to each of four possible types of case if they were or were not treated.}
\label{tabPO}
\end{table} -->


| \small      |          Type a          |            Type b           |         Type c         |          Type d         |
|-------------|:------------------------:|:---------------------------:|:----------------------:|:-----------------------:|
|             | **a**dverse effects | **b**eneficial Effects | **c**hronic cases | **d**estined cases |
| Not treated |          Healthy         |             Sick            |          Sick          |         Healthy         |
| Treated     |           Sick           |           Healthy           |          Sick          |         Healthy         |

Table: (\#tab:PO). Potential outcomes: What would happen to each of four possible types of case if they were or were not treated.


In each column, we have simply written down the outcome that a patient of a given type would experience if they are not treated, and the outcome they would experience if they are treated. 

Throughout the book, we generalize from this toy example. Whenever we have one causal variable and one outcome, and both variables are binary (i.e., each can take on two possible values, 0 or 1), then there are only four sets of possible potential outcomes, or "causal types." More generally, for any pair of causal and outcome variables, the four types are:

* **a**: A negative causal effect of $X$ on $Y$
* **b**: A positive causal effect of $X$ on $Y$
* **c**: No causal effect, with $Y$ "stuck" at 0
* **d**: No causal effect, with $Y$ "stuck" at 1

Put in terms of a set of potential outcomes, this gives us Table \@ref(tab:POGEN):

| \small      |          Type a          |            Type b           |         Type c         |          Type d         |
|-------------|:------------------------:|:---------------------------:|:----------------------:|:-----------------------:|
|             | $Y=$ | $Y=$ | $Y=$ | $Y=$ |
| Make $X=0$  |            1             |              0              |           0            |             1           |
| Make $X=1$  |            0             |              1              |           0            |             1           |
Table: (\#tab:POGEN). Generalizing from Table \@ref(tab:PO), the table gives for each causal type the values that $Y$ would take on if $X$ is set at $0$ and if $X$ is set at 1.



<!-- Let $Y(x)$ denote the "potential" outcome (the value $Y$ would take on) when $X=x$. Then, if $X$ is a binary variable, the effect of $X$ on $Y$ is simply defined as $Y(1) -  Y(0)$.  -->

<!-- These types differ in their ''potential outcomes'' | that is on what outcomes, $Y$,  they *would* have depending on their treatment condition $X$ . More formally, we let $Y(x)$ denote a case or type's potential outcome  when $X=x$. Thus, the potential outcomes are $Y(0)=1, Y(1)=0$ for type $a$; $Y(0)=0, Y(1)=1$ for type $b$; $Y(0)=0, Y(1)=0$ for type $c$; and $Y(0)=1, Y(1)=1$ for type $d$.   -->




<!-- For any given response type, the causal effect of $X$ on $Y$ is $Y(1) -  Y(0)$. Thus, the causal effect is $-1$ for $a$ types, $1$ for $b$ types, and $0$ for both $c$ and $d$ types. -->

Returning to our democratization example, let $X=1$ represent a high level of economic equality and $X=0$ its absence, with $Y=1$ representing democratization and $Y=0$ its absence. An $a$ type, then, is any case in which a high level of equality, if it occurs, *prevents* democratization in a country that would otherwise have democratized. The causal effect of high equality in an $a$ type is $= -1$. A $b$ type is a case in which high equality, if it occurs, generates democratization in a country that would otherwise have remained non-democratic (effect $= 1$). A $c$ type is a case that will not democratize regardless of the level of equality (effect $= 0$); and a $d$ type is one that will democratize regardless of the level of equality (again, effect $= 0$).

In this setting, a causal *explanation* of a given case outcome amounts to a statement about its type. The claim that India democratized because of a high level of equality is equivalent to saying that India democratized and is $b$ type. To claim that Sierra Leone democratized because of low inequality is equivalent to saying that Sierra Leone democratized and is an $a$ type. To claim, on the other hand, that Malawi democratized for reasons having nothing to do with its level of economic equality is to characterize Malawi as a $d$ type (which alreqdy specifies its outcome).

We can also use potential-outcomes reasoning for more complex causal relations. For example, supposing there are two binary causal variables $X_1$ and $X_2$, we can specify any given case's potential outcomes for each of the different possible combinations of causal conditions---there now being four such conditions (as each causal variable may take on $0$ or $1$ when the other is at $0$ or $1$). We now have 16 causal types: 16 different response patterns that $Y$ might display to changes in $X_1$ and $X_2$. The full set is represented in Table \@ref(tab:PO16). (The type numberings are, of course, arbitrary here and included for ease of reference.) 

We can read off this table that a type 4 is a case in which $X_1$ has a positive causal effect on $Y$ but $X_2$ has no effect, whereas a type 6 is case in which $X_2$ has a positive effect but $X_2$ has none. We also capture interactions, or ways in which the effect of one variable might be conditioned by the value of the other. For instance, a type 5 is case in which $X_2$ has a postive causal effect if and only if $X_1$ is 0, and where $X_1$ has a negative causal effect only when $X_2=1$.

As one might imagine, the number of causal types increases rapidly as the number of considered causal variables increases, as it also would if we allowed $X$ or $Y$ to take on more than 2 possible values. However, the basic principle of representing possible causal relations as patterns of potential outcomes remains unchanged.

<!-- let $Y(x_1, x_2)$ denote the outcome when $X_1=x_1$ and  $X_2=x_2$. Then the quantity $\left(Y(1, 1) - Y(0, 1)\right) - \left(Y(1, 0) - Y(0, 0)\right)$ describes the interactive effect of two treatments: it captures how the effect of $X_1$ changing from $0$ to $1$ is different between those situations in which $X_2=1$ and those situations in which $X_2=0$. -->


------------------------------------------------------------------------------------
   **Type**    if $X_1=X_2=0$   if $X_1=0,X_2=1$  if $X_1=1,X_2=0$   if $X_1=X_2=1$ 
-------------  ---------------  ----------------  -----------------  ---------------
      1               0                0             0                    0      

      2               0                0             0                    1      

      3               0                0             1                    0      

      4               0                0             1                    1      

      5               0                1             0                    0      

      6               0                1             0                    1      

      7               0                1             1                    0      

      8               0                1             1                    1      

      9               1                0             0                    0      

      10              1                0             0                    1      

      11              1                0             1                    0      

      12              1                0             1                    1      

      13              1                1             0                    0      

      14              1                1             0                    1      

      15              1                1             1                    0      

      16              1                1             1                    1      
------------------------------------------------------------------------------
Table: (\#tab:PO16) With two binary causal variables, there are 16 causal types: 16 ways in which $Y$ might respond to changes in the two variables.

Readers will note that, in the counterfactual framework, causal relations are conceptualized as deterministic. A given case has a set of potential outcomes. Any randomness enters the analysis as incomplete knowledge of the factors influencing an outcome. But, in principle, if we knew all of the relevant causal conditions and the complete set of potential outcomes for a case, we could perfectly predict the actual outcome in that case. This understanding of causality---as ontologically deterministic, but empirically imperfectly understood---is compatible with views of causation commonly employed by qualitative researchers (see, e.g., @mahoney2008toward), and with understandings of causal determinism going back at least to @laplace1901philosophical. As we will see, we can readily express this kind of incompleteness of knowledge within a causal model framework; indeed, the way in which causal models manage uncertainty is central to how they allow us to pose questions of interest and to learn from evidence.

A further important, if somewhat counter-intuitive implication, of the counterfactual framework lies in how it forces us to think about multiple causes. When seeking to explain the outcome in a case, researchers sometimes proceed as though competing explanations amount to rival causes, where $X_1$ being a cause of $Y$ implies that $X_2$ was not. Did Malawi democratize because it was a relatively economically equal society *or* because of international pressure to do so? In the counterfactual model, however, causal relations are non-rival. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if *either* supporter's vote were different. Put differently, when we say that $X$ caused $Y$ in a given case, we will generally mean that $X$ was *a* cause, $X$ will rarely be *the* cause in the sense of being the *only* thing a change in which would have changed the outcome. Malawi might not have democratized if *either* a relatively high level of economic equality or international pressure had been absent. For most social phenomena that we study, there will be multiple, and sometimes a great many, difference-makers for any given case outcome.

<!-- ## Counter-intuitive implications of the counterfactual model -->

<!-- Although the counterfactual framework is now widely employed, it contains a set of implications that might sit uncomfortably with common conceptions of how causal inference operates. -->

<!-- Second, it is often intuitive to think of causal processes as sets of transitive relations: if $A$ causes $B$ and  $B$ causes $C$, then we might think that $A$ causes $C$. Yet, in the counterfactual model, causal relations are *not* transitive. In a classic illustration, imagine a boulder that rolls down a hill, causing you to duck, and that ducking in turn saves your life. As a counterfactual matter, the boulder clearly caused the ducking and the ducking your survival. But the boulder rolling down the hill did not save your life. To consider an example from the social realm, think about situations in which action begets reaction. For instance, a rebellion causes a military crackdown, and the military crackdown causes the regime to survive; yet the rebellion did not cause the regime to survive. (For discussions see @hall2004two and @paul2013causation.) The insight has implications both for process tracing and for correlational approaches to establishing causation. Finding in a causal link in a case from $A$ to $B$ and a causal link from $B$ to $C$ is not equivalent to finding that $A$ caused $C$. Likewise, identifying a population-level causal effect of $X$ on some suspected mediator $M$, and another effect of $M$ on $Y$, does not establish an $X\rightarrow Y$ causal relationship. -->

<!-- <!-- I do not find the next point convincing. --> 

<!-- Third, notions of causality going back at least to Hume [@hume2000enquiry] treat causal relations as characterized by spatio-temporal contiguity between cause and effect, or at least of the intermediate steps between them. Yet in the counterfactural model, there is no requirement that causes be temporally or spatially connected to their effects. For instance, *potentially* intervening events that did *not* occur can have causal effects, even though they make no spatio-temporal contact with the observable events that seem to lie along the path from $X$ to $Y$. The plague that put Friar John into quarantine meant that he did not deliver the letter to Romeo to inform him that Juliet was not dead, which in turn led to Romeo's death. There is a *causal* path from the plague to Romeo's death, but no *spatio-temporal* one.  -->

<!-- Fourth, hypothesis-testing at the case level sometimes proceeds as though competing explanations amount to rival causes, where $A$ caused $B$ implies that $C$ did not. But in the counterfactual model, causal relations are neither rival nor decomposable. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if *either* supporter's vote were different. The causes are not rival. Now, imagine that all three of three voters supported an outcome. Then the three votes jointly caused the outcome. However, this joint cause is not decomposable into its component parts: none of the individual votes had *any* effect on the outcome. A change in any one vote would have made no difference. -->

<!-- <!-- Does the latter example above  satisfy minimality? Is a graph minimal that points from all three yesses to the outcome? --> 

<!-- Thus, there appear to be some tensions between the counterfactual model and some common notions of causality. These tensions largely disappear, however, once we properly specify causal models as systems of causal relations. For this work, Directed Acyclic Graphs provide a powerful tool.    -->

## Causal Models and Directed Acyclic Graphs

Potential outcomes tables can capture quite a lot. We could, for instance, summarize our beliefs about the relationship between economic equality and democratization by saying that we think that the world is comprised of a mixture of $a$, $b$, $c$, and $d$ types, as defined above. We could get more specific and express a belief about what proportions of cases in the world are of each of the four types. For instance, we might believe that $a$ types and $d$ types are quite rare while $b$ and $c$ types are more common. Moreover, our belief about the proportions of $b$ (positive effect) and $a$ (negative effect) cases imply a belief about equality's *average* effect on democratization as, in a binary setup, this quantity is imply the proportion of $b$ types minus the proportion of $a$ types.

As we have seen, beliefs about even more complex causal relations can be fully expressed in potential-outcomes notation. However, as causal structures become more complex---especially, as the number of variables in a domain increases---a causal model can be a powerful organizing tool. In this section, we show how causal models and their visual counterparts, directed acyclic graphs (DAGs), can represent substantive beliefs about counterfactual causal relationships in the world. The key ideas in this section can be found in many texts (see, e.g., Halpern and Pearl (2005) and Galles and Pearl (1998)), and we introduce here a set of basic principles that readers will need to follow the argumentation in this book. 

To slightly shift the frame of our running example, suppose that we believe the level of economic inequality can have an effect on whether a country democratizes. We might believe inequality affects the likelihood of democratization by generating demands for redistribution, which in turn can cause the mobilization of lower-income citizens, which in turn can cause democratization. We might also believe that mobilization itself is not just a function of redistributive preferences but also of the degree of ethnic homogeneity, which shapes capacities of lower-income citizens for collective action.  We can visualize this model in Figure \@ref(fig:simpleDAG).

```{r simpleDAG, echo = FALSE, out.width='70%', fig.width = 7, fig.height = 7,  fig.align="center", out.width='.5\\textwidth', fig.cap = "A simple causal model in which high inequality ($I$) affects the democratization ($D$) via redistributive demands and mass mobilization ($M$), which is also a function of ethnic homogeneity ($E$). The arrows show relations of causal dependence between variables.  The graph does not capture the ranges of the variables and the functional relations between them."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 2, 1, 3, 3),
       y = c(2, 2, 2, 0, 2, 3),
       names = c(
         "I", 
         "R",
         "M",
         "E", 
         "D", 
         expression(paste(U[D])) 
         ),
       arcs = cbind( c(1, 2, 4, 3, 6),
                     c(2, 3, 3, 5, 5)),
       title = "A Model of Inequality's Effect on Democratization",
       padding = .4, contraction = .15) 

```

In the context of this example, let us now consider the three components of a causal model: variables, functions, and distributions.

**The variables.** The first component of a causal model is the set of variables across which the model characterizes causal relations. On the graph in Figure \@ref(fig:simpleDAG), the 6 included variables are represented by the 6 nodes. 

Notice that some of these variables have arrows pointing *into* them: $R, M$, and $D$ are endogenous variables, meaning that their values are determined entirely by other variables in the model.

Other variables have arrows pointing out of them but no arrows pointing into them: $I, E$ and $U_D$ are exogenous variables. Exogenous variables are those that influence other variables in the model but themselves have no causes specified in the model. While $I$ and $E$ have natural interpretations, we might wonder what $U_D$ represents as it does not feature in our substantive claims about how democratization arises. In the world of causal models, $U$ terms are typically used to capture unspecified exogenous influences. Far from being nuisance terms, $U$ variables constitute a key way in which we express uncertainty about the world and, in turn, are often the locus of learning about the questions we are asking. In the present example, we believe democratization to be potentially affected by mobilization, but we also know that democratization is affected by other things, even if we do not know what they are. We can thus think of $U_D$ as a set of unknown factors---factors other than mobilization---that affect democratization.^[Conventionally, we denote the set of exogenous variables as \(\mathcal{U}\) and the set of endogenous variables as \(\mathcal{V}\).] 

In a causal-model framework, we sometimes use familial terms to describe relations among variables. For instance, two nodes directly connected by an arrow are known as "parent" and "child," while two nodes with a child in common (both directly affect the same variable) are "spouses." We can also say that $I$ is an "ancestor" of $D$ (a node upstream from $D$'s parent) and conversely that $D$ is a descendant of $I$ (a node downstream from $I$'s child).

In identifying the variables, we also need to specify the \emph{ranges} across which they can potentially vary. We might specify, for instance, that all variables in the model are binary, taking on the values 0 or 1. We could, alternatively, define a set of categories across which a variable ranges or allow a variable to take on any real number value or any value between a set of bounds. ^[If we let \(\mathcal{R}\) denote a set of ranges for all variables in the model, we can indicate $X$'s range, for instance, by writing \(\mathcal{R}(X)=\{0,1\}\). The variables in a causal model together with their ranges---the triple \((\mathcal{U}, \mathcal{V}, \mathcal{R})\)---are sometimes called a \emph{signature}, \(\mathcal{S}\).]

<!-- Going forward, we set every variable in our working example to be binary. -->

**The functions.** Next, we need to specify our beliefs about the causal relations among the variables in our model. How is the value of one variable affected by, and how does it affect, the values of others? For each endogenous variable---each variable influenced by others in the model---we need to express beliefs about how its value is affected by its parents, its immediate causes.

The graph already represents some aspects of these beliefs: the arrows, or directed edges, tell us which variables we believe to be direct causal inputs into other variables. So, for instance, we believe that democratization ($D$) is determined jointly by mobilization ($M$) and some exogenous, unspecified factor (or set of factors), $U_D$. We can think of $U_D$ as all of the other influences on democratization, besides mobilization, that we either do not know of or have decided not to explicitly include in the model. We believe, likewise, that $M$ is determined by $I$ and an unspecified exogenous factor (or set of factors), $U_M$. And we are conceptualizing inequality ($I$) as shaped solely by a factors exogenous to the model, captured by $U_I$. (For all intents and purposes, $I$ behaves as an exogenous variable here since its value is determined solely by an exogenous variable.) 

We can also, however, express more specific beliefs about causal relations in the form of a causal function.^[The collection of all causal functions in the model can be denoted as $\mathcal{F}$.] Specifying a function means writing down whatever general or theoretical knowledge we have about the direct causal relations between variables. A function specifies how the value that one variable takes on is determined by the values that other variables---its parents---take on. 

<!-- For each endogenous variable, we can specify two kinds of variables as direct causes:  (i) other endogenous variables, which we call the variable's *parents*;^[For variable $V_i$, we write its parents as $PA_i$.] and (ii) an exogenous variable. Thus, for instance, the variable $Y$ in Figure \@ref(fig:simpleDAG) has as its direct causes the variable $R$, its parent (an endogenous variable itself) and the random-disturbance, $U_Y$.^[Any variable with no parents in $\mathcal V$ must be a function of a member of $\mathcal U$; otherwise, we could not consider it endogenous. A variable that is a function of one or more members of $\mathcal V$, however, can be modeled without a $U_i$ term if we believe that it is fully determined by variables specified in $\mathcal V$.]  -->

We can specify this relationship in a vast variety of ways. Let's consider a few examples, using various variables in our running model:

* Suppose that we believe that an outcome always occurs whenever a causal condition is present. Redistributive demands, for instance, always occur whenever inequality is present. We we can express this belief with a simple function for $R$: $R=I$. 

* We can similarly express set-theoretic relations of any kind among any number of factors. We may, for instance, believe that an outcome occurs when and only when _two_ conditions are present. Redistributive demands and ethnic homogeneity may be individually necessary and jointly sufficient conditions for mobilization. We can express this belief with a slightly more complex function: $M=E R$. According to this function, $M=1$ (mobilization occurs) if _both_ $E=1$ (ethnic homogeneity is present) and $R=1$ (redistributive preferences are present), but $M=0$ (mobilization does not occur) otherwise. Note that this formulation also builds in causal heterogeneity: here, we are saying that redistributive preferences have an effect on mobilization when and only when ethnic homogeneity is present, and vice versa. 

* We can model one variable as a linear function of another. Moreover, we can express uncertainty about the strength of this relationship. For instance, we can write $R=\beta I$, where $\beta$ is a parameter that we do not know the value of at the outset of a study (but, as we will see, may learn about from the data).

* We can, further, encode uncertainty about the role of other, unknown factors in determining a variable's value. If we believe $D$ to be linearly affected by $M$ but also subject to forces that we do not yet understand and have not yet specified in our theory, then we can write: $D=\beta*M+U_D$, where $U_D$ represents a random disturbance. 

* We may even be uncertain about whether or in what direction one variable affects another. Or we may believe that their relationship varies across cases for reasons that we do not yet understand. We can capture this type of uncertainty as an interaction such as: $D=M U_D$. Here, the value of our random-disturbance term does not merely represent noise around an $M \rightarrow D$ relationship. Rather, $U_D$ now conditions, or moderates, the strength, sign, or existence of the relationship itself. Which of these $U_D$ conditions will depend on the $U_D$'s specified range. For instance, if we allow $U_D$ to take on the values $-1, 0$ or $1$, then $U_D$ will determine whether $M$ has a negative effect, no effect, or a positive effect on $D$. If instead, $U_D$ is allowed to vary continuously between $0$ and $1$, inclusive, then it conditions the strength and existence of a positive causal effect of $M$ on $D$, though not the sign of that effect.  As we discuss in later chapters, our investigation might then center on drawing inferences about $U_D$ in order to assess $M$'s causal effect.

* We can express more fundamental uncertainty about functional form, such as whether two variables are related in a linear or exponential fashion. Consider, for instance, $D=\beta M^{U_D}$. Here, $D$ and $M$ are linearly related if $U_D=1$, but exponentially if $U_D$ is anything other than $1$.^[Here the difference between $\beta$ and $U_D$ is that $\beta$ is a parameter that we believe takes a constant value for all units, even if its value is unknown, while $U_D$ represents some unknown factor or combination of factors the value of which may vary across units, over a pre-specified range.]

* Causal functions can take fully non-parametric form. Let us, for instance, allow $U_D$ to range across the values $a, b, c$, and $d$. We can then write down the following causal function for $D$:
  * if $U_D=a$, then $D=1-M$
  * if $U_D=b$, then $D=M$
  * if $U_D=c$, then $D=0$
  * if $U_D=d$, then $D=1$
  
Readers may recognize in this last causal function an expression of our original four causal types from earlier in this chapter.^[The types here map directly into the four types, $a$, $b$, $c$, $d$, used in @humphreys2015mixing and into principal strata employed by Rubin and others. The literature on probabilistic models also refers to such strata as "canonical partitions" or "equivalence classes." Note that this model is not completely general as the multinomial distribution assumes that errors are iid.] Here, $U_D$ is an unknown factor that is conditioning the effect of mobilization on democratization, determining whether $M$ has a negative effect, a positive effect, no effect with democratization never occurring, or no effect with democratization bound to occur regardless of mobilization. 

We will make considerable use of functions similar to this last one in the pages and chapters to come. It is thus worth dwelling for a moment on what this kind of function is doing. We have started with a graph in which mobilization can have an effect on democratization and the understanding that this effect, both its existence and its sign, may vary across cases. Cases, in other words, may be of different causal types. Further, we do not know what it is that shapes $D$'s response to $M$---what makes a case one type versus another. We thus use $U_D$ as a stand-in for the unknown and unspecified moderators of $M$'s effect. We might, at this stage, wonder what the point is of including $U_D$ in the model; are we not essentially just placing a question mark on the graph? We are, and that is precisely the point. As we will see in later chapters, non-substantive, causal-type nodes can play a key role in specifying (a) what we are uncertain about in a causal network and (b) what we would like to find out. Embedding our questions about the world directly into a model of the world, in turn, allows us to answer those questions in ways systematically and transparently guided by prior knowledge.

The larger point is that functions can be written to be quite specific or extremely general, depending on the state of prior knowledge about the phenomenon under investigation. The use of a structural model *does not require precise knowledge of specific causal relations*, even of the functional forms through which two variables are related. 

<!-- In fact, we can include factors as another variable's "parents" even if we are unsure that those factors matter. Including variables on the righthand side in a functional equation allows for the possibility that those variables matter, and in turn sets us up to investigate their possible effect empirically.^[For instance, in $B=AU_B+C$, $A$ will only affect $B$ if $U_B$ takes on a non-zero value. ] -->

A few important aspects of causal functions stand out. First, unlike regression equations and other equations describing data patterns, these functions express *causal* beliefs. When we write $D=\beta M$ as a function, we do not just mean that we believe the values of $M$ and $D$ in the world to be linearly related. We mean that we believe that the value of $M$ *determines* the value of $D$ through this linear function. Functions are, in this sense, meant as *directional* statements, with causes on the righthand side and an outcome on the left.

Second, to specify functions is to unpack a potentially complex web of causal relations into its constituent causal links. For each variable, we do not need to think through entire sequences of causation that might precede it. We need only specify how we believe it to be affected by its parents---that is to say, those variables pointing directly into it. Our outcome of interest, $D$, may be a shaped by multiple, long chains of causality. To theorize how $D$ is generated, however, we write down how we believe $D$ is shaped by its immediate causes, $M$ and $U_D$. We then, separately, express a belief about how $M$ is shaped by _its_ direct causes, $R$ and $E$. A variable's function must include as inputs all, and only, those variables that point directly into that variable.^[The set of a variable's parents is required to be minimal in the sense that a variable is not included among the parents if, given the other parents, the child does not depend on it in any state that arises with positive probability.]

Third, as in the general potential-outcomes framework, all relations in a causal model are conceptualized as in principle deterministic: a variable's value is fully *determined* by the values of its parents---i.e., by the righthand side terms in its functional equation. We express uncertainty about causal relations, however, either as unknown paramaters (e.g., $\beta$, above) or as random disturbances, the $U$ terms. 

Fourth and relatedly, in a properly specified causal model *the values of the exogenous variables*---those with no arrows pointing in to them---*are sufficient to determine the values of all other variables in the model.*  Consistent with more informal usage, we refer to a given set of values for all exogenous terms in a model as a *context*. In causal model, context determines all other values. For instance, in Figure \@ref(fig:simpleDAG), knowing the values of $I$, $E$, and $U_D$ as well as the causal functions (including the values of any parameters they contain) would tell us the values of $R$, $M$, and $D$.

<!-- ]  Variables that have no parents are called *roots*.^[Thus in our usage all elements of $\mathcal{U}$ are roots, but so are variables in $\mathcal{V}$ that depend on variables in $\mathcal{U}$ only.]  We will say that $\mathcal{F}$ is a set of *ordered structural equations* if no variable is its own descendant and if no element in $\mathcal{U}$ is parent to more than one element of \(\mathcal{V}\).^[This last condition can be achieved by shifting any parent of multiple children in $\mathcal{U}$ to $\mathcal{V}$.] -->

<!-- Do we need to define roots? -->

<!-- For notational simplicity we generally write functional equations in the form $c(a,b)$ rather than $f_c(a,b)$. -->

<!-- **The distributions**. So far, we have specified the variables in the model and their causal relations, possibly with uncertainty. These relations imply  -->

<!-- In general, $U$ terms---capturing unspecified disturbances---represent features of the world that we are *not* able to directly observe. In some cases, we may not even have a specific conceptualization of the phenomena in the world to which a $U$ term corresponds. Nonetheless, we may be able to make *inferences* about the value of a $U$ term from observed data. Indeed, given the role of the exogenous terms in a model in determining the operation of the world that the model describes, learning about a case's context becomes central to model-based causal inquiry and, as we shall see, lies at the heart of the framework that we are elaborating. -->

**The distributions.**  Putting these components together gives what is termed a *structural causal model.* In a structural causal model, all endogenous variables are, either directly or by implication, functions of a case's context (the values of the set of exogenous variables).^[More formally, a **structural causal model** *over* signature $\mathcal{S}=<\mathcal{U},\mathcal{V},\mathcal{R}>$ is a pair $<\mathcal{S}, \mathcal{F}>$, where $\mathcal{F}$ is a set of ordered structural equations containing a function  $f_i$  for each element $Y\in \mathcal{V}$. We say that $\mathcal{F}$ is a set of ordered structural equations if no variable is its own descendant and if no element in $\mathcal{U}$ is parent to more than one element of \(\mathcal{V}\). This last condition can be achieved by shifting any parent of multiple children in $\mathcal{U}$ to $\mathcal{V}$. This definition thus includes an assumption of acyclicity, which is not found in all definitions in the literature.] What we have not yet inscribed into the model, however, is any beliefs about how *likely* or *common* different kinds of contexts might be. Thus, for instance, a structural causal model consistent with Figure \@ref(fig:simpleDAG) stipulates that $I$, $E$, and $U_D$ may have effects on $D$, but it says nothing in itself about the distribution of $I$, $E$, and $U_D$ themselves, beyond limitations on their ranges.^[Thus $P(d|i,e, u_D)$ would defined by this structural model (as a degenerate distribution), but $P(i)$, $P(e)$, $P(u_D)$, and $P(i,e, u_D)$ would not be.] We have not said anything, for instance, about how common high inequality is across the relevant domain of cases, how common ethnic homogeneity is, or how unspecified inputs are distributed.  

In many research situations, we will have beliefs not just about how the world works under different conditions, but also about what kinds of conditions are more likely than others. We can express these beliefs about context as probability distributions over the models exogenous terms.^[We assume that the exogenous terms, the elements of $\mathcal{U}$, are generated independently of one another. While this is not without loss of generality, it is not as constraining as it might at first appear: any graph in which two exogenous variables are not independent can be replaced by a graph in which these two terms are listed as endogenous (possibly unobserved) nodes, themselves generated by a third variable. Note also that one could envision "incomplete probabilistic causal models" in which researchers claim knowledge regarding distributions over *subsets* of $\mathcal{U}$.]  For instance, a structural causal model might support a claim of the form: "$R$ has a positive effect on $M$ if and only if $E=1$ holds." We might, then, add to this a belief that $E=1$ in 25\% of cases in the population of interest. Including this belief about context implies, in turn, that $R$ has a positive effect on $M$ a quarter of the time. As with the functions, we can also (and typically would) build uncertainty into this belief by specifying a *distribution* over possible shares of cases with ethnic homogeneity, with our degree of uncertainty captured by the distribution's variance. 

<!-- Am trying to render all the indented paragraphs below as a single, multi-paragraph footnote. Have read up and tried all sorts of things, no luck yet. -->

<!-- Thus, a probabilistic causal model is a structural causal model coupled with a probability distribution over the model's exogenous variables. A corresponding probabilistic model, however, might support a stronger claim of the form: "Condition $C$ arises with frequency $\pi^C$, and so $X$ causes $Y$ with probability $\pi^C$." -->

***

Technical Note on the Markov Property
  The assumptions that no variable is its own descendant and that the $U$ terms are generated independently make the model *Markovian*, and the parents of a given variable are Markovian parents. Knowing the set of Markovian parents allows one to write relatively simple factorizations of a joint probability distribution, exploiting the fact ("the Markov condition")  that all nodes are *conditionally independent* of their nondescendants, conditional on their parents. Variables $A$ and $B$ are "conditionally independent" given $C$ if $P(a|b,c) = P(a|c)$ for all values of $a, b$ and $c$.  
  To see how this Markovian property allows for simple factorization of $P$ for Figure \@ref(fig:simpleDAG), note that $P(X, R, Y)$ can always be written as: 
  $$P(X, R, Y) = P(X)P(R|X)P(Y|R, X)$$ 
  If we believe, as in the figure, that $X$ causes $Y$ only through $R$ then we have the slightly simpler factorization:
  $$P(X, R, Y) = P(X)P(R|X)P(Y|R)$$ 
  Or, more generally:

\begin{equation} 
P(v_1,v_2,\dots v_n) = \prod P(v_i|pa_i)
(\#eq:markov)
\end{equation} 

  The distribution $P$ on $\mathcal{U}$ induces a  joint probability distribution on $\mathcal{V}$ that captures  not just information about how likely different states are to arise but also the relations of conditional independence between variables that are implied by the underlying causal process. For example, if we thought that $X$ caused $Y$ via $R$ (and only via $R$), we would then hold that $P(Y | R) = P(Y | X, R)$: in other words if $X$ matters for $Y$ only via $R$ then, conditional on $R$, $X$ should not be informative about $Y$.   
  In this way, a probability distribution $P$ over a set of variables can be consistent with some causal models but not others. This does not, however, mean that a specific causal model can be extracted from $P$. To demonstrate with a simple example for two variables, any probability distribution on $(X,Y)$ with $P(x)\neq P(x|y)$ is consistent both with a model in which $X$ is a parent of $Y$ and with a model in which $Y$ is a parent of $X$.

***
  
Once we introduce beliefs about the distribution of values of the exogenous terms in a model, we have specified a *probabilistic causal model.* We need not say much more, for the moment, about the probabilistic components of causal models. But to foreshadow the argument to come, our prior beliefs about the likelihoods of different contexts play a central role in the framework that we present in this book. We will see how the encoding contextual knowledge---beliefs that some kinds of conditions are more common than others---forms a key foundation for causal inference. At the same time, our expressions of *uncertainty* about context represent scope for learning: it is the very things that we are, at a study's outset, uncertain about that we can update our beliefs about as we encounter evidence.


### Features of causal models

The diagram in Figure \@ref(fig:simpleDAG) is called a causal graph. Though we have already been making use of this causal graph to help us visualize elements of a causal model, we now explicitly point out a number of general features of causal graphs as we will be using them throughout this book. Causal graphs have their own distinctive "grammar," a set of rules that give them important analytical features.

A causal graph represents elements of a causal model as a set of nodes (or vertices), representing variables, connected by a collection of single-headed arrows (or directed edges).  We draw an arrow from node $A$ to node $B$ if and only if we believe that $A$ can have a direct effect on $B$. The resulting diagram is  a *directed acyclic* graph (DAG) if there are no paths along directed edges that lead from any node back to itself---i.e., if the graph contains no causal cycles. 

The drawing of causal graphs follows two rules. ^[These two conditions define a ''causal DAG'' [@hernan2006instruments]. Further, the specification that $\mathcal{F}$ is a set of ordered structural equations ensures that the graph is acyclic.] First, the *absence* of an arrow between $A$ and $B$ means that $A$ is not a direct cause of $B$.^[By "direct" we mean that the $A$ is a parent of $B$: i.e., the effect of $A$ on $B$ is not fully mediated by one or more other variables in the model.] Here lies an important asymmetry: drawing such an arrow does not mean that we know that $A$ *does* directly cause $B$; but omitting such an arrow implies that we know that $A$ does *not* directly cause $B$. We say more, in other words, with the arrows we omit than with the arrows that we include.

Returning to Figure \@ref(fig:simpleDAG), we have here expressed the belief that redistributive preferences exert no direct effect on democratization; we have done so by *not* drawing an arrow directly from $R$ to $D$. In the context of this model, saying that redistributive preferences have no direct effect on democratization is to say that any effect of redistributive preferences on democratization *must* run through mobilization; there is no other pathway through which such an effect can operate. This might be a way of encoding the knowledge that mass preferences for redistribution cannot induce autocratic elites to liberalize the regime absent collective action in pursuit of those preferences. 

The same goes for the effects of $I$ on $M$, $I$ on $D$, and $E$ on $D$: the graph in Figure \@ref(fig:simpleDAG) implies that we believe that these effects also do not operate directly, but only along the indicated, mediated paths.

Meanwhile, the existence of an arrow from $A$ to $B$ does not imply that $A$ always, or even ever, has a direct effect on $B$. Consider, for instance, the arrow running from $R$ to $M$. The existence of this arrow requires that $M$ appear somewhere in the $M$'s functional equation as a variable's functional equation must include all variables pointing directly into it. Imagine, though, that $M$'s causal function is specified as: $M = RE$. This function allows for the *possibility* that $R$ affects $M$, as it will whenever $E=1$. However, it also allows that $R$ will have no effect, as it will when $E=0$. 

This example also, incidentally, demonstrates another important consequence of context, the values of the exogenous variables: a case's context determines not just the settings on the endogenous variables, but also the causal *effects* that prevail among the variables. Under the functional equation $M=RE$, a case's ethnic-compositional context determines whether or not redistributive preferences will have an effect on mobilization. 

The second rule of drawing DAGs is that any cause common to multiple variables on the graph must itself be represented on the graph. If $A$ and $B$ on a graph are both affected by some third variable, $C$, then we must represent this common cause. Put differently, any two variables without common causes on the graph are taken to be indepedent of one another. Thus, the graph in Figure \@ref(fig:simpleDAG) implies that the values of $I$, $E$, and $U_D$ are all determined independently of one another. If in fact we believed that a country's level of inequality and its ethnic composition were both shaped by, say, its colonial heritage, then this DAG would *not* be an accurate representation of our beliefs about the world. To make it accurate, we would need to add to the graph a variable capturing that colonial heritage and include arrows running from colonial heritage to both $I$ and $E$. 

This rule ensures that the graph captures all potential correlations among variables that are implied by our beliefs. If $I$ and $E$ are in fact driven by some common cause, then this means not just that these two variables will be correlated but also that each will be correlated with any consequences of the other. For instance, a common cause of $I$ and $E$ would also imply a correlation between $R$ and $E$. $R$ and $E$ are implied to be independent in the current graph but would be implied to be correlated if a common node pointed into both $I$ and $E$.

Of particular interest in Figure \@ref(fig:simpleDAG) is the implied independence of $U_D$ from every other node. Imagine, for instance, an additional node pointing into both $I$ and $U_D$. This would represent a classic form of confounding: the assignment of cases to values on the explanatory variable would be correlated with case's potential outcomes on $D$. The omission of any such pathway is precisely equivalent to expressing the belief that $I$ is exogenous, or (as if) randomly assigned.

The flip side of this second rule is that a causal graph, to do the work it must do, does *not* need to include everything we know about a substantive domain of interest. We may know quite a lot about the causes of economic inequality, for example. But we can safely omit these other factors from the graph as long as they do not have effects on other variables in the graph (via some path that does not run through $I$). Likewise, we can choose to capture any number of unspecified factors in a $U$ term. We may be aware of a vast range of forces shaping whether countries democratize, but choose to bracket them for the purposes of an examination of the role of economic inequality. The $U_D$ achieves this bracketing and is permissble as long as none of these unspecified factors also act on the other variables included in the model (since, if they do, then we would have a common cause of two included variables and would then need to capture the implied correlation between them). 

Moreover, as a matter of convention, explicitly including $U$ terms is optional. In practice, $U$'s are often excluded from the visual representation of a model on the understanding that every variable on the graph is subject to some unaccounted-for influence and thus, implicitly, has a $U$ term pointing into it. In this book, we will generally draw the $U$ terms where they are of particular theoretical or analytical interest but will otherwise omit them. Whether we include or omit $U$ terms, we will generally treat those nodes in a graph that have no arrows pointing into them as the exogenous variables that define the context.

As should be clear, a DAG does not represent all features of a causal model. What it does record is which variables enter into the structural equation for every other variable: what can directly cause what. But the DAG contains no other information about the form of those causal relations. Thus, for instance, the DAG in Figure \@ref(fig:simpleDAG) tells us that $M$ is function of both $R$ and $E$, but it does not tell us whether that joint effect is additive ($R$ and $E$ separately increase mobilization), interactive (the effect of each depends on the value of the other), or whether either effect if linear, curvilinear or something else. 

This lack of information about functional forms often puzzles those encountering causal graphs for the first time; surely it would be convenient to visually differentiate, say, additive from conditioning effects. As one thinks about the variety of possible causal functions, it quickly becomes clear that there would be no simple visual way of capturing all possible functional relations. Moreover, as we shall now see, causal graphs are a tool designed with a particular analytic purpose in mind---a purpose to which we now turn.


### Conditional independence from DAGs

If we encode our prior knowledge using the grammar of a causal graph, we can put that knowledge to work for us in powerful ways. In particular, the rules of DAG-construction allow for an easy reading of the *conditional independencies* implied by our beliefs.

To begin thinking about conditional independence, it can be helpful to conceptualize dependencies between variables as generating *flows of information*. Let us first consider a simple relationship of dependence. Returning to Figure \@ref(fig:simpleDAG), the arrow running from $I$ to $R$, implying a direct causal dependency, means that if we expect $I$ and $R$ to be correlated. Put differently, observing the value of one of these variables also gives us information about the value of the other. If we measured redistributive preferences, the graph implies that we would also be in a better position to infer the level of inequality, and vice versa. Likewise, $I$ and $M$ are also linked in a relationship of dependence: since inequality can affect mobilization (through $R$), knowing the the level of inequality would allow us to improve our estimate of the level of mobilization and vice versa.

In contrast, consider $I$ and $E$, which are in this graph indicated as being *independent* of one another. Learning the level of inequality, according to this graph, would give us no information whatsoever about the degree of ethnic homogeneity, and vice-versa.

Moreover, sometimes what you learn depends on *what you already know.* Suppose that we already knew the level of redistributive preferences. Would we then be in a position to learn about the level of inequality by observing the level of mobilization? According to this graph we would not: since the causal link---and, hence, flow of information between $I$ and $M$---runs through $R$, and we already know $R$, there is nothing left to be learned about $I$ by also observing $M$. Anything we could have learned about inequality by observing mobilization is already captured by the level of redistributive preferences, which we have already seen. While $I$ and $M$ are dependent---one is informative about the other---if we have *not* seen $R$, they are independent of one another (uninformative about each other) if we have seen $R$. We can express this idea by saying that $I$ and $M$ are *conditionally independent given $R$*. 

We say that two variables, $A$ and $C$, are "conditionally independent" given a set of variables $\mathcal B$ if, once we have knowledge of the values in $\mathcal B$, knowledge of $A$ provides no information about $C$ and vice-versa. Taking $\mathcal B$ into account thus "breaks" any relationship that might exist unconditionally between $A$ and $C$. 

To take up another example, suppose that war is a cause of both military casualties and price inflation, as depicted in Figure \@ref(fig:warDAG). Casualties and inflation will then be (unconditionally) correlated with one another because of their shared cause. If I learn that there have been military casualties, this information will lead me to think it more likely that there is also war and, in turn, price inflation (and vice versa). However, assuming that war is their only common cause, we would say that military casualties and price inflation are *conditionally independent given war.* If we already know that there is war, then we can learn nothing further about the level of casualties (price inflation) by learning about price inflation (casualties). We can think of war, when observed, as blocking the flow of information between its two consequences; everything we would learn about inflation from casualties is already contained in the observation that there is war. Put differently, if we were just to look at cases where war is present (i.e., if we hold war constant), we should find no correlation between military casualties and price inflation; likewise, for cases in which war is absent.  


```{r warDAG, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "This graph represents a simple causal model in which war ($W$) affects both military casualties ($C$) and price inflation ($P$)."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 1),
       y = c(1, 2, 0),
       names = c(
         "W", 
         "C",
         "P"
         ),
       arcs = cbind( c(1, 1),
                     c(2, 3)),
       title = "A Model of War's Effect on Casualties and Prices",
       padding = .4, contraction = .15) 

```

Relations of conditional independence are central to the strategy of statistical control, or covariate adjustment, in correlation-based forms of causal inference, such as regression. In a regression framework, identifying the causal effect of an explanatory variable, $X$, on a dependent variable, $Y$, requires the assumption that $X$'s value is conditionally independent of $Y$'s potential outcomes (over values of $X$) given the model's covariates. To draw a causal inference from a regression coefficient, in other words, we have to believe that including the covariates in the model "breaks" any biasing correlation between the value of the causal variable and its unit-level effect.

As we will explore, however, relations of conditional independence are of more general interest in that they tell us, given a model, *when information about one feature of the world may be informative about another feature of the world, given what we already know*. By identifying the possibilities for learning, relations of conditional independence can thus guide research design.


<!-- Some possibilities are excluded by the framework, however: for example, one cannot represent uncertainty regarding whether $A$ causes $B$ or $B$ causes $A$. -->

<!-- It would be nice to make a less general statement than the above as it sounds like none of this has any relevance if we think there's reciprocal causation in the causal system of interest; and that sounds like it excludes A LOT of problems political scientists are interested in. Pearl has a bit of discussion of the fact that one can compute the effect of interventions for models with cyclic features as well. So can we put this point more narrowly? Have been looking into this a bit but not yet sure exactly how to do that. -->




<!-- The above point is something I want to get more clarity around. Also, why we are defining roots as we are. -->

<!-- In Figure \@ref(fig:simpleDAG) we show a simple DAG that represents a situation in which $X$ is a parent of $M$, and $M$ is a parent of $Y$. In this example, the three variables $U_X$, $U_M$, and $U_Y$ are all exogenous and thus elements of \(\mathcal{U}\). $X$, $M$, and $Y$ are endogenous and members of  \(\mathcal{V}\). If these three variables were binary, then there would be eight possible realizations of outcomes, i.e., of \(\mathcal{V}\). In the underlying model,  $U_X$ is an ancestor of $X$, $M$, and $Y$ which are all descendants of $U_X$. The elements of $\mathcal{U}$ are all roots, though $X$ is also  a root as it has no parent in $\mathcal{V}$.   -->

 
<!-- In addition we will usually omit variables from a graph only if they are single parents---this has the advantage of clarifying that all uncertainty is over the value of roots, and not over functional forms given roots; this is without loss of generality as parameters for functional equations can themselves be represented as roots.      -->

<!-- As a very simple example one might imagine that $A$ an $B$ are independently generated binary variables; $C$ is an indicator for whether $A$ and $B$ have the same value. Then obviously if you know $C$, then knowing $A$ tells you everything about $B$. -->

To see more systematically ow a DAG can reveal conditional independencies, it is useful spell out three pairs of features of the flow of information in causal graphs:


```{r, echo = FALSE, fig.width = 5, fig.height = 4,  fig.align="center", out.width='.9\\textwidth', fig.cap = "\\label{fig:CI} Three elementary relations of conditional independence."}
par(mfrow = c(3,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(1, 2),
                     c(2, 3)),
       title = "(a) A path of arrows pointing in the same direction.", padding = .4, contraction = .15) 
hj_dag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(2, 2),
                     c(1, 3)),
       title = "(b) A forked path.", padding = .4, contraction = .15) 
hj_dag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "(c) An inverted fork (collision).", padding = .4, contraction = .15) 

```


(1a) Information can flow unconditionally along a path of arrows pointing in the same direction. In Panel 1 of Figure \ref{fig:CI}, information flows across all three nodes. Learning about any one will tell us something about the other two. 

(1b) Learning the value of a variable along a path of arrows pointing in the same direction *blocks* flows of information across that variable. Knowing the value of $B$ in Panel 1 renders $A$ no longer informative about $C$, and vice versa: anything that $A$ might tell us about $C$ is already captured by the information about $B$.

(2a) Information can flow unconditionally across the branches of any forked path. In Panel 2 learning only $A$ can provide information about $C$ and vice-versa.

(2b) Learning the value of the variable at the forking point blocks *flows* of information across the branches of a forked path. In Panel 2, learning $A$ provides no information about $C$ if we already know the value of $B$.^[Readers may recognize this statement as the logic of adjusting for a confound that is a cause of both an explanatory variable and a dependent variable in order to achieve conditional independence.]

(3a) When two or more arrowheads collide, generating an inverted fork, there is no unconditional flow of information between the incoming sequences of arrows. In Panel 3, learning only $A$ provides no  information about $C$, and vice-versa. 

(3b) Collisions can be sites of *conditional* flows of information. In the jargon of causal graphs, $B$ in Panel 2 is a "collider" for $A$ and $C$.^[In the familial language of causal models, a collider is a child of two or more parents.] Although information does not flow unconditionally across colliding sequences, it does flow across them *conditional* on knowing the value of the collider variable or any of its downstream consequences. In Panel 2, learning $A$ *does* provide new information about $C$, and vice-versa, *if* we also know the value of $B$ (or, in principle, the value of anything that $B$ causes). 

The last point is somewhat counter-intuitive and warrants further discussion. It is easy enough to see that, for two variables that are correlated unconditionally, that correlation can be "broken" by controlling for a third variable. In the case of collision, two variables that are *not* correlated when taken by themselves *become* correlated when we condition on (i.e., learn the value of) a third variable, the collider. The reason is in fact quite straightforward once one sees it: if an outcome is a joint function of two inputs, then if we know the outcome, information about one of the inputs can provide information about the other input. For example, if I know that you have brown eyes, then learning that your mother has blue eyes makes me more confident that your father has brown eyes. 

Looking back at our democratization DAG in Figure \@ref(fig:simpleDAG), $M$ is a collider for $R$ and $E$, its two inputs. Suppose that we again have the functional equation $M=RE$. Knowing about redistributive preferences alone provides no information whatsoever about ethnic homogeneity since the two are determined independently of one another. On the other hand, imagine that you already know that there was no mobilization. Now, if you observe that there *were* redistributive preferences, you can figure out the level of ethnic homogeneity: it must be 0. (And likewise in going from homogeneity to preferences.)

Using these basic principles, conditional independencies can be read off any DAG. We do so by checking every path connecting two variables of interest and ask whether, along those paths, the flow of information is open or blocked, given any other variables whose values are already observed. Conditional independence is established when *all* paths are blocked given what we already know; otherwise, conditional independence is absent.

As an exercise, see whether you can identify the relations of conditional independence between $A$ and $D$ in Figure \ref{fig:CItest}. 

```{r, echo = FALSE, fig.width = 5, fig.height = 2,  fig.align="center", out.width='.9\\textwidth', fig.cap = "\\label{fig:CItest} An exercise: $A$ and $D$ are conditionally independent, given which other variable(s)?"}
par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = 1:4, y = c(1, 1,1,1), names = c( "A", "B", "C", "D" ),
       arcs = cbind( c(1, 3, 3),
                     c(2, 2, 4)),
       title = " ", padding = .4, contraction = .15) 

```

Are A and D independent:

* unconditionally?

Yes. $B$ is a collider, and information does not flow across a collider if the value of the collider variable or its consequences is not known. Since no information can flow between $A$ and $C$, no information can flow between $A$ and $D$ simply because any such flow would have to run through $C$.

* if you condition on $B$?

No. Conditioning on a collider opens the flow of information across the incoming paths. Now, information flows between $A$ and $C$. And since information flows between $C$ and $D$, $A$ and $D$ are now also connected by an unbroken path. While $A$ abnd $D$ were independent when we conditioned on nothing, they cease to be independent when we condition on $B$.

* if you condition on $C$?

Yes. Conditioning on $C$, in fact, has no effect on the situation. Doing so cuts off $B$ from $D$, but this is irrelevant to the $A$-$D$ relationship since the flow between $A$ and $D$ was already blocked at $B$, an unobserved collider. 

* if you condition on $B$ and $C$?

Yes. Now we are doing two, countervailing things at once. While conditioning on $B$ opens the path connecting $A$ and $D$, conditioning on $C$ closes it again, leaving $A$ and $D$ conditionally independent.

Analyzing a causal graph for relations of independence represents one payoff to formally encoding our beliefs about the world in a causal model. We are, in essence, drawing out implications of those beliefs: given what we believe about a set of direct causal relations (the arrows on the graph), what must this logically imply about other dependencies and independencies on the graph, conditional on having observed some particular set of nodes? We show in a later chapter how these implications can be deployed to guide research design, by indicating which parts of a causal system are potentially informative about other parts that may be of interest.

### A simple running example

We will illistrate these core ideas with a simple running example of a model of government corruption and survival. 

We begin with two binary features of context. Consider, first, that a country may or may not have a free press ($X$). Second, the country's government may or may not be sensitive to public opinion ($S$).^[Government sensitivity here can be thought of as government sophistication (does it take the actions of others into account when making decisions?) or as a matter of preferences (does it have a dominant strategy to engage in corruption?).] Let us then stipulate what follows from these conditions. The government will engage in corruption ($C=1$) unless it is sensitive to public opinion and there is a free press. Moreover, if and only if there is both government corruption and a free press, the press will report on the corruption ($R=1$). Finally, the government will be removed from office ($Y=1$) if it has acted corruptly and this gets reported in the press; otherwise, the government remains in office. 

As a set of equations, this simple causal model may be written as follows:

$\begin{array}{ll}
C = 1-X\times S &  \mbox{Whether the government is corrupt}\\
R = C\times X &  \mbox{Whether the press reports on corruption}\\
Y = C\times R & \mbox{Whether the government is removed from office}
\end{array}$

One thing that these equations make clear is that the variables in our model function in various places as causal-type nodes for one another. For instance, we can see from equation for $C$ that the causal effect of a free press ($X$) on corruption ($C$) depends on whether the government is sensitive to public opinion ($S$): $S$ determines $C$'s response to $X$ (as does $X$ for $S$'s effect on $C$). A similar relationship holds for $C$ and $X$ in their effect on $R$ and for $C$ and $R$ in their effect on $Y$. As we will see below, the model also implies more complex causal-type relationships. We can, further, substitute through the causal processes to write down the functional equation for the outcome in terms of the two initial causal variables: $Y=(1-S)X$.^[Equivalently, in Boolean terms, where $Y$ stands for the occurrence of government removal, $Y= \neg S \land X$; and the function for the outcome "government retained" can be written  $\neg Y = (S\land X) \lor (S\land\neg X) \lor (\neg S \land \neg X)$ or, equivalently, $\neg Y = S + \neg S \neg X$.] 

Let us, further, allow our two primary causal variables---the existence of a free press and the existence of a sensitive government---to vary probabilistically. In particular, we represent the probability of a free press with the population parameter $\lambda^X_1$ and the probability of a sensitive government with the parameter $\lambda^S_1$. To generate draws based on these  probabilities, we then introduce two random variables with uniform distributions between 0 and 1, $U_X$ and $U_S$, and posit that we get a free press, $\theta^X=\theta^X_1$ whenever $u_X < \lambda^X_1$ and a sensitive government, $\theta^S=\theta^S_1$, whenever $u_S < \lambda_1^S$. 

This gives the following equations for $X$ and $S$:

$\begin{array}{ll}
X = \mathbb{1}(u_X < \lambda^X_1) & \mbox{Whether the press is free}\\
S = \mathbb{1}(u_S < \lambda^S_1) & \mbox{Whether the government is sensitive}\\
\end{array}$


<!-- where $\pi^S$ and $\pi^X$ are parameters governing the probability of $S$ and $X$, respectively, taking on the value of 1. -->

<!-- To generate a probabilistic causal model, we also need distributions on $\mathcal U = (U_S, U_X)$. These are given by:  -->

<!-- $\begin{array}{ll} -->
<!-- u_S \sim \text{Unif}[0,1] &  \mbox{Stochastic component of government type} \\ -->
<!-- u_X \sim \text{Unif}[0,1] &  \mbox{Stochastic component of press freedom} -->
<!-- \end{array}$ -->

Note that in this model, only the most "senior" specified variables, $X$ and $S$, have a stochastic component (i.e., $\lambda^X_1$ and $\lambda^S_1$ lie between 0 and 1). All other endogenous variables are deterministic functions of other specified variables.

<!-- Does that work? The term "specified" is quite useful here -- since otherwise really every variable except the U's is a deterministic function. Do we want to just come out and formally distinguish, earlier, between specified and unspecified variables? -->





```{r running, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align="center", out.width='\\textwidth', fig.cap = "The figure shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel.", fig.align="center", warning = FALSE}


x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)

names = c("S:\nSensitive\ngovernment\n\n", "\nX:\nFree Press", "C:\n Corruption", "R:\n Media report", "Y:\nGovernment\nreplaced")

hj_dag(x =  c(x, 0, 0),
       y = c(y, 0.25, 1.75),
       names = c(names, " ", " "),
       arcs = cbind( c(1,2,2, 3, 4, 3, 6, 7),
                     c(3,3,4, 5, 5, 4, 2, 1)),
       title = "Free Press and Government Survival",
       add_functions = 0, 
       contraction = .15,
       add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS",
       padding = .2)

text(c(0,0), c(.25, 1.75), c(expression(paste(U[X])), expression(paste(U[S]))))

```


<!-- In figure above, can we not put proper indicator variable notation in the structural equations for the U's? NOT EASY -->

The corresponding causal diagram for this model is shown in Figure \@ref(fig:running). The graph explicitly includes the processes determining the two key causal variables variables (the $\lambda$ and $U$ terms). 

## Causal models from the literature

We can provide more of a sense of how one might encode prior knowledge in a causal model by asking how we might construct models in light of extant scholarly works. We undertake this exercise here for three well-known works in comparative politics and international relations: Pierson's seminal book on welfare-state retrenchment (@pierson1994dismantling); Elizabeth Saunders' research on leaders' choice of military intervention strategies (@saunders2011leaders); and Przeworski and Limongi's work on democratic survival (@przeworski1997modernization), an instructive counterpoint to Boix's (@boix2003democracy) argument about a related dependent variable. For each, we represent in the form of a causal model the causal knowledge that we might plausibly think we take away from the work in question. Readers might represent these knowledge bases differently; our present aim is merely to illustrate how causal models are constructed, rather than to defend a particular representation (much less the works in question) as accurate.


### Pierson on dismantling the welfare state

The argument in Pierson's 1994 book  *Dismantling the Welfare State* challenged prior notions of post-1980 welfare-state retrenchment in OECD countries as a process driven primarily by socioeconomic pressures (slowed growth, rising unemployment, rising deficits, aging populations) and the rise of market-conservative ideologies (embodied, e.g., the ascendance of Thatcher and Reagan). Pierson argues that socioeconomic and ideological forces put retrenchment on the policy agenda, but do not ensure its enactment because retrenchment is a politically perilous process of imposing losses on large segments of the electorate. Governments will only impose such losses if they can do so in ways that allow them avoid blame for doing so---by, for instance, making the losses hard to perceive or responsibility for them difficult to trace. These blame-avoidance opportunities are themselves conditioned by the particular social-program structures that governments inherit. 

 <!-- $C$=conservative government; $S$=socioeconomic pressures; $P$=program structure; $A$=retrenchment being on the agenda; $B$=blame-avoidance opportunities; $R$=retrenchment; $U_R$=random influences on retrenchment -->

```{r, echo = FALSE, fig.width = 10, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:DAGPierson} A graphical representation of Pierson (1994).", warning = FALSE, message = FALSE}

par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,3,1,1,2,3),
       y = c(1,1,2,3,2,2,3),
       names = c(
         expression(paste("P: Program structure")),
         expression(paste("B: Blame-avoidance\nopportunity")),  
         expression(paste("R: Retrenchment")),
         expression(paste("C: Conservative\ngovt")),
         expression(paste("S: Socioeconomic\npressures")),
         expression(paste("A: On Agenda")),
         expression(paste(U[R]: "Random influence\n on retrenchment"))),
       arcs = cbind( c(1,2, 4,5,6,7),
                     c(2,3, 6,6,3,3)),
       title = "Government Rentenchment (Pierson, 1994)",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```

While the argument has many more specific features (e.g., different program-structural factors that matter, various potential strategies of blame-avoidance), its essential components can be captured with a relatively simple causal model. We propose such a model in graphical form in Figure \ref{DAGPierson}. Here, the outcome of retrenchment ($R$) hinges on whether retrenhcment makes it onto the agenda ($A$) and on whether blame-avoidance strategies are available to governments ($B$), and on some unspecified random input ($U_R$). Retrenchment emerges on the policy agenda as a consequence of both socioeconomic developments ($S$) and the ascendance of ideologically conservative political actors ($C$). Inherited program structures ($P$), meanwhile, determine the availability of blame-avoidance strategies.

A few features of this graph warrant attention. As we have discussed, it is the omitted arrows in any causal graph that imply the strongest statements. The graph in Panel (a) implies that $C$, $S$, $P$, and $U_R$---which are neither connected along a directed path nor downstream from a common cause---are independent of one another. This implies, for instance, that whether conservatives govern is independent of whether program structures will allow for blame-free retrenchment. Thus, as Pierson argues, a Reagan or Thatcher can come to power but nonetheless run up against an opportunity structure that would makes retrenchment politically perilous. Further, in this graph any effect of program structures on retrenchment *must* run through their effects on blame-avoidance opportunities. One could imagine relaxing this restriction by, for instance, drawing an arrow from $P$ to $A$: program structures might additionally affect retrenchment by conditioning the fiscal costliness of the welfare state, thus helping to determine whether reform makes it onto the agenda.

Where two variables *are* connected by an arrow, moreover, this does not imply that a causal effect will always operate. Consider, for instance, the arrow pointing from $A$ to $R$. The fact that $A$ sometimes affects $R$ and sometimes does not is, in fact, central to Pierson's argument: conservatives and socioeconomic pressures forcing retrenchment on the agenda will *not* generate retrenchment if blame-avoidance opportunities are absent. 

The graph also reflects a choice about where to begin. We could, of course, construct a causal account of how conservatives come to power, how socioeconomic pressures arose, or why programs were originally designed as they were. Yet it is perfectly permissible for us to bracket these antecedents and start the model with $C$, $S$, and $P$, as long as we do not believe that these variables have any antecedents in common. If they do have common causes, then this correlation should be captured in the DAG.^[In DAG syntax, this correlation can be captured by placing the common cause(s) explicitly on the graph or by drawing a dashed line between the correlated nodes, leaving the source of the correlation unspecified.]

The DAG itself tells us about the possible direct causal dependencies but is silent on the ranges of and functional relations among the variables. How might we express these? With three endogenous variables, we need three functions indicating how their values are determined. Moreover, every variable pointing directly into another variable must be part of that second variable's function. Let us assume all variables are binary, with each condition either absent or present. We can capture quite a lot of Pierson's theoretical logic with the following quite simple functional equations:

* $A=CS$, implying that retrenchment makes it on the agenda if and only if both conservatives are in power and socioeconomic pressures are high.
* $B=P$, implying that blame-avoidance opporunities arise when and only when program structures take a particular form
* $R=ABU_R$. 

This last functional equation requires a little bit of explanation. Here we are saying that retrenchment will only occur if retrenchment is on the agenda and blame-avoidance opportunities are present (as the expression zeroes out if either of these are 0). Yet even if both are present, the effect on retrenchment also hinges on the value of $U_R$. $U_R$ thus behaves as a causal-type variable with respect to the effect of an $AB$ combination on $R$ and allows for two possible types. When $U_R=1$, the $AB$ combination has a positive causal effect on retrenchment. When $U_R=0$, $AB$ has no causal effect: retrenchment will not occur regardless of the presence of $AB$. A helpful way to conceptualize what $U_R$ is doing is that is capturing a collection of features of a case's context that might render the case susceptible or not susceptible to an $AB$ causal effect. For instance, Pierson's analysis suggests that a polity's institutional structure might widely diffuse veto power such that stakeholders can block reform even when retrenchment is on the agenda and could be pursued without electoral losses. We could think of such a case as having a $U_R$ value of 0, implying that $AB$ has no causal effect. A $U_R=1$ case, with a positive effect, would be one in which the government has the institutional capacity to enact reforms that it has the political will to pursue.

### Saunders (2011) 

@saunders2011leaders asks why, when intervening militarily abroad, do leaders sometimes seek to transform the *domestic* political institutions of the states they target but sometimes seek only to shape the states' external behaviors. Saunders' central explanatory variable is the nature of leaders' causal beliefs about security threats. When leaders are "internally focused," they believe that threats in the international arena derive from the internal characteristics of other states. Leaders who are "externally focused," by contrast, understand threats as emerging strictly from other states' foreign and security policies. These basic worldviews, in turn, affect the cost-benefit calculations they make about intervention strategies, via two mechanisms. Most simply, these beliefs affect perceptions of the likely security gains from a transformative intervention strategy. In addition, these beliefs affect the kinds of strategic capabilities in which leaders invest, which in turn effects the costliness and likelihood of success of alternative intervention strategies. Calculations about the relative costs and benefits of different strategies then shape the choice between a transformative and non-transformative approach to intervention. Yet leaders can, of course, only choose one of these options if they decide to intervene at all. The decision about whether to intervene depends, in turn, on at least two kinds of considerations. A leader is more likely to intervene against a given target when the nature of the dispute makes the leader's preferred strategy---given their causal beliefs---appear feasible in this situation; yet leaders may also be pushed to intervene by international or domestic audiences.

Figure \ref{fig:DAGSaunders} depicts the causal dependencies in Saunders' argument in DAG form. Working from left to right, we see that causal beliefs ($C$) affect the expected net relative benefits of the two strategies ($B$) both via a direct pathway and via an indirect pathway running through preparedness investments ($P$). Characteristics of a given target state or dispute ($T$) likewise influence $B$. The decision about whether to intervene ($I$) is then a function of three factors: causal beliefs ($C$), the expected relative net benefits of the strategies ($B$), and audience pressures ($A$). Finally, the choice of strategy ($S$) is a function of whether or not intervention occurs at all ($I$), cost-benefit comparisons between the two strategies ($B$), and other, idiosyncratic factors that may operate in various cases ($U_S$).

```{r, echo = FALSE, fig.width = 10, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:DAGSaunders} A graphical representation of Saunders' (2011) argument.", warning = FALSE, message = FALSE}

par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2,1, 3, 3,3,3.5),
       y = c(1,0,2,2, 2, 1,0, .5),
       names = c(
         expression(paste("C: Causal beliefs")),
         expression(paste("P: Preparedness\ninvestment")),  
         expression(paste("B: Benefits expected\nfrom transfers")),  
         expression(paste("T: Target\ncharacteristics")),
         expression(paste("A: Audience pressures")),
         expression(paste("I: Intervention")),
         expression(paste("S: Intervention strategy")),
         expression(paste(U[S]: "Random influence\non strategy")) 
         ),
       arcs = cbind( c(1,1, 2,1, 4, 3, 3,5,6,8),
                     c(2,3, 3,6, 3, 6, 7,6,7,7)),
       title = "(b) Military intervention strategies (Saunders, 2011)",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```

This relatively complex DAG illustrates how readily DAGs can depict the multiple pathways through which a given variable might affect another variable, as with the multiple pathways linking $C$ to $I$ and $B$ (and, thus, all of its causes) to $S$. In fact, this graphical representation of the dependencies in some ways throws the multiplicity of pathways into even sharper relief than does a narrative exposition of the argument. For instance, Saunders draws explicit attention to how causal beliefs operate on expected net benefits via both a direct and indirect pathway, both of which are parts of an indirect pathway from $C$ to the outcomes of interest, $I$ and $S$. What is a bit easier to miss without formalization is that $C$ also acts *directly* on the choice to intervene as part of the feasibility logic: when leaders assess whether their generally preferred strategy would be feasible if deployed against a particular target, the generally preferred strategy is itself a product of their causal beliefs. The DAG also makes helpfully explicit that the two main outcomes of interest---the choice about whether to intervene and the choice about how---are not just shaped by some of the same causes but are themselves causally linked, with the latter depending on the former.

Omitted links are also notable. For instance, the lack of an arrow between $T$ and $A$ suggests that features of the target that affect feasibility have no effect on audience pressures. If instead we believed, for instance, that audiences take feasibility into account in demanding intervention, we would want to include a $T \rightarrow A$ arrow.

Turning to variable ranges and functional equations, it is not hard to see how one might readily capture Saunders' logic in a fairly straightforward set-theoretic manner. All variables except $S$ could be treated as binary with, for instance, $C=1$ representing internally focused causal beliefs, $P=1$ representing preparedness investments in transformation, $B=1$ representing expectations that transformation will be more net beneficial than non-transformation, $T=1$ meaning that a target has characteristics that make transformation a feasible strategy, and so on. Although there are two strategies, we in fact need three values for $S$ because it must be defined for all values of the other variables---i.e., it must take on a distinct categorical value if there is no intervention at all. We could then define functions, such as:

* $B=CPT$, implying that transformation will only be perceived to be net beneficial in a case if and only if the leader has internally focused causal beliefs, the government is prepared for a transformative strategy, and the target has characteristics that make transformation feasible
* $I=(1-|B-C|)+(1-(1-|B-C|))A$, implying that intervention can occur under (and only under) either of two alternative sets of conditions: if the generally preferred strategy and the more net-beneficial strategy in a given case are the same (i.e., such that $B-C=0$) or, when this alignment is absent (i.e., such that $|B-C|=0$), where audiences pressure a leader to intervene.

### Przeworski and Limongi (1997)

Przeworski and Limongi (@przeworski1997modernization) argue that democratization occurs for reasons that are, with respect to socioeconomic or macro-structural conditions, largely idiosyncratic; but once a country has democratized, a higher level of economic development makes democracy more likely to survive. Economic development thus affects whether or not a country is a democracy, but only after a democratic transition has occurred, not before. Thus, unlike in @boix2003democracy, democratization in Przeworski and Limongi's argument is exogenous, rather than being determined by other variables in the model. Moreover, the dynamic component of Przeworski and Limongi's argument---the fact that both the presence of democracy and the causal effect of development on democracy depend on whether a democratic transition occurred at a previous point in time---forces us to think about how to capture over-time processes in a causal model. 

We represent Przeworski and Limongi's argument in the DAG in Figure \@ref(fig:DAGPL). The first thing to note is that we can capture dynamics by considering democracy at different points in time as separate nodes. According to the graph, whether a country is a democracy in a given period ($D_t$) is a function, jointly, of whether it was a democracy in the previous period ($D_{t-1}$) and of the level of per capita GDP in the current period, as well as of other unspecified forces ($U_{D_t}$) that lie outside the model.

```{r DAGPL, echo = FALSE, fig.width = 10, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "A graphical representation of Przeworski and Limongi's argument, where $D_{t-1}$=democracy in the previous period; $GDP_t$=per capita GDP in the current period; $D_t$=democracy in the current period.", warning = FALSE, message = FALSE}

par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,1,2,2),
       y = c(2,0,1,2),
       names = c(
         expression(paste(D[t-1]: "Democracy,\nlast period")),
         expression(paste(GDP[t]: "GDP per capita,\nthis period")),  
         expression(paste(D[t]: "Democracy,\nthis period")),  
         expression(paste(U[D[t]]: "Random influence\non democracy,\nthis period"))
         ),
       arcs = cbind( c(1,2, 4),
                     c(3,3, 3)),
       title = "(c) Democratization (Przeworski and Limongi, 1997)",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```

Second, the arrow running from $GDP_{t-1}$ to $D_t$ means that $GDP$ *may* affect democracy, not that it always does. Indeed, Przeworski and Limongi's argument is that development's effect depends on a regime's prior state: GDP matters for whether democracies continue to be democracies, but not for whether autocracies go on to become democracies. The *lack* of an arrow between $D_{t-1}$ and $GDP_{t-1}$, however, implies a (possibly incorrect) belief that democracy and $GDP$ in the last period are independent of one another.

Finally, we might consider the kind of causal function that could capture Przeworski and Limongi's causal logic. In this function, $GDP$ should reduce the likelihood of a transition *away* from democracy but not affect the probability of a transition *to* democracy, which should be exogenously determnined. One possible translation of the argument into functional terms is:


$$d_t = 1 (p(1-d_{t-1}) + d_{t-1}(1-q(1-gdp)) > u_{D_t})$$


where

* $d_t$ and $d_{t-1}$ are binary, representing current and last-period democracy, respectively
* $p$ is a parameter, varying from 0 to 1, representing the probability that an autocracy democratizes
* $q$ is a parameter, varying from 0 to 1, representing the probability that a democracy with a GDP of 0 reverts to autocracy
* $gdp$ represents national per capita GDP, normalized on a 0 to 1 scale for the population of interest.
* $u_{D_t}$ represents a random, additional input into democracy with a uniform distribution on the 0 to 1 scale
* the indicator function, ${1}$, evaluates the inequality and generates a value of $1$ if and only if it is true

Unpacking the equation, the likelihood that a country is a democracy in a given period rises and falls with the expression to the left of the $>$-operator. This expression itself has two parts, reflecting the difference between the determinants of *transitions to* democracy (captured by the first part) and the determinants of democratic *survival* (captured by the second). The first part comes into play---i.e., is non-zero---only for non-democracies. For non-democracies, the expression evaluates simply to $p$, the exogenous probability of democratization. The second part is non-zero only for democracies, where it evaluates to $1-q$---the inverse of the reversion parameter---times $1-gdp$: thus, the reversion probability falls as national income rises. The inequality is then evaluated by "asking" whether the expression on the left (either $p$ or $(1-q)gdp$) is greater than a number ($u_{D_t}$) randomly drawn from a uniform distribution between 0 and 1. Thus, higher values for the expression increase the likelihood of democracy while the randomness of the $u_{D_t}$ threshold captures the role of other, idiosyncratic inputs.

Note how, while the functional equation nails down certain features of the process, it leaves others up for grabs. In particular, the parameters $p$ and $q$ are assumed to be constant for all autocracies and for all democracies, respectively, but their values are left unspecified. And one could readily write down a function that left even more openness---by, for instance, including an unknown parameter that translates $GDP$ into a change in the probability of reversion or allowing for non-linearities, with unknown parameters, in this effect.


## Steps for constructing causal models

We close with two summaries for how you construct a causal model. First, in Box REF, we summarize the general steps. Second, in Box REF we describe how you can do this in code to produce an object that you can later use for analysis.

### Abstract procedure

***

Box: **Steps for constructing causal models**

1. Identify a set of variables in a domain of interest

  * You should specify the range of each variable: is it continuous or discrete?
  * May include $U$ terms representing unspecified, random influences

2. Draw a causal graph (DAG) representing beliefs about causal dependencies among these variables

  * Capture direct effects only
  * Arrows indicate *possible*, not constant or certain, causal effects
  * The absence of an arrow between two variables indicates a belief of *no* direct causal relationship between them
  * Ensure that the graph captures all correlations among variables. This means that either (a) any common cause of two or more variables is included on the graph (with implications for Step 1) or (b) correlated variables are connected with a dashed, undirected edge.
  
3. Write down one causal function for each endogenous variable

  * Each variable's function must include all variables directly pointing into it on the graph
  * Functions may take any form, as long as each set of possible causal values maps onto a single outcome value
  * Functions may express arbitrary amounts of uncertainty about causal relations
  
4. State probabilistic beliefs about the distributions of the exogenous variables

  * How common or likely to do we think different values of the exogenous variables are?
  * Are they independently distributed? If in step 2 you drew an undirected edge between nodes then you believe that the connected variables are not independently distributed.

***


### Model construction in code

Our `gbiqq` package provides a set of functions to implement all of these steps concisely for *binary* models -- models in which all variables are dichotomous.

```{r, message = FALSE}

# Steps 1 and 2 
# We define a model with three binary variables and specified edges between them:
model <- make_model("X -> M -> Y")

# Step 3
# Unrestricted functional forms are allowed by default, though these can 
# also be reduced. Here we impost monotonicity be removing one type for 
# M and one for Y
model <- set_restrictions(model, 
                          node_restrict = list(M = "10", Y="10"))

# Step 4
# We set priors over the distribution of (remaining) causal types.
# Here we set "jeffreys priors"
model <- set_priors(model, prior_distribution = "jeffreys")

# We now have a model defined as an R object. 
# Later we will ask questions of this model and update it using data.

```

These steps are enough to fully describe a binary causal model. Later in this book we will see how we can ask questions of a model like this but also how to use data to train it. 

<!-- If we want to know whether two variables, $A$ and $C$, are conditionally independent given some set of variables, $\mathcal B$, we need to find out whether any paths between $A$ and $C$ are "active" given $\mathcal B$. Put slightly differently, the question is whether variables in $\mathcal{B}$ block information flows from $A$ to $C$---or rather allow, or even create, such flows. This can be assessed as follows. For each possible path between $A$ and $C$, we check whether there are three connected variables $X, Y, Z$^[Where $X$ and/or $Y$ may be $A$ and/or $B$.] on that path that either: -->

<!-- (a) form a "chain" $X\rightarrow Y \rightarrow Z$ (going either direction) or "fork" $X\leftarrow Y\rightarrow Z$, with $Y \subset \mathcal{B}$,^[For instance, under this criterion, if we have $A\rightarrow W\rightarrow Y \rightarrow B$, and $Y$ is an element of the set $\mathcal{B}$, then this path is not active given $\mathcal{B}$. Similarly, if we have $A \leftarrow X\leftarrow Y\rightarrow Z \rightarrow B$, and $Y$ is in $\mathcal{B}$, then the path is not active given $\mathcal{B}$.] or  -->

<!-- (b) form an "inverted fork" $X \rightarrow Y \leftarrow Z$, with neither $Y$ nor its descendants in $\mathcal C$.^[For instance, if we have $A \rightarrow W \rightarrow Y \leftarrow Z \leftarrow B$, and $Y$ is *not* in $\mathcal{C}$, then the path is not active given $\mathcal{C}$. In other words, "inverted fork" paths are not unconditionally active. Knowing only $A$, in this setup, tells us nothing about $B$, and vice-versa.]  -->

<!-- If either of these conditions holds, then the path is blocked (not active) given $\mathcal C$. In the first case, any possible information flows along the path are *blocked* by a variable in $\mathcal C$. In the second case, *no* variable in $\mathcal C$ is *creating* an information flow that would not otherwise be present. If there are no active paths, then $A$ and $B$ are conditionally independent given $\mathcal C$. In the graph-analytic language of Pearl and others, $A$ and $B$ are said to be "$d$-separated" by $\mathcal{C}$.^[There are multiple techniques for establishing $d-$separation. Pearl's guide "$d-$separation without tears" appears in an appendix to @pearl2009causality.]   -->

<!-- Thus, in Figure \@ref(fig:simpleDAG), we can readily see that $X$ and $Y$ are conditionally independent given $R$: $X$ and $Y$ are *d-*separated by $R$.  Conversely, $R$ and $U_Y$ are unconditionally independent. However, conditioning on $Y$ $d-$*connects* $R$ and $U_Y$, generating a dependency between them. If and only if we know the outcome, $Y$, then learning $R$ yields information about $U_Y$. To foreshadow a point that we develop further later in the book, this analysis reveals how we can, and cannot, learn empirically about elements of our models. For instance, if $U_Y$ is a variable of interest but not directly observable, then information on $R$ is unhelpful if we have not observed the outcome, $Y$, but *is* informative if we have. -->


<!-- ###Interventions in causal graphs -->

<!-- A second advantage of causal graphs is that they provide a useful structure for thinking through causal effects. In a causal-model framework, we think of a variable's causal effect as being the effect of an imagined *intervention*: a manipulation of some variable, $X$, that provides $X$ with a value that is *not* determined by its parents.  In an intervention, it is as if the causal function for $X$ is replaced by the function $X=x$, with $x$ being the constant value to which we have set $X$. If we take $X$ to be binary, to keep things simple, then the causal effect of $X$ on $Y$ is the difference between the value $Y$ would take on under the intervention $X=0$ (which can be written as $do (X)=0$) and the value $Y$ would take on under the intervention $X=1$ ($do(X)=1$).  -->

<!-- As this manipulative account of causation makes clear, analyzing the effect of $X$ requires us we to set aside the "natural causes" of $X$ itself in a particular way. We can readily see how this works graphically by considering a modified version of our free press/government survival model as displayed in Figure \ref{fig:DAGdirect}. As compared with Figure \ref {fig:simpleDAG}, this DAG represents a model in which $X$ has effects on $Y$ both indirectly through $R$ and directly. We might imagine, for instance, that part of the effect of a free press ($X$) on government turnover ($Y$) runs via media reports of official corruption ($R$) while part of the effect runs through a deterrent effect of a free press that reduces graft and thus leaves greater public resources for investment in public goods (neither of which mediating variables are represented on the graph). -->

<!-- Suppose that we want to estimate the effect of an increase in media reports of corruption, $R$, on government survival, $Y$. We thus want to know the difference between the value that $Y$ would take on if the number of media reports were set at a  high level and the value $Y$ would take on if media reports were set to low. As should be clear from the graph, we cannot estimate this difference by comparing the observed value of $Y$ when media report levels are high to the observed value of $Y$ when media report levels are low. The reason is that $R$ has an ancestor, $X$, that affects $Y$ via a path that does not run through $R$. Thus, the value that $Y$ takes on when $R$ is observed to be high (or low) will be determined not just by $R$ but also by $X$, which is itself systematically correlated with $R$ by being its ancestor. The difference between $Y$'s values at two values of $R$ will thus itself be a combination of $R$'s effect and of $X$'s effect. -->

<!-- Instead, the query, "What is the effect of $R$ on $Y$?", must be conceived of in a way that separates out the effect of $X$ on $R$. We can represent the empirical conditions that we would need to estimate this effect in Figure \ref{fig:DAGdirectmut}. We have "mutilated" the original DAG by removing all arrows pointing into our causal variable, $R$ (here, simply the arrow running from $X$ to $R$). This graph represents a world in which $R$ has been manipulated, rather than naturally caused. (Equivalently, it is a world in which $R$ is purely exogenous.) And what the mutilated graph tells us is that, in an empirical situation in which the  dependency of $R$ on $X$ could be removed, $R$'s causal effect on $Y$ *could* be estimated by comparing $Y$'s values at high and low levels of $R$.^[In the formulation used in  @pearl2009causality, an intervention involving an endogenous variable $V_i$ can be written as $do(V_i)=v_i'$, or for notational simplicity $\hat{v}_i'$ (meaning $V_i$ is forced to take the particular value $v_i'$). The resulting distribution can be written as a modified version of Equation \@ref(eq:markov): -->

<!-- \begin{equation}  -->
<!-- P(v_1,v_2,\dots v_n|\hat{v}_i) = \prod_{-i}P(v_j|pa_j)\mathbb{1}(v_i = v_i')(\#eq:eqdo) -->
<!-- \end{equation} -->

<!-- where $-i$ indicates that the product is formed over all variables $V_j$ other than $V_i$, and the indicator function ensures that probability mass is only placed on vectors (or worlds) with $v_i = v_i'$. This new distribution has a graphical interpretation, representing the probability distribution over a graph in which all arrows into $V_i$ are removed.  The difference between Equation \@ref(eq:markov) and \@ref(eq:eqdo) is the difference between an *observed* probability distribution and the effect of an *intervention*. The key differences is that, when we intervene to manipulate a variable, we break the link between the variable and its "natural" causes.] -->

<!-- It is a separate question how such an empirical situation might be generated. But it is not hard to see from Figure \ref{fig:DAGdirect} that the  dependency of $R$ on $X$ could be removed either via (i) directly manipulating $R$ in a manner orthogonal to $X$ or (ii) controlling for $X$, since any variation in $R$ conditional on $X$ would itself be independent of $X$. Equivalently, using graph-analytic logic, we can also see that conditioning on $X$ blocks the path between $R$ and $Y$ that generates the confound (called a "backdoor"), thus expunging the confounding effect from the observed correlation between $R$ and $Y$. -->


<!-- ```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:DAGdirect} As compared with Figure \ref {fig:simpleDAG}, this DAG represents a model in which $X$ has effects on $Y$ both indirectly through $R$ and directly. We might imagine, for instance, that part of the effect of a free press ($X$) on government turnover ($Y$) runs via media reports of offical corruption ($R$) while part of the effect runs through a deterrent effect of a free press that reduces graft and thus leaves greater public resources for investment in public goods (neither of which mediating variables are represented on the graph)."} -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(0, 1, 2), -->
<!--        y = c(2, 3, 2), -->
<!--        names = c("X", "R", "Y"), -->
<!--        arcs = cbind( c(1, 2, 1), -->
<!--                      c(2, 3, 3)), -->
<!--        title = "A DAG with Indirect and Direct Effects", -->
<!--        padding = .4, contraction = .15)  -->

<!-- ``` -->


<!-- ```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:DAGdirectmut} This DAG represents a 'mutilated' version of the previous graph in which the causes of $R$ have been removed, and thus captures the empirical relations that would be required to hold to estimate the effect of $R$ on $Y$."} -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(0, 1, 2), -->
<!--        y = c(2, 3, 2), -->
<!--        names = c("X", "R", "Y"), -->
<!--        arcs = cbind( c(2, 1), -->
<!--                      c(3, 3)), -->
<!--        title = "A 'Mutilated' DAG for Estimating the Effect of R on Y", -->
<!--        padding = .4, contraction = .15)  -->

<!-- ``` -->





<!-- Suppose, for instance, that democracy ($D$) causes higher levels of private investment ($I$) and greater public-goods provision ($P$), and that both of these cause faster economic growth ($G$). If we want to know the joint distribution of public-goods provision and democracy, we would use Equation \ref{eqmarkov}, conditioning on the values of the parents of these two variables. If, however, we want to know what level of growth would be produced by an increase public-goods provision---a causal question---we have to ask  -->



<!-- ```{r, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align="center", out.width='\\textwidth', fig.cap = "\\label{fig:intervention} The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\\pi$ value.", fig.align="center", warning = FALSE} -->

<!-- hj_dag(x = c(1, 2, 2, 3), -->
<!--        y = c(1, 2, 0, 1), -->
<!--        names = c("D","I", "P", "G"), -->
<!--        arcs = cbind( c(1, 1, 3, 2), -->
<!--                      c(2, 3, 4, 4)), -->
<!--        add_functions = 0, -->
<!--        contraction = .2 -->
<!--        ) -->
<!-- title("A causal model") -->
<!-- ``` -->



<!-- Not completely following the logic this next paragraph or sure if it is helpful here -->

<!-- So far, although not completely general, the focus on causal DAGs is consistent with most approaches used in qualitative work on process tracing, in qualitative case analysis, and in econometric approaches. Some of these approaches commonly assume simple functional forms but these impositions are not implied by the general approach. For example econometric models often impose linear assumptions---for example in work on linear structural equations. Qualitative case analysis often assume all units are binary and that outcomes are deterministic. Under some representations the latter assumption implies conditional independencies that cannot be read from the graph, and thus violate stability conditions commonly assumed of the probability distributions that graphs are meant to represent (though it is still always that case that one can tell from the graph whenever two sets of variables are not conditionally independent given some other set). In our running example described below we give an example of such deterministic relations. -->







<!--chapter:end:02-causal-models.Rmd-->

# Theories as causal models {#theory}

```{r, include = FALSE}
source("_packages_used.R")
```

***

We introduce the idea of thinking of (applied) theoretical claims as claims within hieracrchies of causal models. Lower level models serve as a theory for a higher level model if the higher level model can be deduced from the lower level model. The empirical content of a lower level model is the possible reduction in variance of the higher level model that it can provide. 

***


Theory plays an important role in this book's use of causal models for causal inference. Yet the term "theory" in the empirical social sciences means very different things in different contexts.  In this book, we will refer to a theory is an *explanation* of a phenomeon: a theory provides an account of how or under what conditions a set of causal relationships operate. Moreover, we can express both a theory and the claims being theorized as causal models. A theory, then, is a model that explains and implies another model---possibly with the help of some data. 

We discuss toward the end of the chapter how this definition of theory relates to common understandings of theory in the social sciences. First, however, we focus on unpacking our working definition. In embedding theorization within the world of causal models, we ultimately have an empirical objective in mind. Theorizing a causal relationship of interest, in our framework, means elaborating our causal beliefs about the world in greater detail. As we show in later chapters, theorizing in the form of a causal model allows us to generate research designs: to identify sources of inferential leverage and to explicitly and systematically link observations of components of a causal system to the causal questions we seek to answer. 


## Two theories of inequality's effects on democratization {#inequalitytheory}



To see how this works, let us return to our democratization example and consider first the very basic claim that inequality can have an affect on democratization. We represent this simple claim in Figure \@ref(fig:demtheory5), Panel (a). In this simple model, $I$ may sometimes have an effect of $D$, and sometimes not; and that effect may be positive or negative. All of this will depend on the case's causal type. In Chapter \@ref(models), we considered the idea that causal type may be conceptualized as a random disturbance, a $U$ term, that can take on different type values (e.g., $a, b, c, d$) and that points into an outcome variable. To fix this idea going forward, we make a shift in notation and use $\theta$ to indicate that a node is a receptacle for causal types. Thus, $\theta_D$ here captures the case's causal type, or $I$'s causal effect on $D$ for a given case. 

```{r demtheory5, echo = FALSE, fig.width = 6, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a)."}

par(mfrow = c(3,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2),
       y = c(1,1,2),
       names = c(
         expression(paste(I)),
         expression(paste("D")),  
         expression(paste(theta[D]))),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "(a) A Claim: Inequality Causes Democratization",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2,2, 1.5, 1.5),
       y = c(1,1,2, 1  , 2),
       names = c(
         expression(paste(I)),
         expression(paste("D")),  
         expression(paste(theta[D])),
         expression(paste(M)),
         expression(paste(theta[M])) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(4, 2, 4, 2)),
       title = "(b) A Theory: How Inequality Causes Democratization",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2, 2, 1.5),
       y = c(1,1, 2, 2),
       names = c(
         expression(paste(I)),
         expression(paste("D")),  
         expression(paste(theta[D])),
         expression(paste(E)) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(2, 2, 4, 2)),
       title = "(c) Another theory: When Inequality Causes Democratization",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

```


We might then wonder *how* inequality might exert its effect on democratization. One possible answer, drawing on our model in Chapter \@ref{models} is that inequality can affect mass-mobilization, which in turn can affect democratization. This explanatory claim is visually represented in Panel (b) of the figure. Here, we can see that any effect of $I$ on $D$ runs through $M$. There are details of this graph that we will delve into later. But for now, it is sufficient to see that we have partly explained variation left unexplained by model (a). Model (b) allows us to say, for instance, that whether inequality has an effect on democratization has to depend on whether inequality has an effect on mobilization. Model (b) thus theorizes, in one important sense, a part of inequality's effect that is left untheorized in model (a). 

Alternatively, we might wonder *when* inequality causes democratization. Our simple claim, in panel (a), allows that $I$ *can* cause $D$, but provides no information about the conditions under which it does so. Those conditions are implicitly embedded within $\theta^D$, where they are left unspecified. We could, however, theorize some of what is left unsaid in in panel (a). We do this in panel (c), where we posit ethnic homogeneity ($E$) as a moderator of the inequality's effect on democratization. Panel (c) represents a theory of panel (a) in that it can help account for variation in causal effects that is unaccounted for by the model in (a).


### Theory as causal functions

Let's delve a bit deeper into how this works. Thinking through causal functions can help. Consider first a function that might correspond to panel (a). If $\theta^D$ serves as a simple receptacle for our four basic causal types, then we can deploy our four-causal-type function, familiar from Chapter \@ref{models}: 

  * if $\theta^D=a$, then $D=1-I$ ($I$ has a negative causal effect on $D$)
  * if $\theta^D=b$, then $D=I$ ($I$ has a positive causal effect on $D$)
  * if $\theta^D=c$, then $D=0$ ($I$ has no causal effect)
  * if $\theta^D=d$, then $D=1$ ($I$ has no causal effect)

This model thus allows for $I$'s causal effect to vary in ways that are left entirely unaccounted for. 

For the moderation model, in panel (c), consider a logic whereby ethnic homogeneity eases mobilization and thus democratization. Drawing on @boix2003democracy, we theorize that inequality can have a negative effect on democratization by giving the elite more to lose from majority rule, making autocrats less willing to hand over power. Inequality's positive effect, we might further posit, derives from the fact that it gives the poor more to gain from the redistribution that democratization would enable (@acemoglu2005economic). However, this positive effect can only unfold to the extent that the masses are able to mobilize, and the capacity to mobilize will hinge on ethnic homogeneity. Ethnic homogeneity thus defines the causal possibilities in regard to $I$'s effect on $D$. First, homogeneity is necessary for a positive effect of inequality. Second, by enabling mobilization around distributional demands, ethnic homogeneity rules out a net negative effect of inequality (as inequality's mobilizing effects will balance out elite fears of expropriation). Third, by making mass mobilization easier in general, ethnic homogeneity makes possible mobilization and democratization *without* inequality. Under ethnic heterogeneity, on the other hand, inequality can have a negative effect, or it can no effect at all with autocracy entrenched. 

Put differently, under ethnic homogeneity, inequality's effect can only correspond to a $b$ type or a $d$ type, while under heterogeneity the effect can only be of type $a$ or type $c$. $E$ thus allows us to partition the range of causal possibilities that model (a) had lumped together under $\theta^D$. Now we can capture this logic with a functional equation in which $\theta^D$ now takes on just two possible values (0 or 1), rather than four:

\begin{equation}
D=IE(1-\theta^D)+\theta^DE+(1-E)(1-I)\theta^D
\end{equation}

We can work through the arithmetic to observe $E$'s causal-partitioning effect. Whether $E$ is 0 or 1 determines whether we are in a world of posiitve effects ($b$ types) and democratization regardless ($d$ types), or a world of negative effects ($a$ types) and autocracy regardless ($c$ types). Note that the righthand side is a sum of three expressions. We can think of $E$ as a "switch" that turns these expressions "on" or "off." When $E=1$, the third expression goes to 0, leaving only the first two in play "on." Now, $\theta^D$ determines whether $I$ has a positive effect (when $\theta^D=0$) or no effect with $D$ fixed at $1$ (when $\theta^D=1$). Conversely, when $E=0$, the first two expressions both go to $0$, and $\theta^D$ determines whether $I$ will have a negative effect (when $\theta^D=1$) or no effect with $D$ stuck at 0 (when $\theta^D=0$).

Model (c) has thus given substantive meaning to an aspect of the phenomenon that was merely residual variation in Model (a). Model (a) provides no account of why inequality has the effects it does, relying fully on $\theta^D$ as a placeholder for this uncertainty. In Model (c), $\theta^D$ plays a more modest role, with ethnic homogeneity doing a good deal of the work of determining inequality's possible effects. 

In sum, Model (b) and Model (c) each *explain* Model (a), if in slightly different ways. Model (b) answers the explanatory question, "*How* does inequality affect democratization when it does?" Model (c) answers the explanatory question, "*Why* does inequality's effect on democratization vary?" Both theories provide richer, more interpretable accounts of the phenomenon of interest than the simpler model that they are theorizing.

We note one final possibility. Imagine that we started with the quite *specific* claim that inequality sometimes has a positive effect on democratization and sometimes has no effect (with democratization happening for other reasons). Suppose we believed this claim to be true for some, possibly not well defined, domain of cases.^[This claim could be graphically represented by panel (a), but would involve a more restricted range for $\theta^D$ and simpler functional equation, involving only two types.] Model (c) could serve as a theory of this more specific claim in that Model (c), paired with some data, could explain the claim. In particular, Model (c) paired with the data $E=1$---we are in an ethnically homogeneous context---produces the more specific claim. Here, it is the theory *plus an observation* of context that accounts for the specific claim. 

<!-- Similarly, take the functional equation $f_1: Y=X_1X_2$. Coupled with data $X_2=1$, $f_1$ implies the functional equation $f_2: Y=X_1$.  -->


## Theory as a "lower-level" model

We can, likewise, think of theories as implying the models they are theorizing. If you believe Model (b) in Figure \@ref(fig:Demtheory), then you also must believe Model (a): if inequality can affect mobilization, which can affect democratization, then inequality can affect democratization. Similarly, if you believe Model (c), then you must also believe Model (a): if it is true that inequality can have positive, negative, and null effects on democratization that depend on ethnic homogeneity, then it is also true, more simply, that inequality can have positive, negative, and null effects on democratization. 

More formally, let us say that a causal model, $M^\prime$, is a *theory* of $M$ if $M$ is implied by $M^\prime$. Theory is, thus, all relative. $M^\prime$ might itself sit atop a theory, $M^{\prime\prime}$, that implies $M^\prime$. To help fix the idea of theory as "supporting" or "underlying" the model(s) it theorizes, we refer to the theory, $M^\prime$, as a *lower*-level model relative to $M$ and refer to $M$ as a *higher*-level model relative to its theorization, $M^\prime$.^[We note that our definition of theory differs somewhat from that given in @pearl2009causality (p207): there a theory is a (functional) causal model and a restriction over $\times_j \mathcal{R}(U_j)$, that is, over the collection of contexts envisionable. Our definition also considers probabilistic models as theories, allowing statements such as ''the average effect of $X$ on $Y$ is 0.5.''] 

<!-- Higher-level models can be generated from lower-level models in two ways, both of which are consistent with common understandings of what it is for a set of claims to constitute or, conversely, derive from a ''theory.'' -->

Building on our discussion in Section \@ref(inequalitytheory), we can distill two ways in which lower-level models can relate to, or support, higher-level models:


### Disggregating nodes

A lower-level model, $M^\prime$, can be a representation of $M$ in which a node in $M$ has been disaggregated into multiple nodes.



```{r, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:Highlow} Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes."}

par(mfrow = c(3,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2),
       y = c(1,1,2),
       names = c(
         expression(paste(X)),
         expression(paste("Y")),  
         expression(paste(theta^Y))),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "(a) A Higher-Level Model",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2,2, 1.5, 1.5),
       y = c(1,1,2, 1  , 2),
       names = c(
         expression(paste(X)),
         expression(paste("Y")),  
         expression(paste(theta^Y[lower])),
         expression(paste(K)),
         expression(paste(theta[K])) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(4, 2, 4, 2)),
       title = "(b) A Lower-Level Model: Disaggregating into Mediation",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2, 2, 1.5),
       y = c(1,1, 2, 2),
       names = c(
         expression(paste(X)),
         expression(paste("Y")),  
         expression(paste(theta^Y[lower])),
         expression(paste(C)) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(2, 2, 4, 2)),
       title = "(c) A Lower-Level Model: Disaggregating into Moderation",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

```



<!-- ```{r, echo = FALSE, fig.width = 7, fig.height = 5, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:K} A model with one explanatory  variable (top left), two lower level models that can imply it, and one model that does not."} -->

<!-- par(mfrow = c(2,2)) -->
<!-- par(mar=c(1.5,1.5,3.5,1.5)) -->
<!-- hj_dag(x = c(1,1,2,2), -->
<!--        y = c(1,2,1,2), -->
<!--        names = c( -->
<!--          expression(paste(X)), -->
<!--          expression(paste(theta[X])), -->
<!--          expression(paste("Y")), -->
<!--          expression(paste(theta[Y]^{higher}))), -->
<!--        arcs = cbind( c(2,1, 4), -->
<!--                      c(1,3, 3)), -->
<!--        title = "(a) Simplest X causes Y graph", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .2 -->
<!-- ) -->


<!-- hj_dag(x = c(1,1,2,2, 1.5, 1.5), -->
<!--        y = c(1,2,1,2, 1  , 2), -->
<!--        names = c( -->
<!--          expression(paste(X)), -->
<!--          expression(paste(theta[X])), -->
<!--          expression(paste("Y")), -->
<!--          expression(paste(theta[Y]^{lower})), -->
<!--          expression(paste(K)), -->
<!--          expression(paste(theta[K])) -->
<!--          ), -->
<!--        arcs = cbind( c(2,1, 4, 6, 5), -->
<!--                      c(1,5, 3, 5, 3)), -->
<!--        title = "(b) Lower level graph 1:\nMediator specified", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .2 -->
<!-- ) -->

<!-- hj_dag(x = c(1,1,2,2, 1.5, 1.5), -->
<!--        y = c(1,2,1,2, 1.5, 2), -->
<!--        names = c( -->
<!--          expression(paste(X)), -->
<!--          expression(paste(theta[X])), -->
<!--          expression(paste("Y")), -->
<!--          expression(paste(theta[Y]^{lower})), -->
<!--          expression(paste(K)), -->
<!--          expression(paste(theta[K])) -->
<!--          ), -->
<!--        arcs = cbind( c(2,1, 4, 6, 5), -->
<!--                      c(1,3, 3, 5, 3)), -->
<!--        title = "(c) Lower level graph 2:\nOrthogonal second cause", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .2 -->
<!-- ) -->

<!-- hj_dag(x = c(1,1,2,2, 1.5, 1.5), -->
<!--        y = c(1,2,1,2, 1.5, 2), -->
<!--        names = c( -->
<!--          expression(paste(X)), -->
<!--          expression(paste(theta[X])), -->
<!--          expression(paste("Y")), -->
<!--          expression(paste(theta[Y]^{lower})), -->
<!--          expression(paste(K)), -->
<!--          expression(paste(theta[K])) -->
<!--          ), -->
<!--        arcs = cbind( c(2,1, 4, 6, 5, 5), -->
<!--                      c(1,3, 3, 5, 3, 1)), -->
<!--        title = "(d) An incompatible graph", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .2 -->
<!-- ) -->

<!-- ``` -->



For instance, suppose we start with the higher-level model represented in Figure \ref{fig:Highlow}(a). We can then offer the graph in panel (b) as a *theory*, a lower-level model, of (a). Informally, we have added a step, $K$, in the causal chain between $X$ and $Y$, a familiar mode of theorization, exactly parallel to our model of mediation by mobilization just above. More formally, however, we have in fact *split* one node in (a) into two nodes. 

In the higher-level model, (a), $Y$ is a function of $X$ and a disturbance $\theta^Y$, the latter representing all things other than $X$ than can affect $Y$. In our four-type setup, $\theta^Y$ represents all of the (unspecified) sources of variation in $X$'s effect on $Y$. When we add $K$, $X$ now does not directly affect $Y$ but only does so via $K$. Further, we model $X$ as acting on $K$ "with error," with $\theta^K$ representing all of the (unspecified) factors determining $X$'s effect on $K$. The key thing to notice here is that $\theta^K$ now represents *a portion of the variance that $\theta^Y$ represented in the higher-level graph*: some of the variation in $X$'s effect on $Y$ now arises from $X$'s effect on $K$, which is captured by $\theta^K$. So, for instance, $X$ might have no effect on $Y$ because $\theta^K$ takes on a value such that $X$ has no effect on $K$. Likewise, any effect of $X$ on $Y$ must arise from an effect of $X$ on $K$, captured in $\theta^K$'s value. ^[As we emphasize further below, it is in fact only this "error" in the $X\rightarrow K$ link that makes the addition of $K$ potentially informative as a matter of research design: if $K$ were a deterministic function of $X$ only, then knowledge of $X$ would provide full knowledge of $K$, and nothing could be learned from observing $K$.] What $\theta^K$ represents, then, is that part of the original $\theta^Y$ that arose from some force other than $X$ operating at the *first* step of the causal chain from $X$ to $Y$. 

So now, $\theta^Y$ is not quite the same entity in the lower-level graph that it was in the higher-level graph. In the original graph, $\theta^Y$ represented *all* sources of variation in $X$'s effect on $Y$. In the lower-level model, with $K$ as mediator, $\theta^Y$ represents only random variation in $K$'s effect on $Y$. $\theta^Y$ has been expunged of any factors shaping the first stage of the causal process, which now reside in $\theta^K$. Reflecting a convention that we use throughout the book, we highlight this change in $\theta^Y$'s meaning by referring in the second model to $\theta^{Y_\text{lower}}$. 

Theorization here thus starts with the proliferation of substantive variables---adding beliefs about intervening steps in a causal process. But, critically, it also involves an accompanying disaggregation of unexplained variation. Addition and splitting thus go hand-in-hand: the *insertion* of a mediator between $X$ and $Y$ also involves the *splitting* of $Y$'s unspecified parent ($\theta_Y$).   

<!-- Put differently, when we construct the lower-level model in (b), we are taking that part of $Y$ not determined by $X$ and splitting it in two: a non-$X$ input into $K$ and a non-$K$ (and thus also non-$X$) input into $Y$. -->

Consider next panel (c) in Figure \ref{fig:Highlow}, which also supports (implies) the higher-level theory in panel $(a)$. The logical relationship between models $(a)$ and $(c)$, however, is somewhat different. Here the lower-level model *specifies* one of the conditions that comprised $\theta^Y$ in the higher-level model. In specifying a moderator, $C$, we have extracted $C$ from $\theta^Y$, leaving $\theta^{Y_\text{lower}}$ to represent all factors *other than $C$* that condition $X$'s effect on $Y$. (Again, the relabeling as $\theta^{Y_\text{lower}}$ reflects this change in the term's meaning.) While we might add a $\theta^C$ term pointing into $C$, this is not necessary. Whereas in Model (b) we have extracted $\theta^K$ from $\theta^Y$, in Model (c), it is $C$ itself that we have extracted from $\theta^Y$, substantively specifying what had been just a srandom disturbance.


### Generalizing a model

A lower-level model, $M^\prime$, can also be a representation of $M$ in which a node has been introduced that permits variation in a feature of context that is fixed and taken-for-granted in $M$. Here, $M^\prime$ theorizes $M$ in the sense of embedding $M$ within a *more general* set of beliefs about how the world works. $M$ then becomes a special case of the theorized relations, one that holds when we condition on some data, specifying some particularity of context.

To illustrate this approach to theorization, consider again graphs (a) and (c) in Figure \ref{fig:Highlow}. We have discussed how the graph in panel (c) can represent a disaggregation of $\theta^Y$ from panel (a) into $\theta^{Y_\text{lower}}$ and $C$. An alternative possibility, however, is to employ a moderation model that represents a more general claim than the higher-level model that it supports. For instance, the graph in panel (a) might represent the causal function $Y=X+\theta^Y$. In this model, $X$ always has an effect on $Y$. The graph in panel (c), in turn, might represent the more general function, $Y=XC+\theta^Y$ (where $C$ is binary). Now, whether $X$ has an effect depends on the value of $C$. In particular, model (c) combined with the observation $C=1$ directly implies model (a). Model (a) is a special case of model (c) that holds in and only in the context $C=1$. In answer to the question, "Why do you believe model (a)?" one could respond with model (c) plus the observation $C=1$. 

Note the difference in how theorization has proceeded for the two moderation models (both graphically represented in panel (c)). When we introduce moderation via the disaggregation of nodes, we pull content out of $\theta^Y$ and specify it substantively. When we generalize, on the other hand, we *add* a node, a source of variation not factored into the original model at all. We can think of that variation as implicitly conditioned-on in the higher-level model.

As a secondary matter, note that the $\theta$ term pointing into $Y$ may or may not be altered by the addition. In particular, we will explore later, if $\theta^Y$ is a vessel for causal types, then the number of possible causal types---and, thus, $\theta^Y$'s range---must expand as we add nodes pointing into $Y$. But in the functions we have used in our illustration, $\theta^{Y_\text{lower}}$ remains unchanged when we add the $C$ node. In both the higher- and lower-level models, $\theta^Y$ represents precisely the same random disturbance. 



<!-- $M^\prime$ is a *theory* of $M$ in that it, in a sense, helps explain the dependencies of $Y$ on $X$ more fully than does $M$. -->

## Moving between higher- and lower-level models

As we will see, thinking about models as conditionally nested within one another can be empirically useful. It provides a way of generating empirical leverage on a causal question by plumbing more deeply our background knowledge about a domain of interest. When we more fully specify higher-level claims via a more elaborate, lower-level model, we are a making explicit unspecified conditions on which the higher-level relationships depend. In doing this, we are identifying potentially observable nodes that might be informative about our research question.


### Mappings are not one-to-one

As we develop lower-level models to support our claims, or determine which claims are supported by our theories, what kinds of moves are we permitted to make? One important thing to note is that the mappings between higher-level claims and theories may not be one-to-one. A single theory can support multiple higher-level theories. Moreover, a single higher-level relation can be supported by multiple, possibly incompatible lower-level theories. 

To illustrate, consider two "lower level" theories of democratization:


* ($L_1$): $Inequality \rightarrow Democratization  \leftarrow Mobilization$  
* ($L_2$): $Inequality \rightarrow Mobilization \rightarrow Democratization$


Note how these theories are incompatible with one another. While  $Inequality$ and $Democratization$ are independent in $L_1$, they are causally related in $L_2$. Moreover, in $L_2$, $Inequality$ and $Democratization$ are related only through $Mobilization$, while in $L_1$, $Democratization$ is directly affected by $Inequality$.^[Put differently, these two theories record different relations of conditional independence: in $L_1$, $Inequality$ and $Mobilization$ are unconditionally independent, but they are not unconditionally independent in $L_2$. Also, in $L_2$, $Inequality$ is independent of $Democratization$ conditional on $Mobilization$; but this is not the case in $L_1$.]

Now, consider the following three higher-level claims:


* ($H_1$): $Inequality \rightarrow Democratization$
* ($H_2$): $Mobilization \rightarrow Democratization$
* ($H_3$): $Inequality \rightarrow Mobilization$


$H_1$ could be derived from (explained by) either theory, $L_1$ or $L_2$. Although the two theories are incompatible with one another, in both theories $Inequality$ affects $Democratization$. Both theories likewise imply $H_2$, in which $Mobilization$ affects $Democratization$.

$H_3$, however, can be supported only by one of these theories: only in $L_1$, and not in $L_2$, does $Inequality$ cause $Mobilization$.^[In addition, the *conditional* higher-level model $((Inequality \rightarrow Democratization)|Mobilization=1)$ can be supported by model $L_1$ but not by model $L_2$, where holding $Mobilization$ constant would sever the dependence of $Democratization$ on $Inequality$.]

Thus multiple (possibly *incompatible*) theories can usually be proposed to explain any given causal effect.  When seeking an explanation for, say, $H_1$, the choice between $L_1$ and $L_2$ is not dictated by logic; it must be drawn from a substantive belief about which set of causal dependencies operates in the world. On the other hand, $L_2$ *is* logically ruled out as an explanation of $H_3$. Further, any given theory logically implies multiple (necessarily *compatible*) higher-level claims about causal relations.


### Permissible moves across levels

What, more generally, are the permissible moves across levels? 

#### Moving down levels 

We have already discussed two possible forms of theorization --- moves down a level: (i) disaggregating existing nodes, introducing beliefs about mediation or moderation, or (ii) adding nodes representing variation in a feature of context that is implicitly held constant in the higher-level model.

There are other possible ways of elaborating a model. For instance, we can add *antecedent conditions*: causes of nodes that were exogenous in the higher-level model. Likewise, we can add *downstream effects*: outcomes of nodes that were terminal in the higher-level model.


```{r incompat, echo = FALSE, fig.width = 9, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "A higher-level model and a lower-level model that is impermissible."}

par(mfrow = c(2,1))
par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(1,2,1.5,2),
       y = c(1,1,1,2),
       names = c(
         expression(paste(Inequality)),
         expression(paste("Democratization")),
         expression(paste("Mobilization")),
         expression(paste(theta^D[higher]))),
       arcs = cbind( c(1,3,4),
                     c(3,2,2)),
       title = "(a) Higher-level model",
       add_functions = 0,
       contraction = .16,
       padding = .1
)

hj_dag(x = c(1,2,1.5, 2, 1.5),
       y = c(1,1,1, 2, 1.5),
       names = c(
         expression(paste(Inequality)),
         expression(paste("Democratization")),
         expression(paste("Mobilization")),
         expression(paste(theta^D[lower])),
         expression(paste("Ethnic homogeneity"))
         ),
       arcs = cbind( c(1,3, 4, 5, 5),
                     c(3,2, 2, 1, 2)),
       title = "(b) An incompatible lower-level model",
       add_functions = 0,
       contraction = .16,
       padding = .1
)

```

The central principle governing allowable elaborations is that a lower-level model *must not introduce dependencies between variables that were omitted in the higher-level model.* We provide an example of a violation of this principle in Figure \@ref(fig:incompat). 

We start with a higher-level model, in panel (a), in which inequality affects democratization through mobilization. We then elaborate the model in panel (b) by adding ethnic homogeneity as a moderator of mobilization's effect. However, because ethnic homogeneity is also modeled here as affecting inequality, we have now introduced a source of dependence between inequality and democratization that was omitted from the higher-level model. In panel (a), democratization and inequality were dependent only via mobilization; and so they are conditionally independent given mobilization. In panel (b), democratization and mobilization are additionally dependent via their common cause, ethnic homogeneity. By the rules governing causal graphs (see Chapter \ref{models}), the higher-level specifically *prohibited* this second source of dependency---since all dependencies between variables must be represented. 
Put differently, if two variables are independent--- or conditionally independent given a third variable---in one model, then this same relation of independence (or conditional independence) must be captured in any theory of that model. A theory can *add* conditional independencies not present in the higher-level model. For instance, a mediation theory, $X \rightarrow M \rightarrow Y$, implies a conditional independence that is not present in the higher-level model that it supports, $X \rightarrow Y$: in the lower-level model only, $X$ is conditionally independent of $Y$ given $M$. But we may not theorize away (conditional) independencies insisted on by our higher-level claim.


#### Moving up levels

Moving in the other direction, what, in general, are the permissible *simplifications* of lower-level models? In other words, given a theory, what are the higher-level claims that it can support?

When we move up a level --- i.e., eliminate one or more nodes --- the key rule is that the higher-level graph must take into account: 

(a) all *dependencies* among remaining nodes and 
(b) all *variation* generated by the eliminated node. 

We can work out what this means, separately, for eliminating *endogenous* nodes and for eliminating *exogenous* nodes.

##### Eliminating endogenous nodes

Eliminating an endogenous nodes means removing a node with parents (direct causes) represented on the graph. If the node also has one or more children, then the node captures a dependency: it links its parents to its children. When we eliminate this node, preserving these dependencies requires that all of the eliminated node's parents adopt---become parents of---all of the eliminated node's children. Thus, for instance in panel (b) of Figure \ref{fig:Highlow}, if we were to eliminate $M$, $M$'s parents ($X$ and $\theta^M$) need to adopt $M$'s child, $Y$. We see in panel (a) of the figure, the higher-level model, that $X$ is now pointing directly into $Y$. 

As for $\theta^M$, it too must now point directly into $Y$---though we can use a bit of shorthand to make this happen. Recall that $\theta^M$ represents the part of $M$ that is randomly determined. Rather than drawing two separate disturbance ($\theta$) terms pointing into $Y$, however, we more simply represent the combined disturbance term as $\theta^Y_{\text{higher}}$, with the ''higher'' signaling the aggregation of roots. (This is, of course, simply reversing the disaggregation that we undertook earlier to move from the higher- to the lower-level model.)

More intuitively, when we simplify away a mediator, we need to make sure that we preserve the causal relationships being mediated---both those among substantive variables and any random shocks at the mediating causal steps.^[Eliminating endogenous nodes may also operate via "encapsulated conditional probability distributions" [@koller2009probabilistic] wherein a system of nodes, $\{Z_i\}$  is represented by a single node, $Z$,  that takes the parents of  $\{Z_i\}$ not in $\{Z_i\}$ as parents to $Z$ and issues the children of $(Z_i)$ that are not  in $(Z_i)$ as children. However, this is not a fundamental alteration of the graph.]

##### Eliminating exogenous nodes

What about eliminating exogenous nodes---nodes with no parents? For the most part, exogenous nodes cannot be eliminated, but must either be replaced by or incorporated into $U$ (or $\theta$) terms. The reason is that we need preserve any dependencies or variation generated by the exogenous node.  Figure \@ref(fig:elimrules) walks through four different situations in which we might want to simplify away the exogenous node, $X$. (Here we use the more generic $U$ notation, though the same principles apply if these are type-receptacles($\theta$.)

```{r elimrules, echo = FALSE, fig.width = 10, fig.height = 10,  fig.align="center", out.width='.5\\textwidth', fig.cap = "Here we represent the basic principles for eliminating exogenous nodes."}


par(mfrow = c(4,2))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2),
       y = c(2,3,1),
       names = c(
         expression(paste(X)),
         expression(paste(W)),
         expression(paste("Y"))
         ),  
       arcs = cbind( c(1, 1),
                     c(3, 2)),
       title = "(a1) Lower-level: X has two children",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2,2),
       y = c(2,3,1),
       names = c(
         expression(paste(U)),
         expression(paste(W)),
         expression(paste(Y))
       ),
       arcs = cbind( c(1, 1),
                     c(3, 2)),
       title = "(a2) Higher-level: Dependency between W and Y must be preserved",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,1,2),
       y = c(3,1,2),
       names = c(
         expression(paste(X)),
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 2),
                     c(3, 3)),
       title = "(b1) Lower-level: X has a substantive spouse",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,1,2),
       y = c(3,1,2),
       names = c(
         expression(paste(U[Y])),
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 2),
                     c(3, 3)),
       title = "(b2) Higher-level: X must be replaced with U term",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,1,2),
       y = c(3,1,2),
       names = c(
         expression(paste(X)),
         expression(paste(U^Y[lower])),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 2),
                     c(3, 3)),
       title = "(c1) Lower-level: X has a random spouse",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2),
       y = c(1,1),
       names = c(
         expression(paste(U^Y[higher])),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1),
                     c(2)),
       title = "(c2) Higher-level: X absorbed into $U$ term",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,3,2),
       y = c(1,1,1),
       names = c(
         expression(paste(X)),
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 3),
                     c(3, 2)),
       title = "(d1) Lower-level: X has one child and no spouse",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(3,2),
       y = c(1,1),
       names = c(
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(2),
                     c(1)),
       title = "(d2) Higher-level: X can be safely removed",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```





*  *Multiple children.* In (a1), we start with a lower-level model in which $X$ has two children, thus generating a dependency between $W$ and $Y$. If we eliminate $X$, we must preserve this dependency. We can do so, as pictured in (a2), by replacing $X$ with a $U$ term that also points into $W$ and $Y$.^[By DAG convention, we could, alternatively, convey the same information with a dashed, undirected line between $W$ and $Y$.] Though we are no longer specifying what it is that connects $W$ and $Y$, the correlation itself is retained.
*  *Substantive spouse.* In (b1), $X$ has a spouse that is substantively specified, $W$. If we eliminate $X$, we have to preserve the fact that $Y$ is not fully determined by $W$; *something* else also generates variation in $Y$. We thus need to replace $X$ with a $U$ term, $U_Y$, to capture the variation in $Y$ that is not accounted for by $W$.
* *$U$-term spouse.* In (c1), $X$ has a spouse that is *not* substantively specified, $U^{Y_\text{lower}}$. Eliminating $X$ requires, again, capturing the variance that it generates as a random input. As we already have a $U$ term pointing only into $Y$, we can substitute in $U^{Y_\text{higher}}$, which represents both $U^{Y_\text{lower}}$ and the variance generated by $X$.*IS THIS FOOTNOTE RIGHT?*^[This aggregation cannot occur if $U^{Y_\text{lower}}$ also has another child, $W$, that is not a child of $X$ since then we would be representing $Y$'s and $W$'s random components as identical, which they are not in the lower-level graph.]
*  *One child, no spouse.* In (d1), $X$ has only one child and no spouse. Here we can safely eliminate $X$ with no loss of information. It is always understood that every exogenous node has some cause, and there is no loss of information in simply eliminating a node's causes if those causes are exogenous and do not affect  other endogenous nodes in the model. In (d2) we are simply not specifying $Y$'s cause, but we have not lost any dependencies or sources of variance that had been expressed in (d1).


One interesting effect of eliminating a substantive exogenous node can be to render seemingly deterministic relations effectively probabilistic. In moving from (b1) to (b2), we have taken a component of $Y$ that was determined by $X$ and converting it into a random disturbance. Just as we can explain a more probabilistic claim with a less probabilistic theory, we can derive higher-level claims with greater probabilism from theories with greater determinism.


```{r lowercomplexdem, echo = FALSE, fig.width = 10, fig.height = 6,  fig.align="center", out.width='.5\\textwidth', fig.cap = "A lower-level model  from which multiple higher level models can be derived."}


par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,1,2,2,3),
       y = c(2,1,2,1,1.5),
       names = c(
         expression(paste("I: Inequality")),
         expression(paste("E: Ethnic\nhomogeneity")),
         expression(paste("M: Mobilization")),
         expression(paste("R: Redistributive\npreferences")),
         expression(paste("D: Democratization"))
         ),  
       arcs = cbind( c(1, 3, 2, 2, 3, 4),
                     c(3, 4, 4, 3, 5, 5)),
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)
```



```{r runningsubs, echo = FALSE, fig.width = 12, fig.height = 13.5, fig.cap = "Higher level models derived from the lower level model of Figure X. Nodes that are eliminated are marked in grey; circles denote exogenous nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two other nodes could equivalently be indicated as an undirected edge connecting the two.) Note that $M$, $R$, and $D$ are deterministic functions of $I$ and $E$ in this example."}

par(mfrow = c(5,5))
par(mar=c(1,1,3.5,1))
x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)
names = c("I", "E", "M", "R", "D")

M <- matrix(0, 5, 5)
M[1, c(3)] <-1
M[2, c(3,4)] <-1
M[3, c(4,5)] <-1
M[4, 5] <-1

matrix_remove <- function(M, remove = NULL){
  M2 <- M
if(!is.null(remove)) {
  for(j in remove) {M2 <- (M2 + outer(M[, j], M[j,]))}  # connect parents to children
  for(j in remove) {if(sum(M2[,j]>0)) {M2[j,] <- 0; M2[,j] <- 0}}  # disconnect non-roots (roots may have to be renamed)
}
M2[M2>1] <- 1
M2
}

removes <- perm_bb(c(2,2,2,2,2))[-1,]
removes <- removes[rowSums(removes)<=3,]
removes <- removes==1
removes <- removes[order(rowSums(removes) - (1:nrow(removes))/100),]
for(j in 1:nrow(removes)){
GO <- (1:5)[removes[j,]]
    hj_dag(
       x = x,
       y = y,
       names = names,
       arcs = which(matrix_remove(M, GO)==1, arr.ind = TRUE),
       add_points = FALSE,
       solids = c(mysolids[[j]]),
       )
text(x[GO],y[GO],names[GO], col = "grey")
title(j, adj=0)
for(i in 1:ncol(M)) if((sum(M[,i])==0) & (i%in% GO)) points(x[i], y[i], cex = 3, col = "grey")
}
```


We can apply these principles to a model of any complexity. We illustrate a wider range of simplifications by starting with Figure \@ref(fig:lowercomplexdem), which represents a somewhat amended version of our inequality and democratization model from Chapter \@ref(models), with more complex causal relations. Then, in Figure \@ref(fig:runningsubs), we show all permissible reductions of the more elaborate model. We can think of these reductions as the full set of simpler claims (involving at least two nodes) that can be derived from the lower-level theory. In each subgraph, 

* we mark eliminated nodes in grey; 
* those nodes that are circled must be replaced with $U$ terms; and
* arrows represent the causal dependencies that must be preserved. 


Note, for instance, that neither $E$ (because it has a spouse) nor $I$ (because it has multiple children) can be simply eliminated; each must be replaced with a $U$ term. Also, the higher-level graph with nodes missing can contain arrows that do not appear at all in the lower-level graph: eliminating $M$, for instance, forces an arrow running from $X$ to $R$ and another running from $X$ to $Y$, as $X$ must adopt $M$'s children. The simplest elimination is of $D$ itself since it does not encode any dependencies between other variables.
<!-- ^[Put differently, and in language that we introduce below, colliding arrowheads do not represent a path in DAG analysis.]  -->

We can also read Figure \@ref(fig:runningsubs) as telling us the set of claims for which the lower-level graph in Figure \ref{fig:running} can serve as a theory. As we can see, the range of claims that a moderately complex model can theorize is vast. For each simpler claim, moreover, there may be other possible lower-level graphs---theories besides ---consistent with it.



##### Conditioning on nodes

A further permissible "upward" move is conditioning on a node. When we condition on a node, we are restricting the higher-level model in scope to situations in which that node's value is held constant. Doing so allows us to eliminate the node as well as all arrows pointing into it or out of it. Consider three different situations in which we might condition on a node:


* *Exogneous, with multiple children.* In simplifying (a1) in Figure \@ref(fig:elimrules), we need to be sure we retain any dependence that $X$ generates between $W$ and $Y$. However, recalling the rules of conditional independence on a graph (see Chapter \ref{models}), we know that $W$ and $Y$ are *independent* conditional on $X$. Put differently, if we restrict the analysis to contexts in which $X$ takes on a constant value, the lower-level model implies that $Y$ and $W$ will be uncorrelated across cases. As fixing $X$'s value breaks the dependence between $Y$ and $W$, we can drop $X$ (and the arrows pointing out of it) without having to represent that dependence. 
*  *Exogenous, with spouse.* In simplifying (b1) or (c1) Figure \@ref(fig:elimrules), we need to account for the variation generated by $X$. If we fix $X$'s value, however, then we eliminate this variation by assumption and do not need to continue to represent it (or the arrow pointing out of it) on the graph.
*  *Endogenous.* When we condition on an endogenous node, we can eliminate the node as well the arrows pointing into and out of it. We, again, leverage relations of conditional independence here. If we start with graph (b) in Figure \ref{fig:Highlow}, and we condition on the mediator, $M$, we sever the link between $Y$ on $X$, rendering them conditionally independent of one another. We can thus remove $M$, the arrow from $X$ to $M$, and the arrow from $M$ to $Y$. In the new model, with $M$ fixed, $Y$ will be entirely determined by the random disturbance $\theta^{Y_\text{lower}}$.^[Note that such conditioning does not add any variance to the $\theta^Y$ term, so we retain the notation $\theta^{Y_\text{lower}}$.]


In sum, we can work with models that are simpler than our causal beliefs: we may believe a complex lower-level model to be true, but we can derive from it a sparer set of claims. There may be intervening causal steps or features of context that we believe matter, but that are not of  interest for a particular line of inquiry. While these can be removed, we nonetheless have to make sure that their *implications* for the relations remaining in the model are not lost. Understanding the rules of reduction allow us to undertake an important task: checking which simpler claims are and are not consistent with our full belief set.

<!-- Nodes with no parents in $\mathcal{U}\cup\mathcal{V}$ cannot be eliminated as this would entail a loss of information. The graph in Figure \ref{fig:K}(d) illustrates the importance of this. Here $K$ is a cause of both $X$ and $Y$, in other words it is a possible confounder. A higher-level graph that does not include $K$ still requires a $U_K$ node pointing into both $K$ and $Y$ to capture the fact that there is a confounder. -->


<!-- ^[The conditioning approach can also handle theoretical propositions in the form of structural causal models that make no immediate empirical claims but still have "empirical content" in the sense of being able to inform *conditional* claims. The claim "if $X$ then $Y$" says nothing about $P(Y)$ by itself. However, it says a lot about $P(Y)$ if $P(X)$ is known.] -->


<!-- One  effect of elimination is to render seemingly deterministic relations effectively probabilistic. For example, in the lower level graph $C$ is a deterministic function of $X$ and $S$. But in higher level graphs it can depend probabilistically on one of these: in submodel 21, $C$  depends probabilistically on  $X$ since $S$ is now a stochastic disturbance; in 34 $C$ depends probabilistically on $S$. This illustrates how unobserved or unidentified  features render a model "as-if" stochastic. Conversely, models that exclude this form of uncertainty implicitly claim model-completeness. -->



<!-- Consider a second manner in which a higher level model  can be deduced from a lower level model, this time in conjunction with data (or more broadly, ancillary claims): -->

<!-- 2. A higher level model may be formed by conditioning on values of nodes in a lower level model. Conversely, a higher-level functional model, $M$, can be theorized via a lower-level $M^\prime$ in which conditions shaping the operation of the causal effect in $M$, unspecified in $M$, are now specified. -->

<!-- To illustrate this approach, consider again the graphs in Figure \ref{fig:K}. Above we described how the graph in  panel (a) can be produced by aggregating $U_Y^{\text{lower}}$ and $U_K$ from panel (c). An alternative possibility is to simplify by conditioning: we derive a higher-level graph from $M^\prime$ by fixing the value of $K$. For instance, if $Y=XK+U_Y^{\text{lower}}$ in $M'$, then at $K=1$, we have the submodel $M_k$ in which $Y=X+U_Y^{\text{lower}}$. Note that, in generating a submodel by conditioning on $K$, we retain the term $U_Y^{\text{lower}}$ as we have not added causal force into $Y$'s unspecified parent. -->




<!-- Perhaps surprisingly, in this treatment, the theoretical support for a causal model is itself just another causal model: a set of beliefs about structural relations between variables. Thus, a theory is an object that is formally similar to an empirical claim.  -->

<!-- Would like to clarify last sentence above. -->



<!-- This next paragraph is hard to follow. The u's come out of nowhere. What's a mapping from R1 to R1? Generally seems like the points could be made more simply. -->

<!-- I THINK THE UNIVERSALITY AND PRECISION MATERIAL IS INTERESTING BUT NOT SOMETHING WE NEED TO DO ON OUR ROAD TO USING CAUSAL MODELS FOR CAUSAL INFERENCE. AND IT'S NOT EASY. SO I SUGGEST WE CUT.

We can, however, use the approach to assessment of two features sometimes considered important to assess empirical content of a theory: the level of *universality* of a theory and the degree of *precision* of a theory [@popper2005logic, @glockner2011empirical]. ***DEFINE THESE HERE.*** For instance, consider a theory over $X_1, X_2, A, B, Y$ that specified $X_1, X_2 \rightarrow Y \leftarrow A, B, g$ with functional equations: -->

<!-- $$Y = \left\{ \begin{array}{ccc}  -->
<!-- A + BX_1 & \text{ if } & X_2 = 1\\    -->
<!--   g(X_1) &\text{ if } & X_2 = 0 \end{array} \right.$$  -->

<!-- where the domain of $g$, $\mathcal{R}(g)$, is the set of all functions that map from $\mathbb{R}^1$ to $\mathbb{R}^1$, and the ranges of $A$ and $B$ are the real number line. Say the distributions over $A, B, X_1, X_2$, and  $g$ are not specified. Then the theory makes a precise claim conditional on $u_1, u_2, X_1, X_2$, and  $g$. But since the distribution over $\mathcal{R}(g)$ is not provided by the theory, the theory only claims knowledge of a functional form for $Y$ for those cases in which $X_2=1$. Thus in this case the *universality* of the theory for the claim "$Y$ is a linear function of $X$," is  $P(X_2=1)$. This is the domain over which the theory has something to say about this proposition. Note that in this case the universality is not  provided by the theory, but is rather an external proposition that depends on additional data. The *precision* of the theory depends both on the claim of interest and the distribution of root variables. For example, the precision of the theory for the causal effect of $X_1$ on $Y$ when $X_2=1$ depends on the distribution of $B$: the theory is more precise about this causal effect the less uncertainty there is about the value of $B$. Moreover, a theory that specified that $B$ has large variance would be making a precise claim about causal *heterogeneity*, even if it was imprecise about the causal effect. Again this feature cannot be read from the theory without access to ancillary information that the theory itself does not provide. -->

<!-- AJ comments: -->

<!-- - Not clear what the "this approach" is that this is illustrating a feature of. Expressing theory as structural equations?  -->

<!-- - There seems to be a more specific point here than just that we can assess universality and precision. In fact, it's actually that we *can't* assess these things purely from structural models. We need probabilistic models or information on variable values, no? -->

<!-- - Universality seems an odd term for a concept that is continuous. Generality? Coverage? -->

<!-- - There was switching between precision and specificity. I've gone with precision, but could see either. -->



<!-- Better to replace type with variables.  -->

<!-- For example rather thatn $X \rightarrow Y \leftarrow Q$ where $Q$ takes on values of the four tyes.  -->
<!-- ROUGH NOTES -->
<!-- Say $Y = AX +B(1- X)$ in which case  uncertainty over functional forms, or  uncertainty about undefuned causal types  can be reconceptualized as uncertainty around positive and negative drivers -->


<!-- Drawing on different theories for subcomponents; eg theory of human decision making.  -->
<!-- More or less specified theories.  -->

<!-- Encapsulated CPDs are one way to wrap subtheories. Two identical DAGS could have two distinct theoretical underpinnings in the sense of having different encapsulated CPDs.    -->

<!-- When is one theory more general than another? When is one more specified than another? -->

<!-- If you specify a more detailed theory, the theory has more testable implications, but it is less general.  -->

<!-- Theory T'' is implied by theory T' if V'' is a subset of V' and the relations in T'' are implied by the relations in the reduced set T'' -->

<!-- For example:  -->
<!-- T'': A = 1 with prob .5, B = 1 with prob .5, if A = 1; 0 otherwise, C = 1 if B = 1 -->
<!-- T'': A = 1 with prob .5, C = 1 with prob .5, if A = 1; 0 otherwise -->
<!-- Different  subparts of theories may be implied by distinct theories provided implication relations satisfied -->
<!-- eg T'' part 1 may be implied by T, and T' implied by T'''', but T'' and T'''' not mutually consistent -->

<!-- Theory T'' explains more than T' if it identifies more causal relations -->
<!-- Theory T'' is more fertile  than T' if it implies more theories (same?) -->
<!-- Theory T'' has more observable implications that Theory T' if.... -->
<!-- Theory T'' is more falsifiable than theory T'.... -->
<!-- Theory T'' is more general (wide scope) than theory T' if its conditioning set is a  subset of theory T''s conditioning set -->
<!-- Theory T'' is more parsimonious  than theory T' if it predicts the same or more causal relations  with fewer nodes -->
<!-- Theory T'' is more complete (fully specified) than theory T' if it has lower prior variance (?) -->

<!-- Key -- there is no distinction between aleatory and epistemic uncertainty. It is all epistemic.  -->
<!-- eg the goalie might well randomize, but if we do not know which way she will go it is because we do not know  -->
<!-- what randomizing device she is using -->
<!-- Key: priors specified over all relations -->
<!-- Possibly need that nodes have classes -- eg utility -- that are used for some lo level theories -->
<!-- Question; ultimately is htere only one net and the universe is a realization of it? -->


<!-- SAME THING HERE. AN INTERESTING POINT, BUT A SEPARATE ENDEAVOR.

Functional (but not probabilistic)  causal models allow for the representation of logically derived relations between nodes without implying any unconditional empirical claims; that is, all claims may be of the  *if-then* variety, as is typical for example of propositions derived from game theoretic models. The process of connecting such models to the empirical claims can be thought of as the embedding of these incomplete models within larger structures.  -->

<!-- Consider for example the claim that in normal form games,  players play Nash equilibrium.  This claim in itself is not a tautology; that is, it is not a result. It can be contrasted for example with the *analytic result* that when rational players play a game and players have common knowledge of the game structure and of player rationality they will only play ''rationalizable'' strategies. Even still, the Nash claim does provide a set of analytically derived functional equations that relate nodes that describe game forms to actions  taken, and from actions to utilities. Representation as a causal graph can make explicit what conditional independencies are assumed in the move from analytic results to empirical claims. For example, are actions independent of the game form conditional on beliefs about the game form; are utilities independent of expectations conditional on actions, and so on. -->

<!-- We give an example of one such model below when we turn to extensive-form games for a lower-level theory that supports our running example.   -->


**BOXES**

***

**BOX 1**

**Two kinds of theories.**

Theories are lower-level causal models that explain or provide an account of a higher-level, simpler model. There are two forms of theorization:

1. The disaggregation of nodes. A single node in a higher-level model can be split into multiple nodes. For instance, for a higher-level model in which $X \rightarrow Y \leftarrow \theta^Y$:
  * *Mediation*: A mediator, $M$, can be introduced between $X$ and $Y$, thus splitting $\theta^Y$ into $\theta^M$ and $\theta^{Y_\text{lower}}$. The mediation theory thus explains the $X \rightarrow Y$ relationship.
  * *Moderation*: A component of $\theta^Y$ can be extracted and specified as a substantive variable. This variable is now a substantively conceptualized moderator of the $X \rightarrow Y$ relationship. The moderation theory thus provides a fuller explanation of why $X$ has different effects on $Y$ in different contexts.
2. Generalization. A feature of context omitted and implicitly held constant in a higher-level model can be explicitly included in the model. The higher-level model is now explained as a special case of a more general set of causal relations.

***


***

**BOX 2**

**Rules for moving between levels**

*Moving down levels*: 

All (conditional) independencies represented in a higher-level model must be preserved in the lower-level model. 

When we disaggregate or add nodes to a model, new conditional independencies can be generated. But any variables that are independent or conditionally independent (given a third variable) in the higher-level model must also be independent or conditionally independent in the lower-level model.

*Moving up levels*: We can move up levels by eliminating an exogenous node, eliminating an endogenous node, or conditioning on a node. When we eliminate a node from a model, we must preserve any variation and dependencies that it generates: 

1. When eliminating an endogenous node, that nodes parents adopt (become direct causes of) that nodes children. 
2. When eliminating an exogenous node, we must usually replace it with a $U$ term. If the node has more than one child, it must be replaced with a $U$ term pointing into both children (or an undirected edge connecting them) to preserve the dependency between its children. If the node has a spouse, the eliminated nodes variation must also be preserved using a $U$ term. Where the spouse is (already) a $U$ term with no other children, $U$ terms can be combined.  
3. Since conditioning on a node blocks the path through which it connects its children, we can simply eliminate the node and the arrows between it and its children.
4. An exogenous node with no spouse and only one child can be simply eliminated.

***


## Beneath the Graph: Causal Types in Lower-Level Models

We have discussed theorization largely from a graphical perspective, showing how features of causal graphs change (nodes get split, combined, added, or removed) as we move down or up levels. But there is more that happens beneath the surface of a graphical structure when we theorize a claim. In this section, we return to our causal-type framework, in which causal types are captured by $\theta$ terms. While we have already shown how we can introduce causal types into the simplest $X \rightarrow Y$ model, we work through here how the causal-type space changes as we complicate that model. We show, first, how causal types operate a mediation model, and then how they operate in a model with moderation.

### Mediation as Theory {#medtheory}

We begin with a simple claim: there are two binary variables, $X$ and $Y$, and $X$ may have an effect on $Y$. This claim is represented in Figure \ref{fig:Highlow}(a) above. In this graph, $X$ is independent of $\theta^Y$, which means that it is as if $X$ is randomly assigned.

We will let $\theta^Y$ be a variable that ranges across our four different causal types, conditioning how $Y$ responds to $X$. While  $a, b, c$, and $d$ were heuristically useful as a way of introducing the  idea of a causal type, things will soon get more complicated, so it will be useful to have more flexible notation. Going forward, we will usually refer to causal types using $\theta$ notation, with subscripts and superscripts used to denote potential outcomes and outcome variables. In our binary $X \rightarrow Y$ setup, we can indicate the causal type governing $Y$'s response with notation of the form $\theta^Y_{ij}$, where $i$ and $j$ represent $Y$'s potential outcomes. Specifically, $i$ represents the value $Y$ takes on when $X=0$, while $j$ represents the value $Y$ takes on when $X=1$.^[The functional equation for $Y$ is then given by: 
$$Y(x, \theta_{ij}^{Y_\text{higher}}) = \left\{ \begin{array}{cc}  
i & \text{ if } x=0 \\ j & \text{ if } x=1 \end{array}  \right.$$] Thus, the translation from $a, b, c$ and $d$ notation is:


* *a*: $\theta_{10}^Y$. A negative effect implies that $Y$ is $1$ when $X=0$ and $0$ when $X=1$.
* *b*: $\theta_{01}^Y$. A positive effect implies that $Y$ is $0$ when $X=0$ and $1$ when $X=1$.
* *c*: $\theta_{00}^Y$. A null "chronic" effect implies that $Y$ is $0$ regardless of $X$'s value.
* *d*: $\theta_{11}^Y$. A null "destined" effect implies that $Y$ is $1$ regardless of $X$'s value.


To be clear, these $\theta_{ij}^Y$ terms are not random variables; they are the four _values_ (types) that the type-variable $\theta^Y$ can take on.

<!-- represented with the notation $u_{ij}$: we read the subscripts to mean that a unit of type $u_{ij}$ has outcome $i$ when $X=0$ and $j$ when $X=1$. Then let $u_Y^{higher}$ have a multinomial distribution over the four values of  $u_{ij}$ with event probabilities  $\lambda_{ij}^{higher}$. ; for example, let $u_X\sim \text{Unif}[0,1]$ and $X = \mathbb{1}(u_K<\pi^K)$. -->


<!-- For example if $U_Y^{higher}$ is distributed normally and $Y$ takes on the value 1 if $bX+u_Y^{higher}$ is above some threshold, we have a probit model.  -->


Now consider a theory that specifies a variable intervening between $X$ and $Y$. This theory is depicted in Figure \ref{fig:Highlow}(b) above, where $M$ mediates the relationship. We see that there are now two $\theta$ terms, each representing a set of causal types for a different step in the causal chain. While $\theta^Y$ represented $Y$'s response to its parent $X$, $\theta^Y_{\text{lower}}$ represents $Y$'s response to its "new" parent, $M$. We now also need to conceive of a causal type capturing $M$'s response to $X$, and we let $\theta^M$ represent this type.^[This graph assumes no confounding in the mediating relationship either as the two $\theta$ terms and $X$'s assignment are all independent of one another.]

We thus allow $X$ to have a positive, negative, or no effect on $M$, with $\theta^M$ taking on four possible values:


* *a*: $\theta_{10}^M$ 
* *b*: $\theta_{01}^M$
* *c*: $\theta_{00}^M$
* *d*: $\theta_{11}^M$


And we allow for $M$ to have a positive, negative, or no effect on $Y$, with $\theta^Y_{\text{lower}}$'s possible values being:


* *a*: $\theta_{10}^Y$ 
* *b*: $\theta_{01}^Y$
* *c*: $\theta_{00}^Y$
* *d*: $\theta_{11}^Y$


We can now think about _combinations_ of types in the lower-level model as mapping onto types in the higher-level model. We work through these mappings in greater detail later in the book, but consulting table \@ref(tab:highlowmapping), one can readily see how this works. 




|                    | $\theta_{10}^{Y_{lower}}$  | $\theta_{01}^{Y_{lower}}$  | $\theta_{00}^{Y_{lower}}$  | $\theta_{11}^{Y_{lower}}$  |
|--------------------|----------------------------|----------------------------|----------------------------|----------------------------|
| $\theta_{10}^{M}$  | $\theta_{01}^{Y_{higher}}$ | $\theta_{10}^{Y_{higher}}$ | $\theta_{00}^{Y_{higher}}$ | $\theta_{11}^{Y_{higher}}$ |
| $\theta_{01}^{M}$  | $\theta_{10}^{Y_{higher}}$ | $\theta_{01}^{Y_{higher}}$ | $\theta_{00}^{Y_{higher}}$ | $\theta_{11}^{Y_{higher}}$ |
| $\theta_{00}^{M}$  | $\theta_{11}^{Y_{higher}}$ | $\theta_{00}^{Y_{higher}}$ | $\theta_{00}^{Y_{higher}}$ | $\theta_{11}^{Y_{higher}}$ |
| $\theta_{11}^{M}$  | $\theta_{00}^{Y_{higher}}$ | $\theta_{11}^{Y_{higher}}$ | $\theta_{00}^{Y_{higher}}$ | $\theta_{11}^{Y_{higher}}$ |
Table: (\#tab:highlowmapping) Mapping from lower level nodal types on $M$ and $Y$ to higher level causal types on $Y$. 

For instance, in a case in which both $\theta^M=\theta^M_{01}$ (a positive effect of $X$ on $M$) and $\theta^{Y_{\text{lower}}}=\theta_{01}^{Y_{lower}}$ (a positive effect of $M$ on $Y$), we have a positive effect of $X$ on $Y$---meaning that, in the _higher-level_ model, $\theta^{Y_{higher}}=\theta^{Y_{higher}}_{01}$. Two linked negative effects also generate a positive effect of $X$ on $Y$ and so map onto the same higher-level type. Further, it is easy to see that if there is no causal effect at _either_ the $X \rightarrow M$ step _or_ the $M \rightarrow Y$ step, we will have one of the null effect types at the higher level since, in this model, $X$ cannot affect $Y$ unless there are causal effects at both constituent steps.^[These mappings, of course, hinge on the fact that $X$ affects $Y$ _only_ through $M$ in this model (no direct effects or other pathways).]

To foreshadow the discussion in later chapters, these mappings are critical: they allow us to use inferences drawn at a lower level to answer questions posed at a higher level.

<!-- The lower-level functional equations are formally similar though now each unit's outcome (given $X$) depends on two event probabilities: one that determines type with respect to the effect of $X$ on $K$ ($t_{ij}^{K}$), and one with respect to the effect of $K$ on $Y$ ($u_{ij}^{Y}$): -->

<!-- $$Y(K, u_{ij}^{Y}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } K=0 \\ j & \text{ if } K=1 \end{array}  \right.$$ -->
<!-- $$K(X, u_{ij}^{K}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } X=0 \\ j & \text{ if } X=1 \end{array}  \right.$$ -->

<!-- Thus, in the lower-level model, there are sixteen types that derive from the cross product of two independent random terms. -->

<!-- Critically, one can derive the higher-level types from the lower level types, and beliefs about the higher level types from beliefs about the lower level types. For example, using the nomenclature in @humphreys2015mixing: -->

<!-- \begin{eqnarray*} -->
<!-- \text{adverse: }u_{10}^{high} &=& u_{01}^{K}\&u_{10}^{Y} \text{ or } u_{10}^{K}\&u_{01}^{Y} \\ -->
<!-- \text{beneficial: }u_{01}^{high} &=& u_{01}^{K}\&u_{01}^{Y} \text{ or }  u_{10}^{K}\&u_{10}^{Y} \\ -->
<!-- \text{chronic: } u_{00}^{high} &=& u_{00}^{Y} \text{ or }  u_{00}^{K}\&u_{01}^{Y} \text{ or }  u_{11}^{K}\&u_{10}^{Y}\\ -->
<!-- \text{destined: }u_{11}^{high} &=& u_{11}^{Y} \text{ or }  u_{00}^{K}\&u_{10}^{Y} \text{ or }  u_{11}^{K}\&u_{01}^{Y} -->
<!-- \end{eqnarray*} -->

<!-- In the same way, the higher-level probabilities are implied by the lower level probabilities. -->

<!-- \begin{eqnarray*} -->
<!-- \text{adverse: }\lambda_{10}^{high} &=& \lambda_{01}^{K}\lambda_{10}^{Y} + \lambda_{10}^{K}\lambda_{01}^{Y} \\ -->
<!-- \text{beneficial: }\lambda_{01}^{high} &=& \lambda_{01}^{K}\lambda_{01}^{Y} + \lambda_{10}^{K}\lambda_{10}^{Y} \\ -->
<!-- \text{chronic: } \lambda_{00}^{high} &=& \lambda_{00}^{Y} + \lambda_{00}^{K}\lambda_{01}^{Y} + \lambda_{11}^{K}\lambda_{10}^{Y}\\ -->
<!-- \text{destined: }\lambda_{11}^{high} &=& \lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y} -->
<!-- \end{eqnarray*} -->

<!-- Importantly, even without specifying a distribution over $U_K$ or $U_Y^{\text{lower}}$, a lower-level structural model could be informative by restricting the *ranges* of  $U_K$ or $U_Y^{\text{lower}}$. For instance, a lower level theory that imposed a monotonicity condition (no adverse effects) might exclude $t^K_{10}$ and $t^y_{10}$---that is, increasing $X$ never reduces $K$, and increasing $K$ never reduces $Y$.  -->

<!-- We return  to this example below and show how observation  of $K$ can yield inference on causal estimands when  the theory places this kind of a structure on relationships. -->

### Moderation as Theory {#modtheory}

Now consider an alternative lower-level theory, represented in  Figure \ref{fig:HighLow}(c) above. Here $C$ is posited as a second parent of $Y$. This graph contains the substantive assumptions that $C$'s value is determined independently of $X$'s, as well as the assumption that $X$ and $C$ are both as-if randomly assigned.

In this graph, we again have a $\theta_Y^{\text{lower}}$ term, but it is a different object from $\theta_Y^{\text{lower}}$ in the mediation graph. In this moderation model, $\theta_Y^{\text{lower}}$ is more complex as it determines the mapping from two binary variables into $Y$. A causal type in this setup now represents how a case will respond to four different possible combintations of $X$ and $C$ values. Rather than four causal types, we now have 16, as there are 16 possible ways in which a case might respond to two binary variables. Table \{#tab:PO16} in Chapter \ref{models} provides a tabular representation of the complete set of 16 potential outcomes. 

However, we can also represent these types more compactly. For a setup with two binary causes, we will use the notation $\theta_{ij}^{gh}$. The subscripts and superscripts represent $Y$'s potential outcomes for different values of $X$ and $C$. To aid interpretation, we always place $X$ on the horizontal axis and a second variable (here, $C$) on the vertical access. In other words, the value of $X$ increases from 0 to 1 as we move to the _right_, from $i$ to $j$ or from $g$ to $h$. And the value of $C$ increases from 0 to 1 as we move _up_, from $i$ to $g$ or from $j$ to $h$. $\theta_{ij}^{gh}$ thus denotes a unit that has outcome $i$ when $X=0, C=0$, $j$ when $X=1, C=0$, $g$ when $X=0, C=1$, $h$ when $X=1, C=1$.

To illustrate, $\theta_Y^{\text{lower}}=$:


* $\theta_{00}^{11}$ means that $X$ has no effect under any value of $C$, and $C$ has a positive effect under any value of $X$. 
* $\theta_{10}^{10}$ implies that $X$ always has a negative effect, and $C$ never has an effect. 
* $\theta_{01}^{11}$ represents one kind of conditional effect: $X$ has a positive effect only when $C=0$, and $C$ has a positive effect only when $X=0$.
 

Again, we can map between lower- and higher-level types---though we may need to do so in combination with data. For instance, a case has type $\theta_{01}$ in the higher-level model if it has type $\theta_{01}^{11}$ in the lower-level model _and_ $C=0$. This is a case for which $X$ has a positive effect on $Y$ when $C=0$ _and_ in which $C$ _is in fact_ 0. On the other hand, the same lower level type, $\theta_{01}^{11}$, in combination with $C=1$ maps onto the type $\theta_{11}$ in the higher-level model---a type in which $Y$ is 1 regardless of $X$'s value. 

Meanwhile, the lower-level type $\theta_{10}^{10}$ maps onto the higher-level type $\theta_{10}$ regardless of the data ($C$'s value) since this lower-level type implies that $X$ has a negative effect on $Y$ under any value of $C$. 

In later chapters, we represent all lower- to higher-level mappings relevant to a question of interest with the use of "type-reduction" tables that allow one to readily see how inferences drawn at one level inform causal questions posed at another level.

<!-- We let $u_Y^{\text{lower}}$ in this graph denote a multinomial distribution over the sixteen values of  $u_{ij}^{gh}$ with event probabilities  $\lambda_{ij}^{gh}$. -->

<!-- I changed abcd scripts above to ghij and made corresponding (I think) changes below. I don't care what it is but abcd obviously could be confusing in this context. -->

<!-- The sixteen types are illustrated in Table \@ref(tab:types2X) in the appendices. -->

<!-- Again, the types in the higher level mapping are functions of the types in the lower-level mapping. For example,  a unit has type $u_{01}$ in the higher level model if $K=1$ and it is of type $u_{00}^{01},u_{10}^{01},u_{01}^{01}$, or $u_{11}^{01}$, or if $K=0$ and it is of type $\lambda_{01}^{00},\lambda_{01}^{10},\lambda_{01}^{01}$, or $\lambda_{01}^{11}$.  -->

<!-- We write this as: -->

<!-- $$u_{01} =  ((K=1) \land (t^{lower} \in \{u_{00}^{01} \cup u_{10}^{01} \cup  u_{01}^{01} \cup u_{11}^{01} \}) \lor  ((K=0) \land (t^{lower} \in \{\lambda_{01}^{00} \cup \lambda_{01}^{10} \cup \lambda_{01}^{01} \cup \lambda_{01}^{11}\})$$ -->

<!-- In the same way, the probability of type $u_{01}$ can be written in terms of the parameters of the lower-level graph.  Importantly, the parameters of the higher-level distribution  $u_Y^{higher}$ depend on both $u_K$ and $u_Y^{\text{lower}}$. Thus, unlike the mediation case above, the probative value depends on the likelihood of an *observable* event occurring. Specifically, the share of a given higher-level type is given by: -->

<!-- $$\lambda_{ij} = P(u_Y^{higher} = u_{ij}) = \pi^K\left(\lambda_{00}^{gh}+\lambda_{10}^{gh}+\lambda_{01}^{gh}+\lambda_{11}^{gh}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{ij}^{00}+\lambda_{ij}^{10}+\lambda_{ij}^{01}+\lambda_{ij}^{11}\right)$$ -->

<!-- For example: -->

<!-- $$\lambda_{00} = P(u_Y^{higher} = u_{00}) = \pi^K\left(\lambda_{00}^{00}+\lambda_{10}^{00}+\lambda_{01}^{00}+\lambda_{11}^{00}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{00}^{00}+\lambda_{00}^{10}+\lambda_{00}^{01}+\lambda_{00}^{11}\right)$$ -->


<!-- Conditional probabilities follow in the usual way. Consider, for instance, the case where it is known that $X=Y=1$ and so the posterior probability of type $u_{01}$ is simply $P(i \in u_{01} | X=Y=1) = \frac{\lambda_{01}}{\lambda_{01}+\lambda_{11}}$. Note that $\pi^x$ does not appear here as this $X$ is orthogonal to $u_Y$. The probability of type $u_{01}$, knowing that $X=Y=1$, can be written in terms of the parameters of the $u$ distributions in the lower-level graph.  -->

<!-- $$P(i \in u_{01} | X=Y=1) = \frac{ -->
<!-- \pi^K\left(\lambda_{00}^{01}+\lambda_{10}^{01}+\lambda_{01}^{01}+\lambda_{11}^{01}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{01}^{00}+\lambda_{01}^{10}+\lambda_{01}^{01}+\lambda_{01}^{11}\right) -->
<!-- }{ -->
<!-- \sum_{i = 0}^1\left(\pi^K\left(\lambda_{00}^{i1}+\lambda_{10}^{i1}+\lambda_{01}^{i1}+\lambda_{11}^{i1}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{i1}^{00}+\lambda_{i1}^{10}+\lambda_{i1}^{01}+\lambda_{i1}^{11}\right) -->
<!-- \right)}$$ -->

<!-- We return below to this example and describe how the lower-level model can be used to generate inferences on relations implied by the higher level model.  -->


## Conclusion

Before moving on, it is worth considering how the understanding of theory that we work with in this book compares to other prominent understandings of theory. 

**Theory as tautology.**  The claim that the number of Nash equilibria is generically odd in finite games is often understood to be a theoretical claim. Unless there are errors in the derivation of the result, the claim is true in the sense that the conclusions follow from the assumptions. There is no evidence that we could go looking for in the world to assess the claim. The same can be said of the theoretical claims of many formal models in social sciences; they are theoretical deductions of the if-then variety [@clarke2012model]. Theory in this sense is true by tautology. By contrast, theory as we define it in this book refers to claims with *empirical* content: a theory refers to causal relations in the world that might or might not hold, and is susceptible to empirical testing. The deductive _logical_ relations that hold in a causal model are those of conditional independence, as discussed in Chapter \@ref(models): for instance, if $X$ causes $Y$ only through $M$ in a theory, then $X$ and $Y$ are conditionally independent given some value of $M$.

**Theory as a collection of maps.** According to @clarke2012model, building on a semantic view of theory (@giere2010explaining), a theory is a collection of models, together with a set of hypotheses linking them to the real world. As in our usage, Clarke and Primo see theories and models as very similar objects: for them, a theory is a system of models; for us, a theory is a supporting model. In both frameworks, there is no real difference in kind between models and theories.

Our approach also shares with Clarke and Primo the idea that models are not full and faithful reflections of reality; they are maps designed for a particular purpose. In the case of causal models, the purpose is to capture relationships of independence and possible causal dependence. As we have shown, that is a purpose that allows for the stripping away of detail---though it also forbids certain simplifications (such as any simplification that removes a dependency between variables). Clarke and Primo see models as useful to the extent that they are similar to features of the real world in ways related to the model's purpose. Along these lines, a causal model will be useful to the extent that it posits relations of independence that are similar to those prevailing in the domain under investigation.

**Theory as a testable claim** In the hypothetico-deductive framework, often traced back to @popper2014conjectures and highly influential in empirical political science, empirical social science is an activity of theory-*testing*.  Having developed a theory, we then derive from it a set of empirical predictions and then test those predictions against evidence. In @clarke2012model, we also seek to confirm theories by developing and testing hypotheses about the similarity of a model or theory to particular features of the world. In both cases, a theory is posited---possibly on the basis of logic or background knowledge---and then assessed. The value (truth or usefulness) of the model itself is the object of inquiry.

In a causal-model framework, theories are always tentative, and we can subject any model or theory to empirical evaluation, a task to which we turn in Chapter \@ref{evaluation}. However, in the book's setup, theories are first and foremost _expressions of what we already know and don't know_ about a given causal domain when inquiry begins. We encode this background knowledge in order to inform research-design choices and draw inferences from the data.  Models and theories are thus, in this sense, the world within which inquiry unfolds. Indeed, as we explore in Chapter \@ref{questions}, the very questions we ask live within---can be represented as parts of---our theories. 

<!-- **Theory as model.** Although @clarke2012model argue for a separation of the ideas of model and theory, it is common for social scientists to use the terms interchangeably to denote an abstract representation of some part of the world that is of interest. For instance, a model may stipulate that outcome $X$ can have a positive effect on $Y$ because $X$ can cause $M$ and $M$ can cause $Y$.  One can read from a model how things work in the context of the model: for instance, if $M$ does not obtain, then under this model, $X$ does not cause $Y$.   One can use a model to make claims about the world only by assuming a mapping from elements in the model to elements in the worlds.  In this sense a  model is best thought of as an object that may or may not be useful [@clarke2012model]; whether the model itself is true or false is, in this usage, not a coherent question.  -->

<!-- **Theory as empirical claim.** In common usage, "a theory of" a phenomenon is a direct claim about the phenomenon, in the world. The claim that natural resources cause conflict is a theoretical claim of this form. The claim is certainly not true by definition, and empirical evidence can be used to assess it. In this claim, the *theory*, as usually understood, is certainly thin; the claim is no more than an empirical proposition, and it possesses no internal logic. Yet, more elaborate collections of empirical propositions are easily constructed. For instance: natural resources cause conflict because they can finance secessionist claims in resource rich areas.^[This latter claim does seem to possess something like a logic; though it does not take much to see that the logic is just a slightly more elaborate set of empirical claims. The outcomes do not follow  *logically* from the causes----there is no logical reason why secessionist claims would cause conflict, but the theory---as a collection of claims---has implications similar to those in the model in the paragraph above: if there are no secessionist claims, then under this theory, natural resources are not causing conflict.] Theory in this sense can certainly be right or wrong. -->



<!-- In this book we take a somewhat idealist position and assume that we are permanently inhabiting a world of models.  -->

<!-- The distinction between the last two accounts is sometimes confusing, and  @clarke2012model make a case for cleaning up the language on this front. In their account, drawing on @giere2010explaining, a theory might be best thought of as a set of models accompanied by hypotheses linking the model to the question of interest in the  world.  -->

<!-- We see our approach to theory as models as following in the spirit of  @clarke2012model and  @giere2010explaining yet also as being consistent with the treatment of models in the literature on probabilistic causal models with which this book is centrally engaged. A nice feature is that it preserves a close associated between theory and explanation and it incorporates naturally the notion of deduction without requiring that models themselves are statements of the  *if-then* variety. -->

**Theory as generalization** In another of the many uses of "theory," political scientists often think of theorization as generalization. For @Van-Evera:1997 and @przeworski1970logic, for instance, theories are by their nature general statements that we can use to explain specific events. In this view, "Diamond resources caused Sierra Leone's civil war" is a case-specific explanation; "Natural resource endowments cause civil war" is a theoretical formulation. 

In our treatment of theory as a lower-level causal model, however, there is no generic sense in which a theory is more or less general than the higher-level claim that it explains. In this book's framework, we _can_ theorize by generalizing: when we elaborate a model by building in variation in a factor that was held constant in the higher-level claim, we are making the model more general in scope. If our natural resources claim implicitly applies only to weak states, we can theorize this claim by allowing state strength to vary and articulating how the natural-resource effect hinges on that claim. 

However, when we theorize by disaggregating nodes---say, by adding intervening causal steps---we have in fact made a more _specific_ claim. Natural resources may cause civil war under a broad set of circumstances. Natural resources will cause civil war *through looting by rebel groups* under an almost certainly narrower set of circumstances. Here, the more elaborate argument---the theorization of *why* $X$ causes $Y$---is actually a stronger claim, with narrower scope, than the simpler one that it supports. 

**The value of parsimony** @Van-Evera:1997 and @przeworski1970logic also express a common view in characterizing _parsimony_ as a quality of good theory. While they recognize that parsimony must often be traded off against other goods, such as accuracy and generality, _ceteris paribus_ a more parsimonious theory---one that uses fewer causal variables to explain variation in a given outcome---is commonly understood to be a better theory. 

We do not take issue with the idea that simpler models and explanations are, all else equal, better. But the succeeding chapters also demonstrate a distinctive and important way in which all else will often not be equal when we seek to use theory to guide research design and support causal inference. To foreshadow the argument to come, the elaboration of more detailed, lower-level models can direct us to new opportunities for learning. As we unpack a higher-level claim, we will often be identifying additional features of a phenomenon the observation of which can shed light on causal questions of interest. Moreover, our background beliefs---the prior knowledge on which causal inference must usually rest---are often more informative at lower levels than at higher levels: it will, for instance, often be easier for us express beliefs about causal effects for smaller steps along a causal chain than about an overarching $X \rightarrow Y$ effect. 

Making things more complicated, of course, still makes things more complicated. And we should avoid doing so when the payoff is small, as it will sometimes be. But in the pages to come, we will also see a distinct set of benefits that arise from drilling more deeply into our basis of prior knowledge when formulating inferential strategies.

<!-- , with these claims,  perhaps derived or inspired from some model via a statement that the model represents the world faithfully for some purpose.  -->

<!-- The key difference as we see it is between representations of a system, a model, and claims that the model itself represents another system---the world---in some ways. The difference betw  -->



## Appendix:  Illustration of a Mapping from a Game to a DAG

Our running example supports a set of higher level models, but it can also  be *implied* by a lower level models. Here we illustrate with an example in which the lower level model is a game theoretic model, together with a solution.^[Such representations have been discussed as multi agent influence diagrams, for example in @koller2003multi or @white2009settable on "settable systems"--- an extension of the "influence diagrams" described by @dawid2002influence.] 

In Figure \ref{fig:tree} we show a game in which nature first decides on the type of the media and the politician -- is it a media that values reporting on corruption or not? Is the politician one who has a dominant strategy to engage in corruption or one who is sensitive to the risks of media exposure? In the example the payoffs to all players are fully specified, though for illustration we include parameter $b$ in the voter's payoffs which captures utility gains from sacking a politician that has had a negative story written about them *whether or not they actually engaged in corruption*. A somewhat less specific, though more easily defended, theory would not specify particular numbers as in the figure, but rather assume ranges on payoffs that have the same strategic implications.  

The theory is then the  game plus a solution to the game. Here for a solution the theory specifies subgame perfect equilibrium.

In the subgame perfect  equilibrium of the game; marked out on the game tree (for the case  $b=0$) the sensitive politicians do not engage in corruption when there is a free press -- otherwise they do; a free press writes up any acts of corruption, voters throw out the politician if indeed she is corrupt and this corruption is reported by the press.  

As with any structural model, the theory says what will happen but also what *would* happen if things that should not happen happen. 

```{r game1, echo=FALSE, fig.width = 15, fig.height = 12, fig.cap = "\\label{fig:tree} A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which  $b=0$."}

H <-  matrix(c(rep("O", 32), 
rep("X=1, S=1", 8), rep("X=1, S=0", 8), rep("X=0, S=1", 8), rep("X=0, S=0",8 ),
rep(rep(c("C","NC"), each = 4 ), 4),
rep(rep(c("R","NR"), each = 2 ), 8),
rep(c("Y","NY"), 16)), 
 32)[32:1,]

in.history = function(action) rowSums(H==action)>0

P <- cbind(rep(1, 32), 
           rep(2, 32), 
           rep(3, 32), 
           rep(4, 32))[32:1,]
U <- matrix(NA, 32, 4)
U[,2] <- in.history("C") -   
          2*in.history("Y") + 
          2*(in.history("X=0, S=0")+ in.history("X=1, S=0"))*in.history("C") 


# Media gains only when it does reliable story 
U[,3] <- in.history("NR") +   
          2*in.history("R")*in.history("C")*(in.history("X=1, S=0")+ in.history("X=1, S=1")) 


# Voters prefer firing if reports on corrupt politician
U[,4] <- in.history("NY") +   
          2*in.history("Y")*in.history("C")*in.history("R") 

        
gt_tree(H,U,P, player.names = c("Nature", "Gov", "Media", "Voters"),         
  mark.branches=((ncol(H)-1):2),
  print.utilities = c(FALSE, TRUE, TRUE, TRUE),
  force_solution = TRUE, warnings = FALSE)

text(6.6, (1:32)[in.history("Y") & in.history("R")]- .02, expression(italic(+b)) , cex = 1.2) 

```



To draw this  equilibrium as a DAG we include nodes for every action taken, nodes for features that determine the game being played, and the utilities at the end of the game. 

If equilibrium claims are justified by claims about the beliefs of actors then these could also appear as nodes. To be clear however these are not required to represent the game  or the equilibrium, though they can capture assumed logics underlying the equilibrium choice. For instance a theorist might claim that humans are wired so that whenever they are playing a "Stag Hunt" game they play "defect." The game and this solution can be represented on a DAG without reference to the  beliefs of actors about the action of other players. However, if the *justification* for the equilibrium involves optimization given the beliefs of other players, a lower level DAG could represent this by having a node for the  game description that points to beliefs about the actions of others, that then points to choices. In a game with dominant strategies, in contrast, there would be no arrows from these beliefs to actions.

For our running example, nodes could usefully include the politician's expectations, since the government's actions depend on expectations of the actions of others. However, given the game there is no gain from  including the media's expectations of the voter's actions since in this case the media's actions do not depend on expectations of the voters actions then these expectations should be included.  

In Figure \ref{fig:gamedag} we provide two examples of DAGs that illustrate lower level models that support our running example. 

The upper  graph gives a DAG reflecting equilibrium play in the game described in Figure \ref{fig:tree}. Note that in this game there is an arrow between $C$ and $Y$ even though $Y$ does not depend on $C$ for some values of $b$---this is because conditional independence requires that two variables are independent for *all* values of the conditioning set. For simplicity also we mark $S$ and $X$, along with $b$ as features that affect which subgame is being played---taking the subgames starting after Nature's move. Note that the government's expectations of responses by others matters, but the expectations of other players do not matter given this game and solution. Note that the utilities appear twice in a sense. They appear in the subgame node, as they are part of the definition of the game--though here they are the utilities that players expect at each terminal node; when they appear at the end of the DAG they are the utilities that actually arise (in theory at least). 

The lower level DAG  is very low and much more general, representing the theory that in three player games of complete information, players engage in backwards induction and choose the actions that they expect to maximize utility given their beliefs about the actions of others. The DAG assumes that players know what game is being played ("Game"), though this could also be included for more fundamental justification of behavioral predictions. Each action is taken as a function of the beliefs about the game, the expectations about the actions of others, and knowledge of play to date. The functional equations---not shown---are given by optimization and belief formation assuming optimization by others.  


```{r, echo = FALSE, fig.width = 12, fig.height = 10, out.width='\\textwidth', fig.cap = "\\label{fig:gamedag} The upper panel shows a causal graph that describes  relations between nodes suggested by analysis of  the  game  in Figure \\ref{fig:tree} and which can imply the causal graph of  Figure \\ref{fig:running}. The game itself  (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors.   The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''", fig.align="center", warning = FALSE}

par(mfrow = c(2,1))
par(mar=c(1,1,3.5,1))


x = c(0, 1, 2, 2,  3,  3, 4, 5)
y = c(0, 0, 2, -2, 2, -2.5, -2, 0)

names = c("S, X, b",                                        #1 
          "Subgame",                                           #2 
          "E: Gov's Beliefs\nabout responses by\n Media and Voters",    #3
          "Corruption",                                       #4
          "",            #5
          "Report",                                       #6
          "Remove\nGovernment",                                       #7
          "Utilities"                          #8
)

hj_dag(x =  x,
       y = y,
       names = c(names, " ", " "),
       arcs = cbind( c(1,rep(2,5)  ,3, c(4,6,7),  4, 4, 6),
                     c(2,3:4, 6:8,      4,  rep(8, 3), 6, 7, 7)),
       title = "Lower DAG: Backwards induction in a game with 3 players  with one  move  each",
       contraction = .22,
       padding = .5)



x = c(0, 1, 2, 2,  3,  3, 4, 5)
y = c(0, 0, 2, -2, 2, -2.5, -2, 0)

names = c("Context",                                        #1 
          "Game",                                           #2 
          "1's Beliefs\nabout actions \n 2|1 and 3|2,1",    #3
          "Action 1",                                       #4
          "2's Beliefs\nabout actions \n 3|2,1",            #5
          "Action 2",                                       #6
          "Action 3",                                       #7
          "Utilities"                          #8
)

hj_dag(x =  x,
       y = y,
       names = c(names, " ", " "),
       arcs = cbind( c(1,rep(2,6)  ,3, 5, c(4,6,7),  4, 4, 6),
                     c(2,3:8,      4, 6, rep(8, 3), 6, 7, 7)),
       title = "Still lower: Backwards induction, 3 player game with one  move for each player",
       contraction = .2,
       padding = .5)




```

These lower level graphs can themselves provide clues for assessing relations in the higher level graphs. For instance, the lower level model might specify that the value of $b$ in the game affects the actions of the government only through their beliefs about the behavior of voters, $E$. These beliefs may themselves have a stochastic component, $U_E$. Thus  $b$ high  might be thought to reduce the effect of media on corruption. For instance if $b \in \mathbb{R}_+$, we have $C= 1-FG(1-\mathbb{1}(b>1))$. If $X$ is unobserved and one is interested in whether $S=0$ caused corruption, knowledge of $b$ is informative. It is a root node in the causal estimand. If $b>1$ then $S=0$ did not cause corruption. However if $b$ matters only because of its effect on $E$ then the query depends on $U_E$.  In this case, while knowing $b$ is informative about whether $S=0$ caused $C=1$, knowing $E$ from the lower level graph is more informative.

Note that the  model we have examined here involves no terms for $U_C$, $U_R$ and $U_Y$---that is, shocks to outcomes given action. Yet clearly any of these could exist. One could imagine a version of this game with "trembling hands," such that errors are always made with some small probability, giving rise to a much richer set of predictions.  These can be  represented in the game tree as moves by nature between actions chosen and outcomes realized. Importantly in a strategic environment such noise could give rise to different types of conditional independence. For instance say that a Free Press only published its report on corruption with  probability $\pi^R$, then with $\pi^R$ high enough the sensitive government might decide it is worth engaging in corruption even if there is a free press; in this case the arrow from $X$ to $C$ would be removed. Interestingly in this case as the error rate rises, $R$ becomes less likely, meaning that the effect of a $S$ on $Y$ becomes gradually weaker (since governments that are not sensitive become  more likely to survive) and then drops to 0 as sensitive governments start acting just like nonsensitive governments. 

<!--chapter:end:03-theory-as-causal-models.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Causal Questions {#questions}

***

Although a lot  of empirical work focuses on identifying average causal effects, there is a rich array of other well defined causal questions that can be asked about how variables relate to each other causally. We decribe major families of question and illustrate how these can all be described as questions about the values of nodes in a causal model.

***

```{r, include = FALSE}
source("_packages_used.R")
```

The study of causation is central to most empirical social science, whether quantitative analyses of large sets of cases or qualitative, small-$N$ case studies. Yet a general interest in causality masks tremendous heterogeneity in the kinds of causal questions that scholars tend to ask. 

Returning to our inequality and democratization example, we might seek, for instance, to know inequality's average impact on democratization across some set of cases. Alternatively, we might be interested in a particular case---say, Mongolia in 1995---and want to know whether this is a context in which inequality has an effect---a question about causal effects at the case level. Relatedly---but distinctly---we might wonder whether the level of democracy in Mongolia in 1995 is causally attributable to the level of inequality in that case. And we may be interested in _how_ causal effects unfold, inquiring about the pathway or mechanism through which inequality affects democratization---a question we can also ask at two levels. We can ask whether inequality affected democratization in Mongolia through mobilization of the masses; and we can ask how commonly inequality affects democratization through mobilization across a broad set of cases.

Rather separate methodological literatures have been devoted to the study of average causal effects, the analysis of case-level causal effects and explanations, and the identification of causal pathways. It is typically understood that their analysis requires quite distinct sets of tools. In this chapter, we take a key integrative step in showing that each of these queries can be readily captured in a causal model. More specifically, we demonstrate how causal queries can be represented as question about one or more _nodes_ on a causal graph. When we assimilate our causal questions into a causal model, we are placing what we want to know in formal relation to both what we _already_ know and what we can potentially _observe_. As we will see in later chapters, this move allows us then to deploy the model to generate strategies of inference: to determine which observations, if we made them, would be likely to yield the greatest leverage on our query, given our prior knowledge about the way the world works. And by the same logic, once we see the evidence, this integration allows us to  "update" on our query---figure out in systematic fashion what we _have_ learned---in a manner that takes background knowledge into account.

In the remainder of this chapter, we walk through the conceptualization and causal-model interpretation of five key causal queries:

* Case-level causal effects

* Case-level causal attribution

* Case-level explanation

* Average causal effects

* Causal pathways

These five are not exhaustive of the causal questions that can be captured in causal graphs, but they are among the more common foci of social scientific investigation.


<!-- * What is the average effect of a given increase in inequality is on the level of democracy for a given population of cases? -->

<!-- * Case-level causal effects: What is the effect of $X$ on $Y$ in a given case? -->

<!-- * Causal attribution: Did the condition $X$, present in a case, cause the outcome, $Y$, that occurred in that case? -->

<!-- * Actual causes: Which of the antecedent conditions present in the case either was a counterfactual cause of the outcome or *could* have been a counterfactual cause given the way in which events actually played out? -->

<!-- * Average causal effects: What is the mean effect of $X$ on $Y$ across a population of cases? -->

<!-- * Causal pathways: How did $X$ exert its effect on $Y$ in a case? How does $X$ affect $Y$ in a population of cases? -->

<!-- Some causal questions involve realized values of variables only, some involve counterfactual statements, and some involve combinations of these.  -->

 <!-- In what follows we advocate an approach in which causal questions --- which we term *queries* --- can be defined as questions about the *values of exogenous nodes on a causal graph*, including unobservable $U$ terms. ^[With some abuse of notation we use $Q$ generically to refer to the query itself and the the set of variables whose values determine the query. Thus a query may be written as the random variable $Q =\mathbb{1}((u_X = 1) \& (u_Y = 0))$, which takes on a value $q=1$ if both $u_X = 1$ and $u_Y = 0$ and 0 otherwise. Assessing this query requires understanding the values of particular roots, or query nodes, $\{U_X, U_Y\}$ which we also refer to as $Q$.]  Addressing a given causal question then involves using data on observed features of a graph to make inferences about those unobserved or unobservable features of the graph that define the query. These inferences are, of course, always conditional on the graph itself.  -->

<!-- (a) uncertainty about causal questions is represented as uncertainty about  and (b)  -->

<!-- THIS ALL SEEMS LIKE NUANCE WE DON'T REALLY NEED HERE.

In this framework, inferences about causation amount to inferences about the *context* that a case is in: that is, whether conditions in the case (the relevant exogenous-variable values) are such that a given causal effect, causal pathway, etc. would have been operating. We can translate questions about causation into questions about context because, in a structural causal model, the values of all exogenous variables are sufficient to determine the value of all endogenous nodes: context determines outcomes. This further implies that, for any manipulation of an exogenous or endogenous variable, there exist one or more exogenous nodes on the graph that suffice to determine the effect on all endogenous variables in the graph: context determines *effects*. Likewise, the settings on the model's exogenous variables determine the pathway(s) through which one variable in the model will affect another. -->

<!-- It is important to note a difference between this formulation and the conceptualization of causality typically employed in the potential outcomes framework. We characterize causal inference as learning about a unit *as it is*, conditional on a causal model, rather than learning about the unit as it is and as it could be. Suppose, for instance, that in a causal model a car will start if it has gas and if the key is turned.^[A version of this example is in @darwiche1994symbolic.] In a standard potential outcomes setup, the question "Does turning the key cause the car to start?" is equivalent to asking, "Would the car start if the key is turned?" and "Would the car start if the key is not turned?" In our model-based framework, the question of the key-turning's causal effect is somewhat differently framed as a question about an exogenous variable: "Does the car have gas?" In the model-based framework, then, our query becomes a question about the state of affairs in the case---about the case's *context*---rather than a pair of factual and counterfactual questions about outcomes with and without treatment. These two framings are fully consistent with one another, and counterfactual reasoning is no less important in the model-based framework; it has simply been displaced to the causal model, which encodes all counterfactual relations. -->
<!-- <!-- moreover this can always be done formally, even if the causal model contains no additional assumptions about the causal process.      --> -->

## Causal queries

### Case-level causal effects

The simplest causal question is whether some causal effect operates in an individual case. Does $X$ have an effect on $Y$ in this case? For instance, is Yemen in 1995 a case in which a change in economic inequality would produce a change in whether or not the country democratizes? We could put the question more specifically as a query about a causal effect in a particular direction, for instance: Does inequality have a positive effect on democratization in the case of Yemen in 1995?

In counterfactual terms, a query about case-level causation is a question about what would happen if we could manipulate a variable in the case: if we could hypothetically manipulate $X$'s value in the case, would $Y$'s value also change? To ask whether a positive (or negative) effect operates for a case is to ask whether a particular counterfactual relation holds in that case. If we assume a binary setup for simplicity, to ask whether inequality has a positive effect on democratization is to ask: if we set $I$ to $0$ would $D$ take on a value of $0$, _and_ if we set $I$ to $1$, would $D$ take on a value of $1$? (_Both_ of these conditions must hold for $I$ to have a positive effect on $D$.)

<!-- The closely connected question of causal attribution [@yamamoto2012understanding] asks: did $X$ cause $Y$'s value in this case? -->

We can easily represent this kind of query in the context of a causal model. We show the DAG for such a model in Figure \ref{fig:casequery}. As introduced in Chapter \@ref(theory), $\theta^Y$ here represents the causal type characterizing $Y$'s response to $X$ and, if $X$ and $Y$ are binary, can take on one of four values: $\theta^Y_{10}$, $\theta^Y_{01}$, $\theta^Y_{00}$, and $\theta^Y_{11}$ (which map onto our original $a, b, c$ and $d$ types). Importantly, given that the value of nodes (or variables) is allowed to vary across cases, this setup allows for $\theta_Y$---the causal effect of $X$ on $Y$---to vary across cases. Thus, $X$ may have a positive effect on $Y$ in one case (with $\theta^Y=\theta^Y_{01}$), $X$ may have a negative ($\theta^Y=\theta^Y_{10}$) or no effect ($\theta^Y=\theta^Y_{00}$ or $\theta^Y_{11}$) on $Y$ in other cases.

<!-- Consider again our four causal types, above. In this setup, $X$'s causal effects on $Y$ can vary across cases. We can readily translate this setup, in which different cases have different causal effects, into a structural causal model. We can do so by letting $Y$ be a function both of $X$ and of a *causal-type variable* that encodes potential outcomes, a variable that we will denote as $Q$. We represent this simple model graphically in Figure \ref{fig:DAGtypes}. Here $Q$ can be thought of as variable that conditions the effect of $X$ on $Y$.  -->

<!-- We then need to specify the values that $Q$ can take on, $Q$'s range. With a binary causal variable of interest, we can write down a value of $Q$ as $q_{ij}$. The pair of subscripts simply conveys the type's potential outcomes: $i$ represents the value that $Y$ takes on if $X=0$ while $j$ represents the value that $Y$ takes on if  $X=1$. Thus, in a binary framework, $Q$ can take on four values, corresponding to our original four types: $q_{00}$ (a $c$ type), $q_{10}$ (an $a$ type), $q_{01}$ (a $b$ type) and $q_{11}$ (a $d$ type). This setup also allows us to write down a simple, closed-form functional equation for $Y$ in terms of its parents, $X$ and $Q$: $Y(x,q_{ij}) =  i(1-x) + jx$.^[To generate this closed-form function, we decompose $q_{ij}$ into its component parts, $i$ and $j$. Note that there is no loss of generality in the functional form linking $X$ and $Q$ to $Y$. In a causal model framework, the structural equations, such as those linking $X$ and $Y$ conditional on another node, can be entirely non-parametric.] -->

```{r, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:casequery} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\\theta^Y$. With a single binary causal variable of interest, we let $\\theta_Y$ take on values $\\theta^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\\theta^Y$ ranges over the four values: $\\theta^Y_{00}$, $\\theta^Y_{10}$, $\\theta^Y_{01}$ and $\\theta^Y_{11}$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 1),
       y = c(1, 1, 2),
       names = c(
                expression(paste(X)),
                expression(paste(Y)),
                expression(paste(theta^Y))),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "A DAG Capturing a Case-Level Causal Effect",
       padding = .4, contraction = .15)

```



<!-- Let $\lambda_1^Q$ denote a multinomial distribution over these four values and let t -->

In this model, then, the query, "What is $X$'s causal effect in this case?" simply becomes _a question about the value of $\theta_Y$_. 

Of course, $\theta_Y$ is not directly observable: causal types are intrinsically unobserved properties of cases. So, as we will see in later chapters, research design becomes a challenge of determining which _observable_ nodes in the graph are potentially informative about the unobservable nodes that constitute our causal queries. 

<!-- We note that, in this discussion, we are employing a more generic property of causal graphs. In a graph of the general form $X \rightarrow Y \leftarrow Z$, the effect of $X$ on $Y$ in a case will depend on the value of $Z$ in that case. $Z$ in this structure might be a random disturbance term, $U_Y$, or a variable with a substantive interpretation. Either way, where a node has multiple parents, we should generally conceive of the parents as exerting their effects interactively. There are special situations in which $X$'s effect will not depend on the value of $Z$. For instance, if $Z$ operates only additively on $Y$ (say, $Y=X+Z$) and $Y$ is not bounded, then $Z$ is irrelevant to $X$'s causal effect, which will be homogeneous across cases and fixed by the model. But, in general, the causal effect of a parent on its child will depend on the value(s) of the other parent(s) (its spouse(s)).^[Nodes that share a child are spouses.] In this sense, for a given $X \rightarrow Y$ relationship, any other parents of $Y$ can be thought of as causal-type variables; these are the variables that define $X$'s case-level causal effect. Put differently, learning about the case-level effect of a causal variable on an outcome means learning about the outcome's other cause(s). -->

<!-- Note also that, in the above illustration, the variable $Q$ is not specified in substantive terms; it is a carrier for causal information. However, social scientific theories commonly use substantive concepts as causal-type variables. The effect of fiscal stimulus on economic growth is theorized to depend on the unemployment rate; the effect of public opinion on policy is held to depend on institutional arrangements; the effect of natural resources on civil war might depend on the level of economic development. Any time one variable moderates the influence of another, the two variables operate as causal-type nodes for one another's effects on the outcome. Later in this chapter and in other chapters, we work with further examples in which the exogenous variables that define a query have a stronger substantive interpretation.    -->

<!-- More generally, work in graphical models defines the causal effect of $X$ on $Y$ in terms of the changes in $Y$ that arise from interventions on $X$. For example, using the notation for interventions given above we can describe the effect of a change in $X$ from $x'$ to $x''$ on the probability that $Y=1$ in unit $i$ as: -->


### Case-level causal attribution

A query about causal attribution is related to, but different from, a query about a case-level causal effect. When asking about $X$'s case-level effect, we are asking, "*Would* a change in $X$ cause a change in $Y$ in this case?" The question of causal attribution is slightly different: "*Did* $X$ cause $Y$ to take on the value it did in this case?" More precisely, we are asking, "Given the values that $X$ and $Y$ _in fact_ took on in this case, would $Y$'s value have been different if $X$'s value had been different?" 

For instance, given that we know that inequality in Taiwan was relatively low and that Taiwan democratized in 1996, was  low inequality a _cause_ of Taiwan's democratization in 1996? Put differently, given low economic inequality and democratization in Taiwan in 1996, would the outcome in this case have been different if inequality had been high?

This goes beyond simply asking whether Taiwan is a case in which inequality has a causal effect on democratization. Whereas a case-level causal effect is defined in terms of a single $\theta$ node, we define a causal-attribution query in terms of a larger set of nodes. To attribute $Y$'s value in a case to $X$, we need to know not only whether this is the kind of case in which $X$ could have an effect on $Y$ but also whether the context is such that $X$'s value *in fact* made a difference. 

Consider, for instance, the general setup in Figure \@ref(fig:attribquery). Here, $Y$ is a function of two variables, $X$ and $W$. This means that $\theta^Y$ is somewhat more complicated than in a setup with one causal variable: $\theta^Y$ must here define $Y$'s response to different combinations of two other variables, $X$ and $W$, since _both_ of these variables point directly into $Y$. Thus, $\theta^Y$ must cover the full set of possible causal interactions between two binary causal variables. 



```{r attribquery, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:attribquery} This DAG is a graphical representation of the simple causal setup in which $Y$ depends on two variables $X1$ and $X2$. How $Y$ responds to X1 and X2 depnds on $\\theta^Y$, the DAG itself does not provide information on whether or how X1 and X2 interact with each other."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 0, 1, 1),
       y = c(2, 0, 1, 2),
       names = c(
                expression(paste(X1)),
                expression(paste(X2)),
                expression(paste(Y)),
                expression(paste(theta^Y))),
       arcs = cbind( c(1, 4, 2),
                     c(3, 3, 3)),
       title = "A DAG Capturing Causal Attribution",
       padding = .4, contraction = .15)

```

We already saw the set of causal types for a set up like this in Chapter 2 (see Table \@ref(tab:PO16)).  In the table, there are four column headings representing the four possible combinations of $X1$ and $X2$ values. Each row represents one possible pattern of $Y$ values as $X1$ and $X2$ move through their four combinations. 

Labelling is a little difficult with so many types. One approach used  in Chapter \@ref(models) is to represent change in $X1$ on the horizontal axis, and change in the second variable, $X2$, on the vertical axis. The value of $X1$ increases from 0 to 1 as we move to the _right_ (from $i$ to $j$ or from $g$ to $h$). And the value of $X2$ increases from 0 to 1 as we move _up_ (from $i$ to $g$ or from $j$ to $h$).

One way to conceptualize the size of the causal-type "space" is to note that $X1$ can have any of our four causal effects (the four binary types) on $Y$ when $X2=0$; and $X1$ can have any of four causal effects when $X2=1$.^[This is precisely equivalent to noting that $X2$'s effect on $Y$ can be of any of the four types when $X1=0$ and of any of the four types when $X1=1$.] This yields 16 possible response patterns to combinations of $X1$ and $X2$ values. 
<!-- Thus, for instance, $\theta_{00}^{10}$ (type 5) describes a response pattern in which $W$ has a positive effect on $Y$ when $X=0$ but has no effect, with $Y$ stuck at $0$, when $X=1$; and in which $X$ has no effect when $W=0$ and a negative effect when $W=1$. For $\theta_{11}^{00}$ (type 11), $W$ has a negative effect on $Y$ regardless of $X$'s value; and $X$ has no effect regardless of $W$'s value. -->

<!-- \begin{table}[h!] -->
<!--   \centering -->
<!--   \def\arraystretch{1.3} -->
<!--     \begin{tabular}{ccccccc} -->
<!--     \hline -->
<!--     \textbf {} & \textbf {Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $\\ -->
<!--          & & $W=0)$ & W=1)$ & $W=0)$ & $W=1)$ \\  \hline -->
<!--     1 & $\theta_{00}^{00}$ 			&  0     & 0     & 0     & 0  \\ -->
<!--     2 & $\theta_{00}^{01}$ 	& 0     & 0     & 0     & 1 \\ -->
<!--     3 & $\theta_{01}^{00}$ 	& 0     & 0     & 1     & 0 \\ -->
<!--     4 & $\theta_{01}^{01}$ 			& 0     & 0     & 1     & 1 \\ -->
<!--     5 & $\theta_{00}^{10}$ 	& 0     & 1     & 0     & 0 \\ -->
<!--     6 & $\theta_{00}^{11}$ 			& 0     & 1     & 0     & 1 \\ -->
<!--     7 & $\theta_{01}^{10}$ 	& 0     & 1     & 1     & 0 \\ -->
<!--     8 & $\theta_{01}^{11}$ 		& 0     & 1     & 1     & 1 \\ -->
<!--     9 & $\theta_{10}^{00}$			& 1     & 0     & 0     & 0 \\ -->
<!--     10 & $\theta_{10}^{01}$ 	& 1     & 0     & 0     & 1 \\ -->
<!--     11 & $\theta_{11}^{00}$			& 1     & 0     & 1     & 0 \\ -->
<!--     12 & $\theta_{11}^{01}$		& 1     & 0     & 1     & 1 \\ -->
<!--     13 & $\theta_{10}^{10}$			& 1     & 1     & 0     & 0 \\ -->
<!--     14 & $\theta_{10}^{11}$		& 1     & 1     & 0     & 1 \\ -->
<!--     15 & $\theta_{11}^{10}$		& 1     & 1     & 1     & 0 \\ -->
<!--     16 & $\theta_{11}^{11}$			& 1     & 1     & 1     & 1 \\ -->
<!--     \bottomrule -->
<!--     \end{tabular}% -->
<!--    \caption{The table defines the 16 values (causal types) that $\theta^Y$ can take on, given a binary $X$ and $W$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies.} -->
<!--   \label{tab:types2x}% -->
<!-- \end{table} -->


A query about causal attribution---whether $X1 = 1$ caused $Y=1$--for the model in in Figure \@ref(fig:attribquery), would be defined in terms of both $X2$ and $\theta_Y$. Parallel to our Taiwan example, suppose that we have a case in which $Y=1$ and in which $X1$ was also 1, and we want to know whether $X1$ caused $Y$ to take on the value it did. Answering this question requires knowing whether the case's type is such that $X1$ would have had a positive causal effect on $Y$, _given the value of $X2$_ (which we might think of as the context). Thus, given that we start with knowledge of $X1$'s and $Y$'s values, our query about causal attribution amounts to a query about two nodes on the graph: (a) the value of $X2$ and (b) whether the value of $\theta^Y$ is such that $X1$ has a positive causal effect given $X2$'s value.

Suppose, for instance, that we were to observe $X2=1$. We then need to ask whether the causal type, $\theta_Y$, is such that $X1$ has a positive effect when $X2=1$. Consider type 8, or $\theta_{01}^{11}$. This is a causal type in which $X1$ has a positive effect when $X2=0$ but no effect when $X2=1$. Put differently, $X2=1$ is a sufficient condition for $Y=1$, meaning that $X1$ makes no difference to the outcome when $X2=1$. 

<!-- Looking down the table, we can readily identify the causal types that qualify by focusing on the two superscripts---which provide responses to $X$ when $W=1$---and looking for a $01$ in that upper row.  -->
In all we have  four qualifying types: $\theta_{00}^{01}$, $\theta_{01}^{01}$, $\theta_{10}^{01}$, and $\theta_{11}^{01}$ (or 2, 4, 10, and 12).  In other words, we can attribute a $Y=1$ outcome to $X1=1$ when $X2=1$ and the causal type is one of these four. By parallel reasoning, we can also attribute a $Y=1$ outcome to $X1=1$ when $X2=0$ and the causal type is any of $\theta_{01}^{00}$, $\theta_{01}^{01}$, $\theta_{01}^{10}$, and $\theta_{01}^{11}$. 

Thus, a question about causal attribution is a question about the *joint* value of a set of nodes: about whether the _combination_ of context and causal type is such that changing $X$ would have changed the outcome.


### Case-level explanation

So far we have been dealing with causes in the standard counterfactual sense: antecedent conditions a change in which would have produced a different outcome. Sometimes, however, we are interested in identifying antecedent conditions that were not counterfactual difference-makers but that nonetheless _generated_ or _produced_ the outcome. Consider, for instance, a situation in which an outcome was overdetermined: multiple conditions were present, each of which on their own, _could_ have generated the outcome. Then none of these conditions caused the outcome in the counterfactual sense; yet one or more of them may have been distinctively important in *producing* the outcome. The concept of an *actual cause* may be useful in putting a finer point on this kind of causal question.  

Let us first approach the concept at an intuitive level. An antecedent condition, $A$, that played a role in generating an outcome might not be a counterfactual cause because, had it not occurred, some second chain of events set in motion by $B$ would have unfolded, generating the outcome anyway. In the standard counterfactual scenario, $A$ is not a counterfactual cause: take away $A$ and the outcome still happens because of the chain of events emanating from $B$. Yet let us imagine that the fact that $A$ _did_ occur _prevented_ part of $B$'s chain of consequences from unfolding and itself producing the outcome. Then let us imagine a tweaked counterfactual comparison in which we *fix* the observed fact that $B$'s causal sequence did not fully unfold. We can then ask: *conditional on $B$'s sequence not fully unfolding*, would $A$ have been a counterfactual cause of the outcome? If so, then we say that $A$ is an "actual cause"" of the outcome. We have, in a sense, identified $A$ as distinctively important in the production of the outcome, even if it was not a case-level cause in the usual sense.

More formally, and using the definition provided by [@halpern2015modification], building on [@halpern2005causesa] and others, we say that a condition ($X$ taking on some value $x$) was an *actual cause* of an outcome (of $Y$ taking on some value $y$), where $x$ and $y$ may be collections of events, if:

1. $X=x$ and $Y=y$ both happened
2. there is some set of variables, $\mathcal W$, such that if they were fixed at the levels that they actually took in the case, and if $X$ were to be changed, then $Y$ would change (where $\mathcal W$ can also be an empty set)
3. no strict subset of $X$ satisfies 1 and 2 (there is no redundant part of the condition, $X=x$)

The definition thus describes a condition that *would* have been a counterfactual cause of the outcome if we were to imagine holding constant some set of events that in fact occurred (and that, in reality, might not have been constant if the actual cause had not in fact occurred).

A motivating example used in much of the literature on actual causes [e.g.  @hall2004two] imagines  two characters, Sally and Billy, simultaneously throwing stones at a bottle. Both are great shots and hit whatever they aim at. Sally's stone hits first, and so the bottle breaks. However, Billy's stone *would* have hit had Sally's not hit, and would have broken the bottle. Did Sally's throw cause the bottle to break? Did Billy's?

By the usual definition of causal effects, neither Sally's nor Billy's action had a causal effect: without either throw, the bottle would still have broken. We commonly encounter similar situations in the social world. We observe, for instance, the onset of an economic crisis and the breakout of war---either of which would be sufficient to cause the government's downfall---but with the economic crisis occurring first and toppling the government before the war could do so. Yet neither economic crisis nor war made a difference to the outcome.

To return to the bottle example, while neither Sally's nor Billy's throw is a counterfactual cause, there is an important sense in which Sally's action obviously broke the bottle, and Billy's did not. This intuition is confirmed by applying the definition above. Consider first the question: Did Sally's throw break the bottle? Conditions 1 and 3 are easily satisfied, since Sally \emph{did} throw and the bottle \emph{did} break (Condition 1), and "Sally threw" has no strict subsets (Condition 3).

Condition 2 is met if Sally's throw made a difference, counterfactually speaking; and in determining this, we are permitted to condition on (to fix in the counterfactual comparison) any event or set of events that actually happened (or on on none at all). To see why Condition 2 is satisfied, we have to think of there being three steps in the process: Sally and Billy throw, Sally's or Billy's rock hits the bottle, and the bottle breaks. In actuality, Billy's stone did not hit the bottle. And conditioning on this actually occurring event (Billy's stone not hitting), the bottle would *not* have broken had Sally not thrown. From the perspective of counterfactual causation, it may seem odd to condition on Billy's stone not hitting the bottle when thinking about Sally not throwing the stone since Sally's throwing the stone was the very thing that prevented Billy from hitting the bottle. Yet Halpern argues that this is an acceptable thought experiment for establishing the importance of Sally's throw since conditioning is constrained to the actual facts of the case. Moreover, the same logic shows why Billy is not an actual cause. The reason is that Billy's throw is only a cause in those conditions in which Sally did not hit the bottle. But because Sally \emph{did} actually hit the bottle, we are not permitted to condition on Sally not hitting the bottle in determining actual causation. We thus cannot---even through conditioning on actually occurring events---construct any counterfactual comparison in which Billy's throw is a counterfactual cause of the bottle's breaking.

The striking result here is that there can be grounds to claim that a condition was the actual cause of an outcome even though, under the counterfactual definition, the effect of that condition on the outcome is 0. (At the same time, all counterfactual causes are automatically actual causes; they meet Condition 2 by conditioning on nothing at all, an empty set $\mathcal W$.) One immediate methodological implication follows: since actual causes need not be causes, there are risks in research designs that seek to understand causal effects by tracing back actual causes---i.e., the way things actually happened. If we traced back from the breaking of the bottle, we might be tempted to identify Sally's throw as the cause of the outcome. We would be right only in an actual-causal sense, but wrong in the standard, counterfactual causal sense. Chains of events that appear to "generate" an outcome are not always causes. ^[Perhaps more surprising, it is possible that the expected causal effect is negative but that $X$ is an actual cause in expectation. For instance, say that 10% of the time Sally's shot intercepted Billy's shot but without hitting the bottle. In that case the average causal effect of Sally's throw on bottle breaking is $-0.1$ yet 90% of the time Sally's throw is an actual cause of bottle breaking (and 10% of the time it is an actual cause of non-breaking). For related discussions see @menzies1989probabilistic.]

As with other causal queries, the question "Was $X=x$ the actual cause of $Y=y$?" can be redefined as a question about which values for exogenous nodes produce conditions under which $X$ could have made a difference. To see how, let us run through the Billy and Sally example again, but formally in terms of a model. Consider Figure \ref{fig:actualquery}, where we represent Sally's throw ($S$), Billy's throw ($B$), Sally's rock hitting the bottle ($H^S$), Billy's rock hitting the bottle ($H^B$), and the bottle cracking ($C$). Each endogenous variable has a $\theta$ term associated with it, capturing its response to its parents. We capture the possible "preemption" effect with the arrow pointing from $H^S$ to $H^B$, allowing that whether Sally's rock hits to affect whether Billy's rock hits. 

Let us again imagine that Sally threw ($S=1$), Billy threw ($B=1$), and the bottle cracked ($C=1$). Let us say that $\theta^{H^B}$ takes on a value such that (a) $H^B=0$ whenever $H^S=1$ (Sally's hit preempts Billy's) and (b) $B$ has a positive effect on $H^B$ when $H^S=0$  (Billy's throw hits if Sally's doesn't). Further, assume that $S$ has a positive effect on $H^S$. Let us finally posit that $\theta^C$ takes on a value such that $C=1$ if $H^B$ equals $1$.^[That is, $\theta^C$ equals some value $\theta_{ij}^{11}$, where $H^S$ operates along the horizontal axis and $H^B$ along the vertical and $i$ and $j$ can be any 0 or 1 values.] This is a set of $\theta$ values under which the query, "Does $S$ have a causal effect on $C$?" must be answered in the negative. Similarly, this is a context in which $C=1$ cannot be causally attributed to $S=1$. If Sally had not thrown, then Sally's rock would not have hit the bottle, which means that Billy's rock would have hit, and the bottle would still have cracked---still, $C=1$.

However, it is still possible that $S=1$ was an actual cause of $C=1$. To complete this query, we need to ask whether there is some node value that we can hold fixed at the value that it _actually_ assumed in the case such that $S$ would have a causal effect on the outcome. Fixing $B=1$ (Billy throws) cannot help (since if Billy throws, Billy hits, and the bottle cracks anyway). However, under $S=1$ and $B=1$, given the $\theta$ values we have posited, $H^B=0$: Billy's rock does not hit. If we hold constant that $H^B=0$, then there is an "opportunity" for $S$ to matter in that $C$ is no longer forced to 1 (by Billy's rock hitting). But for $S$ to matter under his scenario, something else has to be true: $\theta^C$'s value must allow for $H^S$ to have a positive effect on $C$ when $H^B=0$. 

Using our two-cause notation (with $H^S$ on the horizontal axis, and $H^B$ on the vertical), and given that we have already stipulated that $C=1$ when $H^B=1$, the one permissible value for $\theta^C$ is $\theta^{11}_{01}$. This is causal type in which neither $H^B$ nor $H^S$ can be causal if both Billy and Sally throw: whenever one variable is 1, the other has no effect. But it is also a type in which each has a causal effect if the other is held at 0. 

It is also the case, as we have said, that all counterfactual causes are actual causes. They are, quite simply, counterfactual causes when we hold _nothing_ fixed ($\mathcal W$ is the empty set). Thus, in fact, any $\theta^S$, $\theta^{H^S}$ and $\theta^C$ values in which $S$ has a positive effect when $B=1$ will do. This includes, for instance, a $\theta^C$ value in which Billy's hitting has no effect on the bottle (perhaps Billy doesn't throw hard enough!): e.g., $\theta^{01}_{01}$. Here, Sally's throw is both a counterfactual cause and an actual cause of the bottle's cracking. The larger point is that actual cause queries can, like all other causal queries, be defined as questions about the values of nodes in a causal model.


```{r, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:actualquery} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's causal type, represented by $\\theta^Y$. With a single binary causal variable of interest, we let $\\theta_Y$ take on values $\\theta^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\\theta^Y$ ranges over the four values: $\\theta^Y_{00}$, $\\theta^Y_{10}$, $\\theta^Y_{01}$ and $\\theta^Y_{11}$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 0, 1, 1, 1, 1, 3, 3),
       y = c(2, 0, 2, 3, 0, -1, 1, 2),
       names = c(
                expression(paste(S)),
                expression(paste(B)),
                expression(paste(H^S)),
                expression(paste(theta^H^S)),
                expression(paste(H^B)),
                expression(paste(theta^H^B)),
                expression(paste(C)),
                expression(paste(theta^C))),
       arcs = cbind( c(1, 2, 4, 6, 3, 5, 3, 8),
                     c(3, 5, 3, 5, 7, 7, 5, 7)),
       title = "A DAG Capturing an Actual Cause",
       padding = .4, contraction = .15)

```

Actual causes are conceptually useful whenever there are two sufficient causes for an outcome, but one preempts the operation of the other. For instance, we might posit that both the United States' development of the atomic bomb was a sufficient condition for U.S. victory over Japan in World War II, and that U.S. conventional military superiority was also a sufficient condition and would have operated via a land invasion of Japan. Neither condition was a counterfactual cause of the outcome because both were present. However, holding constant the _absence_ of a land invasion, the atomic bomb was a difference-maker, rendering it an actual cause. The concept of actual cause thus helps capture the sense in which the atomic bomb contributed to the outcome, even if it was not a counterfactual cause.

Similarly, the question of how *common* it is for a condition to be an actual cause can be expressed as values of nodes, possibly including nodes that record parameter values for the relevant exogenous nodes.

<!-- We should try to be more specific here and for notable causes about what the nodes we'd want to learn about are. -->


An extended notion [@halpern2016actual, p 81] of actual causes restricts the imagined counterfactual deviations to states that are more likely to arise (more "normal") than the factual state. We will call this notion a ''notable cause.'' Similarly, one cause, $A$, is "more notable" than another cause, $B$, if a deviation in $A$ from its realized state is (believed to be) more likely than a deviation in $B$ from its realized state.

For intuition, we might wonder why a Republican was elected to the presidency in a given election. In looking at some minimal winning coalition of states that voted Republican, we might distinguish between a set of states that *always* vote Republican and a set of states that usually go Democratic but voted Republican this time. If the coalition is minimal winning, then every state that voted Republican is a cause of the outcome in the standard (difference making) sense. However, only the states that usually vote Democratic are notable causes since it is only for them that the counterfactual scenario (voting Democratic) was more likely to arise than the factual scenario. In a sense, we take the "red" states' votes for the Republican as given---placing it, as it were, in the causal background---and identify as "notable" those conditions that mattered and easily could have gone differently. By the same token, we can say that, among those states that voted Republican this time, those that more commonly vote Democratic are *more* notable causes than those that less commonly vote Democratic.

Again, whether something is a notable cause, or the likelihood in some population that a condition is a notable cause, can be expressed as a claim about the value of a set of root nodes.

Though not a focus of our applied examples we show formally how to estimate these estimands in the Appendix, section XXX.

<!-- So there are 2 things being defined: notable vs. not notable, and more vs. less notable. -->

<!-- The election example seems to be illustrating the first of these since it refers to the volatile states being notable.  -->

<!-- But the reasoning doesn't line up with the definition of notable given here: we haven't said that these are states that usually vote non-Republican. We've only said that their voting non-R is more likely than the other states' voting non-R. -->

<!-- Shall I fix by simply changing the volatile states to ones that usually vote D? -->

<!-- Also, we are not saying what the nodes in Q are, just that there are some. Could we say that they're the same nodes as for a plain actual cause PLUS nodes going into X (a possible notable cause) representing parameters of the distribution of X? -->

### Average causal effects

A more general query asks about an average causal effect in some population. In counterfactual terms, a question about average causal effects is: if we manipulated the value of $X$ for all cases in the population---first setting $X$ to one value for all cases, then changing it to another value for all cases---by how much would the average value of $Y$ in the population change? Like other causal queries, a query about an average causal effect can be conceptualized as learning about a node in a causal model. 

We can do this by conceiving of any given case as being a member of a population composed of different causal types. When we seek to estimate an average causal effect, we seek information about the *shares* of these causal types in the population. 

More formally and adapted from @humphreys2015mixing, we can use $\lambda^Y_{ij}$ to refer to the *share* of cases in a population that has causal type $\theta^Y_{ij}$. Thus, given our four causal types above, $\lambda^Y_{10}$ is the proportion of cases in the population with negative effects; $\lambda_{01}$ is the proportion of cases with positive effects; and so on. We can, of course, also think of these shares as probabilities; that is, we can think of any given case as being ``drawn'' from a multinomial distribution with probabilities $\lambda = (\lambda^Y_{10}, \lambda^Y_{01}, \lambda^Y_{00}, \lambda^Y_{11})$. One nice feature of this setup, with both $X$ and $Y$ as binary, the average causal effect can be simply characterized as the share of positive-effect cases less the share of negative-effect cases: $\lambda^Y_{01} - \lambda^Y_{10}$. 

Graphically, we can represent this setup by including $\lambda^Y$ in a more complex causal graph as in Figure \ref{fig:DAGace}. As in our setup for case-level causal effects, $X$'s effect on $Y$ in a case depends on (and only on) the case's causal type, $\theta^Y$. The key difference is that we now model the case's type not as exogenously given, but as a function of two additional variables: the distribution of causal types in a population and a random process through which the case's type is "drawn" from that distribution. We represent the type distribution as $\lambda^Y$ (a vector of values for the proportions $\lambda^Y_{10}, \lambda^Y_{01}, \lambda^Y_{00}, \lambda^Y_{11}$) and the random process drawing a $\theta^Y$ value from that distribution as $U_\theta$. 


<!-- We might stipulate, for instance, that $U_\theta$ has a uniform distribution, between 0 and 1. We could write down the structural equation for $\theta^Y$ as:  -->

<!-- $\theta^Y=$ -->

<!--   $\theta^Y_{10}$ if $U_\theta < \lambda^Y_{10}$ -->

<!--   $\theta^Y_{01}$ if $\lambda^Y_{10} < U_\theta < \lambda^Y_{10} + \lambda^Y_{01}$ -->

<!--   $\theta^Y_{00}$ if $\lambda^Y_{10} + \lambda^Y_{01} < U_\theta < \lambda^Y_{10} + \lambda^Y_{01} + \lambda^Y_{00}$ -->

<!--   $\theta^Y_{11}$ if $\lambda^Y_{10} + \lambda^Y_{01} + \lambda^Y_{00} < U_\theta < \lambda^Y_{10} + \lambda^Y_{01} + \lambda^Y_{00} + \lambda^Y_{11}$ -->


<!-- \begin{equation}  -->
<!-- (\#eq:Q) -->
<!-- \end{equation}  -->

<!-- *** -->

In this model, our causal query---about $X$'s average causal effect---is thus defined by the vector $\lambda^Y$, and specifically by the shares of negative- and positive-causal-effect cases, respectively, in the population. What is $X$'s average effect on $Y$ amounts to asking: what are the values of $\lambda^Y_{10}$ and $\lambda^Y_{01}$? As with $\theta^Y$, $\lambda^Y$ is not directly observable. And so the empirical challenge is to figure out what we _can_ observe that would allow us to learn about $\lambda^Y$'s component values?^[Note also that $\lambda^Y$ can be thought of as itself drawn from a distribution, such as a Dirichlet. The hyperparameters of this underlying distribution of $\lambda$ would then represent our uncertainty over $\lambda$ and hence over average causal effects in the population.]

<!-- Of course, like $\theta^Y$, $\lambda^Y$ is not directly observable. Thus, inference about average causal effects will necessarily involve using information about *observable* nodes to learn both about unobservables of interest. We might, for instance, use observations of $X$ and $Y$ to learn about a case's causal type ($Q$) and, possibly repeating across many cases, about the share of different types in the population ($\lambda$). -->

<!-- **I have decided not to incorporate $U_\lambda$ in the graph because it would actually require a node for the distribution's hyperparameters as well and I think would in any case cloud the point we want to make here.** -->

<!-- Formally, this kind of average causal effect is also calculated using Equation \ref{ate}, though for a model that is not conditional on the case at hand. -->



```{r, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:DAGace} This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different causal types. As before, $X$'s effect on $Y$ is a function of a causal-type variable, $\\theta^Y$. Yet here we explicitly model the process through which the case's type is drawn from a distribution of types in a population. The variable $\\lambda$ is a vector representing the multinomial distribution of causal types in the population while $U_\\theta$ is a random variable representing the draw of each case from the distribution defined by $\\lambda$. A case's causal type, $\\theta^Y$, is thus a joint function of $\\lambda^Y$ and $U^{\\theta_Y}$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 2, 2, 1, 3),
       y = c(1, 1, 2, 3, 3),
       names = c("X", "Y", expression(paste(theta^Y)), expression(paste(lambda^Y)), expression(paste(U^theta))),
       arcs = cbind( c(1, 3, 4, 5),
                     c(2, 2, 3, 3)),
       title = "A DAG with Causal Type Drawn from a Population-level Distribution of Causal Types",
       padding = .4, contraction = .15) 

```


We can, of course, likewise pose queries about other population-level causal quantities. For instance, we could ask for what proportion of cases in the population $X$ has a positive effect: this would be equivalent to asking the value of $\lambda^Y_{01}$, one element of the $\lambda^Y$ vector. Or we could ask about the proportion of cases in which $X$ has no effect, which would be asking about $\lambda^Y_{00} + \lambda^Y_{11}$.

<!-- **ARE WE IN FACT GOING TO CARRY A DISCUSSION OF ACTUAL AND NOTABLE CAUSES THROUGH THE BOOK? IF NOT, WE SHOULD CUT THE NEXT TWO SUBSECTIONS. IT WILL TAKE PEOPLE A LONG TIME TO GET THE ACTUAL CAUSE IDEA, SO ONLY WORTH THE EFFORT IF THERE'S A PAYOFF.** -->

<!-- MH: STRONLY THINK WE SHOULD -- I SEE A CONTRIBUTION HERE AS BEING AN EXPANSION OF THE QUESTIONS WE ASK -->


### Causal Paths

To develop richer causal understandings, researchers often seek to describe the causal path or paths through which effects propagate. Consider the DAG in Figure \@ref(fig:DAGpaths), in which $X$ can affect $Y$ through two possible pathways: directly and via $M$. Assume again that all variables are binary, taking on values of $0$ or $1$. As we have seen in Chapter \@ref(theory), mediation models require causal-type nodes that point into any mediators as well as into the outcome variable. So here we have drawn in a causal-type variable defining $M$'s response to $X$, $\theta^M$, and a causal-type variable capturing $Y$'s response, $\theta^Y$. Importantly, $\theta^Y$ defines $Y$'s response to _two_ parent variables: $M$ and $X$. 

Suppose that we observe $X=1$ and $Y=1$ in a case. Suppose, further, that we have reasonable confidence that $X$ has had a positive effect on $Y$ in this case. We may nonetheless be interested in knowing whether that causal effect ran *through* $M$. We will refer to this as a query about a causal path. A causal path query, of course, goes beyond assessing whether some mediating event along the path occurred. We cannot, for instance, establish that the top path in Figure \ref{fig:DAGpaths} was operative simply by determining the value of $M$ in this case---though that will likely be useful information. 

Rather, the question of whether the top (mediated) causal path is operative is a composite question of two parts: First, does $X$ have an effect on $M$ in this case? Second, does that effect---the difference in $M$'s value caused by a change in $X$---in turn *cause* a change in $Y$'s value? In other words, what we want to know is whether the effect of $X$ on $Y$ depends on---*will not operate without*---the effect of $X$ on $M$.^[A very similar question is taken up in work on  mediation where the focus goes to understanding quantities such as the "indirect effect"" of $X$ on $Y$ via $M$. Formally, the indirect effect would be $$Y(X=1, M = M(X=1,\theta^M), 
\theta^Y) - Y(X = 1, M = M(X=0, \theta^M), \theta^Y))$$, which captures the difference to $Y$ if $M$ were to change in the way that it would change due to a change in $X$, but without an actual change in $X$ [@pearl2009causality p 132, @imai2010general].] Framing the query in this way makes clear that asking whether a causal effect operated via a given path is in fact asking about a specific set of causal effects lying along that path.


```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:DAGpaths} Here $X$ has effects on $Y$ both indirectly through $M$ and directly."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 2, 1, 2),
       y = c(2, 3, 2, 4, 3),
       names = c("X", "M", "Y", expression(paste(theta^M)), expression(paste(theta^Y))),
       arcs = cbind( c(1, 2, 1, 4, 5),
                     c(2, 3, 3, 2, 3)),
       title = "A DAG with Two Causal Paths",
       padding = .4, contraction = .15) 

```


As we can show, we can also define a causal-path query as a question about specific nodes on a causal graph. In particular, just as we have defined other questions about causal effects in terms of causal-type nodes, a causal path can also be defined in terms of the values of type nodes: specifically, in the present example, in terms of the nodes $\theta^M$ and $theta^Y$. To see why, let us first note that there are two combinations of effects that would allow $X$'s positive effect on $Y$ to operate via $M$: (1) $X$ has a positive effect on $M$, which in turn has a positive effect on $Y$; or (2) $X$ has a negative effect on $M$, which has a negative effect on $Y$. 

Thus, in establishing whether $X$ affects $Y$ through $M$, the first question is whether $X$ affects $M$ in this case. Whether or not it does is a question about the value of the causal-type node, $\theta^M$. Let us assume that $\theta^M$ can take on four possible values corresponding to the four possible responses to $X$: $\theta^M_{10}, \theta^M_{01}, \theta^M_{00}, \theta^M_{11}$.^[In other words, $X$'s effect on $M$ could be negative, positive, absent with $M$ stuck at $0$, or absent with $M$ stuck at $1$, respectively.] For sequence (1) to operate, $\theta^M$ must take on the value $\theta^M_{01}$, representing a positive effect of $X$ on $M$. For sequence (2) to operate, $\theta^M$ must take on the value $\theta^M_{10}$, representing a negative effect of $X$ on $M$.

$\theta^Y$, as for our causal-attribution example, defines $Y$'s response to different combinations of two other variables---here, $X$ and $M$---since _both_ of these variables point directly into $Y$. Another way to think about this setup is that $M$ is not just a possible mediator of $X$'s indirect effect; $M$ is also a potential _moderator_ of $X$'s direct effect. Where $X$ can have both an mediated effect through $M$ and a direct effect, $X$ and $M$ also potentially _interact_ in affecting $Y$. 

This results in sixteeen possible values for $\theta^Y$---again as shown above in Table \@ref(tab:PO16).

<!-- We spell out potential outcomes for the 16 resulting types---each a possible value of $\theta^Y$---in Table \@ref(tab:typespaths), which is parallel to Table \@ref(tab:types2x). Within $\theta$'s sub- and superscripts, the value of $X$ increases from 0 to 1 as we move to the right while the value of $M$ increases from 0 to 1 as we move up. -->

<!-- \begin{table}[h!] -->
<!--   \centering -->
<!--   \def\arraystretch{1.3} -->
<!--     \begin{tabular}{ccccccc} -->
<!--     \hline -->
<!--     \textbf {} & \textbf {Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $\\ -->
<!--          & & $M=0)$ & $M=1)$ & $M=0)$ & $M=1)$ \\  \hline -->
<!--     1 & $\theta_{00}^{00}$ 			&  0     & 0     & 0     & 0  \\ -->
<!--     2 & $\theta_{00}^{01}$ 	& 0     & 0     & 0     & 1 \\ -->
<!--     3 & $\theta_{01}^{00}$ 	& 0     & 0     & 1     & 0 \\ -->
<!--     4 & $\theta_{01}^{01}$ 			& 0     & 0     & 1     & 1 \\ -->
<!--     5 & $\theta_{00}^{10}$ 	& 0     & 1     & 0     & 0 \\ -->
<!--     6 & $\theta_{00}^{11}$ 			& 0     & 1     & 0     & 1 \\ -->
<!--     7 & $\theta_{01}^{10}$ 	& 0     & 1     & 1     & 0 \\ -->
<!--     8 & $\theta_{01}^{11}$ 		& 0     & 1     & 1     & 1 \\ -->
<!--     9 & $\theta_{10}^{00}$			& 1     & 0     & 0     & 0 \\ -->
<!--     10 & $\theta_{10}^{01}$ 	& 1     & 0     & 0     & 1 \\ -->
<!--     11 & $\theta_{11}^{00}$			& 1     & 0     & 1     & 0 \\ -->
<!--     12 & $\theta_{11}^{01}$		& 1     & 0     & 1     & 1 \\ -->
<!--     13 & $\theta_{10}^{10}$			& 1     & 1     & 0     & 0 \\ -->
<!--     14 & $\theta_{10}^{11}$		& 1     & 1     & 0     & 1 \\ -->
<!--     15 & $\theta_{11}^{10}$		& 1     & 1     & 1     & 0 \\ -->
<!--     16 & $\theta_{11}^{11}$			& 1     & 1     & 1     & 1 \\ -->
<!--     \bottomrule -->
<!--     \end{tabular}% -->
<!--    \caption{The table defines the 16 values (causal types) that $\theta_Y$ can take on, given a binary $X$ and $M$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies.} -->
<!--   \label{tab:typespaths}% -->
<!-- \end{table} -->

<!-- ------------------------------------------------------------------- -->
<!--    **Type**    $(Y | X=0,$  $(Y |X=0,$   $(Y | X=1,$   $(Y | X=1,$   -->
<!--                   $M=0)$       $M=1)$       $M=0)$        $M=1)$     -->
<!-- -------------  -----------  -----------  ------------  ------------ -->
<!-- $\theta_{0j}^{gh}$             0            0             0             0       -->

<!--       2             0            0             0             1       -->

<!--       3             0            0             1             0       -->

<!--       4             0            0             1             1       -->

<!--       5             0            1             0             0       -->

<!--       6             0            1             0             1       -->

<!--       7             0            1             1             0       -->

<!--       8             0            1             1             1       -->

<!--       9             1            0             0             0       -->

<!--       10            1            0             0             1       -->

<!--       11            1            0             1             0       -->

<!--       12            1            0             1             1       -->

<!--       13            1            1             0             0       -->

<!--       14            1            1             0             1       -->

<!--       15            1            1             1             0       -->

<!--       16            1            1             1             1       -->
<!-- ------------------------------------------------------------------- -->

<!-- Table: (\#tab:typespaths)$Y$'s 16 causal types---values of $Q^Y$---given binary $X$ and $M$ as parents of $Y$ -->


What values of $\theta^Y$
<!-- , of those displayed in Table \@ref(tab:typespaths),  -->
then are compatible with the operation of the $M$ causal path? Let us first consider this question with respect to sequence (1), in which $X$ has a positive effect on $M$, and that positive effect is necessary for $X$'s positive effect on $Y$ to occur. For this sequence to operate, $\theta^M$ must take on the value of $\theta^M_{01}$. When it comes to $\theta^Y$, then, what we need to look for types in which $X$'s effect on $Y$ _depends on $M$'s taking on the value it does as a result of $X$'s positive effect on $M$_. 

We are thus looking for causal types that represent two kinds of counterfactual causal relations operating on nodes. First, $X$ must have a positive effect on $Y$ when $M$ changes as it should given $X$'s positive effect on $M$. Second, that change in $M$, generated by a change in $X$, must be *necessary* for $X$'s positive effect on $Y$ to operate. The thought experiment here thus imagines a situation in which $X$ changes from $0$ to $1$,^[This is the natural thought experiment when explaining a case with realized value of $X=1$, in which the outcome can be thought of as having been generated by a change from $X=0$. The identification of types does hinge, however, on the direction in which we imagine types changing. In other situations, we might observe $X=Y=0$ and thus conceive of the outcome as having been generated by a change from $X=1$ to $X=0$ (again, assuming a positive effect of $X$ on $Y$). When we do this, query 2 below changes: we are now looking for types in which $Y=1$ when $X=0$ but $M=1$. (Does $Y$ stay at $1$ when $X$ moves to $0$ but $M$ doesn't?) The queries are then satisfied by types $6$ and $8$, rather than $2$ and $6$.] but $M$ does *not* change to the value that it should as a result of this change in $X$. We then inspect our types to see if $Y$ would change from $0$ to $1$ in this situation. It is this counterfactual that isolates the causal significance of the path that runs through $M$. It is only if $Y$ would *not* change to $1$ in this situation that we have identified a causal-type for which the $M$-mediated path matters. 

Assuming a positive effect of $X$ on $M$ ($\theta^M=\theta^M_{01}$), we thus need to apply three queries to $\theta^Y$:^[Using standard potential outcomes notation, we can express the overall query, conditioning on a positive effect of $X$ on $M$, via the inequality $Y(1, M(1)) - Y(0, M(0)) > Y(1, M(0)) - Y(0, M(0))$. The three specific queries formulated below simply correspond to the three unique elements of this expression. We can also readily map the path query that we are defining here---does the positive effect of $X$ on $Y$ depend on $X$'s effect on $M$---onto a query posed in terms of indirect effects. For instance, in our binary setup, conditioning our path query on a positive causal effect of $X$ on $Y$, a positive effect of $X$ on $M$, and an imagined change from $X=0$ to $X=1$ generates precisely the same result (identifies the same $\theta^Y$ types) as asking which $\theta^Y$ types are consistent with a positive indirect effect of $X$ on $Y$, conditioning on a positive total effect and $X=1$.]

1. Is $X=1$ a counterfactual cause of $Y=1$? Establishing this positive effect of $X$ involves two queries:

    a) Where $X=0$, does $Y=0$? As we are assuming $X$ has a positive effect on $M$, if $X=0$ then $M=0$ as well. We thus look down the $X=0, M=0$ column and eliminate those types in which we do not observe $Y=0$. This eliminates types $9$ through $16$.

    b) Where $X=1$, does $Y=1$? Again, given $X$'s assumed positive effect on $M$, $M=1$ under this condition. Looking down the $X=1, M=1$ column, we eliminate those types where we do not see $Y=1$. We retain only types $2, 4, 6,$ and $8$.

2. Is $X$'s effect on $M$ necessary for $X$'s positive effect on $Y$? That is, do we see $Y=1$ *only* if $M$ takes on the value that $X=1$ generates ($M=1$)? To determine this, we inspect the _counterfactual_ condition in which $X=1$ yet $M=0$, and we ask: does $Y=0$? Of the four remaining types, only $2$ and $6$ pass this test. 




<!-- \begin{table}[h!]
  \centering
    \begin{tabular}{cccccc}
    \hline
    **Type** &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $ \\
         & $M=0)$ & $M=1)$ & $M=0)$ & $M=1)$ \\  \hline
        1 			&  0     & 0     & 0     & 0 \\
    2 	& 0     & 0     & 0     & 1 \\
    3 	& 0     & 0     & 1     & 0 \\
    4 			& 0     & 0     & 1     & 1 \\
    5 	& 0     & 1     & 0     & 0 \\
    6 			& 0     & 1     & 0     & 1 \\
    7 	& 0     & 1     & 1     & 0 \\
    8 		& 0     & 1     & 1     & 1 \\
    9			& 1     & 0     & 0     & 0 \\
    10 	& 1     & 0     & 0     & 1 \\
    11			& 1     & 0     & 1     & 0 \\
    12		& 1     & 0     & 1     & 1 \\
    13			& 1     & 1     & 0     & 0 \\
    14		& 1     & 1     & 0     & 1 \\
    15		& 1     & 1     & 1     & 0 \\
    16			& 1     & 1     & 1     & 1 \\
    \bottomrule
    \end{tabular}%
   \caption{$Y$'s 16 causal types---values of $Q^Y$---given binary $X$ and $M$ as parents of $Y$}
  \label{typespaths}%
\end{table}% -->

Under these and only these two values of $\theta^Y$---$\theta_{00}^{01}$ and $\theta_{00}^{11}$---we will see a positive effect of $X$ on $Y$ for which the $M$-mediated path is causally necessary as long as $X$ also has a positive effect on $M$. These two $\theta^Y$ values are also different from one another in an interesting way. For type $\theta_{00}^{11}$, $X$'s effect on $Y$ runs strictly through $M$: if $M$ were to change from $0$ to $1$ *without* $X$ changing, $Y$ would still change from $0$ to $1$. $X$ is causally important for $Y$ _only_ insofar as it affects $M$. In a case of type $\theta_{00}^{11}$, then, anything else that similarly affects $M$ would generate the same effect on $Y$ as $X$ does. In type $\theta_{00}^{01}$, however, both $X$'s change to $1$ *and* the resulting change in $M$ are necessary to generate $Y$'s change to $1$; $X$'s causal effect thus requires both the mediated and the unmediated pathway. Andhere  $X$ itself matters in the counterfactual sense; for a case of type $\theta_{00}^{01}$, some other cause of $M$ would *not* generate the same effect on $Y$. 

<!-- The structural equation for $M$ will include $X$ and $Q_M$ as arguments. Thus, knowing the value of $M$ for any given value of $X$, conditional on a given structural equation for $M$, requires knowing $U_M$. The same logic operates for $U_Y$'s role in determining how $Y$ responds to a given change in $M$, conditional on $Y$'s structural equation.  -->

<!-- Note that, as we saw with causal effects, it is also possible to imagine related estimands of the form "does $X$ cause $Y$ in this case through $M$?", "did $X$ cause $Y$ in this case through $M$?" (which requires knowledge of $X$), and "how often does $X$ cause $Y$ though $M$ in a larger population?" (which requires knowledge of the parameters that give rise to $U_Y$ and $U_M$).   -->

We can undertake the same exercise for sequence (2), in which $X$ first has a negative effect on $M$, or $\theta^M=\theta^M_{10}$. Here we adjust the three queries for $\theta^Y$ to take account of this negative effect. Thus, we adjust query 1a so that we are looking for $Y=0$ when $X=0$ and $M=1$. In query 1b, we look for $Y=1$ when $X=1$ and $M=0$. And for query 2, we want types in which $Y$ fails to shift to $1$ when $X$ shifts to $1$ but $M$ stays at $1$. Types $\theta_{01}^{00}$ and $\theta_{11}^{00}$ pass these three tests. 

In sum, we can define a query about causal paths as a query about the value of $\theta$ terms on the causal graph. For the graph in Figure \ref{fig:DAGpaths}, asking whether $X$'s effect runs via the $M$-mediated path is asking whether one of four combinations of $\theta^M$ and $\theta^Y$ hold in case:

* $\theta^M=\theta^M_{01}$ and ($\theta^Y=\theta_{00}^{01}$ or $\theta_{00}^{11}$)
* $\theta^M=\theta^M_{01}$ and ($\theta^Y=\theta_{01}^{00}$ or $\theta_{11}^{00}$) 

It is worth noting how different this formulation of the task of identifying causal pathways is from widespread understandings of process tracing. Scholars commonly characterize process tracing as a method in which we determine whether a mechanism was operating by establishing whether the events lying along that path occurred. As a causal-model framework makes clear, finding out that $M=1$ (or $M=0$, for that matter) does not establish what was going on causally. Observing this intervening step does not by itself tell us what value $M$ *would* have taken on if $X$ had taken on a different value, or whether this would have changed $Y$'s value. We need instead to conceive of the problem of identifying pathways as one of figuring out the _counterfactual_ response patterns of the variables along the causal chain. As we will demonstrate later in the book, explicitly characterizing those response patterns as nodes in a causal model helps us think systematically about empirical strategies for drawing the relevant inferences.

<!-- Such a  focus on causal paths does not restrict attention to questions of the form "how did $X$ cause $Y$" but more generally, "what paths generated $Y$?" Such questions may have answers of the form "$Y=1$ occurred because $X=0$ led to $M=0$, which, when $Z=1$, gives rise to $Y=1$ and not because $X=1$  led to $M=1$, which, when $Z=0$ gives rise to $Y=1$." Such inquiries can focus on distinct sets of conditions that give rise to an outcome ("equifinality"), as in Qualitative Comparative Analysis (QCA). While QCA analysts sometimes refer to sets of conditions as "paths",  QCA does not generally involve explicit assessment of the causal steps linking conditions to outcomes. When examining paths in a causal-model framework, the analyst can address queries that involve drawing inferences about an entire chain linking $X$ to $Y$ or even an entire causal network. An understanding of a full causal network would, in turn, allow for any more specific estimand to be estimated. -->




## Illustration with the Running Example

We can more fully illustrate the definition of causal queries in terms of exogenous nodes on a graph by thinking through their application to the simple causal model described in Chapter 2. 

We illustrate the model again in figure \@ref(fig:running2).


```{r, echo = FALSE}
names = c("S", "X", "C", "R", "Y")
M <- matrix(0, 5, 5)
M[1, c(3)] <-1
M[2, c(3,4)] <-1
M[3, c(4,5)] <-1
M[4, 5] <-1
f_C <- function(V) 1- V[1]*V[2]
f_R <- function(V) V[2]*V[3]
f_Y <- function(V) V[3]*V[4]
# Histories and effects:

H <- function(do = c(0,0,NA,NA,NA)) {
  do[3] <- ifelse(is.na(do[3]), f_C(do), do[3])
  do[4] <- ifelse(is.na(do[4]), f_R(do), do[4])
  do[5] <- ifelse(is.na(do[5]), f_Y(do), do[5])
  do
  }

edges <- function(S,X) {
  do0 <- c(S,X,NA, NA, NA)
  H1   <- H(do0)
  out <- sapply(1:5, function(i) {
    do1 <- do0
    do1[i] <- 1-H1[i]  # change value for i and put into do
    1*(H1 != H(do1))
  })
  diag(out) <- 0
  out}

```


```{r running2, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align="center", out.width='\\textwidth', fig.cap = "The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\\lambda$ value.", fig.align="center", warning = FALSE}

layout(matrix(c(1,1,2,
                1,1,3,
                4, 5,6), 3, 3, byrow = TRUE))
par(mar=c(1,1,3.5,1))

x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)

names = c("S:\nSensitive\ngovernment\n\n", "\nX:\nFree Press", "C:\n Corruption", "R:\n Media report", "Y:\nGovernment\nreplaced")

hj_dag(x =  c(x, 0, 0),
       y = c(y, 0.25, 1.75),
       names = c(names, " ", " "),
       arcs = cbind( c(1,2,2, 3, 4, 3, 6, 7),
                     c(3,3,4, 5, 5, 4, 2, 1)),
       title = "Free Press and Government Survival",
       add_functions = 0, 
       contraction = .15,
       add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS",
       padding = .2)

text(c(0,0), c(.25, 1.75), c(expression(paste(U[X])), expression(paste(U[S]))))

names = c("S", "X", "C", "R", "Y")

myarcs <- list(
       which(t(edges(0,0))==1, arr.ind = TRUE),
       which(t(edges(1,0))==1, arr.ind = TRUE),
       which(t(edges(0,1))==1, arr.ind = TRUE),
       which(t(edges(1,1))==1, arr.ind = TRUE))

mysolids <- list(H(c(0,0,NA,NA,NA)), 
                 H(c(1,0,NA,NA,NA)), 
                 H(c(0,1,NA,NA,NA)), 
                 H(c(1,1,NA,NA,NA)))

names = c("S", "X", "C", "R", "Y")


titles = c("A: No free press causes Y = 0", 
           "B: No free press is the actual cause\nBut neither S nor X counterfactually cause Y=0",
           "C: Both S = 0 and X = 1 cause Y = 1.\n X = 1 is the notable cause.", 
           "D: Government sensitivity\ncauses Y = 0")
for(j in 1:4){

    hj_dag(
       x = x,
       y = y,
       names = names,
       arcs = myarcs[[j]],
       title = titles[[j]],
       add_points = TRUE,
       solids = c(mysolids[[j]]),
       contraction = .15
       )
}

frame(); box();
text(.1,seq(.95, .05, -.1), 
          c("Structural Equations:",
            "  Y = CR",
            "  C = 1 - XS",
            "  R = CX", 
            expression(paste("  S = 1(", u[S]<lambda^S[1],")")),
            expression(paste("  X = 1(", u[X]<lambda^X[1],")")), "  ",
            "P(U):",
            expression(paste("  ", U[S], "~Unif[01]")), 
            expression(paste("  ", U[X], "~Unif[01]")) 
            ), cex = 1.5, adj = 0)
title("Structural model")
```


THe main panel here is the same as in Chapter 2 but but now we add in a set of another four panels. In these panels we leave the $\lambda$ and $U$ terms implicit as they will not come into play in our analysis of these graphs. In these four panels, we show all possible ''realizations'' of the graph given the four possible contexts defined by the exogenous nodes, $S$ and $X$. We build each of the four possible by assessing outcomes and counterfactual relationships for each possible combination of $S$ and $X$ values. A hollow circle at a node indicates that the variable takes on a value of $0$ while a shaded circle indicates a value of $1$. The arrows indicate causal effects. More specifically, an arrow pointing from one variable to another indicates that a manipulation of the first variable would cause a change in the second variable, *given the values realized by all other variables that are not the first variable's descendants*. Unlike in a conventional DAG, we represent here both the direct effect of each variable on its child and each variable's indirect (mediated) causal effects on its descendants. As we can see from the various arrows in the panels, we can use a single, simple causal model to think through a wide range of causal relationships that might be of interest.^[Though similar, these graphs are not DAGS or natural beams (or submodels). The panels reflect outcomes conditional on the values of $S$ and $X$, but they are not themselves DAGs because they indicate the values taken by nodes and include arrows between two nodes when and only when one causes the other, directly or indirectly. To construct "natural beams" [@pearl2009causality 10.3], we fix a realization of root variables, $U$,  (here, $\mathcal U = (S, X)$); then for each variable, $V_i$ we  partition $pa(V_i)$ into a set of "engaged parents," $S$, and "disengaged parents," with the property that (a) $f_i(S(u), \overline{s}, u) = V_i(u)$ for *all* values of $\overline{s}$ and (b) $f_i(s', \overline{S}(u), u) \neq V_i(u)$ for *some*  $s'$. Thus a natural beam  would connect a parent to a child if, given the particular history, the parent mattered for the child's outcome.] Since the values of all variables in a model are determined by the values of the exogenous nodes, this is equivalent to saying that the arrows show the causal effects that are operating each *context.*

One important feature of DAGs is immediately evident from a comparison of the DAG with subgraphs $A, B, C$, and $D$ in the figure. Consistent with the rules of DAG-construction, the DAG includes arrows between all variables that are under any circumstances directly causally related; but the inclusion of an arrow does not mean that two variables are *always* causally related. For instance, while the DAG (large graph) has an arrow running from $X$ to $R$, we can see from the subgraphs (where we deviate from the standard grammar of DAGs) that the causal effect is contingent on context: it is present only when $S=0$ (panels $A$ and $C$) but not when $S=1$. The arrows in a DAG represent dependencies that exist under *some*, but not necessarily under all, values of the exogenous variables.

These five graphs allow us to define all causal claims of interest. The graphs illustrate, in other words, how causal queries can be represented as the value of the exogenous nodes in a causal diagram. Let us consider each causal query in turn.

**Case-level causal effect.** Working with the four subgraphs, we can show that the query, "What is the effect of one variable on another in this case?" is equivalent to asking about the values of the model's exogenous variables, $X$ and $S$. Consider, for instance, the query: Do media reports of corruption, $R$, have a causal effect on government removal from office, $Y$, in this case? Turning to the subgraphs, we can simply ask in which of these four graphs---in which context---$R$ has a causal effect on $Y$: where is there an arrow running from $R$ to $Y$?^[The subgraphs are derived from application of Equation EQUATION REFERENCE. We can work through the $R \rightarrow Y$ relationship to demonstrate how this is done. Consider the effect of $R$ on $Y$ given $S=0, X=0$. This is the arrow between  $R$ and $Y$ in panel $A$. Removing the arrows pointing to $R$, the distribution over nodes when $R=r'$ is: $P(c,y | \hat{r}=r', s =0, x=0)$. We are interested in $P(y=1| \hat{r}=1,  s =0, x=0) - P(y=1 | \hat{r}= 0,  s =0, x=0)$. The second term is easy as for all cases in which $r=0$, $y=0$; and so  $P(y=1|| \hat{r}= 0)=0$. We focus then on  $P(y=1| \hat{r}=1, s= 0, x= 0)$. Taking the marginal distribution, this can be written $\sum_{c=0}^1P(y=1|r=1, c)P(c|s=0,x=0)$. From the structural equations, we know that $P(c=1|s=0,x=0)=1$ and that $P(y=1|r=1, c=1)=1$. So the marginal distribution is $P(y=1| \hat{r}=1, s= 0, x= 0) = 1$; and the treatment effect of $R$ on $Y$, conditional on the characteristics of this case, is then 1. This positive effect is represented with the arrow from the $R=0$ node to the $Y=0$ node in panel $A$.] We can readily see that $R$ has an effect---a positive effect---on $Y$ in all configurations of exogenous node values (i.e., in all subgraphs) except when $X=1$ and $S=1$ (panel $D$); the absence of an arrow in panel $D$ indicates that $R$'s effect on $Y$ is 0 in that context. Thus, given our model, asking whether there is a case-level causal effect of $X$ on $Y$ is equivalent to asking whether either $S$ or $X$ or both are equal to $0$ in the case.

Another way to put the point is that $S$ and $X$ jointly determine a case's causal type when it comes to the effect of $R$ on $Y$. Returning to our four causal types, the graphs tell us that a case is a $b$ type (positive effect) with respect to the $R \rightarrow Y$ relationship whenever at least one of $S$ or $X$ is $0$. If $S=X=1$, then a case is a $c$ type (no effect, with the outcome fixed at $Y=0$). 

We can work through other relationships in the model similarly. For instance, does a free press have an effect on government removal in a case? See an $X$-to-$Y$ arrow only in panels $A$ and $C$, we can thus conclude that $X$ has a causal effect on $Y$ in (and only in) cases in which $S=0$. 

For now, we are simply using the models to *define* a query about a case-level causal effect. This definition sets the stage for our discussion of research design---how one might go about empirically addressing this query---later in the book. We can point the way toward that discussion by noting making two broad points. If the presence of a free press and government's sensitivity to public opinion are observable, then estimating case-level causal effects will simply be a matter of measuring these two exogenous nodes (or, for some queries, just one of them). However, we will often be in a situation in which the nodes defining our causal query are not observable. Our models of the world often include concepts that are theoretically central to a causal logic but cannot be directly measured. Consider, for instance, government sensitivity to public opinion. When a model's exogenous variables are unobservable, then our research design may require using information from other, *observable* nodes to draw inferences about context. This is a key strategy of process tracing that we develop in later chapters.


<!-- We need to resolve the contradiction between the above footnote and the previous one: one says they're not submodels, the other says they are. -->

 


<!-- In this causal beam with binary variables,  whenever a unique path connects one node to another then the ancestor's node's condition is a cause of the descendant's condition. These case level causal relations cannot be read directly from the graph however if there are multiple paths or non dichotomous variables. To see why multiple paths prevent this inference, return to the boulder example of non transitivity described above; to see why inferences cannot be made along paths with non binary outcomes notice that $A$ and $B$ may be connected because some change in $A$ produces a change in $B$, though not necessarily *all* changes in $A$.    -->

**Average causal effects.** Average causal effects are simply averages of case-level causal effects for the population. Since case-level causal effects are determined by the values of the exogenous nodes in cases, we need to average over the distribution of case-level contexts in the population. Put differently, the average causal effect of any variable on another will depends on how commonly the relevant case-level conditions---those in which the causal effect is and is not present---occur. In our current example, we have seen that the free press makes a difference to government survival if and only if the government is *non-sensitive* (panels $A$ and $C$): the non-sensitive government gets exposed as corrupt if and only if there is a free press while the sensitive government never gets replaced because it adjusts to the presence of a free press by eliminating corruption. Similarly, the sensitivity of the government (and the resulting level of corruption) matters only if there *is* a free press (panels $C$ and $D$). Without a free press, non-sensitive and, thus, corrupt governments do not get exposed and so stay on; with a free press, non-sensitive (and, thus, corrupt) governments get replaced. 

Thus, the *average* effect of each of these initial causes on the outcome will depend on the probability with which the other cause is absent or present. To define a query about average causal effects, we need to examine the full probabilistic causal model as graphed in the large panel in Figure \@ref(fig:running2). What is the average causal effect of a free press ($X$) on government removal ($Y$)? As we have learned from the subgraphs, this effect is fully defined by the value of $S$. In particular, the effect of $X$ on $Y$ is equal to $1$ when $S=0$, and is equal to $0$ when $S=1$. As we've noted, we calculate the average causal effect by averaging causal effects over the distribution of the relevant exogenous variables -- which, here, is only $S$. In the probabilistic model, $S$ is a function of $\lambda^S$ and $U_S$. In particular, $S=1$ whenever $u_S < \lambda^S$. Since $U_S$ has a uniform distribution, this simply means that $S=1$ with probability $\lambda_1^S$; likewise, $S=0$ with probability $1-\lambda_1^S$.  Thus, we calculate $X$'s average causal effect on $Y$ by multiplying each causal effect by the probability of $S$'s taking on the value that generates that effect: $1 \times (1-\lambda_1^S) + 0 \times \lambda_1^S = 1-\lambda_1^S$. Put differently, the causal effect of a free press on government removal is equal to the commonness of insensitive governments in the relevant population of cases. 

We have thus defined our causal query in terms of an exogenous variable, $\lambda_1^S$, in the probabilistic causal model. Note that, just as $S$ acts as a causal-type variable for $X$'s effect on $Y$, querying $\lambda_1^S$ is equivalent to asking about the distribution of causal types in the population. In our four-type framework, cases with $S=0$ are $b$ (positive effect) types with respect to the $X \rightarrow Y$ relationship; cases with $S=1$ are $c$ types (no effect, $Y=0$). (There are, here, no $a$ or $d$ types.) Thus, $\lambda_1^S$ represents the share of $c$ types and $1-\lambda_1^S$ the share of $b$ types in the population, vis-a-vis $X$'s effect on $Y$. 

We can follow the same procedure for all causal relationships in the model. Returning, for instance, to the effect of $R$ on $Y$, we learned from the subgraphs that $R$ has a causal effect of $1$ in panels $A,B$ and $C$---that is, whenever it is not the case that $X=1$ and $S=1$---and otherwise of $0$. Thus, the $R$'s average causal effect is the weighted average $1 \times (1-\lambda_1^S \times \lambda_1^X) + 0 \times \lambda_1^S \times \lambda_1^X$ = $1-\lambda_1^S \times \lambda_1^X$. This is simply the probability of not having both $X=1$ and $S=1$. Here, then, we have defined the causal query in terms of two exogenous nodes in the probabilistic model, $\lambda_1^S$ and $\lambda_1^X$. ^[Likewise, the average causal effect of $R$ conditional on $S=1$ is $1-\lambda_1^X$ (the probability of ending up in panel B, rather than D); and the average causal effect of $R$ given $S=0$ is 1 (since it has an effect in both panels A and C).]

**TO BECOME A LONG FOOTNOTE...**
These quantities can be calculated from the distributions in the same way as we calculated the case-level effects. Removing the arrows pointing to $R$, the distribution over nodes when $R=r'$---but this time not fixing $S$ and $X$---is $P(s,x,c,y | \hat{r}=r')$. Again the key part is $P(y=1| \hat{r}=1)$, which can be written $\sum_x\sum_s\sum_c P(x)P(s)P(c|x,s)P(y|c, r= 1)$. Using the structural equations, this simplifies to $\sum_x\sum_s P(x)P(s)P(c=1|x,s) = P(x=0)P(s=0) + P(x=0)P(s=1) + P(x=1)P(s=0)$, or, $1-\lambda_1^S\lambda_1^X$.

In the same way, we can construct the average treatment effect for each of the exogenous variables:

* $\tau_X = E_S(Y(X=1|S)-Y(X=0|S)) = -(1-\lambda_1^S)$
* $\tau_S = E_X(Y(S=1|X)-Y(S=0|X)) = \lambda_1^X$]
**LONG FOOTNOTE ENDING HERE**

In general, then, we can define queries about average causal effects as queries about the exogenous nodes that represent a causal model's probabilistic components. In the present example, probabilistic components enter only as determinants of the initial substantive causal variables. In other models, variables further downstream might also have stochastic components, a query about average causal effects might include thus further exogenous terms representing population-level distributions. Estimating average causal effects thus amounts drawing inferences about these nodes.^[Given the model, data will be useful for estimating average effects only if one is uncertain about the distributions of $S$ and $X$, which are a function of $U_S$ and $\lambda_1^S$ and $U_X$ and $\lambda_1^X$, respectively. In this example $\lambda_1^S$ and $\lambda_1^X$ are fixed in the model and so we do not learn anything about them from data. If however $\lambda_1^S$ and $\lambda_1^X$ are represented as nodes that are themselves produced by some other distribution -- such as a Beta distribution --- then the question of understanding average effects is the question of making inferences about these nodes.]

<!-- I find the previous paragraph quite confusing in terms of what all of this says about learning about nodes. I would think we would want to set up the example so that we *can* use data to learn about the ATE and so that this runs through learning about root nodes.  -->

**Actual cause.** Returning to a case-level query, the concept of an actual cause becomes useful when outcomes are overdetermined. Suppose there is a case with a sensitive government ($S=1$) and no free press ($X=0$), as in panel B. Then the *survival* of the government is over-determined: neither government sensitivity nor the free press is a counterfactual cause. (A lack of a free press is enough for even a corrupt government to survive; and sensitivity ensures non-corruption and, thus, survival even in the presence of a free press.) 

Nevertheless, we can distinguish between the causes in terms of which one was an actual cause. Conditioning on there being corruption, which there actually was, the lack of a free press *is* a counterfactual cause of government survival: if there had been a free press, holding corruption constant, then the government would have been removed. This makes the lack of a free press an actual cause---that is, a counterfactual cause when some (or no) feature of what actually happened is kept fixed. However, there is no set of realized variable values that we can condition on to make the presence of a sensitive government a counterfactual cause; thus, it is not an actual cause.

The context---the values of the exogenous nodes in the subgraphs, $S$ and $X$---determines which variables will be actual causes through setting the realized values of all endogenous variables in the model and thus restricting the values on which conditioning is permitted for the determination of actual causes. Since corruption *is* present whenever $S=1$ and $X=0$, we are permitted in this context to condition on its presence, and the free press is an actual cause of government retention. In contrast, the sensitivity of the government is not an actual cause in this same context. Given no free press, there will always be corruption but no reporting on corruption, which makes government removal impossible, regardless of government sensitivity; thus, there is no subset of actual events that, when kept fixed, would make a change to a non-sensitive government result in the government's fall. In sum, asking whether a variable in a model was the actual cause of an outcome can equivalently be understood as asking about the values of the model's exogenous nodes. Answering that question will consist either of directly observing those exogenous conditions or drawing inferences about them from other, observable nodes.

<!-- This example has the odd feature that the question of whether S or X is an actual cause is itself already conditional on the values of S and X.   -->

**Notable cause.** In the event that that there is a non-sensitive government ($S=0$) and a free press ($X=1$), as in panel $C$, the government gets replaced and *both* of the two causes matter counterfactually for government replacement. (Absent either one, the government would survive.) Again, however, we can distinguish between them, this time on the basis of notable causation. The question, for identifying a notable cause, is how commonly the causal variable in question takes on its realized, as opposed to a counterfactual, value. Thus, like average causal effects, notable causation depends on *population-level* distributions---in the present example, on the parameters $\lambda_1^S$ and $\lambda_1^X$. If, for instance, governments are more frequently sensitive (the counterfactual) than non-sensitive (the actual value)---i.e., $\lambda_1^S > 0.5$---then the non-sensitive government is a notable cause. However, if free presses are rarer than non-sensitive governments---i.e.  $\lambda_1^X < 1-\lambda_1^S$---then the free press is a *more* notable cause than the non-sensitive government.

<!-- Again, would be more consistent with out queries-as-root-nodes to show the pi's as nodes. -->

**Causal Paths.** Note finally that different causal paths can give rise to the same outcome, where the different paths can be distinguished based on values of root nodes $S$ and $X$. For example, the government may be retained ($Y=0$) because there is no free press ($X=0$) and so no negative reporting on the government, regardless of the value of $S$; or because, there is a free press ($X=1$) and a sensitive government ($S=1$) takes account of this and does not engage in corruption. **\color{red} To be improved to link more closely to our abstract discussion of paths as estimands.**



<!-- What still bugs me a bit is that we don't have a way of showing, in this example, different causal paths for the same *causes* and outcomes. -->

<!--chapter:end:04-causal-questions.Rmd-->

# Bayesian Answers {#bayeschapter}

***

We run through the logic of Bayesian updating and show how it is used for answering queries of interest. We illustrate with applications to correlational and process tracing inferences.

***



```{r, include = FALSE}
source("_packages_used.R")
```


```{r, include = FALSE}

pv = function(k0,k1) k1 - k0

gt.slope.text = function(text, f, xl=0, xh=1, vshift=.05, col="black", fixer=1){
	TX = c(" ", strsplit(text, "")[[1]]," ")
	k = length(TX)
	z = xl+ ((0:(k-1))/k)*(xh-xl)
	angles = 	c(0,(atan2(f(z)[3:k]  - f(z)[1:(k-2)],fixer*2*(xh-xl)/(k))*180/pi),0)
	for(i in 1:k){text(z[i], f(z[i])+vshift, TX[i], col = col, srt=angles[i])}
	}
		
k0 = .1
k1 = .9

# posterior = Prob(data|A)Prior(A)/Prob(Data)
posterior = function(prior, observed, k0, k1) observed*k1*prior/(k1*prior+k0*(1-prior)) +  (1-observed)*(1-k1)*prior/(1-(k1*prior+k0*(1-prior)))
plotit = function(k0, k1, main="", xl=.25, xh=.75){
	prior = seq(0,1,.02)
	plot(prior, posterior(prior, 1, k0, k1), type="l",      
       main = bquote( atop(.(main), 
                           phi[0]~'='~ .(k0)~','~ phi[1]~'='~ .(k1))), 
       
             ylab="Posterior")
	lines(prior, posterior(prior, 0, k0, k1), type="l")
	abline(a=0, b=1)
	gt.slope.text("posterior if clue present", f= function(x) posterior(x, 1, k0, k1), xl = xl, xh=xh) 
	gt.slope.text("posterior if clue absent", f= function(x) posterior(x, 0, k0, k1), xl = xl, xh=xh) 
	gt.slope.text("prior", f= function(x) x, xl = xl, xh=xh) 
	}

```		


Bayesian methods are just sets of procedures to figure out how to update beliefs in light of new information. 

We begin with a prior belief about the probability that a hypothesis is true. New data then allow us to form a posterior belief about the probability of the hypothesis. Bayesian inference takes into account the consistency of the evidence with a hypothesis, the uniqueness of the evidence to that hypothesis, and background knowledge about the problem. 

In the next section we review the basic idea of Bayesian updating. The following section applies it to the problem of updating on causal estimands given a causal model and data.  

## Bayes Basics

For simple problems, Bayesian inference accords well with our intuitions. Once problems get slightly more complex however, our intuitions often fail us. 


### Simple instances

Say I draw a card from a deck. The chances it is a  Jack of Spades is just 1 in 52. If I tell you that the card is indeed a spade and asked you now what are the chances it is a Jack of Spades, you should guess 1 in 13. If I told you it was a heart you should guess there is no chance it is a  Jack of Spades. If I said it was a face card and a spade you should say  1 in 3. 

All those answers are applications of Bayes' rule.  In each case the answer is derived by assessing what is possible, given the new information, and then assessing how likely the outcome of interest among the states that are  possible. In all the cases you calculate:

$$\text{Probability Jack of Spades | Information} = \frac{\text{Is Jack of Spades Consistent with Information?}}{\text{How many cards are consistent with Information?}} $$

The same logic goes through when things are not quite so black and white.

Now consider two slightly trickier examples. 

**Interpreting Your Test Results**. Say that you take a test to see whether you suffer from a disease that affects 1 in 100 people. The test is good in the sense that if you have the disease it will say you have it with a 99% probability. If you do not have it, then with a 99% probability, it will say that you do not have it. The test result says that you have the disease. What are the chances you have it? You might think the answer is 99%, but that would be to mix up the probability of the result given the disease with the probability of the disease given the result. In fact the right answer is 50%, which you can think of as the share of people that have the disease among all those that test positive. For example if there were 10,000 people, then 100 would have the disease and 99 of these would test positive. But 9,900 would not have the disease and 99 of these would test positive. So the people with the disease that test positive are half of the total number testing positive.

As an equation this might be written:

$$\text{Probability You have the Disease | Test} = \frac{\text{How many people have the disease and test positive?}}{\text{How many people test positive?}} $$

**Two-Child Problem** Consider last an old puzzle found described @gardner1961second.  *Mr Smith has two children, $A$ and $B$. At least one of them is a boy. What are the chances they are both boys?* 
To be explicit about the puzzle, we will assume that the information that one child is a boy is given as a truthful  answer to the question "is at least one of the children a boy?" Assuming that there is a 50% probability that a given child is a boy, people often assume the answer is 50%. But surprisingly the answer is 1 in 3. The information provided rules out the possibility that both children are girls and so the right answer is found by readjusting the probability that two children are boys based on this information. As an equation:


$$\text{Probability both are boys | Not both girls} = \frac{\text{Probability  both boys}}{\text{Probability they are not both girls}} = \frac{\text{1 in 4}}{\text{3 in 4}}$$


### Bayes' Rule for Discrete Hypotheses

Formally, all of these equations are applications of Bayes' rule which is a simple and powerful formula for deriving updated beliefs from new data. 

The formula is given as:
\begin{eqnarray}
\Pr(H|\mathcal{D})&=&\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\Pr(\mathcal{D})}\\
                  &=&\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\sum_{H'}\Pr(\mathcal{D}|H')\Pr(H'))}
\end{eqnarray}


where $H$ represents a hypothesis and $\mathcal{D}$ represents a particular realization of new data (e.g., a particular piece of evidence that we might observe). 

Looking at the formula we see that the  posterior belief derives from three considerations. First, the likelihood: how likely are we to have observed these data if the hypothesis were true, $\Pr(\mathcal{D}|H$)? Second, how likely were we to have observed these data regardless of whether the hypothesis is true or false, $\Pr(\mathcal{D})$? These first two questions, then, capture how consistent the data are with our hypothesis and how specific the data are to our hypothesis. As shown in the equation above the second question can usefully be reposed as one about all the different ways (alternative Hypotheses, $H'$) that could give rise to the data. 

Note, that contrary to some claims, the denominator does not require a listing of all possible hypotheses, just an exhaustive collection of hypotheses. For example we might have the notion of the probability that the accused's fingerprints would be on the door if she were or were guilty without having to decompose the "not guilty" into a set of hypotheses regarding who else might be guilty.

Our posterior belief is further conditioned by the strength of our prior level of confidence in the hypothesis, $\Pr(H)$. The greater the prior likelihood that our hypothesis is true, the greater the chance that new data consistent with the hypothesis has *in fact* been generated by a state of the world implied by the hypothesis.

### Bayes' Rule for Continuous Parameters

This basic formula extends in a simple way to collections of continuous variables. For example, say we are interested in the value of some parameter vector $\theta$ (as a vector, $\theta$ can contain many quantities we are uncertain about), we can calculate this, given a prior probability distribution over possible values of $\theta$, $p$, and given data $D$ as:

$$p(\theta|\mathcal{D})=\frac{p(\mathcal{D}|\theta)p(\theta)}{\int_{\theta'}p(\mathcal{D|\theta'})p(\theta')d\theta}$$


#### The Dirichlet distributions

Bayes rule requires the ability to express a prior distribution but it does not require that the prior have any particular properties other than being probability distributions. 

In practice however when we are dealing with continuous parameters, it  can be useful to make use of "off the shelf" distributions. 

In practice we will often be interested in forming beliefs about the share of units that are of a particular type. For this type of question we will make quite heavy use of "Dirichlet" distributions -- a family of distributions that capture beliefs about shares. 

Consider for example the share of people in a population that voted---this is a quantity between 0 and 1. Two people might may both believe that the turnout was around 50\% but may differ in how certain they are about this claim. One might claim to have no information and to believe that any turnout rate between 0 and 100% is equally likely, giving an expected turnout of 50%; another might be completely confident that the number if 50% and entertain no other possibilities.


We can capture such beliefs quite well by using the Beta distribution---a special case of the Dirichlet. The Beta is a distribution over the $[0,1]$ that is governed by two parameters , $\alpha$ and $\beta$. In the case in which both $\alpha$ and $\beta$ are 1, the distribution is uniform -- all values are seen as equally likely. As $\alpha$ rises large outcomes are seen as more likely and as $\beta$ rises, lower outcomes are seen as more likely. If both rise proportionately the expected outcome does not change but the distribution becomes tighter. 

An attractive feature of the Beta distribution is that if one has a prior Beta($\alpha$, $\beta$) over the probability of some event (e.g.  that a coin comes up heads), and then one observes a positive case, the Bayesian posterior distribution is also a Beta with with parameters $\alpha+1, \beta$. Thus in a sense if people start with uniform priors and build up knowledge on seeing outcomes, their posterior beliefs should be Beta distributions.

Figure \@ref(fig:Betas) shows a set of such distributions, starting with one that has greater variance than uniform (this corresponds to the non informative "Jeffrey's prior"), then uniform, then for a case in which multiple negative and positive outcomes are seen, in equal number, and finally a set of priors with mean of 3/4. 

```{r Betas, echo = FALSE, fig.cap="Beta distributions"}
par(mfrow = c(2,2))

x <- seq(0,1,.01)
plot(x, dbeta(x, .5, .5), type = "l", main = expression(paste("Beta distribution: ", alpha, ", ", beta, " = 0.5")))
plot(x, dbeta(x, 1, 1), type = "l", main = expression(paste("Beta distribution: ", alpha, ", ", beta, " = 1")))
plot(x, dbeta(x, 20, 20), type = "l", main = expression(paste("Beta distribution: ", alpha, " =20, ", beta, " = 20")))
plot(x, dbeta(x, 30, 10), type = "l", main = expression(paste("Beta distribution: ", alpha, "=30, ", beta, " = 10")))

```

Dirichlet distributions generalize the Beta to the situation in which there are beliefs not just over a proportion, or a probability, but over collections of probabilities. For example if four outcomes are possible and each is likely to occur with probability $\theta_k$, $k=1,2,3,4$ then beliefs about these probabilities are distributions over the a three dimensional unit simplex---that is, all 4 element vectors of probabilities that add up to 1. The distribution has as many parameters as there are outcomes and these are traditionally recorded in a vector, $\alpha$. Similar to the Beta distribution, an uninformative prior (Jeffrey's prior) has $\alpha$ parameters of  $(.5,.5,.5, \dots)$ and a uniform ("flat") distribution has $\alpha = (1,1,1,,\dots)$.

As with the Beta distribution, the Dirichlet updates in a simple way. If you have a Dirichlet prior with parameter $\alpha = (\alpha_1, \alpha_2, \dots)$ and you observe outcome $1$, for example, then then posterior distribution is also Dirichlet with parameter vector $\alpha' = (\alpha_1+1, \alpha_2,\dots)$. 

#### Moments

In what follows we often refer to the "posterior mean" or the "posterior variance." These are simply summary statistics of the posterior distribution and can be calculated easily once the posterior is known. For example the posterior mean of a parameter $\theta_1$---just one in a collection of parameters stored in $\theta$---is simply $\overline{\theta}_1 | \mathcal{D} = \int \theta_1 p(\theta | \mathcal{D}) d\theta$. Note importantly that this is calculated using the posterior over the entire vector $\theta$, there is no notion of updating parameter $\theta_1$ on its own. Similarly the posterior variance is $\int (\theta_1 - (\overline{\theta}_1 | \mathcal{D})^2  p(\theta | \mathcal{D}) d\theta$. 

#### Bayes estimation in practice

Although the principle of Bayesian inference is quite simple, in practice calculating posteriors for continuous parameters is computationally complex. 

In principle with continuous parameters there is an infinity of possible parameter values. Analytic solutions are not, in general, easy to come by and so in practice researchers use some form of sampling.

Imagine for instance you were interested in forming a posterior on the share intending to vote democrat, given polling data. (This is not truly continuous, but with large elections it might as well be).   

One approach is to coarsen the parameter space---we calculate the probability of observing the polling data given possible values $\theta = 0, \theta = .1, \theta = .2, \dots, \theta = 1$, and, apply Bayes rule to form a posterior for each of these  these possibilities.  The downside of the this approach is that it for a decent level of precision it becomes computationally expensive with large parameter spaces and parameter spaces get large quickly. For instance if you are interested in vote shares you might find .4, .5, and .6 too coarse and want posteriors for 0.51 or even 0.505; this would require calculations for 200 parameter values. If you had two parameters that you wanted to slice up each into 200 possible values, you would then have 40,000 parameter pairs to worry about. What's more, *most* of those calculations would not be very informative if the real uncertainty all lies in some small (though possibly unknown) range -- such as between 40% and 60%.  

An alternative approach is to use variants of Markov Chain Monte Carlo sampling. Under these approaches parameter vectors are sampled and their likelihood is evaluated. If they have high likelihood then new parameter vectors near them are draw with a high probability. Based on the likelihood associated with these new draws, new draws are made. The result is a chain of draws that build up to approximate the posterior distribution. The output from these procedures is not a set of probabilities for each possible parameter vector but rather a a set of draws of parameter vectors from the  posterior distribution. 

Many algorithms have been developed to achieve these tasks efficiently; in all of our applications we rely on the `stan` procedures which involve....


## Bayes applied

### Bayesian Inference on Queries
In Chapter 2 we described estimands of interest as queries over the values of root nodes in directed acyclic graphs. 

Once queries are defined in terms of the values of roots then formation of beliefs, given data $W$,  about estimands follows immediately from application of Bayes rule. 

Let $Q(u)$ define the value of the query in context $u$, the updated beliefs about the query are given by the distribution:

$$P(q | W) = \int_{u:Q(u) = q} P(u|W)du =  \int_{u:Q(u) = q} \frac{P(W|u)P(u)}{\int_{u'}P(W|u')P(u')du'}du$$

This expression gathers together all the contexts that produce a given value of $Q$ and assesses how likely these are, collectively, given the data.^[Learning about roots from observed data is sometimes termed *abduction*; see @pearl2009causality, p 206.] For an abstract representation of the relations between  assumptions, queries, data, and conclusions, see Figure 1 in @pearl2012causal.  


Return now to Mr Smith's puzzle. The two "roots" are the sexes of the two children. The query here is $Q$: "Are both boys?" which can be written in terms of the roots. The statement "$Q=1$" is equivalent to the statement ($A$ is a boy \& $B$ is a boy). Thus it takes the value $q=1$ in just one context. Statement $q=0$ is  the statement ("$A$ is a boy \& $B$ is a girl" or "$A$ is a girl \& $B$ is a boy" or "$A$ is a girl \& $B$ is a girl"). Thus $q=0$ in three contexts.  If we assume that each of the two children is equally likely to be a boy or a girl with independent probabilities, then  each of the four contexts is equally likely. 
 The  result can then be figured out as $P(Q=1) = \frac{1\times \frac{1}{4}}{1\times \frac{1}{4} + 1\times \frac{1}{4}+1\times \frac{1}{4}+0\times \frac{1}{4}} = \frac{1}{3}$. This answer requires summing  over only one context. $P(Q=0)$ is of course the complement of this, but using the Bayes formula one can see that it can be found by summing over the posterior probability of three contexts in which the statement $Q=0$ is true. 



### Bayesian correlational inference

<!-- Put this in DAG framework -->

In all the examples describe above Bayes rule was used to update inferences about a particular case given additional information about that case. But the same logic works just as well for problems in which one tries to update about a general relation given a set of cases. 
The correlational solution to the fundamental problem of causal inference is to focus on *population-level* effects. Rather than seeking to identify the types of particular cases, researchers exploit covariation across cases between the treatment and the outcome variables---i.e., dataset observations---in order to assess the {average effect} of treatment on outcomes for a {population} or {sample} of cases. In the simplest, frequentist approach, under conditions described by \citep{Rubin1974} the average effect of a treatment may be estimated as the difference between the average outcome for those cases that received treatment and the average outcome for those cases that did not receive treatment.

Although this frequentist approach to estimating causal effects from correlational data is more familiar, the problem can also be described in Bayesian terms.^[For a fuller treatment, see for example \citet{heckman2014treatment}.]

<!-- % I think the below is necessary because we do not properly define the Bayesian approach generally; we only do it for a very special illustration -->



```{r simpleXYDAG, echo = FALSE, fig.width = 7, fig.height = 4,  fig.align="center", out.width='.5\\textwidth', fig.cap = "A graph depicting a situation in which it is possible that $X$ causes $Y$; the unit level causal type is $\\theta^Y$ and the distribution of causal types is $\\lambda^Y$."}
hj_dag(x = c(0, 0, 1, 1, 2, -1),
       y = c(1, 0, 0, 1, 1, 1),
       names = c(
         expression(paste(theta^X)),
         "X",
         "Y",
         expression(paste(theta^Y)), 
         expression(paste(lambda^Y)), 
         expression(paste(lambda^X)) 
         ),
       arcs = cbind( c(1, 2, 4, 5, 6),
                     c(2, 3, 3, 4, 1)),
       title = "",
       padding = .2, contraction = .15, box = FALSE) 

```


Suppose we are interested in determining the *distribution* of causal types in a population. We again need to specify our parameters, priors, likelihood, the probability of the data, and the inference strategy.

In turn we have:

**Parameters.** Our hypothesis consists of a set of $\lambda$ values: i.e., the proportion of the population of authoritarian regimes for which economic crisis would generate or has generated collapse ($\lambda_b$); the proportion for which collapse is inevitable ($\lambda_d$); and so on.

We can now define our hypothesis as a vector, $\lambda = (\lambda^X_0,\lambda^X_1,\lambda^Y_{00},\lambda^Y_{10},\lambda^Y_{01}, \lambda^Y_{11})$, that registers a possible set of values for the parameters over which we will update: type proportions in the population and assignment propensities by type. 

**Prior.** We next need to assign a prior probability to $\lambda$. In the general case, we will do so by defining a prior probability distribution, $p(\lambda^j)$, over possible values of the elements of $\lambda^j$. Here $\lambda^Y$ has four possible values and we use a Dirichlet distribution on a 3-simplex.  $\lambda^X$ has has only two possible values and in this case the  Dirichlet distribution reduces to a Beta distribution.  

**Likelihood.** Our data, $\mathcal{D}$, consist of $X$ and $Y$ observations for a sample of cases. With binary $X$ and $Y$, there are four possible data realizations (combinations of $X$ and $Y$ values) for a given case. For a single case, it is straightforward to calculate an event probability $w_{xy}$ | that is, the likelihood of observing the particular combination of $X$ and $Y$ given the type shares and assignment probabilities in $\theta$. For instance:

$$w_{00}=\Pr(X=0, Y=0|\theta)=\lambda^X_0(\lambda^Y_{00} + \lambda^Y_{01})$$


More generally, let $w_{XY}$ denote the vector of these event probabilities for each combination of $X$ and $Y$ values, conditional on $\lambda$. Further,  let $n_{XY}$ denote vector containing the number of cases observed with each $X,Y$ combination. Under an assumption of independence (data are independently and identically distributed), the full likelihood is then given by the multinomial distribution:
$$\Pr(\mathcal{D}|\lambda)= \text{Multinomial}(n_{XY}  | w_{XY})$$

Note again that here we have assumed that data is randomly drawn. More general functions can allow for more complex data gathering processes.

**Probability of the data.** We calculate the unconditional probability of the data, $Pr(\mathcal{D})$, by integrating the likelihood function above over all parameter values, weighted by their prior probabilities.
 

**Inference.** After observing our data, $\mathcal{D}$, we then form posterior beliefs over  $\lambda$ by direct application of Bayes' rule, above:

$$p(\lambda|\mathcal{D}) = \frac{\Pr(\mathcal{D}|\lambda)p(\lambda)}{\int\Pr(\mathcal{D}|\lambda')p(\lambda')d\lambda'}$$


This posterior distribution reflects our updated beliefs about which sets of parameter values are most likely, given the data. Critically, note that, upon observing $X$ and $Y$ data, we  simultaneously update beliefs about all parameters in $\lambda$: both beliefs about causal effects (type shares) in the population *and* beliefs about the assignment propensities for $X$.


<!-- Intuitively, we treat each set of possible values of our parameters of interest---each $\lambda$ vector, that is---as a hypothesis and apply Bayes' rule to assess its probability given the data, that is, the posterior.^[More generally we might think of a hypothesis as being a subset of values of $\theta$ | e.g. "there is a positive treatment effect" corresponds to the set of values for which $b>a$.]  We use three quantities to calculate the posterior. -->

<!-- First, we ask, if this set of parameter values is true, how likely were the observed $X, Y$ values to have emerged? {This calculation in our binary framework is simple. For example, the probability of observing the event $X=1, Y=1$ for a single randomly selected case is given by event probability $w_{11}=b\pi_b+d\pi_d$. Note that we assume in this example that each *type* is drawn independently as would be the case if cases under study were randomly sampled from a large population.} Consider a hypothesis (a specific value of $\theta$) in which most authoritarian countries are assumed to be either susceptible to a regime-collapsing effect of economic crisis or destined to collapse anyway---i.e., a $\theta$ in which $\lambda_b$ and $\lambda_d$ are very high and $\lambda_a$ and $\lambda_c$ very low. Suppose we then observe data in which a large proportion of countries display values $X=1$ and $Y=0$---they experienced crisis and did not collapse---which pegs them as either $a$ or $c$ types. The probability of these data under the hypothesized $\theta$--- $\Pr(\mathcal{D}|\theta)$ | will then be low, reducing our confidence in this hypothesis. On the other hand, such data are far more likely under any $\theta$ vector in which $\lambda_a$ or $\lambda_c$ is high, boosting our confidence in such hypotheses. -->

<!-- Second, we ask, how likely were we to observe these data, $\mathcal{D}$, regardless of whether this particular $\theta$ is true? This value appears in the denominator, where we take into account the likelihood of observing these data for *all* of the possible values of $\theta$, weighted by their prior probabilities. More formally, under the assumption of independence, the probability of observing $\mathcal{D}$, that is, a particular collection of $X,Y$ data, is given by the corresponding value of the multinomial distribution given the event probabilities implied by $\theta$.   -->

<!-- The more likely the data are in general|whether the hypothesis is true or not|the smaller the effect of these data on our beliefs. On the other hand, if the observation of lots of crisis-suffering, collapsing regimes was generally *unlikely* across all $\theta$s, then observing these data will generate a larger shift in our confidence toward any particular $\theta$ vector with which the data are relatively consistent. -->

<!-- Third, we multiply the ratio of these first two quantities by our confidence in the values in this $\theta$ prior to seeing the data ($p(\theta)$). The more prior confidence we have in a hypothesis, the greater the probability that evidence consistent with and unique to the hypothesis in fact indicates that the hypothesis is true. Thus, for instance, suppose that prior evidence and logic suggest that a high proportion of authoritarian regimes in the world are susceptible to a regime-collapsing effect of crisis (are $b$ types). This strong prior belief in a high $\lambda_b$ increases the likelihood that any data pattern consistent with a high $\lambda_b$---say, many $X=1, Y=1$ cases---has *in fact* been generated by a large set of $b$ cases. -->

We illustrate Bayesian correlational inference with a simple case. Suppose we observe for all postwar authoritarian regimes, whether they did or did not suffer economic crisis and did or did not collapse. Say for simplicity we know that all authoritarian regimes were "assigned" to economic crisis with a 0.5 probability during the period under analysis (thus assignment is known to be *as if* random). And assume that, prior to observing $X$, $Y$ data we believe that each of two propositions are true with 0.5 probability. Under proposition **($\theta_1$)** all regimes are of type $b$ (and so the average treatment effect is 1); under proposition **($\theta_2$)** 50\% of regimes are of type $c$ and 50\% are of type $d$ (and so the average treatment effect is 0).^[In this simple case we can think of $\theta$ as being constrained to take on only one of two possible values: $\theta \in  \{\theta_1=\{a=0,b=1, c=0, d=0, \pi_a=0.5,\pi_b=0.5,\pi_c=0.5,\pi_d=0.5\},\{\theta_2=\{a=0,b=0, c=.5, d=.5, \pi_a=0.5,\pi_b=0.5,\pi_c=0.5,\pi_d=0.5\} \}$.] 


<!-- %Suppose that we now randomly draw a set of authoritarian regimes from the population and observe the values on $X$ and $Y$. How should our observation of this data | $\mathcal{D}$ | shift our beliefs about the value $\lambda_b$? -->
<!-- % -->
<!-- %A Bayesian analysis draws on our prior beliefs about three quantities: -->
<!-- % -->
<!-- %\begin{itemize} -->
<!-- % -->
<!-- %\item **$\Pr(\mathcal{D**|b=1$)}: The probability of observing this collection of $X$ and $Y$ values under $H_1$, that is, if all cases are susceptible to a positive treatment effect. Each case should either have values $X=Y=0$ or $X=Y=1$ if indeed $b=1$, and the distribution across these values should follow a binomial distribution with $p=.5$ (since cases of all types were assigned to treatment with 0.5 probability). -->
<!-- % -->
<!-- %\item **$\Pr(b=1$)**: The likelihood that $H_1$ is correct. This belief represents our prior level of confidence in $H_1$, before we observe the new evidence. We have set this belief in the present illustration to 0.5. -->
<!-- % -->
<!-- %\item **$\Pr(\mathcal{D**)$}: The probability of observing this collection of $X$ and $Y$ values *without* conditioning on $H_1$. The expression is an average of the probabilities of observing the data under the two hypotheses, weighted by our prior belief for each hypothesis that it is correct. That is, $\Pr(\mathcal{D}) = \Pr(\mathcal{D}|b=1)\Pr(b=1)+\Pr(\mathcal{D}|b=0)\Pr(b=0)$  -->
<!-- %\end{itemize} -->

<!-- %Thus, the observation of $X$ and $Y$ values in the sample allows us to update our beliefs %on the correlation between treatment and outcomes in the population and, hence,  -->
<!-- %on the average treatment effect. In this simple case  -->
<!-- %%the only data consis: do we observe data in which $X$ and $Y$ are perfectly correlated or not? In this case,  -->
<!-- %if we see a single case that has values $(X=0, Y=1)$, then we will know for certain that $H_1$ is false since this data structure could never arise under $H_1$. If we observe data in which $X$ and $Y$ are perfectly correlated, we may still think it possible that $H_2$ is true. However, such a pattern is {*less*} likely to emerge if $H_2$ is true than if $H_1$ is true.  -->
<!-- % -->
<!-- Suppose we draw a random sample of $n=2$ cases and observe one case in which $X=Y=0$ and one case in which $X=Y=1$. That is, we observe a perfect correlation between $X$ and $Y$ but only two cases. What then should we infer? -->

Applying Bayes' rule, our posterior probability on proposition $\theta_1$, having observed the data, is:

\begin{eqnarray*}
\Pr(\theta_1|\mathcal{D}) 
=\frac{\Pr(\mathcal{D}|\theta_1)\Pr(\theta_1)}{\Pr(\mathcal{D}|\theta_1)\Pr(\theta_1)+\Pr(\mathcal{D}|\theta_2)\Pr(\theta_2)}
\end{eqnarray*}

or equivalently: 

\begin{eqnarray*}
\Pr(b=1|\mathcal{D}) 
%=\frac{\Pr(\mathcal{D}|b=1) \Pr(b=1)}{\Pr(\mathcal{D})}
=\frac{\Pr(\mathcal{D}|\lambda_b=1)\Pr(\lambda_b=1)}{\Pr(\mathcal{D}|\lambda_b=1)\Pr(\lambda_b=1)+\Pr(\mathcal{D}|\lambda_b=0)\Pr(\lambda_b=0)}
\end{eqnarray*}

The event probabilities of each of the observed events is $0.5$ under $\theta_1$ but just $0.25$ under $\theta_2$.  Using the binomial distribution (a special case of the multinomial for this simple case) we know that the chances of such data arising are 1 in 2 under $\theta_1$ but only 1 in 8 under $\theta_2$. Our posterior would then be:

\begin{eqnarray*}
\Pr(\lambda_b=1|\mathcal{D}) =\frac{\frac{1}{2} \times \frac{1}{2}}{\frac{1}{2} \times \frac{1}{2} + \frac{1}{8}\times \frac{1}{2}} = \frac{4}{5} 
\end{eqnarray*}

The key difference between this example and more general applications is simply that in the general case we allow for uncertainty --- and updating --- not simply over whether $\lambda_b$ is 0 or 1, but over a range of possible values for multiple parameters of interest. Though this adds complexity, it does not change the fundamental logic of updating.  


### Simple Bayesian Process Tracing

Process tracing in its most basic form seeks to use within case evidence to draw inferences about the case. For example, with a focus on whether $X$ caused $Y$ , data on a "clue", $K$, is used to make inference about whether or not the outcome in that case was generated by the case's treatment status. We refer to the within-case evidence gathered during process tracing as *clues* in order to underline their probabilistic relationship to the causal relationship of interest. Readers familiar with the framework in @collier2004sources   can usefully think of our "clues" as akin to causal process observations, although we highlight that there is no requirement that the clues be generated by the causal process. 

To make inferences, the analyst looks for clues that will be observed with some probability if the case is of a given type and that will *not* be observed with some probability if the case is *not* of that type.

It is relatively straightforward to express the logic of process tracing in Bayesian terms, a step that will aid the integration of qualitative with quantitative causal inferences. As noted by others (e.g. @BennettBayes, @beachpedersen2013process,  @rohlfing2012case), there is an evident connection between the use of evidence in process tracing and Bayesian inference. .

In a Bayesian setting, we begin with a prior belief about the probability that a hypothesis is true. New data then allow us to form a posterior belief about the probability of the hypothesis. 

Formally, we express Bayes' rule as:
\begin{eqnarray}
\Pr(H|\mathcal{D})=\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\Pr(\mathcal{D})}
\end{eqnarray}

$H$ represents our hypothesis, which may consist of beliefs about one or more parameters of interest. $\mathcal{D}$ represents a particular realization of new data (e.g., a particular piece of evidence that we might observe). Thus, our posterior belief derives from three considerations. First, the ''likelihood'': how likely are we to have observed these data if the hypothesis were true, $\Pr(\mathcal{D}|H$)? Second, how likely were we to have observed these data regardless of whether the hypothesis is true or false, $\Pr(\mathcal{D})$? %These first two questions, then, capture how {consistent} the data are with our hypothesis and how {specific} the data are to our hypothesis. 
Our posterior belief is further conditioned by the strength of our prior level of confidence in the hypothesis, $\Pr(H)$. The greater the prior likelihood that our hypothesis is true, the greater the chance that new data consistent with the hypothesis has *in fact* been generated by a state of the world implied by the hypothesis.

In formalizing Bayesian process tracing, we start with a very simple Bayesian setup, which we then elaborate. Suppose that we already have $X,Y$ data on one authoritarian regime: we know that it suffered economic crisis ($X=1$) and collapsed ($Y=1$). But what caused the collapse? We answer the question by (a.) defining our parameters, which are the key quantities of interest, (b.) stating prior beliefs about the parameters of interest, (c.) defining a likelihood function, (d.) assessing the probability of the data, and (e.) drawing inferences. We discuss each of these in turn.

**Parameters.** The inferential challenge is to determine whether the regime collapsed *because* of the crisis ($b$ type) or whether it would have collapsed even without it ($d$ type). We do so using further information from the case---one or more clues. We use the variable $K$ to register the outcome of the search for a clue (or collection of clues), with $K$=1 indicating that a specific clue (or collection of clues) is searched for and found, and $K$=0 indicating that the clue is searched for and not found.

Let $j\in \{a,b,c,d\}$ refer to the type of an individual case. Our hypothesis, in this initial setup, consists simply of a belief about $j$ for the case under examination: specifically whether the case is a $b$ type ($j=b)$. The parameter of interest is the causal type.^[More formally, we can let our hypothesis be a vector $\theta$ that contains a set of indicators for the causal type of the case $\gamma=(\gamma_b, \gamma_d)$, where $\gamma_j\in\{0,1\}$ and $\sum \gamma_j = 1$.] 

%While the correlational approach observes multiple regimes to determine whether crisis has an effect ''on average,'' the process tracing approach looks for evidence of a *clue*, $K$, within the individual case. As described above, t


**Prior.** We then assign a prior degree of confidence to the hypothesis ($Pr(H)$). This is, here, our prior belief that an authoritarian regime that has experienced economic crisis is a $b$. For now, we  express this belief as a prior point probability.

%Our posterior beliefs then constitute a probability distribution over both the type of the case and $\phi$ values---representing updating over both the causal effect and our empirical assumptions about clue likelihoods.

**Likelihood.** We next indicate the likelihood, $Pr(K=1|H)$. This is the probability of observing the clue, when we look for it in our case, if the hypothesis is true---i.e., here, if the case is a $b$ type. We thus require beliefs relating clues to causal types.

The key feature of a clue is that the probability of observing the clue is believed by the researcher to be a function of the case's causal type. %The *differential* probabilities of observing a clue, of $K=1$, under different types are what allow us to draw inferences from clues to types. We thus need beliefs about the probability of observing the clue, when we look for it, for a case of each type. 
For the present example, we will need two such probabilities: we let $\phi_b$ denote the probability of observing the clue for a case of $b$ type ($\Pr(K=1|j=b)$), and $\phi_d$ the probability of observing the clue for a case of $d$ type ($\Pr(K=1|j=d)$).^[More fundamentally one might think of types being defined over $Y$ and $K$ as a function of $X$. Thus potential clue outcomes could also be denoted $K(1)$ and $K(0)$. High expectations for observing a clue for a $b$ type then correspond to a belief that many exchangeable units for which $Y(X)=X$ also have $K(1)=1$ (whether or not $K(0)=0$).] The key idea in process tracing is that the *differences* between these probabilities provides clues with {''probative value,''} that is, the ability to generate learning about causal types.^[\label{fnPV}More formally, we operationalize the concept of probative value in this paper as twice the expected change in beliefs (in absolute value) from searching for a clue that is supportive of a proposition, given a prior of $0.5$ for the proposition. For example, in determining whether $j=b$ or $j=d$ for a given case, starting from a prior of $0.5$ and assuming $\phi_b > \phi_d$, the expected learning can be expressed as $EL = .5(.5\phi_b/(.5\phi_b + .5\phi_d) -.5)  +.5 (.5 - (1-\phi_b).5/((1-\phi_b).5 + (1-\phi_d).5))$. The probative value, after simplifying, is then: $PV = \phi_b/(\phi_b + \phi_d)  -(1-\phi_b)/((1-\phi_b) + (1-\phi_d))$, which takes on values between 0 and 1.]


In process tracing, analysts' beliefs about the probabilities of observing clues for cases with different causal effects typically derive from theories of, or evidence about, the causal process connecting $X$ and $Y$. Suppose we theorize that the mechanism through which economic crisis generates collapse runs via diminished regime capacity to reward its supporters during an economic downturn. A possible clue to the operation of a causal effect, then, might be the observation of diminishing rents flowing to regime supporters shortly after the crisis. If we believe the theory, then this is a clue that we might believe to be highly probable for cases of type $b$ that have experienced economic crisis (where the crisis in fact caused the collapse) but of low probability for cases of type $d$ that have experienced crisis (where the collapse occurred for other reasons). This would imply a high value for $\phi_b$ and low value for $\phi_d$.

Here the likelihood, $\Pr(K=1|H)$, is simply $\phi_b$.

Note that the likelihood takes account of known features of the data-gathering process. The likelihood given here is based on the implicit assumption that the case is randomly sampled from a population of $X=Y=1$ cases for which share $\phi_b$ of the $b$ cases have clue $K=1$ and share $\phi_d$ of the $d$ cases have clue $K=1$. 

**Probability of the data.** This is the probability of observing the clue when we look for it in a case, *regardless* of its type, $(\Pr(K=1))$. More specifically, it is the probability of the clue in a treated case with a positive outcome. As such a case can only be a $b$ or a $d$ type, this probability can be calculated simply from $\phi_b$ and $\phi_d$, together with our beliefs about how likely an $X=1, Y=1$ case is to be a $b$ or a $d$ type. 
<!-- %^[Specifically, $\Pr(K=1|X=1,Y=1)=\phi_b\Pr(j=b|X=1, Y=1)+\phi_d\Pr(j=d|X=1,Y=1)$.]  -->
This probability aligns (inversely) with Van Evera's concept of ''uniqueness.''

**Inference.**  We can now apply Bayes' rule to describe the learning that results from process tracing. If we observe the clue when we look for it in the case, then our *posterior* belief in the hypothesis that the case is of type *b* is:


\begin{eqnarray*}
%\Pr(j = b |X=Y=K=1)&=& \frac{\Pr(K=1|j = b, X=1) \Pr(j = b| X=1, Y=1) }{\Pr(K=1| X=1, Y=1)}
\Pr(j = b |K=1)= \frac{\Pr(K=1|j = b) \Pr(j = b) }{\Pr(K=1)}= \frac{\phi_b \Pr(j = b) }{\phi_b \Pr(j = b)+\phi_d \Pr(j = d)}
\end{eqnarray*}

Suppose, in our running example, that we believe the probability of observing the clue for a treated $b$ case is $\phi_b=0.9$ and for a treated $d$ case is $\phi_d=0.5$, and that we have prior confidence of $0.5$ that an $X=1, Y=1$ case is a $b$. We then get:

\begin{eqnarray*}
\Pr(j = b |X=Y=K=1)&=&\frac{0.9\times 0.5}{0.9 \times 0.5 + 0.6 \times 0.5}=0.6
\end{eqnarray*}

Analogous reasoning follows for process tracing in cases with other $X,Y$ values. For an $X=0, Y=1$ case, for instance, we need beliefs about whether the case is an $a$ or a $d$ type and, for the clue being searched for, the values $\phi_a$ and $\phi_d$. 

As should be clear from the above, the inferential leverage in process tracing comes from differences in the probability of observing $K=1$ for different causal types. Thus, the logic described here generalizes Van Evera's familiar typology of tests by conceiving of the certainty and uniqueness of clues as lying along a continuum. 

Van Evera's four tests ("smoking gun," "hoop," "straw in the wind," and "doubly decisive") represent, in this sense, special cases---particular regions that lie on the boundaries of a "probative-value space."  To illustrate the idea, we represent the range of combinations of possible probabilities for $\phi_b$ and $\phi_d$ as a square in Figure \ref{CluesInferences1} and mark the spaces inhabited by Van Evera's tests. As can be seen, the type of test involved depends on both the relative *and* absolute magnitudes of $\phi_b$ and $\phi_d$. The probative value of a test depends on the difference between them. Thus, a clue acts as a smoking gun for proposition "$b$" (the proposition that the case is a $b$ type)  if it is highly unlikely to be observed if proposition $b$ is false, and more likely to be observed if the proposition is true (bottom left, above diagonal). A clue acts as a "hoop" test if it is highly likely to be found if $b$ is true, even if it still quite likely to be found if it is false. Doubly decisive tests arise when a clue is very likely if $b$ and very unlikely if not. It is, however, also easy to imagine clues with probative qualities lying in the large space amidst these extremes. 



```{r, fig.cap="\\label{CluesInferences1} A mapping from the probability of observing a clue if the proposition that a case is a $b$ type is true ($\\phi_b$) or false ($\\phi_d$) to a generalization of the tests described in Van-Evera (1997).", echo = FALSE, fig.height=12, fig.width=12}
plot(c(0,1), c(0,1), type="l", col="grey", xlab=expression(paste(phi[d], " (Probability of observing ", italic(K), " given d)")), 
				ylab=expression(paste(phi[b], " (Probability of observing ", italic(K), " given b)")), 
				main="Classification of tests")
text(.1,.95, "K present: \n doubly decisive for b  ")
text(.1,.875, "K absent: \n doubly decisive for d  ")
text(.08,.25, "K present: \n smoking gun for b \n K absent \n hoop test for d")
text(.7,.9, "K present: \n hoop test for b \n K absent: \n smoking gun for d")
text(.4,.6, "K present: \n straw in the wind for b \n K absent: \n straw in the wind for d")

text(.9,.125, "K present: \n doubly decisive for d  ")
text(.9,.05, "K absent: \n doubly decisive for b")
text(.9,.7, "K present: \n hoop test for d \n K absent: \n smoking gun for b")
text(.25,.05, "K present: \n smoking gun test for d \n K absent: \n hoop test for b")
text(.6,.4, "K present: \n straw in the wind for d \n K absent: \n straw in the wind for b ")

arrows(.25,.7, .25, .85, col="red")
arrows(.25,.7, .1, .7, col="red")

text(.35, .775, "More sensitive \n for b", col="red")
text(.1775, .65, "More specific \n for b", col="red")


arrows(.75,.3, .9, .3, col="red")
arrows(.75,.3, .75, .15, col="red")

text(.65, .225, "More specific \n for d", col="red")
text(.825, .35, "More sensitive \n for d", col="red")


```

At the same time, the probative value of a test does not fully describe the learning that takes place upon observing evidence. Following Bayes' rule, inferences also depend on our *prior confidence* in the hypothesis being tested. At very high or very low levels of prior confidence in a hypothesis, for instance, even highly probative evidence has minimal effect on posteriors; the greatest updating generally occurs when we start with moderate prior probabilities. Figure \ref{CluesInferences2} in the Supplementary Materials (\S\ref{AppPriors}) more fully illustrates the effect of prior confidence on learning.

We have so far described a very simple application of Bayesian logic. A further conceptually simple elaboration, however, can place process tracing in a more fully Bayesian setting, allowing for considerable gains in learning. Instead of treating clue probabilities ($\phi$ values) as fixed, we can treat them as parameters to be estimated from the data. In doing so, we allow the search for clues to provide leverage not only on a case's type but also, given a belief about type, on the likelihood that a case of this type generates the clue. In practice, we define our hypothesis as a vector, $\theta$, that includes both the causal type of the case and the relevant $\phi$ values, e.g., $\phi_b, \phi_d$. We can then define our prior as a prior *probability distribution* $p(\theta)$ over $\theta$.^[Here, this distribution could, for example, be given by the product of a categorical distribution over $\gamma$ (indicators of causal type) and a Beta distribution for each $\phi_j$.] We can thus express any prior {uncertainty} about the relationship between causal effects and clues. Our likelihood is then a function that maps each possible combination of type and the relevant $\phi$ values to a probability of observing the clue when we search for it.

Updating then produces a joint posterior distribution over type {and} our $\phi$ values. Observing the clue will shift our posterior in favor of type and $\phi$-value {*combinations*} that are more likely to produce the clue. In sum, and critical to what follows, we can simultaneously update beliefs about {both} the case's type {and} the probabilities linking types to clues|learning both about causal effects and empirical assumptions. We provide further intuition on, and an illustration of, this elaboration in the Supplementary Materials (\ref{AppPriors}).


## Three principles of Bayesian updating


### Priors matter {#AppPriors}


The amount of learning that results from a given piece of new data depends strongly on prior beliefs. We saw this already with the example of interpreting our test results above.  Figure \@ref(CluesInferences2) illustrates the point for process tracing inferneces. 

 In each subgraph of Figure \@ref(CluesInferences2) , we show how much learning occurs under different scenarios. The horizontal axis indicates the level of prior confidence in the hypothesis and the curve indicates the posterior belief that arises if we do (or do not) observe the clue. As can be seen, the amount of learning that occurs---the shift in beliefs from prior to posterior---depends a good deal on what prior we start out with. For a smoking gun test, the amount of learning is highest for values roughly in the 0.2 to 0.4 range---and then declines as we have more and more prior confidence in our hypothesis. For a hoop test, the amount of learning when the clue is *not* observed is greatest for hypotheses in which we have middling-high confidence (around 0.6 to 0.8), and minimal for hypotheses in which we have a very high or a very low level of confidence.


```{r CluesInferences2, fig.cap = "Figure shows how the learning from different types of tests depends on priors regarding the proposition. A smoking gun test has the greatest impact on beliefs when priors are middling low and the clue is observed; a ''hoop test'' has the greatest effect when priors are middling high and the clue is not observed.", echo = FALSE, fig.height=10, fig.width=10}
par(mfrow = c(2,2))
	plotit(.4, .6, main="Straw in the Wind")
	plotit(.6, .95, main="Hoop")
	plotit(.05, .4, main="Smoking Gun")
	plotit(.05, .95, main="Doubly Decisive")
```


The implication here is that our inferences with respect to a hypothesis must be based not just on the search for a clue predicted by the hypothesis but also on the *plausibility* of the hypothesis, based on other things we know. Suppose, for instance, that we fail to observe evidence that we are 90 percent sure we *should* observe if a hypothesized causal effect has occurred: a strong hoop test is failed. But suppose that the existing literature has given us a very high level of confidence that the hypothesis *is* right. This high prior confidence, sometimes referred to as a "base rate," is equivalent to believing that the causal effect exists in a very high proportion of cases. Thus, while any given case with a causal effect has only a 0.1 chance of not generating the clue, the high base rate means that the vast majority of cases that we observe without the clue will nonetheless be cases with causal effects. Thus, the failure of even a strong hoop test, involving a highly certain prediction, should only marginally reduce our confidence in a hypothesis that we strongly expect to be true. 

A similar line of reasoning applies to smoking gun tests involving hypotheses that prior evidence suggests are very unlikely to be true. Innocent people may be very unlikely to be seen holding smoking guns after a murder. But if a very high proportion of people observed are known to be innocent, then a very high proportion of those holding smoking guns will in fact be innocent---and a smoking-gun clue will be far from decisive. 

We emphasize two respects in which these implications depart from common intuitions. First, we cannot make *general* statements about how decisive different categories of test, in Van Evera's framework, will be. It is commonly stated that hoop tests are devastating to a theory when they are failed, while smoking gun tests provide powerful evidence in favor of a hypothesis. But, in fact the amount learned depends not just on features of the clues but also on prior beliefs. 

Second, although scholars frequently treat evidence that goes against the grain of the existing literature as especially enlightening, in the Bayesian framework the contribution of such evidence may sometimes be modest, precisely because received wisdom carries weight. Thus, although the discovery of *disconfirming* evidence---an observation thought to be strongly inconsistent with the hypothesis---for a hypothesis commonly believed to be true is more informative (has a larger impact on beliefs) than *confirming* evidence, this does not mean that we learn more than we would have if the prior were weaker. % But it is not true as a general proposition that we learn more the bigger the "surprise" a piece of evidence is. 
%The effect of disconfirming evidence on a hypothesis about which we are highly confident will be *smaller* than it would be for a hypothesis about which we are only somewhat confident. 
When it comes to very strong hypotheses, the "discovery" of disconfirming evidence is very likely to be a false negative; likewise, the discovery of supporting evidence for a very implausible hypothesis is very likely to be a false positive. The Bayesian approach takes account of these features naturally.^[We note, however, that one common intuition---that little is learned from disconfirming evidence on a low-plausibility hypothesis or from confirming evidence on a high-plausibility one---*is* correct.] 


### Simultaneoues, joint updating

When we update we often update over multiple quantities. When we see a smoking gun, for instance, we might update our beliefs that the butler did it, but we might also update our beliefs baout how likely we are to see smoking guns -- maybe they are not so rare as we thought! 

Intuitively you might think of this updating as happening sequentially -- first of all you update over the general proposition, then you update over the particular claim. But in fact you update over both quantities at once. 

Here we elaborate on the intuition of fully Bayesian process tracing, in which updating occurs over both causal type ($j$) and beliefs about the probabilities with which clues are observed for each type ($\phi$ values). The illustration in the text makes clear how updating over type occurs, given beliefs about $\phi$ values. But how does updating over $\phi$ occur? 

Suppose that we observe a case with values $X=1, Y=1$. We begin by defining a prior probability distribution over each parameter. Suppose that we establish a prior categorical distribution reflecting uncertainty over whether the case is a $b$ type (e.g., setting a probability of 0.5 that it is a $b$ and 0.5 that is a $d$ type). We also start with priors on $\phi_b$ and $\phi_d$. For concreteness, suppose that we are certain that the clue is unlikely for a $d$ type ($\phi_d=.1$), but we are very uncertain about  $\phi_b$; in particular, we have a  uniform prior distribution over $[0,1]$ for $\phi_b$. Note that, even though we are very uncertain about $\phi_b$, the clue still has probative value, arising from the fact that the expected value of $\phi_b$ is higher than that of $\phi_d$. 

Suppose that we then look for the clue in the case and observe it. This observation shifts posterior weight away from a belief that the case is a $b$. See Figure \ref{fig:correlation} for an illustration. Yet it *simultaneously* shifts weight toward a higher value for $\phi_b$ and a lower value for $\phi_d$. The reason is that the observed clue has a relatively high likelihood *both* for combinations of parameter values in which the case is a $d$ and $\phi_b$ is low *and* for combinations in which the case is a $b$ and $\phi_b$ is *high* (or, equivalently, in this example, where $\phi_d$ is low). The marginal posterior distribution of $\phi_b$ will thus be shifted upward relative to its prior marginal distribution. The joint posterior distribution will also reflect a dependency between the probability that the case is a $b$ vs. a $d$, on the one hand, and $\phi_b$ and $\phi_d$ on the other. 




```{r, echo = FALSE, fig.cap = "\\label{fig:correlation} Joint posteriors distribution on whether a case is a $b$ or $d$ and on the probability of seeing a clue for a $b$ type ($\\phi_b$)."}

# X=1 Y=1 case chose. Is it b or d. Say phi_d = .1 and phi_b~ uniform[0,1]. Say prior on b is .5.

k      <- 2000
phi_b  <- seq(0.001,.999, length = k)
theta  <- data.frame(type = c(rep(0,k), rep(1,k)), phi_b = c(phi_b, phi_b), phi_d  <- rep(.1,2*k)) 

posterior_k_seen <- (theta$type*theta$phi_b + (1-theta$type)*theta$phi_d)
posterior_k_seen <-  posterior_k_seen/sum(posterior_k_seen)

posterior_k_not_seen <- (theta$type*(1-theta$phi_b) + (1-theta$type)*(1-theta$phi_d))

posterior_k_not_seen <- posterior_k_not_seen/sum(posterior_k_not_seen)

select1 <- sample(1:(2*k), k, replace = FALSE, prob = posterior_k_seen)
select2 <- sample(1:(2*k), k, replace = FALSE, prob = posterior_k_not_seen)

  par(mfrow=c(1,2))
  plot(theta$type[select2]+(rnorm(2*k)/10)[select2], theta$phi_b[select2], col = rgb(.8,.1,.2,.4), pch =16, cex=.4, xlab = "Is it a b?", ylab = expression(phi[b]), main="Beliefs | K not seen", axes=FALSE) 
  axis(1, at=c(0,1), labels=c("d", "b"));   axis(2)
  box()
  
  plot(theta$type[select1]+(rnorm(2*k)/10)[select1], theta$phi_b[select1], col = rgb(.8,.1,.2,.4), pch =16, cex=.4, xlab = "Is it a b?", ylab = expression(phi[b]), main="Beliefs | K seen", axes=FALSE) 
  axis(1, at=c(0,1), labels=c("d", "b"));   axis(2)
  box()

```



### Posteriors are independent of the ordering of data

We often think of learning as a process in which we start off with some set of beliefs---our priors, we gather data, $D_1$, and update our beliefs, forming a posterior; we then observe new data and we update again, forming a new posterior, having treated the previous posterior as a new prior. In such cases it might seem natural that it matters which data we saw first and which later. 

For instance EXAMPLE

In fact though, Bayesian updating is deaf to ordering. If we learn first that hte card is a face card and second that it is black, our posteriors that it is a Jackk of Spades go from 1 in 52 to 1 in 12 to 1 in 6.  If we learn first that the card is black and second that it is a facecard, our posteriors that it is a Jack of Spades go from 1 in 52 to 1 in 26 to 1 in 6. We end up in the same places in both cases. And we would ave had the same conclusion if we learned in one go that the card is a black facecard.

The math of this is easy enough. Our posterior given two sets of data $D_1, D_2$ can be written:

$$p(\theta | D_1, D_2) = \frac{p(\theta, D_1, D_2)}{p(D_1, D_2)} = \frac{p(\theta, D_1 | D_2)p(D_2)}{p(D_1 | D_2)p(D_2)}= \frac{p(\theta, D_1 | D_2)}{p(D_1 | D_2)}$$

or, equivalently:

$$p(\theta | D_1, D_2) = \frac{p(\theta, D_1, D_2)}{p(D_1, D_2)} = \frac{p(\theta, D_2 | D_1)p(D_1)}{p(D_2 | D_1)p(D_1)}= \frac{p(\theta, D_2 | D_1)}{p(D_2 | D_1)}$$

In other words our posteriors given both $D_1$ and $D_2$ can be thought of as the result of updating on $D_2$ given we already know $D_1$ or the result of updating on $D_1$ given we already know $D_2$. 

This fact will be useful in applications. In practice we might assume that we have beliefs based on background data $D_1$, for example regarding general relations between $X$ and $Y$ and a flat prior, and we then update again with new data on $K$. Rather than updating twice, the fact that updating is invariant to order means that we can assume a flat prior and update once given data on $X$, $Y$, and $K$.   


<!--chapter:end:05-being-Bayesian.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
# (PART) Model-Based Causal Inference {-}

# Process Tracing with Causal Models {#clues}

***

We connect the literature on causal models to qualitative inference strategies used in process tracing. We provide a procedure for inference on case level queries from causal models . In addition we extract a set of implications for process tracing. We show how a key result from the causal models literature provides a condition for when clues may be (or certainly will not be) informative. 

***



```{r, include = FALSE}
source("_packages_used.R")
```


## Process tracing and causal models

This chapter demonstrates how we can use causal models to conduct confirmatory process tracing: that is, to draw causal inferences about a single case from case-level data.  

### The intuition

We first walk through the basic intuition and then provide a more formal account.

When we undertake process tracing, we seek to answer a causal query about a given case. That query could be about any number of case-level causal features, including a case-level causal effect, the pathway through which an effect operates, an actual cause, or causal attribution. We will use observations from the case itself to address this query. We do so via a procedure in which we first encode prior knowledge in the form of a causal model, use data to learn about features of the model, and then take what we have learned about the model and map it into our query.

Given a causal model, we form posteriors over estimands as follows:

1. **Specify all causal types**. A causal type, recall, specifies the values that a unit is expected to take, absent any interventions, but also the values it would take given some interventions on some variables. Examples of types might be:
  * Type 1: $X$ takes the value 1 and $Y$ takes the value 1 but $Y$ would take the value 0 if $X$ were 0.
  * Type 2: $X$ takes the value 0 and $Y$ takes the value 0 but $Y$ would take the value 1 if $X$ were 1.
  * Type 3: $X$ takes the value 1 and $Y$ takes the value 1, as it would were $X$ to take the value 0. 


2. **Specify priors over causal types.** Report how likely you think it is that a given unit is of a particular causal type. In the simplest case one might place 0 weight on some causal types (that might be ruled out by theory, for example) and equal weight on the others. 

3. **Specify the estimand in terms of causal types.** For instance the estimand "$Y$ responds positively to $X$" can be thought of as a collection of causal types: Q={Type 1, Type 2}.^[More generally an estimand might be a function of the distribution of causal types.]

4. **Specify the set of causal types that are consistent with the data.** For instance if we observe $X=1, Y=1$ we might specify the data-consistent set as {Type 1, Type 3.}.

5. **Update.** Updating is done then by adding up the prior probabilities on all causal types that are consistent with both the data and the estimand, and dividing this by the sum of prior probabilities on all causal types that are consistent with the data (whether or not they are consistent with the estimand).


<!-- 1. **Draw a DAG.** We begin by constructing a causal model in graphical form, a DAG, expressing which variables in the domain of interest we think can have a direct effect on which other variables. As we have discussed, the causal model we start with may be derived from theory, from data on other cases, or some combination of the two. (We show, for instance, in Chapter \@ref(mixing) how data from a larger set of cases can inform the priors we bring to single-case process tracing.)  -->

<!-- 2. **Identify causal types**. A DAG, in turn, defines a set of possible causal types: all of the different possible combinations of nodal types that any case might have.  -->

<!-- 3. **Form priors**. We draw further on background knowledge, about the population to which the case belongs, to formulate prior beliefs about the probability that the case is of different causal types. We can generate these priors by ruling out certain nodal types as inconsistent with prior knowledge. Where our prior knowledge supports doing so, we can also place differential quantitative weights on those nodal types that we believe to be more or less common in the population. -->

<!-- 4. **Observe data**. We observe data on some or all of the nodes in the graph. -->

<!-- 5. **Eliminate causal types inconsistent with the data**. Check the consistency of each causal type with the data. Eliminate from contention any causal type that could not have generated the data pattern that we observe. -->

<!-- 6. **Form posteriors**. We now scale up the probabilities on all remaining causal types, providing a posterior probability on each type. -->

<!-- 7. **Map from causal types to query**. As any causal query can be formulated as a question about causal types (see Chapter \@ref(questions)), we can now map from our posteriors on causal types to a posterior probability on the estimand of interest: whether a causal effect, a causal pathway, causakl attribution, or some other case-level causal quantity. -->

```{r, include = FALSE}
library(plotrix)
```

```{r ptvenn, echo = FALSE, fig.width=6, fig.height=6, fig.cap = "Logic of simple updating on arbitrary estimands."}

frame()
draw.circle(.35,.35,.3,nv=100,border=NULL,col=NA,lty=1,density=NULL,
						angle=45,lwd=1)
draw.circle(.65,.35,.3,nv=100,border=NULL,col=NA,lty=1,density=NULL,
						angle=45,lwd=1)
draw.circle(.5,.65,.3,nv=100,border=NULL,col=NA,lty=1,density=NULL,
						angle=45,lwd=1)
text(.5, .8, "Consistent\nwith query")
text(.2, .35, "Consistent\nwith priors")
text(.8, .35, "Consistent\nwith data")
text(.15, .95, "All\ncausal types")
text(.5, .45, "A")
text(.32, .55, "B")
text(.5, .25, "C")


box()
```

This process is represented graphically with Figure \@ref(fig:ptvenn), where we can think of probabilities as proportionate to areas. Our causal model defines the causal type space. We then proceed by a process of elimination. Only some of the causal types in the model are consistent with prior knowledge. Only some are consistent with the data that we observe. Finally, any query itself maps onto a subset of the possible causal types. The causal types that remain in contention once we have observed the evidence are those at the intersection of consistency with priors and consistency with the data. $A$ represents those types that are *also* consistent with a given answer to the query (say, $X$ has a positive effect on $Y$).

Thus, our belief about the query before we have seen the data is the probability of all causal types consistent with our priors and with the query ($A + B$) as a proportion of all types consistent with our priors. Once we have seen the data, we have reduced the permissible types to $A + C$. Our posterior belief on the query is, then, the probabilities of those remaining types that are consistent with the query as a share of the probabilities of *all* remaining types, or $A/(A+C)$.

What we are doing here is straightforward: assessing causal possibilities for their compatibility with both the evidence at hand and our prior knowledge of how the world works. The formalization that we will present ensures that prior knowledge and evidence are all recorded explicitly while forcing logical consistency on the inferences that emerge from them.


### A formalization of the general approach

More formally, the general approach to inference draws on the components we outlined in chapters 2 to 4: graphical causal models (DAGs), types and collections of types, and priors. We now show how these elements formally interact with data to generate causal inferences. We continue to focus on a situation with binary variables, though suggest later in the chapter how this can be extended. Though we walk through the procedure for simple models, the approach outlined here can be applies to *any* causal model with binary variables and to any estimands defined over the model.

The process tracing procedure operates as follows:

**A DAG** We begin with a DAG, or graphical causal model. As we know, a DAG identifies a set of variables and describes the parent-child relations between them, indicating for each variable which other variables are its direct (possible) causes. These relationship, in turn, tell us which (non-descendant) variables a given variable is *not* independent of given the other variables in the model. 

**Nodal types**. Once we have specified a DAG, we have defined the full set of possible nodal types: the types defining the value that a variable will take on given the values of its parents, which we have denoted with $\theta$ values. At each node, the range and number of nodal types is defined by the number of parents that that node has and the number of values the variables can take on. For instance, assuming all variables to be binary, if $Y$ has parents $X$ and $W$ (so $k=2$), then there are $2^{\left(2^2\right)}=16$) possible causal types for the $Y$ node. There are $2^2$ possible combinations of values that two binary causal variables can take on----$(X=0,W=0), (X=0,W=1), (X=1,W=0), (X=1,W=1)$---which implies four possible causal conditions over which $Y$'s possible responses must be defined. For instance, as we have seen, with two causal variables, we can have $\theta^Y_{0000}$, where $Y$ is always 0; $\theta^Y_{0001}$, where $Y$ is 0 unless both $X$ and $W$ are 1; and so on.^[These nodal types can require many indices--$2^k$ for a node with $k$ parents---and the rule we follow is that the $i$th subscript indicates the value the node takes when parent $j \in {1, 2, ..., k}$ take values $\mod(floor((i-1)/(2^{j-1})), 2)$ For instance for `Y0111` the first index means that Y takes the value 0 where both parents are 0,  in all other cases it takes value 1.] To get the total number of nodal types, we simply raise $2$ (since $Y$ is binary) to the number of causal conditions (4), giving the number of possible patterns of $Y$ values that could be generated across these four conditions (16). (The full set of nodal types for two causal variables in a binary setup is given in \@ref(tab:PO16).)^[More generally, let us say that any node $j$ can take on $r_j$ possible values and has parents belonging to set $PA_j$ and that each parent, $i \in PA_j$, can take on $r_i$ values. Then the number of nodal types for node $j$ is equal to $r_j^{\prod_{i \in PA_j}r_i}$. Informally, the exponent in this expression simply multiplies by one another the number of values that each of $j$'s parents can take on. This product tells us the number of causal conditions across which $j$'s responses must be defined. We then raise the number of values that $j$ can take on to the power of the number of causal conditions. With all variables binary, this expression translates to $2^{\left(2^k\right)}$ nodal types for a node with $k$ parents.]

All variables in a model have nodal types defining the value they take on given the value of their parents, including those variables without substantive parents. Suppose that $X$ and $W$, in this model, have no substantively defined parents. We nonetheless define a nodel type for each of them, which simply captures their exogenous assignment to some value. With $X$ binary, for instance, there are two nodal types, $\theta^X_{0}$, where $X$ is set to $0$, and $\theta^X_{1}$, where $X$ is set to $1$.

**Causal types**. We will want to be able to conceive not just of types for individual nodes but of the full collection of nodal types across all nodes in a model. We refer to a unit's full set of nodal types as its *causal type,* which we represent as $\theta$.  A causal type is simply a listing that contains one nodal type for each node in the model. For instance, with a model with variable $X$, $W$, and $Y$, each unit has a *causal* type composed of its *nodal* types on each of the three nodes.^[A model in which each node $j$ has $k_j$ parents has $\prod_j2^{\left(2^{k_j}\right)}$ causal types that uniquely determine what data will be observed for a type under all possible interventions on its exogenous nodes.]  Thus, one causal type in this model could be $\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_1, \theta^Y = \theta^Y_{1101})$; another could be $\theta = (\theta^X = \theta^X_0, \theta^W = \theta^W_1, \theta^Y = \theta^Y_{0001})$; and so on.

We show the mapping between nodal and causal types, for a simply $X \rightarrow Y$ model, in Table \@ref(tab:nodalcausalmatrix). The column headings represent the $8$ permissible causal types, each expressed simply as a concatenated strings of nodal types. The row headings represent the nodal types. In each interior cell, a $1$ or $0$ indicates whether or not a given nodal type is a component of a given causal type. As can be seen, each causal type has two nodal types that are its components since there are two nodes in this model. Each $X$-nodal type is part of four causal types since it can be comined with four different $Y$-nodal types, while each $Y$-nodal type is part of two causal types since it can be combined with two $X$-nodal types.

|             **Causal Types $\rightarrow$** | $\theta^X_0$.$\theta^Y_{00}$ | $\theta^X_1$.$\theta^Y_{00}$ | $\theta^X_0$.$\theta^Y_{10}$ | $\theta^X_1$.$\theta^Y_{10}$ | $\theta^X_0$.$\theta^Y_{01}$ | $\theta^X_1$.$\theta^Y_{01}$ | $\theta^X_0$.$\theta^Y_{11}$ | $\theta^X_1$.$\theta^Y_{11}$ |
|-------------------------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|
|         **Nodal types $\downarrow$**        |                              |                              |                              |                              |                              |                              |                              |                              |
|  $\theta^X_0$ |               1              |               0              |               1              |               0              |               1              |               0              |               1              |               0              |
| $\theta^X_1$ |               0              |               1              |               0              |               1              |               0              |               1              |               0              |               1              |
|               $\theta^Y_{00}$              |               1              |               1              |               0              |               0              |               0              |               0              |               0              |               0              |
|               $\theta^Y_{10}$              |               0              |               0              |               1              |               1              |               0              |               0              |               0              |               0              |
|               $\theta^Y_{01}$              |               0              |               0              |               0              |               0              |               1              |               1              |               0              |               0              |
|               $\theta^Y_{11}$              |               0              |               0              |               0              |               0              |               0              |               0              |               1              |               1              |
Table: (\#tab:nodalcausalmatrix). A mapping between nodal types and causal types for a simple $X \rightarrow Y$ model.






<!-- |                                    **Causal Types $\rightarrow$** | $\theta^X_0$.$\theta^Y_{00}$ | $\theta^X_1$.$\theta^Y_{00}$ | $\theta^X_0$.$\theta^Y_{10}$ | $\theta^X_1$.$\theta^Y_{10}$ | $\theta^X_0$.$\theta^Y_{01}$ | $\theta^X_1$.$\theta^Y_{01}$ | $\theta^X_0$.$\theta^Y_{11}$ | $\theta^X_1$.$\theta^Y_{11}$ | -->
<!-- |------------------------------------------------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:| -->
<!-- |                    **Parameters $\downarrow$**                    |                              |                              |                              |                              |                              |                              |                              |                              | -->
<!-- |               $\theta^X_0 | \theta^Y= \theta^Y_{01}$              |               0              |               0              |               0              |               0              |               1              |               0              |               0              |               0              | -->
<!-- | $\theta^X_1 | \theta^Y_{01}$$\theta^X_1 | \theta^Y= \theta^Y_{01}$ |               0              |               0              |               0              |               0              |               0              |               1              |               0              |               0              | -->
<!-- |             $\theta^X_0| \theta^Y \neq \theta^Y_{01}$             |               1              |               0              |               1              |               0              |               0              |               0              |               1              |               0              | -->
<!-- |             $\theta^X_1 | \theta^Y \neq \theta^Y_{01}$            |               0              |               1              |               0              |               1              |               0              |               0              |               0              |               1              | -->
<!-- |                          $\theta^Y_{00}$                          |               1              |               1              |               0              |               0              |               0              |               0              |               0              |               0              | -->
<!-- |                          $\theta^Y_{10}$                          |               0              |               0              |               1              |               1              |               0              |               0              |               0              |               0              | -->
<!-- |                          $\theta^Y_{01}$                          |               0              |               0              |               0              |               0              |               1              |               1              |               0              |               0              | -->
<!-- |                          $\theta^Y_{11}$                          |               0              |               0              |               0              |               0              |               0              |               0              |               1              |               1              | -->





<!-- |                            | $\theta^X_0$.$\theta^Y_{00}$ | $\theta^X_1$.$\theta^Y_{00}$ | $\theta^X_0$.$\theta^Y_{10}$ | $\theta^X_1$.$\theta^Y_{10}$ | $\theta^X_0$.$\theta^Y_{01}$ | $\theta^X_1$.$\theta^Y_{01}$ | $\theta^X_0$.$\theta^Y_{11}$ | $\theta^X_1$.$\theta^Y_{11}$ | -->
<!-- |-----------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------| -->
<!-- | $\theta^X_0 | \theta^Y= \theta^Y_{01}$ | 0                            | 0                            | 0                            | 0                            | 1                            | 0                            | 0                            | 0                            | -->
<!-- | $\theta^X_1 | \theta^Y= \theta^Y_{01}$ | 0                            | 0                            | 0                            | 0                            | 0                            | 1                            | 0                            | 0                            | -->
<!-- | $\theta^X_0| \theta^Y \neq \theta^Y_{01}$                | 1                            | 0                            | 1                            | 0                            | 0                            | 0                            | 1                            | 0                            | -->
<!-- | $\theta^X_1 | \theta^Y \neq \theta^Y_{01}$                | 0                            | 1                            | 0                            | 1                            | 0                            | 0                            | 0                            | 1                            | -->
<!-- | $\theta^Y_{00}$             | 1                            | 1                            | 0                            | 0                            | 0                            | 0                            | 0                            | 0                            | -->
<!-- | $\theta^Y_{10}$             | 0                            | 0                            | 1                            | 1                            | 0                            | 0                            | 0                            | 0                            | -->
<!-- | $\theta^Y_{01}$             | 0                            | 0                            | 0                            | 0                            | 1                            | 1                            | 0                            | 0                            | -->
<!-- | $\theta^Y_{11}$             | 0                            | 0                            | 0                            | 0                            | 0                            | 0                            | 1                            | 1                            | -->

**Priors**: Our background beliefs about a causal domain usually will consist of more than just beliefs about which variables have causal connections; they will also usually contain beliefs about what *kinds* of effects operate between variables. That is, they will contain beliefs about which types are possible or, more generally, are more or less common in the world. We express these beliefs over causal effects as either restrictions on nodal types or as probability distributions over the nodal types. 

In general, when doing process tracing in this framework, we think of a given case of interest -- the one we are studying and seek to learn about -- as being drawn at random from a population. Thus, our prior beliefs about a *single* case -- before we do the process tracing -- are really beliefs about that population. So, for instance, our prior belief about the probability that inequality has a positive effect on democratization in Mexico in 1999 is our belief about how commonly inequality has a positive effect on democratization in the population of cases that are "like" Mexico in 1999.^[The reference population for a case is defined based on whatever we already know about the case. Thus, for instance, if we already know that the case has $Y=1$ before we begin process tracing, then the relevant population for the formation of prior beliefs is all cases in which $Y=1$.] 

We let $\lambda^j$ denote our belief about the population distribution of nodal types at node $j$. A $\lambda^j$ is simply a vector of proportions, one for each possible nodal type, with the proportions adding up to $1$. So, for instance, $\lambda^Y$ for our current example would be a vector with four values, each of which expresses a proportion for on one of the four nodal types at $Y$. So we might have $\lambda^Y_{0001}=0.1$, $\lambda^Y_{0011}=0.05$, and so on -- with the $\lambda^Y$ values summing to $1$ because these values are defined over the full set of possible nodal types for $Y$. 

We can, in turn, use these population parameters -- these beliefs about nodal-type proportions in the population -- to create prior probabilities over the *causal* type for the case at hand. Since causal types are merely combinations of nodal types, and our case has been drawn at random from the population, we can take a set of posited proprtions of nodal types in the population and readily calculate the probability that our case is of any given causal type. To do so, we need to join together $\lambda$'s across the nodes in a model. 

Let us first see how this works in a situation in which we assume that the nodal types are independent of one another. We can think of this as a situation in which there is no confounding that is not captured in the graph -- no variable missing from the model that is a common ancestor of multiple nodes in the model. Here, our beliefs over causal types are simply the product of our beliefs over the component nodal types (since the joint probability of independent events is simply the product of their individual probabilities). For instance, one causal type might be "a unit in which $X=1$ and in which $Y=1$ no matter what value $X$ takes." In this case the probability that a case is of this causal type might be written $\Pr(\theta^X = \theta^X_1)\Pr(\theta^Y = \theta^Y_{11}) = \lambda^X_1\lambda^Y_{11}$.

The simplest way in which we can express beliefs about the differential probabilities of different causal possibilities is by *eliminating* nodal types that we do not believe to be possible---setting their parameter values to $0$. Suppose, for instance, that we are examining the effect of ethnic diversity on civil war in a case. We might not know whether ethnic diversity causes civil war in this case, but we might have sufficient background knowledge to believe that ethnic diversity never has a *negative* effect on civil war: it never prevents a civil war from happening that would have happened in the absence of ethnic diversity. We would thus want to set the parameter value for a negative causal effect to $0$. If we then know nothing about the relative frequencies of the three remaining nodal types for $Y$, we may (following the  principle of indifference),  frequency of positive effects, null effects with civil war destined to happen, and null effects with civil war never going to happen, assigning a weight of $\frac{1}{3}$ to each of them. 

In a situation of unobserved confounding, our beliefs over causal types are still well defined, though they are no longer the simple product of beliefs over nodal types. Let us imagine for instance, in a simple $X \rightarrow Y$ model, that we believe that some unobserved factor both makes cases more likely to have $X = 1$ and makes it more likely that $X$ has a positive effect on $Y$. This is the same as saying that the probability that $\theta^X = \theta^X_1$ is positively correlated with the probability that $\theta^Y = \theta^Y_{01}$. Now, our probability that *both* $X=1$ and $X$ has a positive effect must be calculated using the joint probability formula, $\Pr(A, B) = \Pr(A)\Pr(B|A)$.^[In words, the probability of $A$ and $B$ occurring is equal to the probability of $A$ occurring times the probability of $B$ occurring *given* that $A$ occurs.] Thus, $\Pr(\theta^Y = \theta^Y_{01}, \theta^X = \theta^X_1) = \Pr(\theta^Y = \theta^Y_{01})\Pr(\theta^X = \theta^X_1 | \theta^Y = \theta^Y_{01})$. To form priors over causal types in this situation, we need to posit beliefs about a set of more complex, conditional proportions for $X$'s type. Specifically, we need to posit, *for those cases* with a positive effect of $X$ on $Y$, what proportion are "assigned" to $X=1$; and, separately, what proportion are assigned to $X=1$ among those cases *without* a positive effect of $X$ on $Y$.

These conditional proportions may, of course, be difficult for the researcher to form beliefs about. Forming a belief about them amounts to saying that we do not know what generates confounding, but we know the correlations it generates in the data. We may wonder how often we will be in that epistemological position. An alternative way to parse the problem, then, is to *model* the confounding by including the confounder (say, $Z$) as a new node in the graph. In the above example, $Z$ would point into both $X$ and $Y$. We would then posit population proportions for a set of nodal types for $X$ -- representing $X$'s possible responses to $Z$ -- and for $Y$ -- representing $Y$'s possible responses to both $X$ and $Z$. We may find it easier to reason and form beliefs about these more complex nodal types than about the conditional proportions involved in unobserved confounding. The two approaches work out to be analytically equivalent given equivalent underlying beliefs, so the choice between them will be a matter of researcher preference.^[As we will see later in the book, another approach is to gather data on additional cases. When analyzing multiple cases, we can set up our priors to allow for the possibility of unobserved confounding and then, potentially, learn about that confounding from the data. This is not possible under our procedure for single-case process tracing, where we treat the population parameters as given and fixed.]

Importantly, in process tracing, we are focused on drawing case-level inferences and, as such, we treat the population-level parameters as given and fixed. In general, these parameters derive from our beliefs about how the world works, and those beliefs will typically be uncertain.  The key point, however, is that in process tracing, the population parameters serve as an *input* into the analysis, conditioning our inferences from the evidence; but we do not *update* on these population-level beliefs once we see the data from a single case. Importantly, as we show later in the book, we *do* update on population-level inferences in the more general setup that we introduce in Chapter \@ref(mixing) for analyzing mixed data in multiple cases. We also show in Chapter \@ref(evaluation) how we can test the sensitivity of conclusions to the values at which we set population parameters. Interestingly, as we also show, process-tracing inferences, including uncertainty about conclusions, are unaffected by the level of uncertainty we might have about population paramaters; we thus do not specify this uncertainty for the purposes of process tracing.

The relationship between causal types, nodal types, and the correlation among nodal types is captured in what we call a *parameter matrix.* We show a parameter matrix for a simple $X \rightarrow Y$ model with no unobserved confounding in Table \@ref(tab:parammatrix). Here each column label (except the last) represents the probability that a case is of a given causal type. Each row label represents a population-level parameter: a belief about the proportions of different nodal types in the population. We indicate a set of possible parameter values in the final column. 

|     **Causal types** $\rightarrow$     | $\theta^X_0,\theta^Y_{00}$ | $\theta^X_1,\theta^Y_{00}$ | $\theta^X_0,\theta^Y_{10}$ | $\theta^X_1,\theta^Y_{10}$ | $\theta^X_0,\theta^Y_{01}$ | $\theta^X_1,\theta^Y_{01}$ | $\theta^X_0,\theta^Y_{11}$ | $\theta^X_1,\theta^Y_{11}$ | Parameter values (population proportions) |
|:--------------------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:-----------------------------------------:|
| **Population parameters** $\downarrow$ |                            |                            |                            |                            |                            |                            |                            |                            |                                           |
|              $\lambda^X_1$             |              0             |              1             |              0             |              1             |              0             |              1             |              0             |              1             |                    0.5                    |
|              $\lambda^X_0$             |              1             |              0             |              1             |              0             |              1             |              0             |              1             |              0             |                    0.5                    |
|            $\lambda^Y_{00}$            |              1             |              1             |              0             |              0             |              0             |              0             |              0             |              0             |                    0.2                    |
|            $\lambda^Y_{10}$            |              0             |              0             |              1             |              1             |              0             |              0             |              0             |              0             |                    0.2                    |
|            $\lambda^Y_{01}$            |              0             |              0             |              0             |              0             |              1             |              1             |              0             |              0             |                    0.4                    |
|            $\lambda^Y_{11}$            |              0             |              0             |              0             |              0             |              0             |              0             |              1             |              1             |                                           |
Table: (\#tab:parammmatrix). A mapping between nodal types and causal types for a simple $X \rightarrow Y$ model (with no unobserved confounding).

To start with the first two rows, these represent the population proportions of each of $X$'s nodal types. For instance, $\lambda^X_{0}$ is our belief about the proportion of cases in the population that are of nodal type $\theta^X_{0}$. The next row, $\lambda^X_{1}$, represents our belief about the inverse: the proportion of cases in the population of type $\theta^X_{1}$. We posit beliefs about these parameters in the final column, indicating that we think that half of cases in the population are "assigned" to $X=0$ and half to $X=1$. Note that, since there are only two possible nodal types for $X$, and their proportions must sum to 1, there is actually just one degree of freedom here: once we've specified one of these parameter values, the other is defined as well.

The last four rows represent the proportion of cases in the population with different $Y$-nodal types: in order, the proportion in which $X$ has  no effect on $Y$, with $Y$ fixed at $0$; the proportion in which $X$ has a negative effect; the proporiton in which $X$ has a positive effect; and the proportion in which $X$ has no effect, with $Y$ fixed at $1$. Again, in the last column, we provide possible values for these proportions, the four of which must also sum to $1$. Here we are stating that positive $X \rightarrow Y$ effects are twice as common in the population as the other three nodal types, which we set at equal prevalence.

The interior cells indicate whether a given population parameter enters into the prior probability of a given causal type. Thus, for instance, to calculate the prior probability of the causal type $\theta^X_1,\theta^Y_{10}$, we need to multiply the two parameters values corresponding to the $1$'s in this causal type's column: $\lambda^X_1$ by $\lambda^Y_{10}$. Given the parameter values we have assigned for this example, then, the prior on this causal type is simply $0.5 \times 0.2 = 0.1$. 

The prior probability that a case is of a given causal type thus comes directly from our beliefs about how nodal types are distributed in the population. All we know before we study a case is whatever we know about cases "like" it in general. It is then these causal-type probabilities -- which represent probabilities that a *given case* is of a particular causal type -- that we will update on once we see the data for this case.

We show the somewhat more complex situation of unobserved confounding in Table \@ref(tab:parammatrixconf). It is the first four rows that allow for unobserved confounding---the correlations across types. In a potential outcomes framework, we could think of these rows as capturing differential "assignment propensities" for $X$. Here, we allow for different probabilities of $X$'s type being $\theta^X_1$ depending on what $Y$'s type is. Thus, $\lambda^X_0 | \theta^Y= \theta^Y_{01}$ is the proportion of $\theta^X_0$ types among cases with $\theta^Y_{01}$ type: put differently, it is the probability of $X$ being assigned to $0$ when $X$ has a positive effect on $Y$. The second row represents the inverse proportion: the proportion of a $\theta^X_1$ types among $\theta^Y_{01}$ types. The next two rows then capture the proportions of the $X$-types among all *other* $Y$-types (i.e., among those cases for which $X$ does *not* have a positive effect on $Y$).

Unobserved confounding in this setup takes the form of a difference in the proportions of a given $X$ type among different $Y$ types. Thus, if $\lambda^X_1, | \theta^Y_{01}$ is not the same as $\lambda^X_1 | \theta^Y \neq \theta^Y_{01}$, we have unobserved confounding. Imagine, for instance, if we are studying the effect of faster economic growth ($X$) on democratization ($Y$), and we believe that there is some unobserved factor that both makes some countries' economies grow more quickly and also makes economic growth more likely to have a positive effect on democratization. This belief amounts to a belief that the probability of a case being assigned to $X=1$ is higher if $Y$'s nodal type is $\theta^Y_{01}$ than if it is not. In other words, in terms of the rows in Table \@ref(tab:parammatrix), we believe here that $\lambda^X_1 | \theta^Y=\theta^Y_{01}$ is greater than $\lambda^X_1 | \theta^Y \neq \theta^Y_{01}$. To illustrate, we provide parameter values along these lines in the final column.

Again, however, a researcher might prefer to specify the confounder (say, $Z$) as a node in the model. The rows in the parameter matrix would then be a set of population parameters defined as proportions of *un*conditional nodal types, with four $X$-types representing possible responses to $Z$, and 16 $Y$ types, representing $Y$'s possible responses to $X$ and $Z$.

| **Causal Types $\rightarrow$** | $\theta^X_0,\theta^Y_{00}$ | $theta^X_1,\theta^Y_{00}$ | $\theta^X_0,\theta^Y_{10}$ | $\theta^X_1,\theta^Y_{10}$ | $\theta^X_0,\theta^Y_{01}$ | $\theta^X_1,\theta^Y_{01}$ | $\theta^X_0,\theta^Y_{11}$ | $\theta^X_1,\theta^Y_{11}$ | Parameter values (population proportions) |
|------------------------------------------------:|:------------------------------:|:------------------------------:|:------------------------------:|:-----------------------------:|:------------------------------:|:------------------------------:|:------------------------------:|:------------------------------:|
|           **Population parameters $\downarrow$**           |                                |                                |                                |                               |                                |                                |                                |                                |                                           |
|     $\lambda^X_0 | \theta^Y= \theta^Y_{01}$     |                0               |                0               |                0               |               0               |                1               |                0               |                0               |                0               |                    0.3                    |
|          $\lambda^X_1 | \theta^Y=\theta^Y_{01}$         |                0               |                0               |                0               |               0               |                0               |                1               |                0               |                0               |                    0.7                    |
|    $\lambda^X_0| \theta^Y \neq \theta^Y_{01}$   |                1               |                0               |                1               |               0               |                0               |                0               |                1               |                0               |                    0.5                    |
|   $\lambda^X_1 | \theta^Y \neq \theta^Y_{01}$   |                0               |                1               |                0               |               1               |                0               |                0               |                0               |                1               |                    0.5                    |
|                 $\lambda^Y_{00}$                |                1               |                1               |                0               |               0               |                0               |                0               |                0               |                0               |                    0.2                    |
|                 $\lambda^Y_{10}$                |                0               |                0               |                1               |               1               |                0               |                0               |                0               |                0               |                    0.2                    |
|                 $\lambda^Y_{01}$                |                0               |                0               |                0               |               0               |                1               |                1               |                0               |                0               |                    0.4                    |
|                 $\lambda^Y_{11}$                |                0               |                0               |                0               |               0               |                0               |                0               |                1               |                1               |                    0.2                    |
Table: (\#tab:parammmatrixconf). A mapping between nodal types and causal types for a simple $X \rightarrow Y$ model *with* unobserved confounding.

<!-- In this case the number of parameters may exceed the number of nodal types, with, for instance parameters $\hat{\lambda}^Y_{11}$ representing $\Pr(\theta^Y = \theta^Y_{11}|\theta^X = \theta^X_1)$  and $\tilde{\lambda}^Y_{11}$ representing $\Pr(\theta^Y = \theta^Y_{11}|\theta^X = \theta^X_0)$.   -->

One special kind of prior that we might wish to set is to disallow a particular (conditional) type altogether. For instance, if studying the effect of we may believe that 

**Possible data types.** A *data type* is a particular pattern of data that we could potentially observe for a given case. More specifically, a data type is a set of values, one for each node in a model. For instance, in our $X, W, Y$ setup, $X=1, W=0, Y=0$ would be one data type. 

Importantly, each possible causal type *maps into a single data type.* One intuitive way to think about why this is the case is that a causal type tells us (a) the values to which all exogenous variables in a model are assigned and (b) how all endogenous variables respond to their parents. Given these two components, only one set of node values is possible. For example, causal type $\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_0, \theta^Y = \theta^Y_{0100})$ imples data $X=1, W=0, Y=1$. There is no other set of data that can be generated by this causal type. 

Equally importantly, however, *the mapping from causal types to data types is not one-to-one.* More than one causal type can generate the same case-level data pattern. For instance, the causal type $\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_0, \theta^Y = \theta^Y_{1101})$ will *also* generate the  data type, $X=1, W=0, Y=1$. Thus, observing this data type leaves us with ambiguity about the causal type by which it was generated.

A full mapping between causal types and data types can be summarized by an "ambiguity matrix." In Table \@ref(tab:ambigmatrix), we provide an example of such a matrix, derived directly from the parameter matrix in Table \@ref(tab:parammatrix). Here, the rows represent causal types and the columns (except for the last) represent data types. The notation for data types is straightforward, with for instance $X0Y0$ meaning that $X=0, Y=0$ has been observed. In the interior cells, the $1$'s and $0$'s indicate whether or not a given data type could arise from a given causal type. We can readily see here that each causal type can generate only one data type. 

We can also see the ambiguity of the data, however, since each data type can be generated by two causal types. For instance, if we observe $X=1, Y=1$, we know that the case is either of causal type $\theta^X_1,\theta^Y_{01}$ or of causal type $\theta^X_1,\theta^Y_{11}$ -- but do not know which.

|  **Data types** $\rightarrow$ | X0Y0 | X1Y0 | X0Y1 | X1Y1 | Priors on causal types |
|:-----------------------------:|:----:|:----:|:----:|:----:|:----------------------:|
| **Causal types** $\downarrow$ |      |      |      |      |                        |
|   $\theta^X_0,\theta^Y_{00}$  |   1  |   0  |   0  |   0  |           0.1          |
|   $\theta^X_1,\theta^Y_{00}$  |   0  |   1  |   0  |   0  |           0.1          |
|   $\theta^X_0,\theta^Y_{10}$  |   0  |   0  |   1  |   0  |           0.1          |
|   $\theta^X_1,\theta^Y_{10}$  |   0  |   1  |   0  |   0  |           0.1          |
|   $\theta^X_0,\theta^Y_{01}$  |   1  |   0  |   0  |   0  |           0.2          |
|   $\theta^X_1,\theta^Y_{01}$  |   0  |   0  |   0  |   1  |           0.2          |
|   $\theta^X_0,\theta^Y_{11}$  |   0  |   0  |   1  |   0  |           0.1          |
|   $\theta^X_1,\theta^Y_{11}$  |   0  |   0  |   0  |   1  |           0.1          |
Table: (\#tab:ambigmatrix). An ambiguity matrix, mapping from data types to causal types for a simple $X \rightarrow Y$ model.

In the last column, we provide prior probabilities for each of the causal types. These have been calculated directly from the parameter matrix (Table \@ref(tab:parammatrix)). To see how the calculation works, start with a causal type in the parameter matrix -- say, $\theta^X_0,\theta^Y_{01}$. We go down that causal type's column and select the rows with $1$'s, representing the parameters for the included nodal types, $\lambda^X_0$ and $\lambda^Y_{01}$. As we want the joint probability of these two nodal types (and a parameter matrix is constructed such that the rows represent independent events),^[That is, when there is unobserved confounding, we express conditional proportions, making all of the proportions conditionally independent of one another.] we simply multiply together the values for these included parameters: $0.5 \times 0.4 = 0.2$. As noted, our prior belief about whether the case at hand is of a given causal type is a straightforward function of our beliefs about how prevalent each of the component nodal types is in the population. 

As models get more complex, the numbers of causal and data types simply multiply. In Table \@ref(tab:ambigmatrixmed), we show the ambiguity matrix for a simple mediation model ($X \rightarrow M \rightarrow Y$). Here, the causal types are combinations of three nodal types, one for each variable in the model. Similarly, the data types have three elements, one for each variable. We now have 8 data types and 32 causal types. 

|       **Data types** $\rightarrow$      | X0M0Y0 | X1M0Y0 | X0M1Y0 | X1M1Y0 | X0M0Y1 | X1M0Y1 | X0M1Y1 | X1M1Y1 | Priors on causal types |
|:---------------------------------------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:----------------------:|
|      **Causal types** $\downarrow$      |        |        |        |        |        |        |        |        |                        |
| $\theta^X_0,\theta^M{00},\theta^Y_{00}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{00},\theta^Y_{00}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{10},\theta^Y_{00}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{10},\theta^Y_{00}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{01},\theta^Y_{00}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          |
| $\theta^X_1,\theta^M{01},\theta^Y_{00}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.04          |
| $\theta^X_0,\theta^M{11},\theta^Y_{00}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{11},\theta^Y_{00}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{00},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{00},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{10},\theta^Y_{10}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{10},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{01},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.04          |
| $\theta^X_1,\theta^M{01},\theta^Y_{10}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.04          |
| $\theta^X_0,\theta^M{11},\theta^Y_{10}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{11},\theta^Y_{10}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{00},\theta^Y_{01}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          |
| $\theta^X_1,\theta^M{00},\theta^Y_{01}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          |
| $\theta^X_0,\theta^M{10},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.04          |
| $\theta^X_1,\theta^M{10},\theta^Y_{00}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          |
| $\theta^X_0,\theta^M{01},\theta^Y_{01}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.08          |
| $\theta^X_1,\theta^M{01},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.08          |
| $\theta^X_0,\theta^M{11},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.04          |
| $\theta^X_1,\theta^M{11},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.04          |
| $\theta^X_0,\theta^M{00},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.02          |
| $\theta^X_1,\theta^M{00},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{10},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.02          |
| $\theta^X_1,\theta^M{10},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          |
| $\theta^X_0,\theta^M{01},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.04          |
| $\theta^X_1,\theta^M{01},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.04          |
| $\theta^X_0,\theta^M{11},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.02          |
| $\theta^X_1,\theta^M{11},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.02          |
Table: (\#tab:ambigmatrixmed). An ambiguity matrix, mapping from data types to causal types for a simpe mediation model, $X \rightarrow M \rightarrow Y$.

Again, the ambiguities arising from data patterns are apparent. For instance, if we observe $X=1, M=0, Y=0$, we see that there are four causal types that could have generated this pattern. To unpack the situation a bit, these data tell us that $\theta^X = \theta^X_1$. But they do not tell us whether $M$'s type is such that $X$ has a negative effect on $M$ ($\theta^M_{10}$) or $X$ has no effect with $M$ fixed at $0$ ($\theta^M_{00}$). Similarly, we do not know whether $M$ has a positive effect on $Y$ ($\theta^Y_{01}$) or no effect with $Y$ fixed at $0$ ($\theta^Y_{00}$). This leavs four combinations of nodal types---four causal types---that are consistent with the data.

Our priors here derive from a set of parameter values, much like in the previous example, in which the $X$ types are equally common (0.5 each); a positive effect of $X$ on $M$ is twice as common (0.4) as the other $M$ types (all set to 0.2); and a positive effect of $M$ on $Y$ is twice as common (0.4) as all other $Y$ types (all at 0.2). We can then easily see why we thus get priors on some causal types are higher than those on others: for instance, the two causal types with priors of 0.08 both have two positive effects (at the $X \rightarrow Y$ and $M \rightarrow Y$ stages) while the causal types with priors of 0.02 include no positive effects at either stage.


```{r, echo = FALSE}
XY <- make_model("X -> Y") %>% set_parameters(c(.5, .5, .2, .2, .4, .2))
ambiguityXY  <- get_ambiguities_matrix(XY)


XMY <- make_model("X -> M -> Y") %>% set_parameters(c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2))
ambiguityXMY  <- get_ambiguities_matrix(XMY)
```

<!-- ```{r ambigmatrix, echo = FALSE} -->
<!-- ambXY_with_priors <- data.frame(cbind(ambiguityXY, prior = draw_type_prob(XY, using = "parameters"))) -->
<!-- kable(ambXY_with_priors), caption = "Ambiguity matrix for X -> Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows.") -->
<!-- ``` -->


<!-- For an $X \rightarrow M \rightarrow Y$ model: -->

```{r, echo = FALSE}

ambXMY_with_priors <- data.frame(cbind(ambiguityXMY, prior = draw_type_prob(XMY, using = "parameters")))

kable(ambXMY_with_priors, caption = "Ambiguity matrix for X -> M -> Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows.")

# View(x)
```

**Updating on types given the data.** Once we observe actual data in a case, we can then update on the probabilities assigned to each causal type. The logic is simple. When we observe a set of data from a case, we place $0$ probability on all causal types that could not have produced these data; we then scale up the probabilities on all causal types that could have. 

We can see how this works within an ambiguity matrix. Let's return to the ambiguity matrix in Table \@ref(tab:ambigmatrix). We start out with a set of probability weights on all rows (causal types). Now, suppose that we observe the data $X=1, Y=1$, i.e., data type $X1Y1$. We then look down the $X1Y1$ column, and we know that all rows with a $0$ in them represent causal types that *could not have* generated these data. These causal types are thus excluded. What is left are two rows: $\theta^X_1, \theta^Y_{01}$ and $\theta^X_1, \theta^Y_{11}$. Returning now to the probabilities, we put 0 weight on all of the excluded rows; and then we scale up the remaining probabilities so that they sum to 1 (preserving the ratio between them).  The priors of 0.2 and 0.1 in the retained rows scale up to $\frac{2}{3}$ and $\frac{1}{3}$, which become our *posterior* probabilities on the causal types. We display an updated ambiguity matrix, with excluded data types and causal types removed, in Table \@ref(tab:ambigupdate). 

Before we see any data on the case at hand, then, we believe (based on our beliefs about the population to which the case belongs) that there is a 0.2 probability that the case is one in which $X$ is assigned to $1$ and has a positive effect on $Y$; and 0.1 probability that it's a case in which $X$ gets assigned to $1$ and has no effect on $Y$ with $Y$ fixed at $1$. Seeing the $X=1, Y=1$ data, we now believe that there is a 0.667 probability that the case is of the former type, and a 0.333 probability that it is of the latter type.

|  **Data types** $\rightarrow$ | X1Y1 | Priors on causal types | Posteriors on causal types |
|:-----------------------------:|:----:|:----------------------:|----------------------------|
| **Causal types** $\downarrow$ |      |                        |                            |
|   $\theta^X_1,\theta^Y_{01}$  |   1  |           0.2          |          0.6667            |
|   $\theta^X_1,\theta^Y_{11}$  |   1  |           0.1          |          0.3333            |
Table: (\#tab:ambigupdate). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrix), after observing $X=1, Y=1$ in a case.



<!-- ```{r, echo = FALSE} -->
<!-- ambXY_with_priors%>% -->
<!--     mutate(type = rownames(ambXY_with_priors)) %>% -->
<!--     select(type, X1Y1, prior) %>% -->
<!--     filter(X1Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable() -->
<!-- ``` -->

We can also see how this works for our $X \rightarrow M \rightarrow Y$ model, and the ambiguity matrix (partially represented) in Table \@ref(tab:ambigmatrixmed). If we observe the data $X=1, M=0, Y=0$, for instance, this exercise would yield the updated ambiguity matrix in Table \ref@(tab:ambigmedupdate). , we have eliminated all rows (causal types) with a $0$ in the relevant data-type column ($X1M0Y0$) and formed the posteriors by scaling up the priors in the retained rows. 


|       **Data types** $\rightarrow$      | X1M0Y0 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |                        |                            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{00}$ |    1   |          0.02          |          0.1667            |
| $\theta^X_1,\theta^M_{10},\theta^Y_{00}$ |    1   |          0.02          |          0.1667            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{01}$ |    1   |          0.04          |          0.3333            |
| $\theta^X_1,\theta^M_{10},\theta^Y_{01}$ |    1   |          0.04          |          0.3333            |
Table: (\#tab:ambigmedupdate). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing $X=1, M=0, Y=0$ in a case.

A notable feature of the logic of single-case process tracing is that the relative probabilities on the retained causal types never change. If we start out believing that causal type $A$ is twice as likely as causal type $B$, and both $A$ and $B$ are retained once we see the data, then $A$ will be twice as likely as $B$ in our posteriors. All updating occurs by *eliminating* causal types from consideration and zeroing in on those that remain.

```{r, echo = FALSE}
ambXMY_with_priors%>%
    mutate(type = rownames(ambXMY_with_priors)) %>%
    select(type, X1M0Y0, prior) %>%
    filter(X1M0Y0 ==1)%>%
   mutate(posterior = prior/sum(prior)) %>%
   kable()
```



<!-- ```{r, echo = FALSE} -->
<!-- data.frame(cbind(ambiguityXMY)) %>% -->
<!--     mutate(type = rownames(ambiguityXMY), prior = draw_type_prob(XMY)) %>% -->
<!--     select(type, X1M1Y1, prior) %>% -->
<!--     filter(X1M1Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable(digits = 2) -->
<!-- ``` -->

A similar logic applies if partial data are observed: that is, if we do not collect data for all nodes in the model. The one difference is that, now, rather than reducing to one column we entertain the possibility of any data *type* consistent with the *observed data*. In general, more than one data type will be consistent with partial data. For instance, suppose that we observe $X=1, Y=0$ but do not observe $M$'s value. These are data that are consistent with both the data type $X1M0Y0$ and the data type $X1M1Y0$ (since the unobserved $M$ could be either $0$ or $1$). We thus retain both of these data-type columns as well as all causal types consistent with *either* of these data types. This gives the updated ambiguity matrix in Table \@ref(tab:ambigmedupdatepartial). We note that, with these partial data, we are not able to update as strongly. For instance, for the causal type $\theta^X_1,\theta^M_{00},\theta^Y_{00}$, instead of updating to a posterior probability of 0.1667, we update to a posterior of only 0.0833 -- because there is a larger set of causal types with which these partial data are consistent.


|       **Data types** $\rightarrow$      | X1M0Y0 | X1M1Y0 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |        |                        |                            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{00}$ |    1   |    0   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{00}$ |    1   |    0   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{00}$ |    0   |    1   |          0.04          |           0.1667           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{00}$ |    0   |    1   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{10}$ |    0   |    1   |          0.04          |           0.1667           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{10}$ |    0   |    1   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{00},\theta^Y_{01}$ |    1   |    0   |          0.04          |           0.1667           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{01}$ |    1   |    0   |          0.04          |           0.1667           |
Table: (\#tab:ambigmedupdatepartial). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing partial data in case: $X=1, Y=0$, with $M$ unobserved.


```{r, echo = FALSE}
ambXMY_with_priors%>%
    mutate(type = rownames(ambXMY_with_priors)) %>%
    select(type, X1M0Y0, X1M1Y0, prior) %>%
    filter(X1M0Y0 ==1 | X1M1Y0 ==1)%>%
   mutate(posterior = prior/sum(prior)) %>%
   kable()
```


<!-- ```{r, echo = FALSE} -->
<!-- data.frame(cbind(ambiguityXMY)) %>% -->
<!--     mutate(type = rownames(ambiguityXMY), prior = draw_type_prob(XMY)) %>% -->
<!--     select(type, X1M0Y1, X1M1Y1, prior) %>%  -->
<!--     filter(X1M1Y1 ==1 | X1M0Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable(digits = 2) -->
<!-- ``` -->

**Updating on estimands.**  We now have a posterior probability for each causal type for the case at hand. The causal question we are interested in answering, our estimand, may not be about causal types *per se.* It is about an estimand that can be expressed as a *combination* of causal types. 

For instance, suppose we are working with the model $X \rightarrow M \rightarrow Y$. And suppose that our question is, "Did $X=1$ cause $Y=1$?" This question is asking both:

1. Does $X=1$ in this case? 

2. Does $X$ have a positive effect on $Y$ in this case?  

The causal types that qualify are those, and only those, in which the answer to both is "yes."  

Meeting condition (1) requires that $\theta^X=\theta^X_1$. 

Meeting condition (2) requires that $\theta^M$ and $\theta^Y$ are such that $X$ has an effect on $M$ that yields a positive effect of $X$ on $Y$. This could occur via a positive $X \rightarrow M$ effect linked to a positive $M \rightarrow Y$ effect or via a negative $X \rightarrow M$ effect linked to a negative $M \rightarrow Y$ effect. 

Thus, the qualifying causal types in this model are:

* $\theta^X_1, \theta^M_{01}, \theta^Y_{01}$
* $\theta^X_1, \theta^M_{10}, \theta^Y_{10}$

Our *prior* on the estimand---what we believe before we collect data on the case at hand---is given simply by summing up the prior probabilities on each of the causal types that correspond to the estimand. Note that we must calculate the prior from the full ambiguity matrix, before excluding types for inconsistency with the data. Returning to the full ambiguity matrix in Table \@ref(tab:ambigmatrixmed), we see that the priors on these two types (given the population parameters assumed there) are 0.08 and 0.02, respectively, giving a prior for the estimand of 0.1. 

The posterior on any estimand is, likewise, given by summing up the posterior probabilities on each of the causal types that correspond to the estimand, drawing of course from the updated ambiguity matrix. For instance, if we observe the data $X=1, $M=1$ Y=1$, we update to the ambiguity matrix in Table \@ref(tab:ambigmedupdate2).  Our posterior on the estimand, "Did $X=1$ cause $Y=1$?" is the sum of the posteriors on the above two causal types. Since $\theta^X_1, \theta^M_{10}, \theta^Y_{10}$ is excluded by the data, this just leaves the posterior on $\theta^X_1, \theta^M_{01}, \theta^Y_{01}$, 0.4444, which is the posterior belief on our estimand. 

If we observe only the partial data, $X=1, Y=1$, then we update to the ambiguity matrix in Table \@ref(tab:ambigmedupdatepartial2). Now both corresponding causal types are included, and we sum their posteriors to get the posterior on the estimand: $0.0769 + 0.3077 = 0.3846$.


```{r, echo = FALSE}
ambXMY_with_priors%>%
    mutate(type = rownames(ambXMY_with_priors)) %>%
    select(type, X1M1Y1, prior) %>%
    filter(X1M1Y1 ==1)%>%
   mutate(posterior = prior/sum(prior)) %>%
   kable()
```


|       **Data types** $\rightarrow$      | X1M1Y1 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |                        |                            |
| $\theta^X_1,\theta^M_{01},\theta^Y_{01}$ |    1   |          0.08          |          0.4444            |
| $\theta^X_1,\theta^M_{11},\theta^Y_{01}$ |    1   |          0.04          |          0.2222            |
| $\theta^X_1,\theta^M_{01},\theta^Y_{11}$ |    1   |          0.04          |          0.2222            |
| $\theta^X_1,\theta^M_{11},\theta^Y_{11}$ |    1   |          0.02          |          0.1111            |
Table: (\#tab:ambigmedupdate2). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing $X=1, M=1, Y=1$ in a case.



|       **Data types** $\rightarrow$      | X1M0Y0 | X1M1Y0 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |        |                        |                            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{10}$ |    1   |    0   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{10}$ |    1   |    0   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{01}$ |    0   |    1   |          0.08          |           0.3077           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{01}$ |    0   |    1   |          0.04          |           0.1538           |
| $\theta^X_1,\theta^M_{00},\theta^Y_{11}$ |    0   |    1   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{11}$ |    0   |    1   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{11}$ |    1   |    0   |          0.04          |           0.1538           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{11}$ |    1   |    0   |          0.02          |           0.0769           |
Table: (\#tab:ambigmedupdatepartial2). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing partial data in case: $X=1, Y=0$, with $M$ unobserved.



<!-- ```{r, echo = FALSE} -->
<!-- ambXMY_with_priors%>% -->
<!--     mutate(type = rownames(ambXMY_with_priors)) %>% -->
<!--     select(type, X1M0Y1, X1M1Y1, prior) %>% -->
<!--     filter(X1M0Y1 ==1 | X1M1Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable() -->
<!-- ``` -->


For more complex models and estimands, it can be more difficult to eyeball the corresponding causal types. In practice, therefore, we use a function in the `gbiqq` package to do this for us. 


<!-- For example, supposer we want to know whether $X$ has some causal effect on $Y$ in our simple mediation model. The estimand, "$X$ haa a causal effect on $Y$" maps onto a relatively large, though still easily calculated, collection of types. Using `gbiqq`'s get_types function, we would define our query as a search for all causal types in which $Y$'s potential outcome when $X=1$ is different from $Y$'s potential outcome when $X=0$. The function then reports back all causal types meeting this condition: -->

<!-- ```{r, eval = FALSE} -->
<!-- get_types(XMY, "Y[X=1] != Y[X=0]") -->
<!-- ``` -->


```{r, echo = FALSE, comment = ""}
types <- get_types(XMY, "Y[X=1] != Y[X=0]")
cat(paste0(names(types$types[types$types]), collapse = ", "))
```


This completes the abstract representation of the process tracing procedure. We now build up the intuition by walking through the procedure for simple mediation and moderation models.






### Illustration with code

```{r, eval = FALSE}

XMY <- make_model("X -> M -> Y") %>% 
       set_parameters (c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2))

query_model(model = XMY, 
              queries = list(PC = "Y[X=1] > Y[X=0]"), 
              subsets = list(TRUE, "X==1 & Y==1", "X==1 & Y==1 & M==0", "X==1 & Y==1 & M==1"),
              using = "parameters")
```

```{r, echo = FALSE}
make_model("X -> M -> Y") %>% 
       set_parameters (c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2)) %>%
       query_model(
         queries = list(PC = "Y[X=1] > Y[X=0]"), 
         subsets = list(TRUE, "X==1 & Y==1", "X==1 & Y==1 & M==0", "X==1 & Y==1 & M==1"),
         using = "parameters") %>%
      kable()

```

## Five principles

#### Classic qualitative tests are special cases of updating on a model

The approach we have described here updates on the model given data on all variables, and from the model makes inferences to estimands. 

This procedure appears different to the approach described for example in  @collier2004sources and outlines in Chapter 5 in which one seeks specific evidence that is directly informative about causal propositions: "clues" that are arise with different probabilities if one proposition or another is true. In fact however the approaches are deeeply connected.  This  "probative value of clues" approach can indeed be *justified* by reference to more fully elaborated  models of the world. 

To see this  we can write down the probability of observing $K=1$ conditional on causal type and $X$, using the $\phi$ notation from @humphreys2015mixing and introduced in Chapter 5. Here $\phi_{jx}$ refers to the probability of observing a clue in a case of type $j$ when $X=x$. Starting with our prior distribution over the lower-level causal types (the $\lambda$'s), we can derive, for an $X=1$ case, the probability of seeing the clue if the case is of type $b$ (positive effect) or of type $d$ (no effect, $Y$ always $1$):

\begin{equation}
\begin{split}
\phi_{b1} & = \frac{\lambda_{01}^{K}\lambda_{01}^{Y}}{\lambda_{01}^{K}\lambda_{01}^{Y}+\lambda_{10}^{K}\lambda_{10}^{Y}}\\ 
\phi_{d1} & = \frac{\lambda_{11}^{Y}(\lambda_{01}^{K}+\lambda_{11}^{K})+\lambda_{11}^{K}\lambda_{01}^{Y}}{\lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y}}
\end{split}
(\#tab:eq:phisfromlambdas)
\end{equation}


These quantities allow for easy mapping between our prior beliefs about our causal query---as expressed in the lower level model---and the classic process-tracing tests in @Van-Evera:1997. Figure \@ref(fig:phis) illustrates. In each panel, we manipulate a prior for one or more of the lower-level causal effects, keeping all other priors flat, and we see how probative value changes. As the curves for $\phi_b$ and $\phi_d$ diverge, probative value is increasing since there is an increasing difference between the probability of seeing the clue if $X$ has a positive effect on $Y$ and the probability of seeing the lcue if $X$ has no effect. 

In the left panel, we see that as we place a lower prior probability on $K$'s being negatively affected by $X$,^[For a given value of $\lambda^K_{01}$, we hold the other $\lambda^K$ values equal by assigning a value of $(1-\lambda^K_{01})/3$ to each.] seeking $K=1$ increasingly takes on the quality of a hoop test for $X$'s having a positive effect on $Y$. The clue, that is, increasingly becomes something we must see if $X$ positively affects $Y$, with the clue remaining moderately probable if there is no effect. Why? The less likely we believe it is that $K=0$ was caused by $X=1$, the less consistent the observation of $K=0$ is with $X$ having a positive causal effect on $Y$ via $K$ (since, to have such an effect, if $X=1$ and $K=0$, would precisely have to mean that $X=1$ *caused* $K=0$). 

In the second graph, we simultaneously change the prior probabilities of zero effects at both stages in the sequence: of $K$ and $Y$ being $1$ regardless of the values of $X$ and $K$, respectively.^[For a given value of $\lambda^K_{11}$, we hold the other $\lambda^K$'s equal by assigning a value of $(1-\lambda^K_{11})/3$ to each; likewise for $\lambda^Y_{11}$ and the other $\lambda^Y$ values.] We see here that, as the probabilities of zero effects jointly diminish, seeking $K=1$ increasingly becomes a smoking-gun test for a positive effect of $X$ on $Y$: the probability of seeing the clue if the case is a $d$ type diminishes. The reason is that, as zero effects at the lower level become less likely, it becomes increasingly unlikely that $K=1$ could have occurred without a positive effect of $X$ on $K$, and that $Y=1$ could have occurred (given that we have seen $K=1$) without a posiitve effect of $K$ on $Y$.

<!-- This example also helps clarify the kind of theoretical knowledge required for drawing inferences from clues. As we have emphasized, the structural equations comprising a causal model can be fully non-parametric. As the example illustrates, $\theta_Y$ can be a type variable that determines different the equation for an endogenous variable in a causal model can  can take the form of beliefs about the proportions of  -->


```{r phis, echo = FALSE,  fig.align="center", out.width='.85\\textwidth', fig.width = 9, fig.height = 4, fig.cap = "The probability of observing $K$ given causal type for different beliefs on lower-level causal effects. In the left figure, priors on all lower-level causal effects are flat except for the probability that $X$ has a negative effect on $K$. If we believe that it is unlikely that $X$ has a negative effect on $K$, $K$ becomes a `hoop' test for the proposition that a case is of type $b$. The righthand figure considers simultaneous changes in $\\lambda_{11}^K$ and  $\\lambda_{11}^Y$---the probabilities that $K=1$ regardless of $X$, and that $Y=1$  regardless of $K$, with flat distributions on all other lower-level effects. With $\\lambda_{11}^K$, $\\lambda_{11}^Y$ both close to 0, $K$ becomes a 'smoking gun' test for the proposition that $X$ has a positive effect on $Y$ ($b$ type).", errors = FALSE, warning = FALSE, message = FALSE, comment = FALSE}

# sim_data <- function(sims=100, pX = 1, K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25)){
#   X <- rmultinom(sims, 1, c(1-pX, pX))
#   X <- t(X) %*% (0:1)  
#   uK <- rmultinom(sims, 1, K_eventprobs) 
#   uK <- t(uK) %*% (1:4)  
#   uY <- rmultinom(sims, 1, Y_eventprobs) 
#   uY <- t(uY) %*% (1:4)  
#   K <- (uK==1)*(X==0) +  (uK==2)*(X==1) +  (uK==4)
#   Y <- (uY==1)*(K==0) +  (uY==2)*(K==1) +  (uY==4)
#   
#   a_higher <- (uK==2)*(uY==1) + (uK==1)*(uY==2)
#   b_higher <- (uK==2)*(uY==2) + (uK==1)*(uY==1)
#   c_higher <- (uY == 3) + (uK==3)*(uY==2) + (uK==4)*(uY==1)
#   d_higher <- (uY == 4) + (uK==3)*(uY==1) + (uK==4)*(uY==2)
#   
#   data.frame(X, K, Y, a_higher, b_higher, c_higher, d_higher)
#  }
# 
# 
# pv <- function(sims=100, K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25)){
#   D <- sim_data(sims = sims, K_eventprobs = K_eventprobs, Y_eventprobs = Y_eventprobs)  
#   c(mean(D$K[D$X==1 & D$Y==1 & D$b_higher ==1]), mean(D$K[D$X==1 & D$Y==1 & D$d_higher ==1]))
# }

pv_analytic_b <- function(K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25))
  {K_eventprobs[2]*Y_eventprobs[2]}/{K_eventprobs[2]*Y_eventprobs[2]+K_eventprobs[1]*Y_eventprobs[1]}

pv_analytic_d <- function(K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25))
  {Y_eventprobs[4]*(K_eventprobs[2]+K_eventprobs[4])+K_eventprobs[4]*Y_eventprobs[2]}/{Y_eventprobs[4]+K_eventprobs[3]*Y_eventprobs[1]+K_eventprobs[4]*Y_eventprobs[2]}

par(mfrow = c(1,2))

plot(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_b(K_eventprobs = c(i,(1-i)/3,(1-i)/3,(1-i)/3), Y_eventprobs = c(1/4,1/4,1/4,1/4))), type = "l", xlab = expression(paste(lambda[10]^{K})), ylab = "", main = "A Hoop Test", ylim = c(0,1))

points(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_d(K_eventprobs = c(i,(1-i)/3,(1-i)/3,(1-i)/3), Y_eventprobs = c(1/4,1/4,1/4,1/4))), type = "l", lty = 2)
text(.1,.8, expression(phi[b]))
text(.1,.5, expression(phi[d]))


plot(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_b(K_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i), Y_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i))), ylim = c(0,1), type = "l", xlab = expression(paste(lambda[11]^{K}," and ", lambda[11]^{Y})), ylab = "", main = "A Smoking Gun Test")

points(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_d(K_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i), Y_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i))),  type = "l", lty = 2)
text(.1,.55, expression(phi[b]))
text(.1,.25, expression(phi[d]))

```



### Conditional independence alone does not provide probative value 

### Uncertainty does not alter inference for single case causal inference

In the procedure described for process tracing in this chapter (and different to what we introduce in Chapter 8) we assume that $\lambda$ is known and we do not place uncertainty around it.

This might appear somewhat heroic, but in fact for single case inference it is  without loss of generality. The expected inferences we would make for any estimand accounting for priors  is the same as the inferences we if we use the expectation only.  

To see this, let $\pi_j$ denote the probability of observing causal type $j$ and $p(D)$ te probability of observing data realization $D$. Say that $j \in D$ if type $j$ produces data type $D$ and say $j \in E$ if casual type $j$ is an element of the estimand set of interest. For instance in an $X \rightarrow Y$ model, if we observe $X=Y=1$ then $D$ consists of causal types $D={(\theta^X_1, \theta^Y_{01}), (\theta^X_1, \theta^Y_{11})})$ and the estimand set for "$X$ has a positive effect on  $Y$" consists of  $E={(\theta^X_1, \theta^Y_{01}), (\theta^X_0, \theta^Y_{01})})$. 

The posterior on an estimand $E$ given data $D$ given prior over $\pi$, $p(\pi)$ is:

$$\Pr(E | D) = \int_\pi  \frac{\sum_{j \in E \cap D}\pi_j}{\sum_{j \in D}\pi_j} f(\pi)d\pi$$

However, since for any $\pi$, $\sum_{j \in D}\pi_j = p(D)$ we have:

$$\Pr(E | D) = \int_\pi  \sum_{j \in E \cap D}\pi_j f(\pi)d\pi/p(D) = \sum_{j \in  E \cap D} \overline{\pi}_j/p(D)$$

### Probative value requires $d-$connection

<!-- Rules for inferring information about one variable from another are th stuff of graphoids  [@pearl1985graphoids] see also [@geiger1987non] and [@pearl1987logic]...  -->

As we have argued, causal estimands can be expressed as the values of exogenous nodes in a causal graph. Case-level causal effects and causal paths can be defined in terms of response-type nodes; average effects and notable causes in terms of population-level parameter nodes (e.g., $\pi$ or $\lambda$ terms); and questions about actual causes in terms of exogenous conditions that yield particular endogenous values (conditioning on which makes some variable a counterfactual cause). 

We thus define causal inference more generally as *the assessment of the value of one or more unobserved (possibly unobservable) exogenous nodes on a causal graph, given observable data.*  To think through the steps in this process, it is useful to distinguish among three different features of the world, as represented in our causal model: there are the things we want to learn about; the things we have already observed; and the things we could observe. As notation going forward, let:

* $\mathcal Q$ denote the exogenous variables that define our *query*; we generally assume that $\mathcal Q$ cannot be directly observed so that its values must be inferred
* $\mathcal W$ denote a set of previously observed nodes in the causal model, and 
* $\mathcal K$ denote a set of additional  variables---clues---that we have not yet observed but could observe.

Now suppose that we seek to design a research project to investigate a causal question. How should the study be designed? Given that there are some features of the world that we have already observed, which additional clues should we seek to collect to shed new light on our question? In terms of the above notation, what we need to figure out is whether a given $\mathcal K$ might be informative about---might provide additional leverage on---$\mathcal Q$ given the prior observation of $\mathcal W$. 

To ask whether one variable (or set of variables) is informative about another is to ask whether the two (sets of) variables are, on average, *correlated* with one another, given whatever we already know. Likewise, if two variables' distributions are fully *independent* of one another (conditional on what else we have observed), then knowing the value of one variable can provide no new information about the value of the other. 

Thus, asking whether a set of clues, $\mathcal K$, is informative about $\mathcal Q$ given the prior observation of $\mathcal W$, is equivalent to asking whether $\mathcal K$ and $\mathcal Q$ are conditionally independent given $\mathcal W$. That is, $\mathcal K$ can be informative about $\mathcal Q$ given $\mathcal W$ only if $\mathcal K$ and $\mathcal Q$ are *not* conditionally independent of one another given $\mathcal W$. 

As we have shown, as long as we have built $\mathcal Q$, $\mathcal K$, and $\mathcal W$ into our causal model of the phenomenon of interest, we can answer this kind of question by inspecting the structure of the model's DAG. In particular, what we need to go looking for are relationships of *$d$-separation*. The following proposition, with only the names of the variable sets altered, is from @pearl2009causality (Proposition 1.2.4): 

**Proposition 1:**  If sets $\mathcal Q$ and $\mathcal K$ are $d$-separated by $\mathcal W$ in a DAG, $\mathcal G$, then $\mathcal Q$ is independent of $\mathcal K$ conditional on $\mathcal W$ in every distribution compatible with $\mathcal G$. Conversely, if $\mathcal Q$ and $\mathcal K$ are *not* $d$-separated by $\mathcal W$ in DAG $\mathcal W$, then $\mathcal Q$ and $\mathcal K$ are dependent conditional on $\mathcal W$ in at least one distribution compatible with $\mathcal G$.

We begin with a causal graph and a set of nodes on the graph ($W$) that we have already observed. Given what we have already observed, *a collection of clue nodes, $\mathcal K$, will be uninformative about the query nodes, $\mathcal Q$, if $\mathcal K$ is  $d$-separated from $\mathcal Q$ by $\mathcal W$ on the graph.* When $\mathcal W$ $d$-separates $\mathcal K$ from $\mathcal Q$, this means that what we have already observed already captures all information that the clues might yield about our query. On the other hand, if $\mathcal K$ and $\mathcal Q$ are $d$-connected (i.e., not $d$-separated) by $W$, then $K$ is *possibly* informative about $Q$.$K$ is not  $d$-separated from $\mathcal Q$ by $\mathcal W$.^[This proposition is almost coextensive with the definition of a DAG. A DAG is a particular kind of dependency  model ("graphoid") that is a summary of a  collection of "independency statements", $(I)$, over distinct subsets of $\mathcal V$ (Pearl and Verma 1987), where $I(\mathcal Q,\mathcal W,\mathcal K)$ means  "we learn nothing about $\mathcal Q$ from $\mathcal K$ if we already know $\mathcal W$". More formally:
$$I(\mathcal K, \mathcal W,\mathcal Q) \leftrightarrow P(\mathcal K,\mathcal Q|\mathcal W)=P(\mathcal K|\mathcal W)P(\mathcal Q|\mathcal W)$$
A Directed Acyclic Graph Dependency model is one where the set of independencies correspond exactly to the relations that satisfy $d$-separation  (Pearl and Verma 1987, p376).  Thus on DAG $\mathcal G$, $I(\mathcal K,\mathcal W,\mathcal Q)_\mathcal G$ implies that $\mathcal K$ and $\mathcal Q$ are $d$-separated by $\mathcal W$.] Note, moreover, that under quite general conditions (referred to in the literature as the *faithfulness* of a  probability distribution) then there are at least *some* values of $\mathcal W$ for which $\mathcal K$ *will* be informative about $\mathcal Q$.^[Put differently, there will not be any conditional independencies that are *not* captured in the DAG.] 


<!-- ^[In Pearl's terminonology, the graph may *represent* the probability distribution but not be *faithful* to it.] -->

<!-- This can be put another way. An $I$-map of $M$ is a model with no extra independencies; a $D$-map is a model that contains all of $M$ with,  possibly aditional independencies; a *perfect* map is a model with the same set of dependencies. Given an independency model $M$, a DAG, $G$, may be an $I$-map of $M$ in the sense that whenever $D$ separates $K$ from $Q$ then $I(K,D,Q)_M$, yet there may still be indepenencies in $M$ not captured by $G$; that is, it may also be htat $I(K,D,Q)_M$ but not $I(K,D,Q)_G$. Pearl refers to such cases as instances of a violation of *stability*, though in simple graphs with discrete variables such violations may be plausible.  -->

<!-- In the example given by Pearl with two matching pennies, $X_1$ and $X_2$ and $Y$ is 1 if the pennies match, $X_1$ adn $X_2$ are probabilisitcally independent of $Y$, yet $Y$ depends on both of them.  -->
<!-- The problem is that $d$-separation satisfies composition, that is, if $I(X_1, D, Q)$ and $I(X_2, D, Q)$ then $I(X_1X_2, D, Q)$; but since we cannot have $I(X_1X_2, D, Q)$ then we cannot have   $I(X_1, D, Q)$ and $I(X_2, D, Q)$ either (see also [@bouckaert1994conditional]). -->

<!-- Note that this example depends on infomration about the probability distribution over $V$, that is, the functional equations, and cannot be inferred from the structure of $S$ alone.   -->

<!-- [Note for us: We seek a  related proposition holds however using $d-separation$ on partially discovered submodels.] -->

Let us examine Proposition 1 in practice. We begin with the simplest case possible, and then move on to more complex models. 

The very simplest probabilistic causal graph has $X$ influencing $Y$, with $X$ determined by a coin flip. Assuming that there is some causal heterogeneity---that is, it is unknown in any particular case whether $X$ causes $Y$---we also include a response-type variable, $Q$, pointing into $Y$, as shown in Figure \ref{fig:d-sepsimple}. Here, $Q^Y$ determines the value of $Y$ that will be generated by $X$. Asking about the causal effect of $X$ in a case thus means learning the value of $Q^Y$ in that case. As will be recalled, in a binary setup with one causal variable, a response-type variable can take on one of four values, $q^Y_{00}$, $q^Y_{10}$, $q^Y_{01}$ and $q^Y_{11}$,^[As a reminder, we read $q^Y_{ij}$ (when $X$ is binary) as meaning that $Y$ will take on value $i$ when $X=0$ and value $j$ when $X=1$.] corresponding to the four possible causal types in this setting.

```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:d-sepsimple} A simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's response type for $Y$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 1),
       y = c(1, 1, 2),
       names = c("X", "Y", expression(paste(theta[Y]))),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "Simplest X, Y, graph",
       padding = .4, contraction = .15) 

```

Let us assume that we have observed nothing yet in this case and then ask what clue(s) might be informative about $Q^Y$, the node of interest. The other two nodes in the graph are $X$ and $Y$: these are thus the possible clues that we might go looking for in our effort to learn about $Q^Y$ (i.e., they are the possible members of $\mathcal K$). 




First, can we learn about $Q^Y$ by observing $X$? We can answer this question by asking whether $X$ is $d$-connected to $Q^Y$ on the graph given what we have already observed (which is nothing). We can see visually that there is no active path from $X$ to $Q^Y$: the only path between $X$ and $Q$ is blocked by colliding arrow heads. Thus, $X$ and $Q^Y$ are $d$-separated, meaning that $X$ will not be informative about $Q^Y$: observing the value that a causal variable takes on in a case---having seen nothing else in the case---tells us nothing whatsoever about that variable's effect on the outcome. If we want to know whether a case is of a type in which the presence of natural resources would cause civil war, observing only that the case has natural resources does not help answer the question.

<!-- **LONG FOOTNOTE STARTING HERE....** -->
<!-- In the case where we observe only $X$, the posterior on $Q^Y$ is: -->
<!-- \begin{eqnarray*} -->
<!-- P(Q^Y=q^Y | X=x) &=& \frac{\sum_{j=0}^1p(X=x)P(Q^Y=q^Y)P(Y=j|X=x, Q^Y=q^Y)}{\sum_{q^{Y'}}\sum_{j=0}^1p(X=x)P(Q^Y=q^{Y'})P(Y=j|X=x, Q^Y=q^{Y'})}\\ -->
<!-- &=&\frac{P(Q^Y=q^Y)}{\sum_{q^{Y'}}P(Q^Y=q^{Y'})} -->
<!-- \end{eqnarray*} -->
<!-- which is simply the prior on $Q^Y$. Thus, nothing is learned about $Q^Y$ from observing $X$ only.]  -->
<!-- <!-- &=& \frac{p(Q=q)\sum_{j=0}^1p(Y=j|X=x, Q=q)}{\sum_{q'}p(Q=q')\sum_{j=0}^1p(Y=j|X=x, Q=q')}\\ --> -->
<!-- **...ENDING HERE** -->

What, then, if we instead were to observe only $Y$? Is $Y$ $d$-connected to $Q$ given what we have already observed (which, again, is nothing)? It is: the arrow from $Q^Y$ to $Y$ is an active path. Observing only the *outcome* in a case does tell us something about causal effects. Returning to the natural resources and civil war example, observing only that a country has had a civil is informative about the case's causal type (the value of $Q^Y$). In particular, it rules out the possibility that this is a case in which nothing could cause a civil war: that is, it excludes $q^Y_{00}$ (i.e., $c$-type) as a possible value of $Q^Y$.

<!-- **LONG FOOTNOTE STARTING HERE....** -->
<!-- In the case where we observe $Y$ only we have: -->
<!-- $$P(Q=q | Y=y) = \frac{\sum_{j=0}^1p(X=j)P(Q=q)P(Y=y|X=j, Q=q)}{\sum_{q'}\sum_{j=0}^1p(X=j)P(Q=q')P(Y=y|X=j, Q=q')}$$ -->
<!-- Here terms involving $Y$ and $Q$ cannot be separated, so the same kind of reduction is not possible. This implies scope for learning about $Q$ from $Y$.  For instance, if  we have $P(Q=j) = 1/4$ for type $j \in \{a,b,c,d\}$  and $P(X=j) = \frac{1}{2}$, then we have $P(Q=a | Y=1)=P(Q=b | Y=1) =\frac{1}{4}$, $P(Q=c | Y=1)=0$ and $P(Q=d | Y=1)=1$. -->
<!-- **...ENDING HERE** -->

Suppose now, having observed $Y$, that we were to consider also observing $X$. Would we learn anything further about $Q^Y$ from doing so? We have already seen that observing $X$ alone yields no information about $Q^Y$ because the two nodes are unconditionally $d$-separated, the path between them blocked by the colliding arrowheads at $Y$. However, as we have seen, observing a collider variable (or one of its descendants) *unblocks* the flow of information, generating relations of conditional dependence across the colliding arrowheads. Here, $X$ and $Q^Y$ are $d$-connected by $Y$: thus, if we have *already* observed $Y$, then observing $X$ does confer additional information about $Q^Y$. Knowing only that a country has natural resources tells us nothing about those resources' effect on civil war in that country. But if we already know that the country has a civil war, then learning that the country has natural resources helps narrow down the case's possible response types. Having already used the observation of $Y=1$ to rule out the possibility that $Q^Y=q^Y_{00}$, observing $X=1$ *together with* $Y=1$ allows us to additionally rule out the possibility that natural resources *prevent* civil war, i.e., that $Q^Y=q^Y_{01}$.^[That is, we can rule out that the case is an $a$ type, or one with a negative causal effect.]

<!-- **LONG FOOTNOTE STARTING HERE....** -->
<!-- Where we observe both $Y$ and $X$,  we have: -->
<!-- $$P(Q=q | Y=y, X=x) = \frac{P(X=x)P(Q=q)P(Y=y|X=x, Q=q)}{\sum_{q'}P(X=x)P(Q=q')P(Y=y|X=x, Q=q')}$$ -->
<!-- which does not allow separation either of  $Q$ and $X$ or of $Q$ and $Y$. Thus, there is again learning from $Y$ and, given $Y$, there is *also* learning from $X$. Put differently, we have $P(Q|Y,X) \neq P(Q|Y)$.  -->

<!-- **...ENDING HERE** -->

Finally, what if we observe $X$ first and are considering whether to seek information about $Y$? Would doing so be informative? $X$ does not $d-$separate $Q^Y$ from $Y$; thus, observing $Y$ will be informative about $Q^Y$. In fact, observing $Y$ if we have already seen $X$ is *more* informative than observing $Y$ alone. The reasoning follows the logic of collision discussed just above. If we observe $Y$ having already seen $X$, not only do we reap the information about $Q^Y$ provided by $Y$'s correlation with $Q^Y$; we simultaneously open up the path between $X$ and $Q^Y$, learning additionally from the conditional dependence between $X$ and $Q^Y$ given $Y$. 


```{r, echo = FALSE, include = FALSE, errors = FALSE}
# Graphing all dags in some class, X,Y,K,Q



# Append to list
lappend <- function(L, obj) {
  L[[length(L)+1]] <- obj
  return(L)
  }


translate_matrix <- function(A, var_names = c("K", "X", "Y", "Q")){
 paste(var_names[which(A==1, arr.ind = TRUE)[,1]], "causes", var_names[which(A==1, arr.ind = TRUE)[,2]])
  }

translate_dagitty <- function(A, var_names = c("K", "X", "Y", "Q")){
  paste(var_names[which(A==1, arr.ind = TRUE)[,1]], "->", var_names[which(A==1, arr.ind = TRUE)[,2]])
  }

make_daggity     <- function(A) dagitty(paste("dag{", paste(translate_dagitty(A), collapse = ";"), "}"))

kinformative2 <- function(A){
   paste(c(
       dseparated(make_daggity(A), "Q", "K", c()),
       dseparated(make_daggity(A), "Q", "K", c("X")),
       dseparated(make_daggity(A), "Q", "K", c("Y")),
       dseparated(make_daggity(A), "Q", "K", c("X","Y"))
       ), collapse = ",")
   }

kinformative <- function(A){
  paste(c("\U2205", "X", "Y", "XY")[!c(
    dseparated(make_daggity(A), "Q", "K", c()),
    dseparated(make_daggity(A), "Q", "K", c("X")),
    dseparated(make_daggity(A), "Q", "K", c("Y")),
    dseparated(make_daggity(A), "Q", "K", c("X","Y"))
  )], collapse =",")
}


# 34 graphs
# All paths. All have Q --> Y. None have Y --> X
#   KXYQ
# K 0???
# X ?0??
# Y ?000
# Q ??10
var_names <- c("K", "X", "Y", "Q")

check.A <- function(A, iterations = 4){
  x <- 1
  if( sum(sapply(1:iterations, function(j) sum(diag((A %^% j))))) > 0) x<- 0 # Cycles
  if( sum(sapply(1:iterations, function(j) (A %^% j)[2,3])) == 0) x<- -1 # "No path from X to Y (2 to 3)"
  if(min(rowSums(A) + colSums(A))==0) x<- -2 # "Unconnected element"
  if(sum(A[,4])>0) x<- -3 # "No causes of Q allowed in graph"
  x}

.A <-  matrix(c( 0,NA,NA,NA, 
                 NA, 0,NA,NA,
                 NA, 0, 0, 0,
                 NA,NA, 1, 0), 4, 4, byrow = TRUE)
rownames(.A) <- var_names; colnames(.A) <- var_names

possibilities <- perm_bb(rep(2,9))

new_A <- function(j) {A <- .A; A[c(2:4, 5, 8:10, 13:14)] <- possibilities[j,]; A}

accept <- list()
for(i in 1:nrow(possibilities)) {
  A <- new_A(i)
  if(check.A(A)==1) accept <- lappend(accept, A)
}

```

We put Proposition 1 to work in a slightly more complex set of models in Figure \ref{fig:34graphs}. Here we investigate the informativeness of a clue that is neither $X$ nor $Y$. Each graph in Figure \ref{fig:34graphs} has four variables: $X$; $Y$; a possible clue, $K$; and a response-type variable, $Q$. We draw all 34 possible graphs with variables $X$, $Y$, $K$, and $Q$ for causal models in which (a) all variables are connected to at least one other variable, (b) $X$ causes $Y$ either directly or indirectly, and (c) $Q$ is a direct cause of $Y$ but is not caused by any other variable in the model and is thus exogenous. The title of each panel reports $K$'s conditional informativeness using principles of $d$-separation: it tells us when $K$ is possibly informative about $Q$ depending on whether $X$, $Y$, both or none are observed.^[Note the "possibly" can be dropped under the assumption that the underlying probability model is "stable" (Pearl 2009, section 2.9.1) and with the interpretation that $K$ is informative about $Q$ for some, but not necessarily all, values of $W$.]

<!-- Above footnote: do we want to say "faithful" rather than stable, as we do earlier? -->

```{r, echo = FALSE, fig.width = 11, fig.height = 12, fig.cap = "\\label{fig:34graphs} All connected directed acyclic graphs over $X,Y,K,Q$, in which $Q$ is an exogenous variable that directly causes $Y$, and $X$ is a direct or indirect cause of $Y$. The title of each graph indicates the conditions under which $K$ can be informative about (i.e., is not $d$-separated from) $Q$, given the prior observation of $X$, $Y$, both, or neither (...).", errors = FALSE, warning = FALSE, message = FALSE, comment = FALSE}


par(mfrow = c(6, 6) )
  par(mar=c(1,1,3.5,1))
  for(i in 1:length(accept)){
    kinf <- kinformative(accept[[i]])
    hj_dag(x = c(0,0,1,1), y = c(1,0,0,1), names = var_names, 
           arcs = which(accept[[i]]==1, arr.ind = TRUE), 
           title = title(ifelse(
                         length(kinf)==0, 
                         paste0(i, ". K never informative"),
                         paste0(i, ". K possibly informative\n given: ", kinf))), 
           padding = .15, cex = 1.2, length = .15) 
    }

```

### Probative value 

The results show us not just what kinds of variables can be informative about a case's response-type but also what combinations of observations yield leverage on case-level causal effects. A number of features the graphs are worth highlighting:

* **Clues at many stages.** Process tracing has focused a great deal on observations that lie "along the path" between suspected causes and outcomes. What we see in Figure \ref{fig:34graphs}, however, is that observations at many different locations in a causal model can be informative about causal effects. We see here that $K$ can be informative when it is pre-treatment (causally prior to $X$---e.g. panel (3)), post-treatment but pre-outcome (that is, "between" $X$ and $Y$ as, e.g., in panel (20)), an auxiliary effect of $X$ that itself has no effect on $Y$ (e.g., in panel (19)), post-outcome (after $Y$---e.g., in panel (15)), or a joint effect of both the suspected cause and the outcome (e.g., panel (31)). 

* **Mediator Clues**. While clues that lie in between $X$ and $Y$ may be informative, they can only be informative under certain conditions. For instance, when a clue serves *only* as a mediator in our model (i.e., its only linkages are being caused by $X$ and being affected by $Y$) and $Q$ only affects $Y$, as in panels (20) and (21), the clue is only informative about $Q$ if we have also observed the outcome, $Y$. Of course, this condition may commonly be met---qualitative researchers usually engage in retrospective research and learn the outcome of the cases they are studying early on---but it is nonetheless worth noting why it matters: in this setup, $K$ is unconditionally $d$-separated from $Q$ by the collision at $Y$; it is only by observing $Y$ (the collider) that the path between $K$ and $Q$ becomes unblocked. (As we saw above, the very same is true for observing $X$; it is only when we know $Y$ that $X$ is informative about $Q$.)

In short, observations along causal paths are more helpful in identifying causal effects to the extent that we have measured the outcome. Importantly, this is not the same as saying that mediator clues are *only* informative about causal effects where we have observed the outcome. Observing $Y$ is necessary for the mediator to be informative about a $Q$ term that is connected only to $Y$. Observing a mediator without the outcome, however, could still be informative about the overall effect of $X$ on $Y$ by providing leverage on how the mediator responds to $X$, which is itself informative about $X$'s effect on $Y$ via the mediator.^[In other words, the clue would then be providing leverage on a response-type variable pointing into the mediator itself.] Moreover, observing the mediator could be informative without the observation of $Y$ if, for instance, $Q$ also points into $K$ itself or into a cause of $K$. As we discuss below, the clue then is informative as a "symptom" of the case's response type, generating learning that does not hinge on observing the outcome.

* **Symptoms as clues.** Some clues may themselves be affected by $Q$: that is to say, they may be symptoms of the same conditions that determine causal effects in a case. For instance, in our illustrative model involving government survival, government sensitivity functions as a response-type variable for the effect of a free press ($X$) on government removal ($Y$): a free press only generates government removal when the government is non-sensitive to public opinion.  Sensitivity to public opinion thus represents our query variable, $Q$, if we seek to learn whether a free press causes government removal in a case. While it may not be possible to observe or otherwise measure the government's sensitivity, there may be *consequences* of government sensitivity that are observable: for instance, whether government officials regularly consult with civil-society actors on policy issues. While consultations would not be part of the causal chain generating the free press's effect, observing consultations (or the lack of them) would be informative about that effect because consultations are a symptom of the same conditions that enable the effect. 

We see that $K$ is a child or descendant of $Q$ in several of the graphs in Figure \ref{fig:34graphs}: $Q$ directly causes $K$ in panels (7) through (14), (17), (18), (25)-(30), (33), and (34); $Q$ causes (K) only indirectly through $X$ in panels (22) through (24); $Q$ causes (K) only indirectly through $Y$ in panels (15), (16), and (31); and $Q$ causes $K$ only indirectly through $X$ and through $Y$ in panel (32). We can then use the principle of $d$-separation to figure out when the symptom clue is potentially informative, given what we have already observed. It is easy to see that $K$ is potentially informative, no matter what we have already observed, if $K$ is directly affected by $Q$; there is nothing we could observe that would block the $Q \rightarrow K$ path. Thus, $Q$'s "symptom" can, in this setup, contain information about type above and beyond that contained in the $X$ and $Y$ values. However, where $Q$ affects $K$ only through some other variable, observing that other variable renders $K$ uninformative by blocking the $Q$-to-$K$ path. For instance, where $Q$ affects $K$ indirectly through $X$, once we observe $X$, we already have all the information about $Q$ that would be contained in $K$. 

* **Surrogates as clues.** Clues may be consequences of the outcome, as in graphs (15) and (16). If $K$ is a consequence *only* of $Y$, then it will contain no new information about $Q$ where $Y$ is already known. However, in situations where the outcome has not been observed, $K$ can act as a "surrogate" for the outcome and thus yield leverage on $Q$ (@frangakis2002principal). A researcher might, for instance, seek to understand causal effects on an outcome that is difficult to directly observe: consider, for instance, studies that seek to explain ideational change. Ideas themselves, the $Y$ in such studies, are not directly observable. However, their consequences---such as statements by actors or policy decisions---will be observable and can thus serve as informative surrogates for the outcome of interest.

Clues may similarly serve as surrogates of a cause, as in graphs (19) and (22). Here $X$ causes $K$, but $K$ plays no role in the causal process generating $Y$. $K$ is of no help if we can directly measure $X$ since the latter $d$-separates $K$ from $Q$. But if an explanatory variable cannot be directly measured---consider, e.g., ideas or preferences as causes---then its consequences, including those that have no relationship to the outcome of interest, can provide leverage on the case-level causal effect.

Clues can also be a consequence of both our suspected cause and the outcome of interest, thus serving as what we might call "double surrogates," as in panels (31) and (32). Here $X$ is a direct cause of $Y$, and $K$ is a joint product of $X$ and $Y$. A double surrogate can be informative as long as we have not already observed both $X$ and $Y$. Where data on either $X$ or $Y$ are missing, there is an open path between $K$ and $Q$. If we have already observed both, however, then there is nothing left to be learned from $K$.

* **Instruments as clues.** Clues that are causally prior to an explanatory variable, and have no other effect on the outcome, can sometimes be informative. Consider, for instance, graph (3). Here $K$ is the only cause of $X$. It can thus serve as a proxy. If we have seen $X$, then $X$ blocks the path between $K$ and $Q$, and so $K$ is unhelpful. $K$ can be informative, though, if we have *not* observed $X$. Note that informativeness here still requires that we observe $Y$. Since $Y$ is a collider for $Q$ and the $K \rightarrow X \rightarrow$ chain, we need to observe $Y$ in order to $d$-connect $K$ to $Q$.

A rather different setup appears in graph (5), where both $K$ and $Q$ cause $X$. Now the conditions for $K$'s informativeness are broader. Observing $X$ still makes $K$ uninformative as a proxy for $X$ itself. However, because $X$ is a collider for $K$ and $Q$, observing $X$ *opens up* a path from $K$ to $Q$, rendering a dependency between them. Still, we have to observe at least one of $X$ or $Y$ for the instrument to be informative here. This is because both of $K$'s paths to $Q$ run through a collision that we need to unblock by observing the collider. For one path, the collider is $X$; for the other path, the collider is $Y$.^[As a simple example one might imagine a system in which $X = K$ if  $q \in {a,b}$  and $X = 1-K$ if  $q \in {c,d}$. Then if we observe, say, $X=Y=K=1$, we can infer that $q = b$. Another way to think about what is happening in graph (5) is that $K$ is providing information about the *assignment process*. In this graph, the causal effect ($Y$'s potential outcomes, determined by $Q$) is also a partial determinant of the assignment of cases to values on $X$. In terms of cross-case correlational inference, then, we would think of this as a situation of confounding. Observing another cause of $X$, then, allows us to more fully characterize the process of assignment.] 

<!-- Graph (5) is similar to one discussed in [@hausman1999independence] in which there is learning from a pretreatment clue because $X$ is a collider for $K$ and $Q$.  -->

<!-- To return to our government-removal model, government sensitivity to public opinion is a response-type variable (a $Q$ term), with non-sensitivity a pre-condition for the positive effect of a free press on removal. Yet it is possible (though we did not include it in our original model) that government sensitivity also affects whether or not a government gets a free press: more sensitive governments may impose tighter media restrictions. In that case, when governments are not sensitive, we would expect to see a free press and government removal.   -->

Other patterns involving instrumentation are also imaginable, though not graphed here. For example, we might have a causal structure that combines instrumentation and surrogacy. Suppose that $X$ is affected by $Q$ and by an unobservable variable $\theta_X$; and that $\theta_X$ has an observable consequence, $K$. Then $K$, though not a cause of $X$, is a "surrogate instrument" [@hernan2006instruments] as it is a descendant of an unobserved instrument, $U$, and thus allows us to extract inferences similar to those that we could draw from a true instrument.

* **Confounders as clues.** In several of the graphs, $K$ is a confounder in that it is a direct cause of both $X$ and $Y$ (panels (4), (6), (12), and (14)). Let us focus on graph (4), which isolates $K$'s role as a confounder. Here $K$ can be informative via two possible paths. First, if $X$ is not observed but $Y$ is, then $K$ is $d$-connected to $Q$ along the path $K \rightarrow X \rightarrow Y \leftarrow Q$. $K$ is in this sense serving as a proxy for $X$, with its path to $Q$ opened up by the observation of the collider, $Y$. Second, with $Y$ observed, $K$ can provide information on $Q$ via the more direct collision, $K \rightarrow Y \leftarrow Q$. If $X$ *is* observed, then the first path is blocked, but the second still remains active. As with any pre-outcome variable, for a confounder clue to provide purchase on $Y$'s response type, $Y$ itself must be observed.

In a sense, then, the role of confounders as clues in case-level inference is the mirror image of the role of confounders as covariates in cross-case correlational inference. In a correlational inferential framework, controlling for a variable in $K$'s position in graph (5) renders the $X, Y$ correlation (which we assume to be observed) informative about $X$'s average causal effect. When we use confounders as evidence in within-case inference, it is our observations of other variables that determine how informative the confounder *itself* will be about $X$'s causal effect.


It is important to be precise about the kinds of claims that one can make from graphs like those in Figure \{fig:34graphs}. The graphs in this figure allow us to identify informativeness about an unobserved node $Q$ that is a parent of $Y$. This setup does not, however, capture all ways in which clues can be informative about the causal effect of $X$ on $Y$ or about other causal estimands of interest. For instance, as noted above, even if a clue is uninformative about a $Q$ node pointing into $Y$, it may still help establish whether $X$ causes $Y$: the statement that $X$ causes $Y$ will for some graphs be a statement about a *collection* of nodes that form the set of query variables $\mathcal Q$. This is the case, for instance, in any graph of the form $X \rightarrow M  \rightarrow Y$, where we are interested not just in $Y$'s response to $M$ (the mediator) but also in $M$'s response to $X$. Of interest, thus, are not just a $Q^Y$ response-type node pointing into $Y$ but also a $Q^M$ response-type node that is a parent of $M$. Observations that provide leverage on either $Q$ term will thus aid an inference about the overall causal effect. A clue $K$ that is $d-$separated from $Q^Y$ may nevertheless be informative about $X$'s effect on $Y$ if it is not $d-$separated from $Q^M$; this opens up a broader range of variables as informative clues. 

Additionally, as our discussion in Chapter 2 makes clear, estimands other than the case-level causal effect---such as average causal effects, actual causes, and causal paths---involve particular features of context: particular sets of exogenous nodes as members of our query set, $\mathcal Q$. Thus, even for the same causal model, informativeness will be defined differently for each causal question that we seek to address. The broader point is that we can identify what kinds of observations may address our estimand if we can place that estimand on a causal graph and then assess the graph for relationships of $d$-separation and -connection.

Further, we emphasize that a DAG can only tell us when a clue *may* be informative (conditional some prior observation): $d-$connectedness is necessary but not sufficient for informativeness. This fact derives directly from the rules for drawing a causal graph: the absence of an arrow between two variables implies that they are *not* directly causally related, while the presence of an arrow does not imply that they always are. As we saw in our analysis of the government-removal example in Chapter 2, whether variables connected to one another by arrows in the original DAG were in fact linked by a causal effect depended on the context. Likewise, whether a clue $K$ is in fact informative may depend on particular values of $\mathcal W$---the variables that have already been observed. As a simple example, let $q = k_1w + (1-w)k_2$, where $W$ is a variable that we have already observed and $K_1$ and $K_2$ are clues that we might choose to observe next. Here, if $w=1$ then learning $K_1$ will be informative about $Q$, and learning $K_2$ will not; but if $w=0$, then $K_1$ will be uninformative (and $K_2$ informative). 


In general, then, graphical analysis alone can help us exclude unhelpful research designs, given our prior observations and a fairly minimal set of prior beliefs about causal linkages. This is no small feat. But identifying those empirical strategies that will yield the greatest leverage requires engaging more deeply with our causal model, as we explore next.




<!--chapter:end:06-process-tracing-with-models.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
# Application: Process Tracing with a Causal Model


```{r, include = FALSE}
source("_packages_used.R")
```

***

We apply the causal-model-based approach to process tracing to a major substantive issue in comparative politics: the relationship between inequality and democratization. We illustrate how one can convert theories in this domain into relatively simple causal models. Drawing on data from @haggard2012inequality, we use qualitative restrictions on causal types together with flat priors to draw inferences about the probabilty with which inequality facilitated or hindered democratization.

***

<!-- We show that systematically integrating theory into process tracing has substantial implications for inference. We also show how inferences depend on how exactly one specifies the causal model.** -->




```{r loadUDs2, include = FALSE}
UDs <- read.csv("saved/ineq_democ_lambdascorrected.csv")
UDs <- UDs[,c(1,3,2,4,5,7,6,8)] # Reorder to reflect PIMD ordering

perms <- perm(c(2,2,2)); colnames(perms) <- c("P", "I", "M")
kable(perms, caption = "Data permutations considered")              
```

In this chapter, we demonstrate how causal-model-based process-tracing works using real data. We undertake this illustration on a substantive issue that has been of central interest to students of comparative politics for decades: the causes of democratization. As the literature and range of arguments about democratization are vast, we focus on just a piece of the debate---specifically on causal claims about the relationship between economic inequality and democratization, with particular attention to the work of @boix2003democracy, @acemoglu2005economic, and @haggard2012inequality. In this chapter, we demonstrate process tracing with causal models, while in a later chapter we demonstrate the integration of process-tracing with correlational analysis. Our focus in this chapter is on using process tracing to assess *the case-level causal effect of inequality on democracy.*



## Inequality and Democratization: The Debate

Sociologists, economists, and political scientists have long theorized and empirically examined the relationship between inequality and democracy (e.g., @dahl1973polyarchy, @bollen1985political, @acemoglu2005economic, @boix2003democracy, @ansell2014inequality). In recent years, the work of @boix2003democracy,  @acemoglu2005economic, and @ansell2014inequality represent major theoretical advances in specifying when and how inequality might generate transitions to democracy (as well as its persistence, which we bracket here). The first and third of these books also provide large-n cross-national and historical tests of their theories' key correlational predictions. @haggard2012inequality, moreover, derive causal process observations from a large number of "Third Wave" cases of democratization in order to examine these theories' claims about the centrality of distributional issues to regime change. We provide a very condensed summary of the core logic of @boix2003democracy and @acemoglu2005economic before seeking to translate that logic into a causal model for the purposes of process tracing, using a transformed version of Haggard and Kaufman's causal-process data.

We briefly summarize the core logics of and differences among these three sets of arguments here, bracketing many of their moving parts to focus on the basic theorized relationship between inequality and democracy. Both Boix's and Acemoglu and Robinson's theories operate within a Meltzer-Richard (@meltzer1981rational) framework in which, in a democracy, the median voter sets the level of taxation-and-transfer and, since mean income is higher than median income, benefit from and vote for a positive tax rate, implying redistribution from rich to poor. The poorer the median voter, the more redistribution she will prefer. Democracy, with its poorer median voter, thus implies greater redistribution than (rightwing) authoritarianism---a better material position from the poor at the expense of the rich elite. Thus, in each of these approaches, struggles over political regimes are conflicts over the distribution of material resources.

In Boix's model, the poor generally prefer democracy for its material benefits. When they mobilize to demand regime change, the rich face a choice as to whether to repress or concede, and they are more likely to repress as inequality is higher since, all else equal, they have more to lose from democracy. Thus, with the poor always preferring democracy over rightwing authoritarianism, inequality reduces the prospects for democratization. 

In Acemoglu and Robinson's model, inequality simultaneously affects the expected net gains to democracy for both rich and poor. At low levels of inequality, democracy is relatively unthreatening to the elite, as in Boix, but likewise of little benefit to the poor. Since regime change is costly, the poor do not mobilize for democracy when inequality is low, and democratization does not occur. At high levels of inequality, democracy is of great benefit to the poor but has high expected costs for the elite; thus, democratization does not occur because the elite repress popular demands for regime change. In Acemoglu and Robinson's model, democracy emerges only when inequality is at middling levels: high enough for the poor to demand it and low enough for the rich to be willing to concede it.

Ansell and Samuels, finally, extend the distributive politics of regime change in two key ways. First, they allow for a two-sector economy, with a governing elite comprising the landed aristocracy and an urban industrial elite excluded from political power under authoritarian institutions. Total inequality in the economy is a function of inequality in the landed sector, inequality in the industrial sector, and the relative size of each. Second, authoritarian (landed) elites can tax the industrial bourgeoisie, thus giving the industrial elite an incentive to seek constraints on autocratic rule. Third, in Ansell and Samuels' model, rising industrial inequality means a rising industrial elite, generating a larger gap between them and industrial workers, though the industrial masses are richer than the peasantry. A number of results follow, of which we highlight just a couple. Rising land inequality reduces the likelihood of bourgeois rebellion by giving the landed elite greater repressive capacities and increasing their expected losses under democracy. As industrial inequality rises, however, the industrial elite have more to lose to confiscatory taxation and thus greater incentive to push for partial democracy (in which they have the ability to constrain the government, though the poor remain politically excluded) as well as greater resources with which to mobilize and achieve it. Full democracy, brought on by joint mass and bourgeois rebellion, is most likely as the industrial sector grows in relative size, giving the urban masses more to lose to autocratic expropriation and more resources with which to mobilize and rebel. 

These three theoretical frameworks thus posit rather differing relationships between inequality and democracy. Taking these theoretical logics as forms of background knowledge, we would consider it possible that inequality reduces the likelihood of democracy or that it increases the likelihood of democracy. Yet one feature that all three theories have in common is a claim that distributional grievances drive demands for regime change. Moreover, in both Boix and Acemoglu and Robinson, less economically advantaged groups are, all else equal, more likely to demand democracy the worse their relative economic position.  Ansell and Samuels' model, on the other hand, suggests that relative deprivation may cut both ways: while poorer groups may have more to gain from redistribution under democracy, better-off groups have more to fear from confiscatory taxation under autocracy. In all three frameworks, *mobilization* by groups with material grievances is critical to transitions to democracy: elites do not voluntarily cede power.

In their qualitative analysis of "Third Wave" democratizations, Haggard and Kaufman point to additional factors, aside from inequality, that may generate transitions. Drawing on previous work on 20th century democratic transitions (e.g., @huntington1993third, @linz1996problems), they pay particular attention to international pressures to democratize and to elite defections. 

## A Structural Causal Model

We now need to express this background knowledge in the form of a structural causal model. Suppose that we are interested in the case-level causal effect of inequality on democratization of a previously autocratic political system. Suppose further, to simplify the illustration, that we conceptualize both variables in binary terms: inequality is either high or low, and democratization either occurs or does not occur. This means that we want to know, for a given case of interest, whether high inequality (as opposed to low inequality) causes democracy to emerge, prevents democracy from emerging, or has no effect (i.e., with democratization either occurring or not occurring independent of inequality). We can represent this query in the simple, high-level causal model shown in Figure \@ref(fig:dagdemochigh). Here, the question, "What is the causal effect of high inequality on democratization in this case?" is equivalent to asking what the value of $\theta^D$ is in the case, where the possible values are $\theta_{00}^D, \theta_{01}^D, \theta_{10}^D$, and $\theta_{11}^D$. We assume here that the case's response type, $\theta^D$, is not itself observable, and thus we are in the position of having to make inferences about it.

Drawing on the grammar of causal graphs discussed in Chapter 5, we can already identify possibilities for learning about $\theta^D$ from the other nodes represented in this high-level graph. Merely observing the level of inequality in a case will tell us nothing since $I$ is not $d-$connected to $\theta^D$ if we have observed nothing else. On the other hand, only observing the outcome---regime type---in a case *can* give us information about $\theta^D$ since $D$ *is* $d-$connected to $\theta^D$. For instance, if we observe $D=1$ (that a case democratized), then we can immediately rule out $\theta_{00}^D$ as a value of $\theta^D$ since this type does not permit democratization to occur. Further, conditional on observing $D$, $I$ is now $d-$connected to $\theta^D$: in other words, having observed the outcome, we can additionally learn about the case's type from observing the status of the causal variable. For example, if $D=1$, then observing $I=1$ allows us additionally to rule out the value $\theta_{10}^D$ (a negative causal effect).

Now, observing just $I$ and $D$ alone will always leave two response types in contention. For instance, seeing $I=D=1$ (the case had high inequality and democratized) would leave us unsure whether high inequality caused the democratization in this case ($\theta^D=\theta_{01}^D$) or the democratization would have happened anyway ($\theta^D=\theta_{11}^D$). This is a limitation of $X, Y$ data that we refer to in @humphreys2015mixing as the "fundamental problem of type ambiguity." Note that this does not mean that we will be left indifferent between the two remaining types. Learning from $X, Y$ data alone---narrowing the types down to two---can be quite significant, depending on our priors over the distribution of types. For example, if we previously believed that a $\theta_{00}^D$ type (cases in which democracy will never occur, regardless of inequality) was much more likely than a $\theta_{11}^D$ type (democracy will always occur, regardless of inequality) and that positive and negative effects of inequality were about equally likely, then ruling out the $\theta_{00}^D$ and $\theta_{10}^D$ values for a case will shift us toward the belief that inequality caused democratization in the case.^[This is because we are ruling out both a negative effect and the type of null effect that we had considered the most likely, leaving a null effect that we consider relatively unlikely.]

```{r dagdemochigh, echo = FALSE, fig.width = 6, fig.height = 4, fig.align="center", fig.cap = "Simple democracy, inequality model"}

hj_dag(x = c(1,2,2),
       y = c(1,1,2),
       names = c(
         "Inequality (I)",
         "Democ'n (D)",  
         expression(paste(theta[D]))
         ),
       arcs = cbind( c(1,3),
                     c(2,2)),
       title = "A high-level model of democratization",
       add_functions = 0, 
       contraction = .28, 
       padding = .2
)

```


Nonetheless, we can increase the prospects for learning by *theorizing* the relationship between inequality and democratization. Given causal logics and empirical findings in the existing literature, we can say more than is contained in Figure \@ref(fig:dagdemochigh) about the possible structure of the causal linkages between inequality and democratization. And we can embed this prior knowledge of the possible causal relations in this domain in a lower-level model that is consistent with the high-level model that most simply represents our query. If we were to seek to fully capture them, the models developed by Boix, Acemoglu and Robinson, and Ansell and Samuels would, each individually, suggest causal graphs with a large number of nodes and edges connecting them. Representing all variables and relationships jointly contained in these three models would take an extremely complex graph. Yet there is no need to go down to the lowest possible level---to generate the *most* detailed graph---in order to increase our empirical leverage on the problem.

We represent in Figure \ref{fig:lowdem} one possible lower-level model consistent with our high-level model. Drawing on causal logics in the existing literature, we unpack the nodes in the high-level model in two ways:

1. We interpose a mediator between inequality and democratization: mobilization ($M$) by economically disadvantaged groups expressing material grievances. $M$ is a function of both $I$ and of its own response-type variable, $\theta^M$, which defines its response to $I$. In inserting this mediator, we have extracted $\theta^M$ from $\theta^D$, pulling out that part of $D$'s response to $I$ that depends on $M$'s response to $I$.

2.  We specify a second influence on democratization, international pressure ($P$). Like $\theta^M$, $P$ has also been extracted from $\theta^D$; it represents that part of $D$'s response to $I$ that is conditioned by international pressures.


```{r, echo = FALSE, fig.width = 8, fig.height = 5, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:lowdem} A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type."}

hj_dag(x = c(1,1.5,3,3, 1.5, 2.2),
       y = c(1,2,2,3, 3, 3),
       names = c(
         "Inequality (I)",
        "Mobilization (M)",  
         "Democratization (D)",  
         expression(paste(theta^{D[lower]})),
        expression(paste(theta^M)),
         "Int'l pressure (P)"
         ),
       arcs = cbind( c(1,2, 4, 6, 5, 1),
                     c(2,3, 3, 3, 2, 3)),
       title = "A lower-level model of democratization",
       add_functions = 0, 
       contraction = .2, 
       padding = .2
)


```



In representing the causal dependencies in this graph, we allow for inequality to have (in the language of mediation analysis) both an "indirect" effect on democratization via mobilization and a "direct" effect. The arrow running directly from $I$ to $D$ allows for effects of inequality on democratization beyond any effects running via mobilization of the poor, including effects that might run in the opposite direction. (For instance, it is possible that inequality has a positive effect on democratization via mobilization but a negative effect via any number of processes that are not explicitly specified in the model.)  The graph also implies that there is no confounding: since there is no arrow running from another variable in the graph to $I$, $I$ is assigned as-if randomly.

The lower-level graph thus has two exogenous, response-type nodes that will be relevant to assessing causal effects: $\theta^M$ and $\theta^{D_{lower}}$. $\theta^M$, capturing $I$'s effect on $M$, ranges across the usual four values for a single-cause, binary setup: $\theta_{00}^M, \theta_{01}^M, \theta_{10}^M$, and $\theta_{11}^M$. 

$\theta^{D_{lower}}$ is considerably more complicated, however, because this node represents $D$'s response to three causal variables: $I$, $M$, and $P$. One way to put this is that the values of $\theta^{D_{lower}}$ indicate how inequality's direct effect will depend on mobilization (and vice versa), conditional on whether or not there is international pressure. We need more complex notation than that introduced in Chapter 5 in order to represent the possible response types here. In Chapter 5, we used $\theta_{ij}^{gh}$ to represent the potential outcomes for the four combinations of values that two binary variables can take on. With a third binary variable, we now use left as well as right super- and subscripts, as in: $_{ij}^{gh}\theta_{mn}^{kl}$. In the current application, we let the left scripts represent the values that $D$ will take on for different values of $I$ and $M$ *when P=0* while the right scripts represent $D$'s potential outcomes for different values of $I$ and $M$ *when P=1*. For each set of four scripts (left or right), moving horizontally from left to right represents a move from $I=0$ to $I=1$, while moving vertically from bottom to top represents a move from $M=0$ to $M=1$. (In other words, we can think of there being a separate $I$-by-$M$ graph on either side of $t$, one for $P=0$ and one for $P=1$, each with the origin in the lower lefthand corner.)

Thus, to illustrate, $n$ represents the value that $D$ will take on when $I=1, M=0$, and $P=1$, while $j$ represents the outcome for those same values of $I$ and $M$ but when $P=0$. Filling in numbers, $_{01}^{01}\theta_{10}^{10}$ represents a type in which inequality always (regardless of mobilization) has a positive direct effect on democratization when $P=0$ and always has a negative direct effect on democratization when $P=1$; in which mobilization itself never has an effect; and in which international pressure has a positive effect when inequality is low but a negative effect when inequality is high.

The result is $2^8=256$ possible response types for $D$. With 4 response types for $M$, we thus have 1024 possible combinations of causal effects between named variables in the lower-level graph. How do these lower-level response types map onto the higher-level response types that are of interest? In other words, which combinations of lower-level types represent a positive, negative, or zero causal effect of inequality on democratization? 

To define a  causal effect of $I$ in this setup, we need to define the "joint effect" of two variables as being the effect of changing both variables simultaneously (in the same direction, unless otherwise specified). Thus, the joint effect of $I$ and $M$ on $D$ is positive if changing both $I$ and $M$ from $0$ to $1$ changes $D$ from $0$ to $1$. We can likewise refer to the joint effect of an increase in one variable and a decrease in another. Given this definition, a positive causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns:


1. **Linked positive mediated effects.** $I$ has a positive effect on $M$; and $I$ and $M$ have a *joint* positive effect on $D$ when $P$ takes on whatever value it takes on in the case. Formally, this condition is satisfied when $\theta^M=\theta^M_{01}$ and $\theta^D_{lower}$ takes on any value consistent with $_{0j}^{g1}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{0n}^{k1}$ if $P=1$.

2. **Linked negative mediated effects** $I$ has a negative effect on $M$; and $I$ and $M$ have a *joint* negative effect on $D$ when $P$ takes on whatever value it takes on in the case. This condition is satisfied when $\theta^M=\theta^M_{10}$ and $\theta^D_{lower}$ takes on any value consistent with $_{i1}^{0h}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{m1}^{0l}$ if $P=1$.

3. **Positive direct effect** $I$ has no effect on $M$ and $I$ has a positive effect on $D$ at whatever value $M$ is fixed at and whatever value $P$ takes on in the case. This condition is satisfied when $\theta^M=\theta^M_{00}$ and $\theta^D_{lower}$ takes on any value consistent with $_{01}^{gh}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{01}^{kl}$ if $P=1$; or when $\theta^M=\theta^M_{11}$ and $\theta^D_{lower}$ takes on any value consistent with $_{ij}^{01}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{mn}^{01}$ if $P=1$.


If we start out with a case in which inequality is high and democratization has not occurred (or inequality is low and democratization *has* occurred), we will be interested in the possibility of a negative causal effect. A negative causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns:


4. **Positive, then negative mediated effects** $I$ has a positive effect on $M$; and $I$ and $M$ have a *joint* negative effect on $D$ when $P$ takes on whatever value it takes on in the case. This condition is satisfied when $\theta^M=\theta^M_{01}$ and $\theta^D_{lower}$ takes on any value consistent with $_{1j}^{g0}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{1n}^{k0}$ if $P=1$.

5. **Negative, then joint negative mediated effects** $I$ has a negative effect on $M$; and jointly increasing $I$ while decreasing $M$ generates a decrease in $D$ when $P$ takes on whatever value it takes on in the case. This condition is satisfied when $\theta^M=\theta^M_{10}$ and $\theta^D_{lower}$ takes on any value consistent with $_{i0}^{1h}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{m0}^{1l}$ if $P=1$.

6. **Negative direct effects** $I$ has no effect on $M$ and $I$ has a negative effect on $D$ at whatever value $M$ is fixed at and whatever value $P$ takes on in the case. This condition is satisfied when $\theta^M=\theta^M_{00}$ and $\theta^D_{lower}$ takes on any value consistent with $_{10}^{gh}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{10}^{kl}$ if $P=1$; or when $\theta^M=\theta^M_{11}$ and $\theta^D_{lower}$ takes on any value consistent with $_{ij}^{10}\theta_{mn}^{kl}$ if $P=0$ or with $_{ij}^{gh}\theta_{mn}^{10}$ if $P=1$.

Finally, all other response patterns yield *no* effect of inequality on democratization.

Thus, for a case in which $I=D=1$, our query amounts to assessing the probability that $\theta^M$ and $\theta^D_{lower}$ jointly take on values falling into conditions 1, 2, or 3. And for a case in which $I \neq D$, where we entertain the possibility of a negative effect, our query is an assessment of the probability of conditions 4, 5, and 6.

### Forming Priors

We now need to express prior beliefs about the probability distribution from which values of $\theta^M$ and $\theta^D_{lower}$ are drawn. We place structure on this problem by drawing a set of beliefs about the likelihood or monotonicity of effects and interactions among variables from the theories in Boix, Acemoglu and Robinson, and Ansell and Samuels. As a heuristic device, we weight more heavily those propositions that are more widely shared across the three works than those that are consistent with only one of the frameworks. We intend this part of the exercise to be merely illustrative of how one might go about forming priors from an existing base of knowledge; there are undoubtedly other ways in which one could do so from the inequality and democracy literature.

Specifically, the belief that we embed in our priors about $\theta^M$ is:

*  **Monotonicity of $I$'s effect on $M$**: In Acemoglu and Robinson, inequality should generally increase the chances of---and, in Boix, should never prevent---mobilization by the poor. Only in Ansell and Samuels' model does inequality have a partial downward effect on the poor's demand for democracy insofar as improved material welfare for the poor increases the chances of autocratic expropriation; and this effect is countervailed by the greater redistributive gains that the poor will enjoy under democracy as inequality rises.^[In addition, as the industrial bourgeoisie become richer, which increases the Gini, this group faces a greater risk of autocratic expropriation. If we consider the rising bourgeosie's mobilization to be mobilization by a materially disadvantaged group, then this constitutes an additional positive effect of inequality on mobilization.] Consistent with the weight of prior theory on this effect, in our initial run of the analysis, we rule out negative effects of $I$ on $M$. We are indifferent in our priors between positive and null effects and between the two types of null effects (mobilization always occurring or never occurring, regardless of the level of inequality). We thus set our prior on $\theta^M$ as: $p(\theta^M=\theta^M_{10})=0.0$, $p(\theta^M=\theta^M_{00})=0.25$, $p(\theta^M=\theta^M_{11})=0.25$, and $p(\theta^M=\theta^M_{01})=0.5$. We relax this monotonicity assumption, to account for the Ansell and Samuels logic, in a second run of the analysis.

For our prior on democracy's responses to inequality, mobilization, and international pressure ($\theta^D_{lower}$), we extract the following beliefs from the literature:

*  **Monotonicity of direct $I$ effect: no positive effect**: In none of the three theories does inequality promote democratization via a pathway *other than* via the poor's rising demand for it. In all three theories, inequality has a distinct negative effect on democratization via an increase in the elite's expected losses under democracy and thus its willingness to repress. In Ansell and Samuels, the distribution of resources also affects the probability of success of rebellion; thus higher inequality also reduces the prospects for democratization by strengthening the elite's hold on power. We thus set a zero prior probability on all types in which $I$'s direct effect on $D$ is positive for any value of $P$. This excludes all of the families of type  $_{01}^{xx}\theta_{xx}^{xx}$,  $_{xx}^{01}\theta_{xx}^{xx}$, $_{xx}^{xx}\theta_{01}^{xx}$, and  $_{xx}^{xx}\theta_{xx}^{01}$. Apologies for dense notation.

* **Monotonicity of $M$'s effect: no negative effect**: In none of the three theories does mobilization reduce the prospects of democratization. We thus set a zero probability on all types in which $M$'s effect on $D$ is negative at any value of $I$ or $P$. This excludes all of the families of type  $_{1x}^{0x}\theta_{xx}^{xx}$, $_{x1}^{x0}\theta_{xx}^{xx}$, $_{xx}^{xx}\theta_{1x}^{0x}$, and $_{xx}^{xx}\theta_{x1}^{x0}$.

* **Monotonicity of $P$'s effect: no negative effect**: While international pressures are only discussed in Haggard and Kaufman's study, none of the studies considers the possibility that international pressures to democratize might prevent democratization that would otherwise have occurred. We thus set a zero probability on all types in which $P$'s effect is negative at any value of $I$ or $M$. This excludes all of the families of type  $_{1x}^{xx}\theta_{0x}^{xx}$,  $_{x1}^{xx}\theta_{x0}^{xx}$,  $_{xx}^{1x}\theta_{xx}^{0x}$, and  $_{xx}^{x1}\theta_{xx}^{x0}$.

For all remaining, allowable types, we set flat priors.

We summarize the set of excluded  types in Table \ref{tab:apptypes}. To ease interpretation of types, we also indicate, for all excluded types, the reason for the exclusion.



|         Family or Type        | Excluded? |   Reason for Exclusion  | Nature of interaction | Prior probability |
|:-----------------------------:|:---------:|:-----------------------:|:---------------------:|:-----------------:|
| $_{01}^{xx}\theta_{xx}^{xx}$ |     Yes     | No + $I$ effects |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{01}\theta_{xx}^{xx}$ |     Yes     | No + $I$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{xx}\theta_{01}^{xx}$ |     Yes     | No + $I$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{xx}\theta_{xx}^{01}$ |     Yes     | No + $I$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{1x}^{0x}\theta_{xx}^{xx}$ |     Yes     | No - $M$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{x1}^{x0}\theta_{xx}^{xx}$ |     Yes     | No - $M$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{xx}\theta_{1x}^{0x}$ |     Yes     | No - $M$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{xx}\theta_{x1}^{x0}$ |     Yes     | No - $M$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{1x}^{xx}\theta_{0x}^{xx}$ |     Yes     | No - $P$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{x1}^{xx}\theta_{x0}^{xx}$ |     Yes     |No - $P$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{1x}\theta_{xx}^{0x}$ |     Yes     |No - $P$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
| $_{xx}^{x1}\theta_{xx}^{x0}$ |     Yes     |No - $P$ effects        |                       |         0         |
|                               |           |                         |                       |                   |
Table: (\#tab:apptypes) Possible (families of) types defining the possible effects of inequality ($I$), mobilization, ($M$), and international pressure ($P$) on democratization. We exclude families based on a set of monotonicity assumptions, which reduces the allowable types from 256 to 20. 


In Table \ref{tab:apptypes2} we show, for the remainig 20  allowable types, how international pressure moderates the effects of inequality (direct) and mobilization. We use the following notation to characterize conditioning effects of pressure on the effect of another variable, $X$:

* $N$: $P$ has no moderating effect

* $O_X$: $P=1$ creates an "opportunity" for $X$ to have an effect that it does not have at $P=0$; at $P=1$ and $X=0$, $D$ takes on the value it does when $X=0$ and $X$ has an effect, but does not take on this value when $P=0$ and $X=0$

* $C_X$: $P=1$ is a causal "complement" to $X$, allowing $X$ to have an effect it did not have at $P=0$; at $P=1$ and $X=1$, $D$ takes on the value it does when $X=1$ and $X$ has an effect, but does not take on this value when $P=0$ and $X=1$

* $S_X$: $P=1$ "substitutes" for $X$, generating the outcome that $X=1$ was necessary to generate at $P=0$; at $P=1$ and $X=0$, $D$ takes on the value it does when $X=1$ and $X$ has an effect, but does not take on this value when $P=0$ and $X=0$

* $E_X$: $P$ "eliminates" $X$'s effect, preventing $X=1$ from generating the outcome it generates when $P=0$; at $P=1$ and $X=1$, $D$ does not take on the value it does when $X=1$ and $X$ has an effect, but does take on this value when $P=0$ and $X=1$



|         Family or Type        | Excluded? |   Reason for Exclusion  | Nature of interaction | Prior probability |
|:-----------------------------:|:---------:|:-----------------------:|:---------------------:|:-----------------:|
| $_{00}^{00}\theta_{00}^{00}$ |           |                         |          $N$          |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{00}\theta_{00}^{10}$ |           |                         |       $C_M, O_I$      |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{00}\theta_{00}^{11}$ |           |                         |         $C_M$         |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{00}\theta_{10}^{10}$ |           |                         |         $O_I$         |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{00}\theta_{10}^{11}$ |           |                         |    $C_M, S_M, O_I$    |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{00}\theta_{11}^{11}$ |           |                         |          $N$          |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{10}\theta_{00}^{10}$ |           |                         |          $N$          |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{10}\theta_{00}^{11}$ |           |                         |       $C_M, E_I$      |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{10}\theta_{10}^{10}$ |           |                         |       $S_M, O_I$      |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{10}\theta_{10}^{11}$ |           |                         |    $S_M, E_I, O_I$    |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{10}\theta_{11}^{11}$ |           |                         |       $S_M, E_I$      |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{11}\theta_{00}^{11}$ |           |                         |          $N$          |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{11}\theta_{10}^{11}$ |           |                         |       $S_M, O_I$      |        0.05       |
|                               |           |                         |                       |                   |
| $_{00}^{11}\theta_{11}^{11}$ |           |                         |         $S_M$         |        0.05       |
|                               |           |                         |                       |                   |
| $_{10}^{10}\theta_{10}^{10}$ |           |                         |          $N$          |        0.05       |
|                               |           |                         |                       |                   |
| $_{10}^{10}\theta_{10}^{11}$ |           |                         |    $C_M, E_I, O_I$    |        0.05       |
|                               |           |                         |                       |                   |
| $_{10}^{10}\theta_{11}^{11}$ |           |                         |         $E_I$         |        0.05       |
|                               |           |                         |                       |                   |
| $_{10}^{11}\theta_{10}^{11}$ |           |                         |          $N$          |        0.05       |
|                               |           |                         |                       |                   |
| $_{10}^{11}\theta_{11}^{11}$ |           |                         |       $S_M, E_I$      |        0.05       |
|                               |           |                         |                       |                   |
| $_{11}^{11}\theta_{11}^{11}$ |           |                         |          $N$          |        0.05       |
Table: (\#tab:apptypes2) Table of allowable types. The table characterizes the nature of the interaction, using the notation explained in the main text. We impose flat priors across all non-excluded types.

Since $P$ conditions the effect of $I$, we must also establish a prior on the distribution of $P$. In this analysis, we set the prior probability of $P=1$ to 0.5, implying that before seeing the data we think that international pressures to democratize are present half the time.

## Results

We can now choose nodes other than $I$ or $D$ to observe from the lower-level model. Recall that our query is about the joint values of $\theta^M$ and $\theta^{D_{lower}}$. By the logic $d-$separation, we can immediately see that both $M$ and $P$ may be informative about these nodes when $D$ has already been observed. Conditional on $D$, both $M$ and $P$ are $d-$connected to both $\theta^M$ and $\theta^{D_{lower}}$. Let us see what learn, then, if we search for either mobilization of the lower classes or international pressure or both, and find either clue either present or absent.

We consider four distinct situations, corresponding to four possible combinations of inequality and democratization values that we might be starting with. In each situation, the nature of the query changes. Where we start with a case with low inequality and no democratization, asking if inequality caused the outcome is to ask if the lack of inequality caused the lack of democratization. Where we have high inequality and no democratization, we want to know if democratization was prevented by high inequality (as high inequality does in Boix's account). For cases in which democratization occurred, we want to know whether the lack or presence of inequality (whichever was the case) generated the democratization. 

Inference is done by applying Bayes rule to the observed data given the priors. Different "causal types" are consistent or inconsistent with possible data observations. Conversely the observation of data lets us shoft weight towards causal types that are consistent with the data and  away from those that are not. Table \@ref(tab:typemat) illustrates. Each row corresponds to an admissible value for  $\theta^Y$; the columns then indicate the value for $Y$ that we should observe for this causal type given values of  $P$, $I$, and $M$. For instance the first row corresponds to a type that has $D=0$ no matter what the value of $P$, $I$, and $M$. A case that displays $D=1$ when, say. $P=M=I=0$ can not be of type 1. Indeed it can only of type 15-20. Thus observation of  $P=M=I=0, D=1$ would force a shift in weight towards these nodal types.    


```{r basicPIMDmodel, eval = TRUE, message = FALSE, include = FALSE}

model <- make_model("I -> M -> D <- P; I -> D") %>%
         set_restrictions(causal_type_restrict =  
           "(M[I=1] < M[I=0]) |
            (D[I=1, M=., P=.] > D[I=0, M=., P=.]) |
            (D[M=1, I=., P=.] < D[M=0, I=., P=.]) | 
            (D[P=1, M=., I=.] < D[P=0, M=., I=.])")
  
```


```{r typemat, echo = FALSE}
rownames(UDs) <- 1:nrow(UDs)
kable(cbind(row = 1:nrow(UDs), UDs), caption = "Mapping from admissible types to responses to ancestor variables. Cell entry shows value for D, given row, for each combination of $P$,$M$,$I$, as indicated in column header.")
```


```{r, include = FALSE}
I1D1 <- conditional_inferences(model, 
                            query = "D[I=1] > D[I=0]", 
                            parameters = model$lambda, 
                            given = "D==1 & I==1")

I0D1 <- conditional_inferences(model, 
                            model$lambda, 
                            query = "D[I=1] < D[I=0]", 
                            given = "D==0 & I==0")

```

```{r, echo = FALSE}

## Functions to extract key inferences given different data strategies

possible_inferences <- function(m, p, results){
  c(m, p,
    dplyr::filter(results, is.na(M) & is.na(P))$posterior,
    dplyr::filter(results, M == m   & is.na(P))$posterior,
    dplyr::filter(results, is.na(M) & P == p)$posterior,
    dplyr::filter(results, M == m   & P == p)$posterior)}

cases_table <- function(results, case_names, digits = 3){
  out <- rbind(
                      possible_inferences(0,0,results),
                      possible_inferences(0,1,results),
                      possible_inferences(1,0,results),
                      possible_inferences(1,1,results))
  out <- round(out, digits)
  
  x <- data.frame(cbind(case_names, out))
  
  colnames(x) <- c("Case", "M: Mobilization?", "P: Pressure?", "No clues", "M only", "P only", "M and P")
  x
  }
```



ADD ESTIMAND: DOES INEQUALITY REDUCE THE EFFECT OF MOBILIZATION.






### Cases with incomplete data

We consider first  causal relations for cases that did not democratize. These cases are not included in @haggard2012inequality.


The results for cases that did not democratize (at the time in question) are presented in tables \@ref(tab:Tapp1) and Table \@ref(tab:Tapp2). Each table shows, for one kind of case, our posterior beliefs on the causal responsibility of $I$ for the outcome for different search strategies.



#### $I=0, D=0$: Non democracy with low inequality

To begin with $I=0, D=0$ cases, did the lack of inequality cause the lack of democratization (as, for instance, at the lefthand end of the Acemoglu and Robinson inverted $U$-curve)? 


```{r Tapp1, echo = FALSE,  cache = FALSE, message = FALSE, warning = FALSE} 


# Did I=0 cause D = 0
I0D0 <- conditional_inferences(model, 
                            model$lambda, 
                            query = "D[I=1] > D[I=0]", 
                            given = "D==0 & I==0")

kable(dplyr::select(I0D0, P,M, posterior),digits = 3, caption = "No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.")
```


We start out, based on the $I$ and $D$ values and our model, believing that there is a `r I0D0[1,5]` chance that low inequality prevented democratization. We then see that our beliefs shift most dramatically if we go looking for mobilization and find that it was present. The reason is that any positive effect of $I$ on $D$ has to run through the pathway mediated by $M$ because we have excluded a positive direct effect of $I$ on $D$ in our priors. Moreover, since we do not allow $I$ to have a negative effect on $M$, observing $M=1$ when $I=0$ must mean that $I$ has no effect on $M$ on this case, and thus $I$ cannot have a positive effect on $D$ (regardless also of what we find if we look for $P$). If we do *not* observe mobilization when we look for it, we now think it is somewhat more likely that $I=0$ caused $D=0$ since it is still possible that high inequality *could* cause mobilization. 

We also see that observing whether there is international pressure has a substantial effect on our beliefs. When we observe $M=1$ (or don't look for $M$ at all), the presence of international pressure increases the likelihood that low inequality prevented democratization. Intuitively, this is because international pressure, on average across types, has a positive effect on democratization; so pressure's presence creates a greater opportunity for low inequality to counteract international pressure's effect and prevent democratization from occurring that otherwise would have (if there had been high inequality and the resulting mobilization).


#### $I=1, D=0$: Non democracy with high inequality

In cases with high inequality and no democratization, the question is whether high inequality prevented democratization via a negative effect, as theorized by Boix. That negative effect has to have operated via inequality's direct effect on democratization since our monotonicity restrictions allow only positive effects via mobilization. Here, the consequence of observing $P$ is similar to what we see in the $I=0, D=0$ case: seeing international pressure greatly increases our confidence that high inequality prevented democratization, while seeing no international pressure moderately reduces that confidence. There is, returning to the same intuition, more opportunity for high inequality to exert a negative effect on democratization when international pressures are present, pushing toward democratization. 



```{r Tapp2, echo = FALSE}

I1D0 <- conditional_inferences(model, 
                            model$lambda, 
                            query = "D[I=1] < D[I=0]", 
                            given = "D==0 & I==1")

kable(dplyr::select(I1D0, P,M, posterior), digits = 3, caption = "Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.")
```

Here, however, looking for $M$ has more modest effect than it does in an $I=0, D=0$ case. This is because we learn less about the indirect pathway from $I$ to $D$ by observing $M$: as we have said, we already know from seeing high inequality and no democratization (and under our monotonicity assumptions) that any effect could not have run through the presence or absence of mobilization. 

However, $M$ provides some information because it, like $P$, acts as *moderator* for $I$'s direct effect on $D$ (since $M$ is also pointing into $D$). As we know, learning about moderators tells us something about (a) the rules governing a case's response to its context (i.e., its response type) and (b) the context it is in. Thus, in the first instance, observing $M$ together with $I$ and $D$ helps us eliminate types inconsistent with these three data points. For instance, if we see $M=0$, then we eliminate any type in which $D$ is 0, regardless of $P$'s value, when $M=0$ and $I=1$. Second, we learn from observing $M$ about the value of $M$ under which $D$ will be responding to $I$. Now, because $M$ is itself potentially affected by $I$, the learning here is somewhat complicated. What we learn most directly from observing $M$ is *the effect of $I$ on $M$* in this case. If we observe $M=1$, then we know that $I$ has no effect on $M$ in this case; whereas if we observe $M=0$, $I$ might or might not have a positive effect on $M$. Learning about this $I \rightarrow M$ effect then allows us to form a belief about how likely $M$ would be to be 0 or 1 if $I$ changed from $0$ to $1$; that is, it allows us to learn about the context under which $D$ would be responding to this change in $I$ (would mobilization be occurring or not)? This belief, in turn, allows us to form a belief about how $D$ will respond to $I$ given our posterior beliefs across the possible types that the case is. 

The net effect, assuming that we have not observed $P$, is a small upward effect in our confidence that inequality mattered if we see no mobilization, and a small downward effect if we see mobilization. Interestingly, if we *do* observe $P$, the effect of observing $M$ reverses: observing mobilization increases our confidence in inequality's effect, while observing no mobilization reduces it. 


### Inferences for cases with observed democratization

We now turm to  cases in which democratization has occurred---the category of cases that Haggard and Kaufman examine.

For these cases we  use data from @haggard2012inequality to show the inferences we would draw using this procedure and the actual observations made for a set of 8 cases. 

Haggard and Kaufman consider only cases that democratized, so all cases in this table have the value $D=1$. We show here how confident we would be that the level inequality caused democratization if (a) we observed only the cause and effect ($I$ and $D$); (b) we additionally observed either the level of mobilization by disadvantaged classes or the level of international pressure; and (c) if we observed both, in addition to $I$ and $D$. Note that countries labels are marked in the  "full data" cells in the lower right  quadrant, but their corresponding partial data cells can be read by moving to the left column or the top row (or to the top left cell for the case with no clue data).

In coding countries' level of inequality, we rely on Haggard and Kaufman's codings using the Gini coefficient from the Texas Inequality dataset.  In selecting cases of democratization, we use the codings in @cheibub2010democracy, one of two measures used by Haggard and Kaufman. Our codings of the $M$ and $P$ clues come from close readings of the country-specific transition accounts in @haggard2012distributive, the publicly shared qualitative dataset associated with @haggard2012inequality. We code $M$ as $1$ where the transition account refers to anti-government or anti-regime political mobilization by economically disadvantaged groups, and as $0$ otherwise. For $P$, we code $P=0$ is international pressures to democratize are not mentioned in the transition account. The main estimates refer to analyses with only qualitative, monotonicity restrictions on our priors. We also show in square brackets the estimates if we allow for a negative effect of inequality on mobilization but believe it to be relatively unlikely.


#### $I=0, D=1$: Low inequality democracies


In a case that had low inequality and democratized, did low inequality cause democratization, as Boix's thesis would suggest? Looking at the first set of cases in Table \@ref(tab:HK8cases1), did Mexico, Albania, Taiwan, and Nicaragua democratize because they had relatively low inequality? Based only on observing the level of inequality and the outcome of democratization, we would place a `r I0D1[1,1]` probability on inequality having been a cause. What can we learn, then, from our two clues?

We are looking here for a negative effect of $I$ on $D$, which in our model can only run via a direct effect, not through mobilization. Thus, the learning from $M$ is limited for the same reason as in an $I=1, D=0$ case. And $M$ is modestly informative as a moderator for the same reasons and in the same direction, with observing mobilization generally reducing our confidence in inequality's negative effect relative to observing no mobilization. In our four cases, if we observe the level of mobilization, our confidence that inequality mattered goes up slightly (to `r I0D1[2,1]`) in Mexico and Taiwan, where mobilization did not occur, and goes down slightly in Albania and Nicaragua (to `r I0D1[3,1]`) where mobilization did occur.



```{r HK8cases1, echo = FALSE}
I0D1_Cases <- cases_table(
  conditional_inferences(
    model, model$lambda,  query = "D[I=1] < D[I=0]",   given = "D==1 & I==0"), 
  case_names  = c("Mexico (2000)", "Taiwan (1996)", 
                  "Albania (1991)", "Nicaragua (1984)"))

kable(I0D1_Cases, digits = 3, caption = "Four cases with low inequality and  democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.")

```


Looking for the international pressure clue is, however, highly informative, though the effect runs in the opposite direction as in an $I=1, D=0$ case.  It is observing the absence of international pressure that makes us more confident in low inequality's effect. Since democratization *did* occur, the presence of international pressure makes it less likely for low inequality to have generated the outcome since international pressure could have generated democratization by itself. Once we bring this second clue into the analysis, Mexico and Taiwan sharply part ways: seeing no international pressure in Mexico, we are now much more confident that inequality mattered for the Mexican transition (`r I0D1[2,2]`); seeing international pressure in Taiwan, we are now substantially less confident that inequality mattered to the Taiwanese transition (`r I0D1[2,3]`). Similarly, observing $P$ sharply differentiates the Albanian and Nicaraguan cases: seeing no international pressure in the Albanian transition considerably boosts our confidence in inequality's causal role there (`r I0D1[3,2]`), while observing international pressure in the Nicaraguan transition strongly undermines our belief in an inequality effect there (`r I0D1[3,3]`).


#### $I=1, D=1$: High inequality democracies

Where we see both high inequality and democratization, the question is whether high inequality caused democratization via a positive effect. Considering the second set of cases in Table \ref{tab:HK8cases2}, did high inequality cause Mongolia, Sierra Leone, Paraguay, and Malawi to democratize?

Observing only the level of inequality and the democratization outcome, we would have fairly low confidence that inequality mattered, with a posterior on that effect of `r I1D1[1,1]`. Let us see what we can learn if we also observe the level of mobilization and international pressure.

As in an $I=0, D=0$ case, $M$ can now be highly informative since this positive effect has to run through mobilization. Here it is the observation of a lack of mobilization that is most telling: high inequality cannot have caused democratization, given our model, if inequality did not cause mobilization to occur. There is no point in looking for international pressure since doing so will have no effect on our beliefs. Thus, when we observe no mobilization by the lower classes in Mongolia and Paraguay, we can be certain (given our model) that high inequality did *not* cause democratization in these cases. Moreover, this result does not change if we also go and look for international pressure: neither seeing pressure nor seeing its absence shifts our posterior away from `r I1D1[1,1]`. 


```{r HK8cases2, echo = FALSE}

I1D1_Cases <- cases_table(
  conditional_inferences(
    model, model$lambda,  query = "D[I=1] > D[I=0]",   given = "D==1 & I==1"), 
  case_names  = c("Mongolia (1990)", "Paraguay (1989)", "Sierra Leone (1996)", "Malawi (1994)"))

kable(I1D1_Cases, caption = "Four cases with high inequality and  democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.")

```

If we do see mobilization, on the other hand---as in Sierra Leone and Malawi---we are slightly more confident that high inequality was the cause of democratization (`r round(I1D1[3,1], 3)`). Moreover, if we first see $M=1$, then observing international pressure can add much more information; and it substantially differentiates our conclusions about the causes of Sierra Leone's and Malawi's transitions. Just as in an *$I=0, D=1* case, it is the absence of international pressure that leaves the most "space" for inequality to have generated the democratization outcome. When we see the absence of pressure in Sierra Leone, our confidence that high inequality was a cause of the transition increases to `r round(I1D1[3,2],3)`; seeing pressure present in Malawi reduces our confidence in inequality's effect to `r round(I1D1[3,3],3)`.

## Model definition and inference in code

How is this model defined and used in practice?

Using the `gbiqq` package we can set this model up, along with restrictions, in a few lines a follows.

```{r, eval = FALSE}

model <- make_model("I -> M -> D <- P; I -> D") %>%
         set_restrictions(causal_type_restrict =  
           "(M[I=1] < M[I=0]) |
            (D[I=1, M=., P=.] > D[I=0, M=., P=.]) |
            (D[M=1, I=., P=.] < D[M=0, I=., P=.]) | 
            (D[P=1, M=., I=.] < D[P=0, M=., I=.])")
```

We can then calculate conditional inferences as follows:

```{r, eval = FALSE}
conditional_inferences(model, 
                       query = "D[I=0]==0",
                       given = "D==1 & I==1")
```


## Concluding thoughts

Haggard and Kaufman set out to use causal process observations to test inequality-based theories of democratization against the experiences of "Third Wave" democratizations. Their principal test is to examine whether they see evidence of distributive conflict in the process of democratization, defined largely as the presence or absence of mobilization prior to the transition. They secondarily look for other possible causes, specifically international pressure and splits in the elite. 

In interpreting the evidence, Haggard and Kaufman generally treat the absence of mobilization as evidence against inequality-based theories of democratization as a whole (p. 7). They also see the *presence* of distributive mobilization in cases with high inequality and democratization as evidence against the causal role of inequality (p. 7). These inferences, however, seem only loosely connected to the logic of the causal theories under examination. Haggard and Kaufman express concern that inequality-oriented arguments point to "cross-cutting effects" (p. 1) of inequality, but do not systematically work through the implications of these multiple pathways for empirical strategy. Our analysis suggests that a systematic engagement with the underlying models can shift that interpretation considerably. Under the model we have formulated, where inequality is *high*, the absence of mobilization in a country that democratized is indeed damning to the notion that inequality mattered. However, where inequality is *low*---precisely the situation in which Boix's theory predicts that we will see democratization---things are more complicated. If we assume that inequality cannot prevent mobilization, then observing no mobilization does not work against the claim that inequality mattered for the transition; indeed, it slightly supports it, at least given what we think is a plausible model-representation of arguments in the literature. Observing the absence of inequality in such a case, however, can undercut an inequality-based explanation if (and only if) we believe it is possible that inequality might prevent mobilization that would otherwise have occurred. Further, in cases with high inequality and democratization, it is the *absence* of mobilization by the lower classes that would least consistent with the claim that inequality mattered. Observing mobilization, in contrast, pushes in favor of an inequality-based explanation.

Moreover, it is striking that Haggard and Kaufman lean principally on a mediator clue, turning to evidence of international pressure and elite splits (moderators, or alternative causes) largely as secondary clues to identify "ambiguous" cases. As we have shown, under a plausible model given prior theory, it is the moderator clue that is likely to be much more informative.

Of course, the model that we have written down is only one possible interpretation of existing theoretical knowledge. It is very possible that Haggard and Kaufman and other scholars in this domain hold beliefs that diverge from those encoded in our working model. The larger point, however, is that our process tracing inferences will inevitably *depend*---and could depend greatly---on our background knowledge of the domain under examination. Moreover, formalizing that knowledge as causal model can help ensure that we are taking that prior knowledge systematically into account---that the inferences we draw from new data are consistent with the knowledge that we bring to the table.

The analysis also has insights regarding case selection. Haggard and Kaufman justify their choice of only $D=1$ cases as a strategy "designed to test a particular theory and thus rests on identification of the causal mechanism leading to regime change" (p. 4). Ultimately, however, the authors seem centrally concerned with assessing whether inequality, as opposed to something else, played a key causal role in generating the outcome. As the results above demonstrate, however, there is nothing special about the $D=1$ cases in generating leverage on this question. The tables for $D=0$ show that, given the model, the same clues can shift beliefs about as much for $D=0$ as for $D=1$ cases. We leave a more detailed discussion of this kind of issue in model-based case-selection for Chapter REFERENCE.

Finally we emphasize that all of the inference in this chapter depends on a model that is constrained by theoretical insights but not one that is trained by data. Although we are able to make many inferences using this model, given the characteristics of a case of interest, we have no empirical grounds to justify these inferences. In Chapter REFERENCE we show how this model can be trained with broader data from multiple cases and in Chapter REFERENCE we illustrate how the model itself can be put into question. 

<!-- THE FOLLOWING TWO CONDITIONS ARE REMOVED BECAUSE, I AM ALMOST POSITIVE, THEY ARE ALREADY COVERED BY THE MONOTONICITY CONDITIONS. \item **$I=1$ dampens $M$'s effect**: Acemoglu and Robinson model inequality's effect on elite preferences (our direct effect) and and its effect on the poor's preferences (essentially, our effect via mobilization) as countervailing. While the poor want democracy more as inequality rises, the elite become more determined to avoid it. Thus, as inequality rises (holding the level of mobilization constant), mobilization should be less likely to succeed. The studies under consideration, moreover, provide no reason to think the opposite: that mobilization should become more likely to succeed as inequality goes up, holding constant the level of mobilization. To reflect these beliefs, we place zero probability on those types in which democracy will not occur at low levels of inequality but in which mobilization has a positive effect at a high level of inequality at any value of $P$. This yields average prior beliefs weighted toward inequality dampening mobilization's effect. -->
<!-- \item **$M=1$ dampens $I$'s effect**: Likewise, where disadvantaged groups have mobilized, inequality should be less likely to have a negative direct effect. At high levels of mobilization, it will be more difficult for elite preferences (which higher inequality drives against democratizing) to prevail. And, based on the logics in the literature, it should never be the case that mobilization enhances inequality's negative effect on democratization. We thus place zero probability on those types in which, for any value of $P$, democratization would occur at high levels of inequality but in which $I$ has a negative effect when $M=1$, generating a belief set weighted toward the belief that mobilization dampens inequality's negative effect. -->


<!--chapter:end:07-PT-application.Rmd-->

# Integrated inferences {#mixing}

***

We argue that mixed methods can be thought of as the analysis of single cases with vector valued variables. Reconceptualizing as large n is useful prmarily for computation reasons and often comes with hidden independence assumptions. We illustrate the single case approach and provide a set of models for the many case approach.

***


```{r, include = FALSE}
source("_packages_used.R")
```

<!-- Lots of this likely to change with integration with DAGs. -->

The main goal of this chapter is to  generalize the model developed in Chapter 6 to problems with data on many cases.  In doing so we generalize the model in @humphreys2015mixing to one that  in which rather than the probative value of clues being  *assumed*, they are  derived from a causal structure. 

We start however  with a conceptual point: the exact structure introduced in Chapter 6 for single case analysis can be used *as is* for multi-case analysis. To see this  you should think of the the nodes as vector-valued, and the estimands as just a particular summary of the vector-valued case level causal effects. Thought of this way the  conceptual work for mixed methods inference from models has been done already and our goal here is more technical---how to exploit assumptions on independence across cases to generate simpler theories of repeated phenomena.


## There's only ever one case

Conceptualized correctly, there is no difference at all between the data types or the inference used in within-case and cross-case inference. The reason is not, as @king1994designing suggest, that all causal inference is fundamentally correlational, even in seemingly single case studies. Nor is the point that, looked at carefully, single "case studies" can be disaggregated into many cases. The intuition runs in the opposite direction: fundamentally, model-based inference always involves comparing *a* pattern of data with the logic of the model. Looked at carefully, studies with multiple cases can be conceptualized of as single-case studies: the drawing of inferences from a single collection of clues.

The key insight is that, when we move from a causal model with one observation to a causal model with multiple observations, all that we are doing is replacing nodes with a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors). 

To illustrate the idea that multi-case studies are really single-case studies with vector valued variables, consider the following situation. There are two units studied, drawn from some population, a binary treatment $X$ is assigned independently with probability .5 to each case; an outcome $Y$ along with clue variable $K$ is observable.  We suppose  $X$ can affect $Y$ and in addition there is a background, unobserved, variable $\theta$ (causal type) that takes on values in $\{a,b,c,d\}$, that affects both $K$ and $Y$.  We will assume that $\theta$ is not independently assigned and that the two units are more likely to have the same values of $\theta$ than different values. For simplicity, we will suppose that for any given case $K=1$ whenever $X$ causes $Y$, and $K=1$ with a 50% probability otherwise. Thus, $K$ is informative about a unit's causal type.

Note that we have described the problem at the unit level. We can redescribe it at the population level however as a situation in which a treatment vector $X$ can take on one of four values, $(0,0), (0,1), (1,0), (1,1)$ with equal probability (or more strictly: as determined by $\theta$). $\theta$ is also a vector with two elements that can take on one of 16 values $(a,a), (a,b),\dots (d,d)$ as determined by $U_\theta$. In this case we will assume that the 16 possibilities are not equally likely, which captures the failure of independence in the unit level assignments.  $Y$ is a vector that reflects the elements of $\theta$ and $X$ in the obvious way (e.g $X=(0,0), \theta=(a,b)$ generates outcomes $Y=(1,0)$; though it is immediately obvious that representing nodes in vector forms allows for more general vector-level mappings to allow for SUTVA violations. $K$ has the same domain as $X$ and $Y$, and element $K[j]=1$ if $\theta[j]=b$.

Note that to describe the estimand, the Sample Average Treatment Effect, we also need to consider operations and queries defined at the vector level. In practice we consider three operations, one in which both units have $X$ forced to 0 and two in which one nit has $X$ set to 0 and the other has $X$ set to 1. Thus we are interested in the average effect of changing one unit to treatment while the other is held in control. Note also that before our estimands were binary---of the form: is it a $b$ type?--and our answer was a probability; now our estimand is categorical and our answer is a distribution (what is the probability the SATE is 0, what is the probability the SATE is .5, etc...)

Represented in this way we can use the tools of Chapters 6 and 7 to fully examine this seemingly multi-case study. In the below we examine a situation in which we consider the value of observing $K$ on one case --- in this set up this is equivalent to observing part of the vector $K$ and making inferences on the full vector $\theta$.


## General procedure 

In practice however thinking of nodes as capturing the outcomes on all units leads to enormous complexity. For example an exogeneous variable $X$ which takes on values of 0 or 1 at random for 10 units has $2^10$ types in this conceptualization, rather than just two when thought of at the case level. 

We reduce complexity however by thinking of models as operating on units and learning about models by observing *multiple* realizations of processes covered by the model, rather than just one. Thinking about it this way is not free however as it requires invoking some kind of independence assumptions --- that outcomes in two units do not depend on each other. If we cannot stand by that assumption then we have to build independence failures into our models

With multiple cases we...

### The parameter matrix
### The ambiguity matrix
### Likelihood
### Estimation
### Mixed data


Say a data strategy seeks data on $X$ and $Y$ in 2 cases and seeks data on $K$ if ever $X=Y=1$.

The probability of each data type is as given in table below:


|type:     |prob:                        |
|----------|-----------------------------|
|$X0Y0$    |$\lambda^X_0(\lambda^Y_{00}+\lambda^Y_{01}))$                               |
|$X0Y1$    |$\lambda^X_0(\lambda^Y_{11}+\lambda^Y_{10}))$                               |
|$X1Y0$    |$\lambda^X_1(\lambda^Y_{00}+\lambda^Y_{10}))$                               |
|$X1M0Y1$  |$\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{11}+\lambda^Y_{10}))$|
|$X1M1Y1$  |$\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01}))$|

The two observations can be thought of as a multinomal draw from these five event types.

Alternatively they can also be thought of as the product of a draw from a strategy in which a set of units is drawn with observations on $X,Y$ only and another set is drawn with observations on $X, M,Y$.

In the single multinomial view we have the probability of seeing data with $X=Y=0$ in one case and $X=1, M=0, Y=1$ in another is:

* $2P(X=0, Y=0)P(X=1, M=0, Y=1)$

In the conditional strategy view we have

* $2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)$

In the two strategy view we have

* $P(X=0, Y=0)P(X=1, M=0, Y=1)$

which is the same up to a constant.

Say rather than conditioning $X=Y=1$ to examine $M$ one of the two cases were chosen at random to observe $M$ and it just so happend to be be a case with $X=Y=1$:

| type:    | prob:                                                                         |
|----------|-------------------------------------------------------------------------------|
|$X0Y0$    |$.5\lambda^X_0(\lambda^Y_{00}+\lambda^Y_{01}))$                                |
|$X0Y1$    |$.5\lambda^X_0(\lambda^Y_{11}+\lambda^Y_{10}))$                                |
|$X1Y0$    |$.5\lambda^X_1(\lambda^Y_{00}+\lambda^Y_{10}))$                                |
|$X1Y1$    |$.5\lambda^X_1(\lambda^Y_{11}+\lambda^Y_{01}))$                                |
|$X0M0Y0$  |$.5\lambda^X_0(\lambda^M_{00}+\lambda^M_{01}))(\lambda^Y_{00}+\lambda^Y_{01}))$|
|$X0M1Y0$  |$.5\lambda^X_0(\lambda^M_{11}+\lambda^M_{10}))(\lambda^Y_{00}+\lambda^Y_{10}))$|
|...       |                                                                               |
|$X1M0Y1$  |$\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{11}+\lambda^Y_{10}))$   |
|$X1M1Y1$  |$\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01}))$   |


In the single multinomial view we have the probability of seeing data with $X=Y=0$ in one case and $X=1, M=0, Y=1$ in another is now:

* $2P(X=0, Y=0)P(X=1, M=0, Y=1)$

In the conditional strategy view we have

* $2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)$

In the two strategy view we have

* $P(X=0, Y=0)P(X=1, M=0, Y=1)$

which is the same up to a constant.


## Illustration 

Consider a generalization of the models introduced in Chapter 6 in which a treatment $X$ is a cause of both $K$ and $Y$, and outcome $Y$ is a product of both $X$ and $K$. Though $K$ is both a mediator and a moderator for the effect of $X$. There are now 16 nodal types for $Y$, 4 for $K$ and 2 for $X$, yielding 32 causal types.

To allow for the possibility of non-random selection of $X$ we will assume that the assignment probability for $X$ depends on $U^Y$. This is a feature shared also in the baseline model when we specify $\pi$ as a function of types $a$,$b$,$c$,$d$.

Our piors requires specifying:

1. A distribution over the 15-dimensional simplex representing possible values of $\lambda^Y$--which in turn determine types $u^Y$.
2. A distribution over the 3-dimensional vector representing possible values of $\lambda^K$,  which in turn determine types $u^K$.


The model is restricted in various ways. We assume now confounding in the assignemnt of $X$. Less obviously we implicitly assume that $K$ is independent of $\theta^Y$ conditional on $X$.

With these elements in hand, however, all we need now is to provide a mapping from these fundamental parameters to the parameters used in the baseline model to form the likelihood. 


The key transformation is the identification of causal types resulting from the 64 combinations of $\lambda^Y$ and $\lambda^K$. These are shown below.

TABLE TO SHOW CAUSAL TYPES

Consider the following matrices of values for $u_Y$ and $u_K$, where $\lambda_{pq}^{rs}$ is the probability that $u^Y = t_{pq}^{rs}$, meaning that $Y$ would take the value $p$ when $X=0, K=0$,  $q$ when $X=0, K=1$,  $r$ when $X=1, K=0$,  and $s$ when $X=1, K=1$. Similarly $\lambda_{w}^{z}$ is the probability that $u^K$ takes value  $t_{w}^{z}$  meaning that $K$ takes the value $w$ when $X=0$ and $z$ when $X=1$.


TABLE TO SHOW CONDITIONAL PROBABILITIES OF K GIVEN X=1 AND TYPE

These types are the *transformed parameters*; the probability of a type is just the sum of the probabilities of the fundamental types that compose it, formed by taking the product of the $\lambda^Y$ and $\lambda^K$ values marked in the rows and columns of  table \ref{tab:types}. 

Similarly $\phi_{tx}$ can be constructed as the probability of observing $K$ conditional on this type (again, sums of products of probabilities associated with cells in table  \ref{tab:types}). For instance, using the row and column indices in exponents (GIVE FULL LABELS) from table \ref{tab:types}:

$$\phi_{b1}=\frac{\lambda_K^2(\lambda_Y^2+\lambda_Y^4+\lambda_Y^6+\lambda_Y^8)+\lambda_K^4(\lambda_Y^2+\lambda_Y^4+\lambda_Y^{10}+\lambda_Y^{12})}{
\lambda_K^1(\lambda_Y^3+\lambda_Y^4+\lambda_Y^7+\lambda_Y^8)+\lambda_K^2(\lambda_Y^2+\lambda_Y^4+\lambda_Y^6+\lambda_Y^8)+\lambda_K^3(\lambda_Y^3+\lambda_Y^4+\lambda_Y^11+\lambda_Y^{12})+\lambda_K^4(\lambda_Y^2+\lambda_Y^4+\lambda_Y^{10}+\lambda_Y^{12})}$$



With these transformed parameters in hand, the likelihood is exactly the same as that specified in the baseline model.

## Illustrated inferences


### XY model

Consider the simple model in which $X$ causes $Y$ without confounding. 

Assuming flat priors on types, what inferences do we draw from different sorts of (small) datasets. Do we learn more about effects from two cases that are the same, two cases that differ on X and Y only or two cases that differ on both.

The results are given in table \@ref(tab:XYresultstable).

```{r XYresultstable, echo = FALSE}

model <- make_model("X->Y") %>% set_parameter_matrix()

if(!exists("fit")) fit <- fitted_model()

if(do_diagnosis){
types_inferences <- function(model, data = data.frame(X = 0, Y = 1), stan_model, label){

  M <- gbiqq::gbiqq(model, data = data, stan_model = stan_model)

  a <- get_estimands(M, using = "posteriors", queries = "Y[X=1]<Y[X=0]")
  b <- get_estimands(M, using = "posteriors", queries = "Y[X=1]>Y[X=0]")
  c <- get_estimands(M, using = "posteriors", queries = "(Y[X=.]==0)", join_by = "&")
  d <- get_estimands(M, using = "posteriors", queries = "(Y[X=.]==1)", join_by = "&")
  data.frame(data = label, a = a$mean, b = b$mean, c = c$mean, d = d$mean, ate = b$mean - a$mean, bd = b$mean/d$mean, bc = b$mean/c$mean)
 }


M_1_0 <- types_inferences(model, data = data.frame(X = 0, Y = 0),stan_model = fit, label = "00")
M_1_1 <- types_inferences(model, data = data.frame(X = 0, Y = 1),stan_model = fit, label = "01")
M_1_2 <- types_inferences(model, data = data.frame(X = 1, Y = 1),stan_model = fit, label = "11")
M_2_1 <- types_inferences(model, data = data.frame(X = c(0,0), Y = c(1,1)),stan_model = fit, label = "01, 01")
M_2_2 <- types_inferences(model, data = data.frame(X = c(1,1), Y = c(1,1)),stan_model = fit, label = "11, 11")
M_2_3 <- types_inferences(model, data = data.frame(X = c(0,1), Y = c(1,1)),stan_model = fit, label = "01, 11")
M_2_4 <- types_inferences(model, data = data.frame(X = c(1,1), Y = c(0,1)),stan_model = fit, label = "10, 11")
M_2_5 <- types_inferences(model, data = data.frame(X = c(0,1), Y = c(0,1)),stan_model = fit, label = "00, 11")
M_3_1 <- types_inferences(model, data = data.frame(X = c(1,1, 1), Y = c(1,1,1)),stan_model = fit, label = "11, 11, 11")

xy_results_table <- rbind(M_1_0, M_1_1, M_1_2, M_2_1, M_2_2, M_2_3, M_2_4, M_2_5, M_3_1)
write_rds(xy_results_table, "saved/xy_results_table.rds")
}
xy_results_table <- read_rds("saved/xy_results_table.rds")

kable(xy_results_table, digits = 2, caption = "Inferences for different data observations in a simple X->Y model")
```

We note a number of features:

* $X=1, Y=1$ data does not discriminate between $\theta^Y_{01}$ and $\theta^Y_{11}$ and so while more of this data puts greater weight on both   $\lambda^Y_{01}$ and $\lambda^Y_{11}$, it does nothing to discriminate between them.
* Similarly $X=0, Y=0$ data does not discriminate between $\theta^Y_{01}$ and $\theta^Y_{00}$ though it puts greater weight (uniformly) on $\lambda^Y_{01}$ and $\lambda^Y_{00}$,
* For this reason, greatest weight is placed on  $\theta^Y_{01}$ when data on both $X=Y=0$ and $X=Y=1$ cases are found. 
* The fractions suggest a common formula:

$$\lambda^Y|n_{xy} \sim Dirichlet\left(1+\frac{n_{01} + n_{10}}2, 1+\frac{n_{00} + n_{11}}2, 1+\frac{n_{00} + n_{10}}2, 1+\frac{n_{01} + n_{11}}2\right)$$

Posterior mean on ATE is then $\frac{n_{00} + n_{11} - n_{01} - n_{10}}n$.

```{r XMYresultstable, echo = FALSE}
if(do_diagnosis){
XMY_model <- make_model("X ->M -> Y")
med_1 <- types_inferences(XMY_model, data = data.frame(
  X = c(1),
  M = c(1),
  Y = c(1)),
  stan_model = fit, label = "111")
med_2 <- types_inferences(XMY_model, data = data.frame(
  X = c(1,1),
  M = c(1,1),
  Y = c(1,1)),
  stan_model = fit, label = "111, 111")
med_3 <- types_inferences(XMY_model, data = data.frame(
  X = c(0,1),
  M = c(0,1),
  Y = c(0,1)),
  stan_model = fit, label = "000, 111")
med_4 <- types_inferences(XMY_model, data = data.frame(
  X = c(1,1, 1),
  M = c(1,1, 1),
  Y = c(1,1, 1)),
  stan_model = fit, label = "111, 111, 111")
xmy_results_table <- rbind(med_1, med_2, med_3, med_4)

write_rds(xmy_results_table, "saved/xmy_results_table.rds")
}

xmy_results_table <- read_rds("saved/xmy_results_table.rds")

kable(xmy_results_table, digits = 2)
```

## Considerations 
### The identification problem

```{r}
model <- make_model("X1 -> M1 -> Y <- M2 <- X2")

# restrict such that *only* M1 OR M2 could cause Y -- can we create a DD test? / achieve identification

```


### Continuous data

We can similarly shift from binary to continuous variable values through an expansion of the causal types. Suppose that $Y$ can take on $m$ possible values. With $k$ explanatory variables, each taking on $r$ possible values, we then have $m^{r^k}$ causal types and, correspondingly, very many more elements in $\phi$. Naturally, in such situations, researchers might want to reduce complexity by placing structure onto the possible patterns of causal effects and clue probabilities, such as assuming a monotonic function linking effect sizes and clue probabilities.


### Measurement error

We have assumed no measurement error; in applications there could be considerable interest in measurement error. On one hand clue information may contain information about possible mismeasurement on $X$ and $Y$; on the other hand there might interest in whether measured clues adequately capture those features of a causal process that is thought to be measureable.  

The probability of different types of measurement error can be included among the set of parameters of interest, with likelihood functions adjusted accordingly. Suppose, for instance, that with probability $\epsilon$ a $Y=0$ case is recorded as a $Y=1$ case (and vice versa). Then the event probability of observing an $X=1$,$Y=1$ case, for example, is $\epsilon \lambda_a \pi_a + (1-\epsilon) \lambda_b \pi_b + \epsilon \lambda_c \pi_c + (1-\epsilon) \lambda_d \pi_d$. %If instead there were measurement error on $X$ but not on $Y$, then the event probability would be: $\epsilon \lambda_a (1-\pi_a) + (1-\epsilon) \lambda_b \pi_b + \epsilon \lambda_d (1-\pi_d) + (1-\epsilon) \lambda_d \pi_d$. 
Similar expressions can be derived for measurement error on $X$ or $K$. Specifying the problem in this way allows us both to take account of measurement error and learn about it.

### Spillovers

Spillovers may also be addressed through an appropriate definition of causal types. For example a unit $i$ that is affected either by receiving treatment or via the treatment of a neighbor, $j$, might have potential outcomes $Y_i(X_i,X_j)=\max(X_i,X_j)$ while another type that is not influenced by neighbor treatment status has  $Y_i(X_i,X_j)=\max(X_i)$. With such a set-up, relevant clue information might discriminate between units affected by spillovers and those unaffected.   

### Parameteric models

## Conclusion

ADD REFERENCE TO TABLE 1 OF FOR MIXED DATA "Ability and Achievement" Otis Duncan


<!--chapter:end:08-mixing-methods.Rmd-->

# Mixed-Method Application: Inequality and Democracy Revisited {#mixingapp}



```{r, include = FALSE}
source("_packages_used.R")
```

```{r, include=FALSE}
# do_diagnosis = FALSE
```

***

In chapter 7 we drew inferences from a 'theory based' democracy and inequality model on data. Here we train the  model on data before making case level inferences, allowing for the possibility of confounding in the assignment of inequality. In this case we update our beliefs over the population parameters and not just over the case level parameters. Thus we simultaneously learn about our thoery and use our theory to learn about cases. The data informed inferences are, on the whole, weaker than the theory based inferences of Chapter 7.

***


## A trained model

We now apply these ideas on mixed method research to our model of democracy and inequality. The key difference to the exercise in in Chapter 7 is that whereas there we took the model as given, and sought to draw inferences to case given the model, the model now becomes an object that we both learn from and learn about. In essence we use the data on many cases to update our beliefs about the general model and then use this "trained' model to then make inferences about cases. 

Along with this change in goals come some changes in the structure of the model:


* Instead of specifying beliefs about causal types, $\theta$,  we now need to specify a belief over the *distribution* of causal types, $\lambda$. Concretely whereas in the simple process tracing model we *specified* that inequality has a positive effect on mobilization for some share of units, we now specify a distribution over the share of units for which inequality could have a positive effect, ranging anywhere from 0 to 100%  

* As we train the model we allow now for the possibility that there is unobserved confounding -- in particular the possibility that inequality is less (or more!) likely in places in which inequality would induce mobilization. We do not impose confounding in one direction of another, but we do allow for the **possibility** of confounding. As a result we may have correlations in our posteriors over beliefs regarding confounding and beliefs about effects. 


***

**Model definition in code**

We can now define the model compactly using `gbiqq` as follows.

```{r, message = FALSE}
model <- make_model("I -> M -> D <- P; I -> D") %>%
  
         set_restrictions(causal_type_restrict =  
           "(M[I=1] < M[I=0]) |
            (D[I=1, M=., P=.] > D[I=0, M=., P=.]) |
            (D[M=1, I=., P=.] < D[M=0, I=., P=.]) | 
            (D[P=1, M=., I=.] < D[P=0, M=., I=.])") %>%

          set_confound(list(I = "(M[I=1] == 1) & (M[I=0] == 1)")) %>%
  
          set_priors()
  
```

***

This model, with confounding, is represented graphically as in Figure \@ref(fig:pimdgraph)


```{r pimdgraph, echo = FALSE}
plot_dag(model)
```



## Data

We use the same  data as before, based on the analysis in @haggard2012inequality, but now rather than implementing analysis case by case, the joint distribution of the data will become important for training the model. Here is a snapshot of the data:


```{r, eval = FALSE, echo = FALSE}
data <- gbiqq::democracy_data 
```


```{r, echo = FALSE, warning = FALSE}
data <- gbiqq::democracy_data 
kable(head(data))

```

Note that although data is gathered on $D$ and $I$ in all cases, data on mobilization, $M$, and external pressure, $P$, was only gathered for some (in fact, for those cases in which there was democratization). $M$ in particular derives from qualitative research that assessed whether mobilization took place in those cases that democratized. 

The raw correlations berween variables is as shown in Table \@ref(pimdcorr). Note that some of these correlations are missing because data was only gathered on some variables conditional on the values of others. For those quantities where do see correlations they are not especially strong. There is a weak relation between inequality and democratization -- though this is consistent with inequality having different types of effect. the strongest correlation here is between $P$ and $M$, which are assumed to be uncorrelated in the model, though this correlation is also quite weak.   

```{r pimdcorr, echo = FALSE}
kable(cor(data[,-1], use = "pairwise.complete.obs"), digits = 3)
```



## Inference

With data and model in hand we can update our model to get posteriors on the distribution of all admissible causal types. In practice this is done by constructing a `stan` model that maps from a set of parameters to a distribution on causal types which in turn provide a likelihood function for observable data. (Using the `gbiqq` package the posterior is calculated by `gbiqq(model, data)`)

The parameters 

```{r}
param_mat <- get_parameter_matrix(model)
dim(param_mat)
```


```{r, include = FALSE}

if(do_diagnosis){
  updated <- gbiqq(model, data)
  write_rds(updated, "saved/PIMD_updated.rds")}

updated <- read_rds("saved/PIMD_updated.rds")

```



### Did inequality *cause* democracy?


```{r, include = FALSE}
prob_b <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=1] > D[I=0]")

PC <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=1] > D[I=0]",
                   subset = "D==1 & I==1")


PC2 <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=1] > D[I=0]",
                   subset = "D==1 & I==1 & M==1")

PC3 <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=1] > D[I=0]",
                   subset = "D==1 & I==1 & M==0")

```


```{r mixedhet, echo =FALSE}
par(mfrow = c(2,2))
hist(prob_b, main = "I=1 causes D=1", xlim = c(0,1))
hist(PC,     main = "D=1 caused by I=1", xlim = c(0,1))
hist(PC2,    main = "D=1 caused by I=1 | M = 1", xlim = c(0,1))
hist(PC3,    main = "D=1 caused by I=1 | M = 0", xlim = c(0,1))

```


### Did inequality *prevent* democracy?

```{r, include = FALSE}
prob_b <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]")

PC <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   subset = "D==0 & I==1")

PC2 <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   subset = "D==0 & I==1 & M==1")

PC3 <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   subset = "D==0 & I==1 & M==0")
```

```{r, mixedhist2, echo = FALSE}
par(mfrow = c(2,2))
hist(prob_b, main = "I=1 causes D=0", xlim = c(0,1))
hist(PC,     main = "D=0 caused by I=1", xlim = c(0,1))
hist(PC2,    main = "D=0 caused by I=1 | M = 1", xlim = c(0,1))
hist(PC3,    main = "D=0 caused by I=1 | M = 0", xlim = c(0,1))

```

We see that inequality appears more likely to prevent democratization than to cause it. We are most confident that inequality played a preventative role in those cases in which there was mobilization but still no democratization. We are most confident that inequality had a positive effect in those cases in which we observe mobilization---but even then the probability that inequality made the difference is small. 

## Prior / posterior comparison for multiple estimands

Estimands can be calculated for both the prior and posterior distributions.

```{r, include = FALSE}
updated <- set_prior_distribution(updated)

subsets <- list(TRUE, "D==1 & I==1", "D==1 & I==1 & M ==1", "D==1 & I==1 & M==0")

result_priors <- gbiqq::query_model(
                   model = updated, 
                   using = "priors",
                   queries =  list("D[I=1] > D[I=0]"),
                   subset = subsets)

result_posteriors <- gbiqq::query_model(
                   model = updated, 
                   using = "posteriors",
                   queries =  list("D[I=1] > D[I=0]"),
                   subsets = subsets)

subsets <- list(TRUE, "D==0 & I==1", "D==0 & I==1 & M ==1", "D==0 & I==1 & M==0")

prevent_priors <- gbiqq::query_model(
                   model = updated, 
                   using = "priors",
                   queries =  list("D[I=0] > D[I=1]"),
                   subsets = subsets)

prevent_posteriors <- gbiqq::query_model(
                   model = updated, 
                   using = "posteriors",
                   queries =  list("D[I=0] > D[I=1]"),
                   subsets = subsets)
```

Inequality causes democratization:

```{r, echo = FALSE}
kable(result_priors, caption = "Prior", digits = 3)
kable(result_posteriors, caption = "Posterior", digits = 3)
```


Inequality prevents democratization:

```{r, echo  = FALSE}
kable(prevent_priors, caption = "Prior", digits = 3)
kable(prevent_posteriors, caption = "Posterior", digits = 3)
```


## Discussion


<!--chapter:end:09-mixed-application.Rmd-->

# (PART) Design Choices {-}

# Elements of Design


```{r, include = FALSE}
source("_packages_used.R")
```


***

A fully specified causal model includes the information needed to assess the properties of a research design that seeks to learn from or learn about the model. We talk through how to go from defining causal models to "declaring" research designs and use this framework in later chapters to inform decisions about details of design choices.  

***

So far we have described a way to think about causal models, a way to specify causal estimands, and a Bayesian approach to inference, given models and estimands. Together with a strategy for data gathering these elements are enough to fully characterize a research design. If in addition we provide criterea for evaluating a design we have enough to be able to simulate the behavior of a research design and assess whether a design is up to the task fo answering the questions we want to answer. 

Once we have a  method to assess the performance of a given design we can can start asking what kind of design is optimal, given some beliefs about the world (see @blair2016declaring for more on this general approach to design declaration and diagnosis). In the next chapters we use this approach to assess a set of design choices including choices regarding the clues about which data is sought, the types of cases for which data is sought, and the number of cases for which different types of data is sought.  

In the remainder of this chapter we discuss a simple evaluative criterion for a design and give examples for design declaration for a simple single case process tracing design and a mixed methods design.



## Declaring  a process tracing design

### Steps

We use the MIDA approach (model, inquiry, data strategy, answer strategy) approach to declare a simple process tracing design with an arbitrary model.


*  **Model.** We will define a model as introduced in Chapter 2. For this we need to define:

    * A directed acyclic graph. In doing so we declare the set of variables we are interested and the relations of independence between them. In defining the variables we generally also define the ranges of the  variables---indicating, for example whether  they are binary, categorical, or continuous. In defining the edges we identify the set of parents of any node. 
    * Any restrictions on functional forms relating parents to children. In the binary set up, imposing  functional form assumptions is the same as restructuring causal types. 
    * A declaration of structures of unobserved confounding.  
    * Priors. Beliefs about the distribution of shocks. When defined as part of the model we think of these priors as being the priors from the vantage point of someone assessing a design and they need not be the same as the priors used in the analysis. 

* **Inquiry** As discussed in Chapter 4, an inquiry is a question asked of a model. This is typically a question about the distribution of a variable in some controlled or natural condition, or some summary of such distributions. We refer to the quantity being targeted by a query as the estimand.

* **Data strategy.** The data strategy describes how data will be gathered. In typical  `DeclareDesign` applications this includes both randomization and data gathering (sampling) strategies. We focus on data gathering though we highlight that randomization strategies can be implemented via a modification of the confounds allowed by the model.   A sampling strategy might indicate a sequence of conditional data gathering schemes, for instance: gather data on $X$ and $Y$ for 100 cases, then gather data on $M$ for all cases in which $X=Y$.
Note that in some cases we might want to think of the estimand as being defined *after* the data strategy. This would be the case for instance if we chose a case and we seek to work out some feature *about that case* rather than about the population.

* **Answer strategy**. The answer strategy combines the observed data with a causal model to generate an updated model from which inferences can be drawn. Importantly, the model used in the answer strategy does not need to be the same model as assumed at the model step since we could imagine analysts coming to the data with quite different models in mind. Of course any model used in the answer strategy should generally involve the same variables as in the model itself.

A design is a concatenation of these four steps. 

The concatenated lets us examine instances of the application of a design. A single instance would involve 

1. a single draw of a true parameter vector from the distribution given in the model definition
2. a calculation of the value of an estimand given this true parameter draw
3. the generation of a dataset given the model implied by step 1 and the data strategy
4. the generation of an answer to the inquiry generated from the realized data from 3. and the answer strategy

With the observation of multiple instances we get to assess the distribution of our answers --- and our uncertainty around these -- over repeated draws, and each time we get to see how well the answer we get maps onto the assumed truth in that draw.


### Illustration in code

In the `gbiqq` package there is a single function that lets you declare a full design in one go by letting you supply arguments to declare a model, an inquiry, a data strategy, and an answer strategy

```{r, include = FALSE}
library(DeclareDesign)
library(gbiqq)
library(dplyr)
```

```{r ch10designer, message = FALSE, warning = FALSE}
# A single function can be used to declare a model, an inquiry, a data strategy, and an answer strategy

my_design <- gbiqq_designer(
  
   model           = make_model("X -> M -> Y"),
   inquiry         = list(ATE = "Y[X=1] - Y[X=0]"),
   data_strat      = list(n_obs = 5, 
                          vars = list(c("X", "Y"), "M"), 
                          probs = list(1, .5), 
                          subsets = list(NULL, "X==1 & Y==0"),
                          n = NULL),
   answer_strat  = NULL
)

```


With this model in hand you can use it to draw likely data, run analyses, an drun diagnostics. 

```{r, eval = FALSE}
sample_data      <- draw_data(my_design)
sample_estimands <- draw_estimands(my_design)
sample_estimates <- get_estimates(my_design, df)
```



### Diagnosands: Evaluating a model


The  observation that theories vary in their precision points to a method for describing the learning that is attributable to a lower-level theory relative to a higher level theory. When a lower-level theory represents a disaggregation, the lower-level theory identifies a set of potentially observable variables that are not listed by the the higher-level theory. This allows one to assess the gains in precision (for some collection of unobserved variables) that can arise from  learning the values of additional observables in the lower-level theory. 

Suppose that the contribution of a lower-level theory is to allow for inferences from new data $K$ about some set of query variables $Q$, after we have already observed variables $W$ from the higher-level model.  

Then we can use the expected squared error from the mean posterior estimate as a measure of precision for collection  $Q$, as a measure of loss:

$$E_{k, q} \left(\left( \int q' P(q' | k, w)dq' - q\right)^2\right)$$
where the expectation is taken over the joint distribution of $K$ and $Q$, given $W$. This is an expected loss---or the *Bayes risk*. The inner term $P(q'|k, w)$ is the posterior distribution on $q'$ given observation of $k$ and $w$. 

Another way to think of the gains is as the expected reduction in the variance of the Bayesian posterior: how certain do you expect you will be after you make use of this new information?

In fact these two quantities are equivalent  (see for example @scharf1991statistical). Moreover, it is easy to see that whenever inferences are sensitive to $K$, the expected variance of the posterior will be lower than  the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of $K$, given what is already known from observation of $W$.^[A similar expression can be given for the expected posterior variance from learning $K$ in addition to $W$ when $W$ is not yet known. See, for example, Proposition 3 in @geweke2014analysis.]  
$$Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))$$

The contribution of a theory can then be defined as the mean reduction in Bayes risk:

$$\text{Gains from theory} = 1- \frac{E_{K|W}(Var(Q|K,W))}{Var(Q|W)}$$

This is a kind of $R^2$ measure (see also @gelman2006bayesian). 

Other loss functions could be used, including functions that take account of the costs of collecting additional data,^[Further, one might call into question the value of a theory if the gains in precision depend upon data that are practically impossible to gather.] or to the risks associated with false diagnoses.^[For instance, in @heckerman1991toward, an objective function is generated using  expected utility gains from diagnoses generated based on new information over diagnoses based on what is believed already. In their treatment [@heckerman1991toward, Equation 6],  the expected value of new information $K$, given existing information $W$ is: $\sum{K}P(K|W)( EU(d(Q,W,K)|W, K) - EU(d(Q, W)|W, K))$ where $EU$ is expected utility and $d$ is the optimal inference (diagnosis) given available data. Note that the diagnosis can take account of $K$ when it is observed, but the expected utility depends on $K$ whether or not it is observed, as $K$ carries information about the state of interest.] 

For illustration say that it is known that $X=1, Y=1$ and that, given this information (playing the role of $W$), the posterior probability that a unit is of type $b$ (and not type $d$) is $p$. Say then that a theory specifies that $K$ will take a value 1  with probability $\phi_j$ if the unit is of type $j$. Then what is the value added of this theory? Define $Q$ here as the query regarding whether the unit is a $b$ type. Then the prior variance, $Var(Q|W)$, is simply $p(1-p)^2 +(1-p)p^2 = p(1-p)$. 

<!-- Would be best to  write down the theory as a structural equation that has phi_j as p(K=1|j) -->

To calculate $E_{K|W}(Var(Q|K,W))$, note that the posterior if $K$ is observed is $\frac{\phi_bp}{\phi_bp+\phi_d(1-p)}$. Let us call this $\hat{q}_K$, and the belief when $K$ is not observed $\hat{q}_{\overline{K}}$.
In that case the  *expected error* is: 

$$\text{Expected Error} = p\phi_b\left(1-\hat{q}_K\right)^2+(1-p)\phi_d\hat{q}_K^2+p(1-\phi_b)\left(1-\hat{q}_{\overline{K}}\right)^2+(1-p)(1-\phi_d)\hat{q}_{\overline{K}}^2$$

where the four terms are the errors when $K$ is seen for a $b$ type, when $K$ is seen for a $d$ type, when $K$ is not seen for a $b$ type, and when $K$ is not see for a $d$ type.


Defining $\rho_K = (p\phi_b+(1-p)\phi_d)$ as the probability of observing $K$ given the prior, we can write the posterior variance as:

$$\text{Expected Posterior Variance} = \rho_K\hat{q}_K(1-\hat{q}_K)+(1-\rho_K)\hat{q}_{\overline{K}}(1-\hat{q}_{\overline{K}})$$


<!-- Making use of the fact that $\rho_K\hat{q}_K = ({\phi_bp+\phi_d(1-p)})\frac{\phi_bp}{\phi_bp+\phi_d(1-p)} = \phi_bp$ and similarly  -->
<!-- $(1-\rho_K)\hat{q}_{\overline{K}} = (1-\phi_b)p$, this can be written in terms of primitives as: -->

With a little manipulation, both of these expressions simplify to:

$$\text{Expected Posterior Variance} =p(1-p)\left(\frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} + \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\right)$$


The gains are then:

$$\text{Gains} =1- \frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} - \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}$$

Let's consider the same question using a particular model and calculate these quantities given this model.

```{r}
model <- make_model("X -> Y; K -> Y") %>%
         set_parameters(c(
           .5, .5, 
           .5, .5, 
           0,.5,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,.5))
```

The probability that $K=1$ given $X=Y=1$ is:

```{r}
prob_K <- query_distribution(model, 
                   query = "K[Y=1]==1",
                   using = "parameters", 
                   subset = "X==1 & Y==1")
```

The inferences conditional on the observation of $K$ are:

```{r}
inferences <- gbiqq::query_model(
                   model, 
                   queries = "Y[X=1]>Y[X=0]",
                   using = "parameters", 
                   subsets = list("X==1 & Y==1", "X==1 & Y==1 & K==1", "X==1 & Y==1 & K==0"))


```

```{r, include = FALSE}
z <- as.numeric(inferences$mean)
```

The expected error is then reduced from `r round(z[1]*(1-z[1]), 2)` to 
`r round(prob_K*z[2]*(1-z[2]) + (1-prob_K)*z[3]*(1-z[3]), 2)`

### Other measures of a gain from a theory

Other natural measures of gains from theory might include the simple correlation between $K$ and $Q$, or entropy-based measures (see @zhang2003properties for many more possibilities). 

For this problem the correlation is given by (see appendix):

$$\rho_{KQ} = \frac{(\phi_b+\phi_d)(1-2p)(p(1-p))^{.5}}{
(p\phi_b+(1-p)\phi_d)(1-(p\phi_b+(1-p)\phi_d)))^{.5}}$$

One might also use a measure of "mutual information" from information theory:

$$I(Q,K) = \sum_q \sum_k P(q,k)\log\left(\frac{P(q,k)}{P(q)P(k)}\right)$$

<!-- here: -->


<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- I(Q,K) ={} & p\phi_b\log\left(\frac{\phi_b}{p\phi_b+(1-p)\phi_d}\right)+ (1-p)\phi_d\log\left(\frac{\phi_d}{p\phi_b+(1-p)\phi_d}\right) \\ -->
<!--       & +p(1-\phi_b)\log\left(\frac{1-\phi_b}{1-p\phi_b-(1-p)\phi_d}\right)+ -->
<!-- (1-p)(1-\phi_d)\log\left(\frac{1-\phi_d}{1-p\phi_b-(1-p)\phi_d}\right) -->
<!-- \end{aligned} -->
<!-- \end{equation} -->

To express this mutual information as a share of variation explained, we could divide $I(Q,K)$ by the entropy of $Q$, $H(Q)$ where $H(Q) = -\sum_qP(q)\log(P(q))$. The resulting ratio can  be interpreted as 1 minus the ratio of the entropy of $Q$ conditional (on $K$) to the unconditional entropy of $Q$.

For this example, Figure \ref{fig:probative_value} shows gains as a function of $\phi_b$ given a fixed value of $\phi_d$. The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely. 

```{r, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:probative_value} The solid line shows gains in precision (reduced posterior variance) for different values of $\\phi_b$ given $\\phi_d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\\phi_b - \\phi_d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$."}



gains = function(p, phi_b, phi_d){
  1- (phi_b*phi_d)/(phi_b*p +phi_d*(1-p)) - (1-phi_b)*(1-phi_d)/((1-phi_b)*p+(1-phi_d)*(1-p))
}

corr_qk <- function(p, phi_b, phi_d){
  ((phi_b-phi_d)*(p*(1-p))^{.5})/
  (((p*phi_b+(1-p)*phi_d)*(1-(p*phi_b+(1-p)*phi_d)))^{.5})
  }
# Mutual Information
mi_qk <- function(p, phi_b, phi_d, base = 2){
 p*phi_b*        log({phi_b}   / {p*phi_b+(1-p)*phi_d}, base = base)+
(1-p)*phi_d*     log({phi_d}   / {p*phi_b+(1-p)*phi_d}, base = base)+
p*(1-phi_b)*     log({1-phi_b} / {1-p*phi_b-(1-p)*phi_d}, base = base)+
(1-p)*(1-phi_d)* log({1-phi_d}/ {1-p*phi_b-(1-p)*phi_d}, base = base)
  }

norm_mi_qk <-  function(p, phi_b, phi_d, base = 2){
    -mi_qk(p, phi_b, phi_d, base = base)/(p*log(p, base = base)+(1-p)*log(1-p, base = base))}

phi_b = seq(0,1,.01)

plot(phi_b, gains(.75, phi_b, .25), type = "l", xlab = expression(paste(phi[b])), ylab = "Probative Value")
  points(phi_b, abs(corr_qk(.75, phi_b, .25)), type = "l", lty=2)
  points(phi_b, (norm_mi_qk(.75, phi_b, .25)), type = "l", lty = 3)
  points(phi_b, abs(phi_b - .25), type = "l", lty = 4)
  title("Reduced posterior variance, correlation, mutual information")
text(.8, c(.15, .25, .3, .62, .41), c("I(K,Q)/H(Q)","(Reduced posterior variance)", "Gains",  expression(paste(abs(phi[b]-phi[d]))), "Cor(K,Q)"))

#plot(abs(corr_qk(.75, phi_b, .25)), gains(.75, phi_b, .25), type = "l", xlab = "Probative value")
#lines(abs(phi_b - .25), gains(.75, phi_b, .25), type = "l", lty=2)
#lines(norm_mi_qk(.75, phi_b, .25), gains(.75, phi_b, .25), type = "l", lty=2)


```



## Declaring a mixed methods design

### Model
### Data strategies
### Estimands
### Answer Strategies

```{r}
# detach(package:DeclareDesign, unload=TRUE)
```

<!--chapter:end:10-elements-of-design.Rmd-->

# Clue Selection as a Decision Problem



```{r, include = FALSE}
source("_packages_used.R")
```

***

We draw out the implications of the causal model approach for clue selection strategies. We introduce a tool for generating an optimal decision tree for clue selection given.

***

Consider now the problem of determining what qualitative data to gather on a case. Evidently it makes sense to gather information on clues that have large probative value, but whether or not clues have probative value can depend on what clues have already been collected: Finding out that the Butler had no motive may be informative for the claim that he is innocent, but it may not be useful if you already know he had no opportunity. 

In our running example, we can see many situations where researchers have a choice of observations that could be informative, and situations in which the informativeness of an observation can depend on what is already known. In Figure \ref{fig:running}, we showed  how one can use the structural equations to provide a set of conditional causal graphs that let one see easily what caused what at different values of the root nodes $S$ and $X$. Each of these plots graphs a particular context. We can thus readily see which collection of root nodes constitutes a given query, or estimand.  Turning things around, we can see, given a query, which nodes are informative of the probability that the query is true.^[With larger graphs, continuous variables, and more stochastic components, it may not be feasible to graph every possible context; but the strategy for inference remains the same.]  

For example, suppose one can see that $X=0$ and  $Y=0$ but does not know the causal effect of $X$ on $Y$  This is equivalent to saying that we know that we are in either panel $A$ or $B$ but we do not know which one. Defining the query in terms of root nodes, the question becomes  $S \stackrel{?}{=} 1$, or $P(S=1|X=0,Y=0)$; the difference between the contexts in the two panels is that $S=0$ when, and only when, $X=0$ causes $Y=0$ . Given the structural equation for $S$, $P(S|X=0,Y=0) = P(S|X=0)$, and given independence of $X$ and $S$, $P(S=1|X=0)= \pi^S$.  Figuring out $S$ fully answers the query: that is, given what we know already, $S$ is doubly decisive for the proposition.^[Graphically what is important is that $S$ is informative not because it is $d-$connected with $Y$, but because it is $d-$connected to the query variable---here, simply, to itself.]

We can also see instances in this example of  how existing data can make clues uninformative. Say one wanted to know if $X$ causes $C$ in a case. As we can see from inspection of the panels, this query is equivalent to asking whether $S=1$ (as $X$ causes $C$ only in those two panels ($B$ and $D$) where $S=1$. Data on $R$ is unconditionally informative about this query as $R$ is not $d-$separated from $S$. For example, $R=1$ implies $S=0$. However, if $C$ and $X$ are already known, then $R$ is no longer informative because $C$ and $X$ together *d*-separate $R$ from $S$.^[We can come to the same conclusion by reasoning with the graphs: if $X=0$ and $C=1$, we know we are in subfigure $A$ or $B$, and $X$ causes $C$ only in panel $B$. However, $R$ is of no help to us in distinguishing between the two contexts as it takes the same value in both graphs.]

The running example also lets us demonstrate how informative clues can be found in many different places in a graph. 

1. **Informative spouses** Spouses---parents of the same child---can inform on one another. As we have seen in other examples, when an outcome has multiple causes, knowing the value of one of those causes helps assess the effect(s) of the other(s). For example, here, $S$ and $X$ are both parents of $C$; $S$ is thus informative for assessing whether $X$ causes $C$. Indeed this query, written in terms of roots, is simply $P(S)$:  $X$ causes $C$ if and only if $S=1$. Likewise, $S$ causes $C$ (negatively) if and only if $X=1$. 

2. **Pre-treatment clues.** Did the absence of media reports on corruption ($R=0$) cause government survival ($Y=0$)? Look to the pre-treatment clue, $X$: $X=0$ is a smoking gun establishing that the absence of a report produced government survival. Or, substantively, if there were a free press, then a missing report would never be a cause of survival since it would occur only in the absence of corruption, which would itself be sufficient for survival. More broadly, this example illustrates how knowledge of selection into treatment can be informative about treatment effects. 

3. **Post-outcome clues.** Suppose we observe the presence of a free press ($X=1$) and want to know if it caused a lack of corruption ($C=0$), but cannot observe the level of corruption directly. Observing $Y$---which occurs after the outcome---is informative here: if $X=1$, then $X$ causes $C$ (negatively) if and only if $Y=0$. When an outcome is not observed, a consequence of that outcome can be informative about its value and, thus, about the effect of an observed suspected cause. 

4. **Mediators as clues**: We see a politically sensitive government ($S=1$) and its survival ($S=0$). Did the government survive because of its sensitivity to public opinion? Here, the mediation clue $C$ is helpful: a lack of corruption, $C=0$, is evidence of $S$'s negative effect on $Y$. -->

And of course, different clues can be  informative in different ways for different types of estimand.

Needed then is a systematic way for identifying what clues to look for, and perhaps, in what order to look for them.

## A strategic approach

The representation of inference problems as one of querying a Bayesian model points to a relatively simple method for answering this question, at least for small problems. Consider first a situation where one has access to data $W$ and wants to know the expected probative value of all possible collections of data one could gather. 

This can be done as follows:

1. First define a model, including a signature $S = (\mathcal{U}, \mathcal{V}, \mathcal{R})$, structural equations $\mathcal{F}$, and beliefs on $\mathcal{U}$, $P()$. 
2. Second, define a query on the model, as a statement about values of $\mathcal{V}$ given different $\mathbb{do}$ operations. 
3. Third, use $P$ to draw a vector of $U$ values and assess whether the query is true or not given $U$ and whether $W$ obtains. Then, over many repeated draws from $P$ calculate the *share* of times that the query  is true  among those cases in which  $W$ is  true. This gives posterior probability on $Q$, $P(Q|W)$.
4. Fourth, given posterior $P(Q|W)$ calculate the probability of observing any realization of values $K'$ given the set of clues sought. For each possible realization calculate posterior variance using $P(Q|W, K')$, itself calculated as the share of draws in which the query is true given both $W$ and the particular set of findings $K'$ obtains.  Calculate the *expected* posterior variance by taking an average of these variances with weights given by the probability of observing the clue pattern in question.
5. Repeat step 4 for all possible collections of clues that one could search for. 

This procedure then returns expected posterior variance associated with a planned search for a collection of clues. A more sophisticated strategy would determine which clues to search for later given findings from clues that are sought first. This reflects the possibility that a given clues $K_2$ may be informative if another clue $K_1$ turns up positive but not if it comes out negative. 

We provide some tools for both of these  approaches and illustrate them below.

## Clue selection for the running example

Lets return to the running example and assess the informativeness of different clue strategies. 

Recall that a model consists of an ordered set of variables $V$, a set of exogenous variables $U$, with a distribution over these, given by $P(u)$ and a set of functions, one for each $V\in\mathcal{V}$, $f_v(v',u_v)$ which takes as arguments a subset of variables in $\mathcal{V}$ that must be prior to $V$ in the ordering plus an element of $U$ associated with $V$.  


TRANSITIONAL TEXT LINKING FROM LAST CHAPTER


In the same way we  can figure out outcomes for all possible profiles of data one might have on $m$ binary variables. With five variables there are 243 ($3^5$) combinations of 0s, 1s and unknowns.  We provide a function which allows specific examines or else examinations of the form "all strategies that seek up to $m_k$ clues when up to $m_w$ variables are already observed."


```{r ch11strategies_chunk_slowxx, warning = FALSE}
model <- 
  
  make_model("S -> C -> Y <- R <- X; X -> C -> R") %>%
  
  set_restrictions(node_restrict = list(C = "C1110", R = "R0001", Y = "Y0001"), action = "keep")

plot_dag(model)
```

```{r, echo = FALSE, warning = FALSE, include = FALSE}
my_strategies <-  conditional_inferences(model, 
                                         query = list(COE = "(Y[S=0] > Y[S=1])"), 
                                         given = "Y==1 & S==0")

```

This produces a matrix shown here as table \@ref(tab:showstrats5xx) for a situation in which $S=0$ and $Y=0$ is already observed. 


```{r showstrats5xx, echo = FALSE, warning = FALSE}
# colnames(my_strategies)[1:2] <- c("Prior var", "Var given W")

kable(my_strategies[my_strategies$prob >0, ], caption="A fragment of the table of expected posterior variance for all two clue strategies given observations on two nodes. Firs column gives prior variance, second gives variance condition on data pattern $W$, as indicated by row labels; subsequent columns give expected variance when different clues are sought. ")
```

From there it is easy to assess the *expected* gains from seeking any kind of clue. See Table \@ref(tab:scxrylearning)


```{r ch11ExpectedLearning, warning = FALSE, include = FALSE}

expected_learning(model, query = list(COE = "(Y[X=1] > Y[X=0])"), strategy = "R", given = "X==0 & Y==0", parameters = NULL)

learning <- sapply(list("S", "C", "R", c("C", "R"), c("C", "S"), c("S", "R"), c("C", "S", "R")),  function(strategy) {
expected_learning(model, query = list(COE = "(Y[X=1] > Y[X=0])"), strategy = strategy, given = "X==0 & Y==0", parameters = NULL)})
```


```{r scxrylearning, echo = FALSE}
kable(t(learning), col.names = c("Strategy", "Given", "Prior belief", "Prior Uncertainty", "Posterior Uncertainty"))
```


In this case if we know $X=0$ and $Y=0$ and we are interested in finding out whether$X=0$ *because* $Y$ is 0 we should look for evidence on $S$. Given this simple model, knowledge of $S$ is enough to answer teh question at hand and no othe information is useful at all.


### Dynamic Strategies

The clue collection strategies described above assume that researchers identify the full set of clues to be gathered in advance and do not alter their in Given $n$ nodes, a data collection strategy will be of the form:
$$\sigma = \{K_1, (K_2|K_1 = 1), (K_2|K_1 = 0), (K_3|K_1=1, K_2 =0)\dots\}$$

where each $K_j$ is en element of the nodes on the graph, or is the empty set. Each of these strategies has an associated expected reduction in variance as well as an associated expected cost. Such a strategy vector specifies the first clue, and then subsequent clues condition on what was found from previous searches. We restrict strategies to those in which each clue is sought at most once (though possibly sought at times that depends on findings), and in which if a clue is sought it is sought immediately. For a risk neutral decision maker, this may be sufficient to choose among them. 

In the running example with five binary nodes the strategies needs to specify up to $2^4$ decision points, reflecting the initial choice and the decisions made after learning about four nodes. An example of a strategy, summarizing contingent plans is the below:




```{r, include = FALSE, eval = FALSE}
random_list_strategy <- function(n){
  out <- list()
  for(j in 1:n){
  out[[j]] <- sample(1:n, 2^{j-1}, replace = TRUE)
  }
  out
  }
random_list_strategy(4)


# Records which cases are sought given a list of choice instructions and a realization, r
sought <- function(strategy, r){
  r <- unlist(r)
  n <- length(strategy)
  z <- rep(NA, n)
  z[1] <- strategy[[1]]
  for(j in 2:n){
    z[j] <- strategy[[j]][1 + sum( sapply(1:(j-1), function(i) r[z[i]]*2^(j-i-1)))]
    }
  z}

k = 2
strategy <- random_list_strategy(k)
r = sample(0:1, k, replace = TRUE)
print("Strategy"); strategy
print("World"); r
print("Sought"); sought(strategy,r)
```



```{r, include = FALSE, eval = FALSE}
strategy_cost_benefit <- function(
  model, 
  ops, 
  query, 
  sims=10, 
  U=NULL, 
  n    =  nrow(U),
  cost = rep(1,n),
  strategy = NULL,  # Or a matrix with one column per node indicating whther to be sought or not
  asraw = FALSE # PRovide raw matrix of clues sought and variance; otherwise only summary provided
  ){

  if(is.null(U)) U <- (replicate(sims, model$P()))
  sims <- ncol(U)
  n    <- nrow(U)
  if(n != length(strategy)) stop ("strategy should have n components")
  result <- biqq::pt_which(
                         model,
                         ops = ops,
                         query = query,
                         U=U)
  V  <- t(as.matrix(result$V))
  Ks0 <- sapply(1:sims, function(j) sought(strategy, V[,j]))
  Kf <- function(v) 1:n %in% v
  Ks <- as.matrix((apply(Ks0, 2, Kf)))   # TRUE if v[i] is sought
  K  <- matrix(NA, n, sims)
  K[Ks] <- V[Ks]
  En <- mean(apply(Ks, 2, sum))  # Expected number of clues sought
  Ec <- mean(t(Ks)%*%matrix(cost, 5, 1))  # Expected cost of clues sought

  # Posterior variance that will be achieved in each state of the world
  pV <- sapply(1:sims, function(j) {
      inK <- sapply(1:sims, function(i) !(0 %in% (1*(V[,i] == K[,j]))))
      var(result$A[inK])})
  # Note need to think about this step: the  posterior is calculated for each set of observations counting as possible all worlds that have these observations no matter how different they look on other fronts. This is because all data used to select the observed data is in the observed data. eg say two worlds  have  V1=1 and differ on everything else. Say that W contains V1=1 and no other data. Then the posterior relative likelihood of these two worlds is the same as the prior relative likelihoods. Note also that sims have to be large to avoid bias in estimate of variance.   
  
  
out <- c("Expected posterior variance"=mean(pV, na.rm = TRUE), "Expected number of clues sought"=En, "Expected Cost"=Ec)
if(asraw) out <- cbind(t(Ks0), pV)
return(out)  
}

```


```{r Strat_Illxx, eval = FALSE}
prices <- .5+.5*runif(5)

strategy <- random_list_strategy(5)

strategy_cost_benefit(  model,  my_operations, my_query, sims=100,   U=NULL,   cost = prices,   strategy = strategy  # Or a matrix with one column per node indicating whether to be sought or not
  )

```


Note that strategies cannot use information unavailable ex ante. To ensure the right structure we specify the strategy as $n$ vectors, of length $1, 2, 4, \dots$. For example of the form $\{\{1\}, \{2,3\}, \{3, 4, \emptyset, \emptyset\}\}$

This has the interpretation: seek evidence on node 1 first, if one finds $V_1=1$ seek evidence on node 2, otherwise seek evidence on node 3, if $V_1=1$ and $V_2=1$ seek evidence on node 3, but if $V_1=1$ and $V_2=0$ seek $V_4$, if $V_3$ is positive stop seeking.

For each strategy we can then assess the expected variance reduction; in addition, if collecting different clues comes at different costs---but collection depends on past findings---then we can also calculated the expected costs of each strategy.

NEED TO REDO STRATEGIES CALCULATIONS

```{r ch6_slow_chunk2xx, include = FALSE, cache = TRUE, eval = FALSE}
U <- replicate(100, model$P()) # RAMP UP TO 1000 or more
strats <- list()
out <- list()

k <- 5 # RAMP UP TO 100 OR MORE
for(i in 1:k){
  print(i)
  strategy <- random_list_strategy(5)
  strats[[i]] <- strategy
  out[[i]] <- strategy_cost_benefit(  model,  my_operations, my_query, U=U,   cost = prices,
    strategy = strategy  # Or a matrix with one column per node indicating whther to be sought or not
    )
  }

v <- unlist(sapply(1:length(out), function(i) out[[i]][1]))
n <- unlist(sapply(1:length(out), function(i) out[[i]][2]))
c <- unlist(sapply(1:length(out), function(i) out[[i]][3]))
best <- (1:k)[(v+n) == min(v+n)]
lowestvar <- (1:k)[(v) == min(v)]

find_optimals <- function(r=2000){
  best_s <- function(s) (1:k)[(v+s*n) == min(v+s*n)]
  unique(sapply(c(1/(r:1), 1:r), best_s))}
optimals <- find_optimals()

plot(v,n, xlab = "Expected Posterior Variance from Strategy", ylab =  "Expected Number of Clues Sought", col = "orange", pch = 19)
points(v[optimals], n[optimals], pch = 21, cex = 1.2, col = "red")
#print(strats[[best]])
#print(strats[[lowestvar]])

k <- floor((length(optimals)+1)/2)
median_strat = optimals[k]  # Not great for even number
print(strats[[median_strat]])
```

Figure  below plots a collection of strategies based on two criteria---the variance reduction and the expected number of clues sought, which could be an indicator for cost. One can see a frontier of optimal strategies, depending on how these two desiderata trade-off against each other.

```{r, echo = FALSE, eval= FALSE, include = FALSE, fig.cap = "\\label{somesstrats} Non dominated strategies are circled."}
plot(v,n, xlab = "Expected Posterior Variance from Strategy", ylab =  "Expected Number of Clues Sought.", col = "grey", pch = 19)
points(v[optimals], n[optimals], pch = 21, cex = 1.2, col = "black")
```


```{r graph_strategyxxx, include = FALSE, eval = FALSE}

g_strat <- function(strategy){
  n = length(strategy)
  plot(1:n, type = "n", xlim = c(0,n), ylim = c(-2^(n-2)+.5, 2^(n-2))-.5, axes = FALSE, xlab = "", ylab = "", main = "Decision Tree")
  for(j in 1:length(strategy)){
    sj <- strategy[[j]]
    x <- rep(j, length(sj)) -1
    y <- ((0:(length(sj)-1) - (length(sj)-1)/2))
    if(max(y)>0) y <- y*(j/max(y))
    points(x, y)
    text(x+.25, y, sj, col = "red")
  }
    for(j in 1:(length(strategy)-1)){
    sj  <- strategy[[j]]
    sj2  <- strategy[[j+1]]
    x <- rep(j, length(sj)) -1
    y <- ((0:(length(sj)-1) - (length(sj)-1)/2))
    if(max(y)>0)  y <- y*(j/max(y))
    y1 <- rep(y, each = 2)
    y2 <- ((0:(length(sj2)-1) - (length(sj2)-1)/2))
    if(max(y2)>0)y2 <- y2*((j+1)/max(y2))
    segments(x,y,x+.5, y)
    segments(x+.5,y1,rep(x+1, each = 2), y2)
    text(x+.75, .5*(y1+y2)[rep(c(TRUE, FALSE), length(y2)/2)] - .2, 0, cex = .8)
    text(x+.75, .5*(y1+y2)[rep(c(FALSE, TRUE), length(y2)/2)]+.2, 1, cex = .8)
    }
}
```

Below we graph one of these strategies on the frontier (the median strategy on the frontier) as a decision tree. The interpretation is to seek the first clue first; it will be revealed to be present (1) or not(0), a subsequent branch is then chosen depending on what is found and the indicated clue then sought, and so on. (Note currently the strategy sets we examine include ones in which the same clue is sought multiple times)

```{r, echo = FALSE, fig.height = 8, eval = FALSE, fig.width = 6, fig.cap = "\\label{dtree} A clue seeking strategy, indicating what to seek first; then what to seek second conditional on first findings, and so on."}
g_strat(strats[[median_strat]])
```  
  
  
## Clue selection for the Democracy model 

With a model in hand we are also in a position to assess what we *could* learn from different data stratgies and what we would infer upon discovery of different data. 


```{r, echo = FALSE}

model <- make_model("I -> M -> D <- P; I -> D") %>%
         set_restrictions(causal_type_restrict =  
           "(M[I=1] < M[I=0]) |
            (D[I=1, M=., P=.] > D[I=0, M=., P=.]) |
            (D[M=1, I=., P=.] < D[M=0, I=., P=.]) | 
            (D[P=1, M=., I=.] < D[P=0, M=., I=.])")

```

```{r, echo = FALSE}

inferences <-
  conditional_inferences(model, query = "D[I=1] > D[I=0]", given = "I==1 & D==1")

kable(inferences, caption = "\\label{possible_outcomes} Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization.", digits = 3)

```

We show in Table \ref{CaseLearn}  how  uncertainty is likely to be reduced with  different research designs. We show these reductions here for the two kinds of cases in which democratization does occur. The first row displays the variance on our posterior belief about the effect if $I$ on $D$ before we observe anything at all. The second row shows what happens to that uncertainty when we observe just cause and outcome, $I$ and $D$. The next four rows show the results for four possible choices in regard to process tracing: looking for neither $M$ nor $P$ (which is identical to doing no process tracing at all); looking for $P$; looking for $M$; and looking for both. The clearest message here is that, if we had to choose between clues, we should observe $P$: given our model (including our priors on the types), we reduce our uncertainty more by learning about an alternative cause than by learning about a mediator. We also see that the mediator is much more informative when the causal effect we are looking for is one that *could* have operated via the mediator, as compared to when the mediator is informative only as a moderator of the cause's direct effects.




```{r, echo = FALSE}

EL1 <-
  rbind(
expected_learning(model, query = "D[I=0] == 0", strategy = NULL,       given = "I==1 & D==1"),
expected_learning(model, query = "D[I=0] == 0", strategy = "P",        given = "I==1 & D==1"),
expected_learning(model, query = "D[I=0] == 0", strategy = "M",        given = "I==1 & D==1"),
expected_learning(model, query = "D[I=0] == 0", strategy = c("P","M"), given = "I==1 & D==1"))

kable(EL1[,-1], caption = "\\label{CaseLearn}Variances and expected variances given different  clue seeking  stratgies for cases in which we have observed inequality and democratization.", digits = 3)

```


```{r, echo = FALSE}

EL2 <-
  rbind(
expected_learning(model, query = "D[I=1] == 0", strategy = NULL,       given = "I==0 & D==1"),
expected_learning(model, query = "D[I=1] == 0", strategy = "P",        given = "I==0 & D==1"),
expected_learning(model, query = "D[I=1] == 0", strategy = "M",        given = "I==0 & D==1"),
expected_learning(model, query = "D[I=1] == 0", strategy = c("P","M"), given = "I==0 & D==1"))

kable(EL2[,-1], caption = "\\label{CaseLearn}Variances and expected variances given different  clue seeking  stratgies for cases in which we have observed low inequality and democratization.", digits = 3)

```


**To come**: applied case-level analyses involving causal pathways, actual causes, and notable causes.



  
<!-- ## More complex problems -->

<!-- Illustration of clue inference for a continuous problem. -->

## Conclusion

Explicit statement of a causal model---including prior beliefs over roots---allows one to assess what will be inferred from all possible observations. This opens the way for simple strategies for assessing what data is most valuable, and in what order it should be gathered. 

We are conscious that here we are pushing the basic logic to the limits. In practice researchers will often find it difficult to describe a model in advance and to place beliefs on nodes. Moreover the collection of new data could easily give rise to possibilities and logics that were not previously contemplated. Nothing here seeks to deny these facts; the claim here is a simpler one: insofar as one can specify a model before engaging in data gathering, the model provides a powerful tool to assess what data is most useful to gather. 




<!--chapter:end:11-clue-selection.Rmd-->

# Going wide and going deep {#wide}

***

Researchers often need to choose between collecting data on more cases or collecting more data within cases. We discuss the tradeoffs and communicate an intuition that clue data, even on a small number of cases, can be informative even when there is $X, Y$ data on a very large number of cases, but only if it provides information that cannot be gathered from $X,Y$ data, such as selection into treatment. Simulations suggest that going deep is especially valuable for observational research, situations with homogeneous treatment effects, and, of course, when there is strong probative value.

***

```{r, include = FALSE}
source("_packages_used.R")
```


How does this approach guide researchers in making choices about research designs?

We address this question with a focus  on characterizing the kind of learning that emerges from different combinations of investment in the collection of correlational as compared with process-tracing data *under different research conditions*. We report the results here of simulation-based experiments designed to tell us under what research conditions different mixes of methods can be expected to yield more accurate inferences. We also discuss, at a high level, the implications of the framework for strategies of qualitative case-selection.  


## Intuitions: Does a sufficiently large $N$ always trump $K$?

We begin by considering the learning that occurs upon observing outcomes from varying numbers of cases given different $XY$ data ranging from small to quite large. 

The goal here is to build up intuitions on how beliefs change given different observations and hw this affects posterior variance. We address the question is a very controlled setting in which 

* a researcher is confronted with balanced $X,Y$ data that exhibits no correlation
* the  researcher can seek a doubly decisive clue on cases in the $X=Y=1$ cell 
* though not known in advance, it turns out that each time the researcher finds evidence suggesting that the case in question is a $b$ type
* the selection probabilities are either unknown or known with near certainty

In this case, we can expect that seeing evidence of $b$ types will shift the researcher to increase her beliefs on the average causal effect. But how strong will these shifts be and how does this depend on the amount of XY data available? Does the signal from the $XY$ data drown out any signal from the  $K$ data?

Intuitions to answer these questions can be gathered from the simulations reported in Figure \ref{morn}. For these simulations we varied the size of the $XY$ data from 5  observations in each cell to 5000. The key features of the simulations are:

1. When assignment propensities are unknown---as for example with observational data---the clue information shifts beliefs independent of how many $XY$ cases there are. The key insight is that the clue information provides inforation on assignment propensities which are informative about the share of each type in each cell and these shares determine treatment effects no matter how large or small the cells are.

2. When assignment propensities are known with large data there is a lot of learning over the distribution of types in a population (at least up to differences in types rather than the distribution of fundamental types). Clue information shifts beliefs about the types of the particular cases for which clue data is gathered but has almost no effect on estimates of the population estimand. 

3. Not visible from the figure however: in the case with large $N$ and known propensities, observation on many $b$ types in the $X=Y=1$ cell, while not changing estimates of *average* treatment effects ($\lambda_b- \lambda_s$) does affect beliefs on *heterogneity*, because the data is more consistent with a world with many $a$s and $b$s than one with many $c$s and $d$s.  For example if there were 10,000 data points in each $X,Y$ and clue information on  20 cases in the $X=Y=1$ cell suggest that these are all $b$ types, then the conclusion would be that 95% of the cases are $a$ and $b$ types, in equal proportion.   


```{r}
model <- make_model("X -> Y <- K") %>%
  set_restrictions(causal_type_restrict = "(Y[X=0, K=1]==1) | (Y[X=0, K=0]==0)") %>%
  set_parameters(c(0.001, .999, .5, .5, .25, .25, .25, .25))
```

```{r, echo = FALSE}
kable(rbind(
  query_model(model, queries = list(ATE = "Y[X=1] - Y[X=0]"), using = "parameters", 
              subsets = list(TRUE, "K==1", "K==0")),
  query_model(model, queries = list(ATE = "Y[X=1] - Y[X=0]"), using = "priors", 
              subsets = list(TRUE, "K==1", "K==0"))))
```


```{r, echo = FALSE}
if(!exists("fit") & do_diagnosis) fit  <- fitted_model()

wide_or_deep <- function(model, n_K, n_fold, query = "Y[X=1] - Y[X=0]") {

  df <- data.frame(X = c(0,0,0,1,1,1), Y = c(0,0,1,0,1,1), K = NA) %>%
        slice(rep(row_number(), n_fold))
  
  given              <- collapse_data(df, model)
  reference_model    <- gbiqq(model, df, stan_model = fit)
  
  possible_data      <- make_possible_data(model = model, given = given, N = n_K)
  estimates_database <- make_estimates_database(model, given = given, possible_data = possible_data, queries = query)
  
  out <- diagnose_strategy(reference_model, model, given, queries = query,
  									estimates_database = estimates_database,
  									possible_data = possible_data)
  out
}  

if(do_diagnosis){
  wd_1_2 <- wide_or_deep(model, 1, 2)
  write_rds( wd_1_2, "saved/wd_1_2.rds")
}

wd_1_2 <- read_rds("saved/wd_1_2.rds")

```

```{r, echo = FALSE}

wd_sim <- function(model, n_K, n_fold) {
  
    df <- data.frame(X = c(0,0,1,1), Y = c(0,0,0,1), K = NA) %>%
      slice(rep(row_number(), n_fold))
    df <- mutate(df, K = c(rep(1, n_K), rep(NA, n()-n_K)))
    
    given <-  collapse_data(df, model)

    updated <- gbiqq::gbiqq(model, data = df, stan_model = fit)
    query_model(updated, 
                  queries = list(ATE = "Y[X=1] - Y[X=0]"), 
                  using = "posteriors")
  
}

if(do_diagnosis){
  if(!exists("fit")) fit <- gbiqq::fitted_model()
  out <- sapply(c(4,8), function(n_K) {sapply(c(2, 10, 20), function(k) wd_sim(model, n_K, k))})
  write_rds(out, "saved/wide_or_deep_XMY.rds")
  }

wd <- read_rds("saved/wide_or_deep_XMY.rds")
wd <- t(wd[c(4,9,14), ])
rownames(wd) <- c("Clues on 4 cases", "Clues on 8 cases")
colnames(wd) <- c("N=8", "N=40", "N=80")
kable((wd))
```

```{r}
model_confound <- make_model("X -> Y <- K") %>%
  set_restrictions(causal_type_restrict = "(Y[X=0, K=1]==1) | (Y[X=0, K=0]==0)") %>%
  set_parameters(c(0.001, .999, .5, .5, .25, .25, .25, .25)) %>%
  set_confound(list(X = "(Y[X=1]==1)"))

draw_parameters(model_confound)


kable(rbind(
  query_model(model_confound, queries = list(ATE = "Y[X=1] - Y[X=0]"), using = "parameters", 
              subsets = list(TRUE, "K==1", "K==0")),
  query_model(model_confound, queries = list(ATE = "Y[X=1] - Y[X=0]"), using = "priors", 
              subsets = list(TRUE, "K==1", "K==0"), n_draws = 2)
  ))
x <- (get_types(model_confound, query = "Y[X=1] - Y[X=0]", join_by = "|")$types)
```


```{r, echo = FALSE}
if(do_diagnosis){
  if(!exists("fit")) fit <- gbiqq::fitted_model()
  out <- sapply(c(4,8), function(n_K) {sapply(c(2, 10, 20), function(k) wd_sim(model_confound, n_K, k))})
  write_rds(out, "saved/wide_or_deep_XMY2.rds")
  }

wd2 <- read_rds("saved/wide_or_deep_XMY2.rds")
wd2 <- t(wd2[c(4,9,14), ])
rownames(wd2) <- c("Clues on 4 cases", "Clues on 8 cases")
colnames(wd2) <- c("N=8", "N=40", "N=80")
kable((wd2))
```



## Evaluating strategies
As a metric of the returns from different research strategies we calculate the *expected* inaccuracy in the estimation of the average treatment effect, as given in equation \ref{Loss}.
\begin{equation}
\mathcal{L}=\mathbb{E}_\theta(\mathbb{E}_{\mathcal{D}|\theta}(\tau(\theta)-\hat{\tau}(\mathcal{D}))^2) 
\label{Loss}
\end{equation}


where $\tau(\theta)$ is the value of $\lambda_b-\lambda_a$ (the average treatment effect) given $\theta$, and $\hat{\tau}(\mathcal{D})$  is the *estimate* of this treatment effect (the mean posterior value) that is generated following some realization of data $\mathcal{D}$. Thus, if some $\theta$ characterized the true state of the world, then $\mathbb{E}_{\mathcal{D}|\theta}(\tau^\theta-\hat{\tau})^2$ is the expected error in estimation of the causal effect given different realizations of the data, $\mathcal{D}$,  that could obtain in this state of the world.  $\mathcal{L}$ is then the expected value of these errors given prior beliefs over possible values of $\theta$.

Note that, while we focus on errors on estimated average causal effects, similar exercises could assess how cross- and within-case observations distinctively contribute to other estimands | including the causal explanations for individual cases and the validity of causal theories | as well as to learning about inferential assumptions themselves (assignment and clue probabilities). 
For all simulations, prior distributions are drawn with parameters as described in the Supplementary Materials (\S \ref{AppSimNotes}, Table \@ref(tab:sims)). Priors on the type distribution are drawn from a Dirichlet distribution; priors for each of the $\pi$ and $\phi$ values are drawn independently from Beta distributions. We note that, while by construction priors on each parameter are independent, this will not generally be the case for posterior distributions. In most cases we simulate the prior distribution using 5200 draws of each parameter. For most experiments we then systematically vary the prior distribution for one parameter of the research situation between two extreme positions. We then calculate the expected posterior from each possible data realization and, in turn, the expected loss in estimates of treatment effects for a range of levels of investment in qualitative and quantitative evidence. 


A few further features of the experiments below are worth noting. First, our illustrations focus on learning about population-level causal effects; however, the model can yield results about the benefits of alternative research designs for estimating a wide range of other quantities of interest, such as case-specific causal explanations or clue probabilities. Second, while we focus on the search for a *single* clue in each case, the analysis can be extended to the case of an arbitrarily large set of clues. Third, in many of these experiments, the probative values are set at doubly decisive levels for all $\phi$ parameters, and thus focus on the very optimistic case of maximally informative process tracing. Fourth, we illustrate tradeoffs at low levels of $n$, but the model can be employed to make choices for arbitrarily large numbers of cases. Finally, we note that some results may be sensitive to the choice of priors. The results below should thus be understood as an illustration of the utility of the BIQQ framework for guiding research choices, rather than as a set of more general prescriptive design rules.



## Varieties of mixing {#varieties}

What are the marginal gains from additional pieces of correlational and process-tracing evidence for the  accuracy of causal estimates? Figure \ref{morn} displays the results,  plotting the errors associated with different mixes of correlational and process data. Each dot represents a single possible research design, with the $x$-axis charting the total the number of cases examined. For all cases, $X$ and $Y$ data are collected. The shading of the dots in each column then represents the proportion of cases for which process-tracing is also carried out. An unshaded dot is a design in which *only* correlational data has been collected for all cases; a black dot is a design in which the process-tracing clue is sought in *all* cases; and shades of grey, as they darken, indicate process tracing for increasing shares of cases. For $n\leq 4$ we report results for all designs; for $n>4$ we report only results when within case information is sought for all, half, or none of the cases. 

We see first from the graph that, as one would expect, moving from lower-$n$ to higher-$n$ designs reduces the expected error of estimates. Further, both adding a correlational case and doing process tracing on an additional case improve accuracy. The figure also suggests that there are diminishing marginal returns to both types of data: in particular the grey point reflecting 50\% process tracing is generally well below the mid point of the white and black dots, and converges toward the black dot (100\% process tracing) as sample size increases. Other, less obvious results also emerge, including:

\begin{itemize}
\item **Qualitative and quantitative data can act as partial substitutes for assessing causal effects**. We see, in the smaller sample sizes, that the marginal gains from adding an extra case are lower when there is more within-case information on existing cases. Similarly, the marginal gains from gathering more within-case information are lower when there are more correlational cases (for example adding one case study when $n=1$ has about the same effect as adding 8 cases studies when $n=16$). 
\item **The *relative* marginal gains from going wider and going deeper vary with the study design**. Suppose that the costs of gathering $X,Y$ data and gathering clue data were the same per case. Suppose further that we have an $n$ of 2 and only correlational data. Then, for the case illustrated in Figure \ref{morn}, if we have additional resources to invest, it is better to gather $K$-type data for 1 of these cases than to add another case with $X,Y$ data only. However, at this point, the tradeoff shifts. If we now have a choice of gathering $K$-type data on the second case (moving to the black dot) or adding a third case (resulting in $n=4,m=1$), the latter strategy now results in an error rate at least as low as the former. %
% Sometimes it is better to go deeper than wider. For example an $n$=3 (or $n$=2) study with process tracing on one case produces greater accuracy than a purely correlational $n=4$ (or $n=3$) study. 
\item **Optimal strategies might involve going deep in a subsample of cases only**. Suppose again that the costs for gathering $X,Y$ data and gathering $K$-type data were the same and, now, that researchers can gather four pieces of data of any type. The results in Figure \ref{morn} suggest that, for the priors chosen here, gathering $X,Y$ data on 3 cases and $K$-type data on one produces more accurate estimates than either going maximally wide (gathering $X,Y$ data on four cases) and at least as accurate an estimate as going maximally deep (gathering $X,Y,K$ data on 2 cases).
 %At the margins and once one has done some process tracing, the results suggest that it is better to invest in additional $X,Y$ data *without* clues than to seek to do process tracing for all cases on which one has $X,Y$ data.

\end{itemize}




\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/m_or_n.pdf}
\caption{{Expected errors in the estimation of average treatment effects for designs in which $X, Y$, data is sought in $n$ studies (horizontal axis) and clue data is sought within $m$ of these. The shading of dots indicates the proportion of cases for which within-case data is sought (white = none; black = all). For small sample sizes ($n \in \{1,2,3,4\}$) we show results for all designs ($m \in \{1,2,\dots, n\})$. For larger sample sizes, we show only designs with clues sought in 0, half, and all cases.}}
\label{morn}
\end{figure}



\subsection{Designs in Context}
More generally, we would expect that the optimal level of mixing depends on the context|on features of the research situation that affect the problem and available tools of inference. In the next subsections, we report results from experiments in which we vary the researcher's priors about (a.) the probative value of clues, (b.)  heterogeneity of treatment effects (c.) uncertainty regarding assignment processes, and (d.) uncertainty regarding the probative value of clues. In all cases we report the expected loss for the design in question, as given in Equation \ref{Loss}. %

\subsubsection{Probative value of clues}
If clues have no probative value| in the sense that priors over $\phi_{jx}$ do not depend on type, $j$, then gathering data on clues does not affect inference. Probative value does not get picked up during analysis, it must be imported. Less clear, however, is the extent to which gains in inference depend on the degree of probative value, defined here as in Section \ref{PTintro} (see footnote \ref{fnPV}). Our simulation evidence from full BIQQ estimation (Figure \ref{experiments}) suggests that in some ranges at least the gains are also convex, that is *increasingly* more is learned as the gaps between pairs such as $\phi_{b1}$ and $\phi_{d1}$ increases. The top left panel of Figure \ref{experiments} shows an example of these convex gains, showing expected losses for settings where there is not probative value, where all tests are doubly decisive, and a case half way between these extremes.        

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/experiments.pdf}
\caption{Figure shows the expected error in mean posterior estimates of average treatment effects for different designs. In these graphs the horizontal axis denotes some feature of the research setting (captured in priors); the white and black circles represent errors from designs in which within case information is sought for no and all cases, respectively; the numbers marked in the circles indicate the number of data points in the study design.}
\label{experiments}
\end{figure}


\subsubsection{Effect Heterogeneity}
We might expect that the optimal research design for estimating average treatment effects would depend on how *heterogeneous* the true causal effects are in the population. If we believe that effects are strongly homogeneous, then confidence that one case is affected by treatment provides a great deal of information about population treatment effects. However, if effects are believed to be highly heterogeneous, then knowing that one case is affected by treatment provides less information regarding effects on different cases. 

Heterogeneity can be conceptualized in different ways. Here we define heterogeneity as increasing in the amount of *variance* in causal effects across cases in the population. In the binary environment, for any $\tau \in [0,1]$, maximum effect heterogeneity is  obtained when $\lambda_a=(1-\tau)/2$ and  $\lambda_b=(1+\tau)/2$: i.e., when all cases have either a positive or negative treatment effect, with no destined or chronic cases. For a positive treatment effect, maximum homogeneity occurs when $a=0, b=\tau$, with the remaining share $1-\tau$ consisting of types $c$ and $d$.^[For negative treatment effects, homogeneity is maximized with $\lambda_b=0$.] There are two very different ways to have an average treatment effect of 0: there may be no treatment effect for any case (maximal homogeneity), or there is a positive effect for half the cases and a negative effect for the other half (maximal heterogeneity).  
%Here there is no variance in causal effects across cases. 

Using this conceptualization of heterogeneity, our simulation results confirm that higher heterogeneity increases the marginal value of going ''wide'' rather than ''deep.'' At low levels of heterogeneity, there are considerable gains to collecting clues on cases at a given sample size; but the gains to process tracing diminish and then disappear as heterogeneity rises (see Supplementary Materials, \S\ref{AppE2}).

%Above, why is homogeneity defined only for a=0? Also, I don't understand why maximal homogeneity isn't a, b, c, or d = 1. How does learning about one case tell you so much more about the population under the definition of homogeneity above?
% that's homogeneity of type, but not of treatment effect; e.g. all c is no more homogeneous than half a and half b. the second het analysis looks at a case like this though


\subsubsection{Uncertainty Regarding Assignment Processes}

Here we examine the implications of uncertainty over treatment assignment (confounding). 

Any differences in assignment probabilities that are *known* are built into our priors in a Bayesian setting and do not produce biases (just as known confounds can be controlled for in a standard regression model). However, *uncertainty* about assignment processes still generates higher variance in posterior estimates \citep[see][]{GerGreKap04}. % where variance of the prior distribution of bias is infinite. In this case, they show that it is optimal to place resources in strategies that have no uncertainty around sources of bias---such as in experimental data collection rather than observational data collection. Similarly, authors in the process tracing tradition have argued that a key feature of process tracing is its capacity to detect reverse causation, omitted variables, and other confounding processes (e.g., \citep{Lieberman2005nested}. 
In the BIQQ framework, however, clues provide discriminatory leverage on case types that is *independent* of assignment probabilities: with strong probative value, $b$ and $d$ type treated units can be told apart thus eliminating the identification problem that certainty over assignment processes helps to solve.  
%How does the level of uncertainty about assignment affect the optimal design mix? 
In our simulations (bottom left panel of Figure \ref{experiments}), we find that greater uncertainty over assignment processes indeed results in greater errors for correlational analysis | most obviously for higher $n$. However, for the parameter space we examine (and given strong assumptions on the probative value of clues), uncertainty about assignment does not reduce accuracy for mixed methods analysis. %Mixed methods analysis, that is, appears more robust to violations of the ignorability assumption. 
(See Supplementary Materials, \S\ref{AppE3}.)

\subsubsection{Uncertainty regarding the probative value of clues}

%The critical assumption for drawing inferences from clues is that researchers know the likelihood of clues being present as a function of type: that is, that we know the likelihood with which a given piece of process-based evidence should be observed if a given causal effect is truly present. This is clearly a strong assumption. %, dependent on identifying both the right theoretical logic for each causal effect and  the ways in which that logic would make itself observable in within-case data. 
%In this sense, process tracing rests on a set of assumptions that may be just as uncertain as the assumption of particular (conditional) assignment probabilities made in observational quantitative research. 
As with assignment probabilities, researchers may be uncertain regarding the probative value of clues for discriminating between types. How much does this uncertainty matter for the relative gains to qualitative evidence?

Surprisingly, our simulations suggest that uncertainty over the probative values of clues is unimportant for expected errors (see Supplementary Materials, \S\ref{AppE4}). Our experiment fixes the expected probative value of a clue and allows for variance around that expected value. Informally, we are thus comparing a situation in which one believes that a clue has moderate probative value to one in which one believes that it may have strong probative value or it may have none at all. 

To be clear, this analysis does *not* imply that there is no penalty to being *wrong* about the probative value of clues. %If a researcher is convinced that a clue has more probative value than it actually has, then the researcher will draw the wrong inferences. 
The result suggests rather, that having more, rather than less, *uncertainty* about that probative value may be relatively inconsequential for the choice of research strategy.



## Notes on Simulations {#AppSimNotes}
Here we provide statistical details and some further interpretation for the paper's simulations assessing the benefits of different designs conditional on different priors regarding the probative value of clues, the heterogeneity of causal effects, uncertainty regarding assignment probabilities, and uncertainty regarding the probative value of clues. Table \@ref(tab:sims) provides details on all parameters used in simulations and Table \ref{simdetails} provides detail on the number of runs, iterations, and related information used in the estimation.

### Probative values {#AppE1}
For these simulations we simultaneously vary the probative value for tests for all $X,Y$ combinations. Specifically, we vary the differences between $\phi_{b0}$ and $\phi_{c0}$ (for $X=Y=0$ cases), between  $\phi_{a0}$ and $\phi_{d0}$ (for $X=0, Y=1$ cases); between   $\phi_{a1}$ and $\phi_{c1}$ (for $X=1, Y=0$ cases); and between   $\phi_{b1}$ and $\phi_{d1}$ (for $X=Y=1$ cases).  For each $X,Y$ combination, we compare the relevant $\phi$ pairs across values of $(.5,.5)$ (no probative value), $(.25,.75)$ (middling probative value) and $(0.01,0.99)$ (strong probative value). Using the definition of probative value (PV) we provide (Section \ref{PTintro}, see footnote \ref{fnPV}), these correspond to cases with probative value of 0, .5, and close to 1 respectively. 


### Effect heterogeneity {#AppE2}
We note that heterogeneity makes going ''wide'' relatively more beneficial for two reasons. First, when all cases are affected either positively or negatively, all of the information needed to identify types is provided by information on $X$ and $Y$. If $X=Y$ then a case was (or could have been) positively affected; if $X \ne Y$ then a case was (or could have been) negatively affected. In this extreme case of maximal heterogeneity, causal process information provides no additional inferential gains. Where there is high homogeneity, on the other hand, the core difficulty is distinguishing $a$ and $b$ types, from $c$ and $d$ types. Then, the information contained in clues may provide greater benefits (see Table \ref{FP}). Second, the more heterogeneous effects are across cases, the less we learn about *population-level* causal effects by getting an individual case right. Thus, again, we would expect greater relative gains to more extensive analysis as heterogeneity increases.

### Uncertainty about assignment processes {#AppE3}
Note that in our binary setup, infinite bias cannot arise, and the harm done by uncertainty over selection processes can be more moderate.  In this set of simulations, the expected value of $\pi_j$ is fixed at $0.5$ and we vary the variance in  $\pi_j$ between 0 and a maximum of 0.289.     


### Uncertainty regarding the probative value of clues {#AppE4}
In this experiment, the expected probability that a clue will be observed is set to 0.75 if one hypothesis is right, and 0.25 if the alternative hypothesis is correct. The simulations vary from a situation in which those probabilities are known with certainty (uncertainty low) to a situation in which the researcher admits the possibility of many possible values of $\phi$ (uncertainty set to its maximum of 0.25). Uncertainty is simultaneously varied for all pairs of $\phi$ values (see \S\ref{AppE1}). The displayed results suggest that uncertainty about the probative value of clues plays little role in the assessment of optimal strategies. Fixing the penumbra of uncertainty around a given expected $\phi$ value allows for the possibility that the clue may have weak probative value, but also that it may have exceptionally strong probative value. The effects of these possibilities appear to wash out when we update beliefs about causal effects upon observation of the clue (or its absence).


### Details on simulation experiments

<!-- \begin{table}[htbp]
  \centering
    \begin{tabular}{ccc|c|ccc|ccc|ccc|ccc}
          &       &       & 1: m or n & \multicolumn{3}{|c|}{2:  Probative Value} & \multicolumn{3}{|c|}{3: Effect Heterogeneity} & \multicolumn{3}{|c|}{4: Assignment Uncertainty} & \multicolumn{3}{c}{5: Clue Uncertainty}  \\
\hline
    $\theta$ & Dist & arg   &       & Low     & $\rightarrow$ & High     & Low      & $\rightarrow$ & High     & Low     & $\rightarrow$ & High     & Low     & $\rightarrow$ & High \\ \hline 
    $\lambda_a$ & Dirichlet& $\alpha_a$   & 1  & 0.20  & $\rightarrow$ & 0.20       & 0.10  & $\rightarrow$ & **2.00**  & 1  & $\rightarrow$ & 1  & 1  & $\rightarrow$ & 1 \\
    $\lambda_b$      &       & $\alpha_b$ & 1  & 0.20  & $\rightarrow$ & 0.20  	& 02.10  & $\rightarrow$ & **4.00**   & 1  & $\rightarrow$ & 1  & 1  & $\rightarrow$ & 1 \\
    $\lambda_c$      &       & $\alpha_c$ & 1  & 0.20  & $\rightarrow$ & 0.20 	& 02.00  & $\rightarrow$ & **0.10**   & 1  & $\rightarrow$ & 1  & 1  & $\rightarrow$ & 1 \\
    $\lambda_d$      &       & $\alpha_d$ & 1  & 0.20  & $\rightarrow$ & 0.20  	& 02.00 	& $\rightarrow$ & **0.10**   & 1  & $\rightarrow$ & 1  & 1  & $\rightarrow$ & 1 \\
    \hline
    $\pi_a$  & Beta  & $\mu$  	& 0.50   & 0.50   & $\rightarrow$ & 0.50  				 & 0.50  & $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50 \\
          &       & $\sigma$    & 0.10 & 0.10 & $\rightarrow$ & 0.10 & 0.10 			& $\rightarrow$ & 0.10   & 0.01 & $\rightarrow$ & **0.289** & 0.10 & $\rightarrow$ & 0.10\\ \hline
    $\pi_b$  & Beta  & $\mu$  	& 0.50   & 0.50   & $\rightarrow$ & 0.50   		& 0.50   & 	$\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50 \\
          &       & $\sigma$    & 0.10 & 0.10 & $\rightarrow$ & 0.10 & 0.10 & 			$\rightarrow$ & 0.10 & 0.01 & $\rightarrow$ & **0.289** & 0.10 & $\rightarrow$ & 0.10 \\ \hline
    $\pi_c$  & Beta  & $\mu$  	& 0.50   & 0.50   & $\rightarrow$ & 0.50   & 0.50   			& $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50 \\
          &       & $\sigma$    & 0.10 & 0.10 & $\rightarrow$ & 0.10 & 0.10 			& $\rightarrow$ & 0.10 & 0.01 & $\rightarrow$ & **0.289** & 0.10 & $\rightarrow$ & 0.10 \\ \hline
    $\pi_d$  & Beta  & $\mu$  	& 0.50   & 0.50   & $\rightarrow$ & 0.50   & 0.50   			& $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50   & 0.50   & $\rightarrow$ & 0.50 \\
          &       & $\sigma$    & 0.10 & 0.10 & $\rightarrow$ & 0.10 & 0.10 			& $\rightarrow$ & 0.10 & 0.01 & $\rightarrow$ & **0.289** & 0.10 & $\rightarrow$ & 0.10 \\
\hline
    $\phi_{a0}$ & Beta  & $\mu$  	& 0.01  & 0.50  & $\rightarrow$ & **0.01**  	& 0.01  & $\rightarrow$ & 0.01  & 0.01  & $\rightarrow$ & 0.01  & 0.01  & $\rightarrow$ & 0.25 \\
          &       & $\sigma$   	& 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\\hline
    $\phi_{a1}$ & Beta  & $\mu$  	& 0.99  & 0.50  & $\rightarrow$ & **0.99**  	& 0.99  & $\rightarrow$ & 0.99  & 0.99  & $\rightarrow$ & 0.99  & 0.75  & $\rightarrow$ & 0.75 \\
          &       & $\sigma$      & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\\hline
    $\phi_{b0} $& Beta  & $\mu$  	& 0.01  & 0.50  & $\rightarrow$ & **0.01**  	& 0.01  & $\rightarrow$ & 0.01  & 0.01  & $\rightarrow$ & 0.01  & 0.25  & $\rightarrow$ & 0.25 \\
          &       & $\sigma$    & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\\hline
    $\phi_{b1}$ & Beta  & $\mu$  	& 0.99  & 0.50  & $\rightarrow$ & **0.99**  	& 0.99  & $\rightarrow$ & 0.99  & 0.99  & $\rightarrow$ & 0.99  & 0.75  & $\rightarrow$ & 0.75 \\
          &       & $\sigma$    & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\\hline
    $\phi_{c0}$ & Beta  & $\mu$  	& 0.99  & 0.50  & $\rightarrow$ & **0.99**  	& 0.99  & $\rightarrow$ & 0.99  & 0.99  & $\rightarrow$ & 0.99  & 0.75  & $\rightarrow$ & 0.75 \\
          &       & $\sigma$    & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\\hline
    $\phi_{c1}$ & Beta  & $\mu$  	& 0.01  & 0.50  & $\rightarrow$ & **0.01**  	& 0.01  & $\rightarrow$ & 0.01  & 0.01  & $\rightarrow$ & 0.01  & 0.25  & $\rightarrow$ & 0.25 \\
          &       & $\sigma$    & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\\hline
    $\phi_{d0}$ & Beta  & $\mu$  	& 0.99  & 0.50  & $\rightarrow$ & **0.99**  	& 0.99  & $\rightarrow$ & 0.99  & 0.99  & $\rightarrow$ & 0.99  & 0.75  & $\rightarrow$ & 0.75 \\
          &       & $\sigma$    & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\ \hline
    $\phi_{d1}$& Beta  & $\mu$  	& 0.01  & 0.50  & $\rightarrow$ & **0.01**  	& 0.01  & $\rightarrow$ & 0.01  & 0.01  & $\rightarrow$ & 0.01  & 0.25  & $\rightarrow$ & 0.25 \\
          &       & $\sigma$    & 0.01 & 0.01 & $\rightarrow$ & 0.01 			& 0.01 & $\rightarrow$ & 0.01 & 0.01 & $\rightarrow$ & 0.01 & 0.001 & $\rightarrow$ & **0.25** \\
\hline
    \end{tabular}%
  \caption{Simulation parameters. Each column details parameters used to generate prior distributions for one of the simulations below. The prior distribution for the full parameter vector is formed from independent draws from Beta distributions for all probabilities and the Dirichlet distribution for shares. Note that the mean and standard deviation parameterization we provide for Beta distributions can be mapped directly to the more standard $\alpha, \beta$ parameterization. 
 }
  \label{sims}%
\end{table}% -->

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      \           \           \      1: m or n\   2: Probative Value\       \              \        3: Effect Heterogeneity\                                4: Assignment Uncertainty\                                  5: Clue Uncertainty\                               
  $\theta$       Dist        arg                         Low          $\rightarrow$       High                Low             $\rightarrow$       High                 Low               $\rightarrow$       High               Low            $\rightarrow$       High     
------------  ---------  ----------  ----------  -------------------  -------------  -------------  ------------------------  -------------  -------------  --------------------------   -------------  --------------  --------------------  -------------  -------------
 $\lambda_a$  Dirichlet  $\alpha_a$      1\              0.20\        $\rightarrow$       0.20\               0.10\           $\rightarrow$     **2.00**                1\               $\rightarrow$         1\                1\            $\rightarrow$        1\      
    
 $\lambda_b$             $\alpha_b$      1\              0.20\        $\rightarrow$       0.20\              02.10\           $\rightarrow$     **4.00**                1\               $\rightarrow$         1\                1\            $\rightarrow$        1\      
    
 $\lambda_c$             $\alpha_c$      1\              0.20\        $\rightarrow$       0.20\              02.00\           $\rightarrow$     **0.10**                1\               $\rightarrow$         1\                1\            $\rightarrow$        1\      
    
 $\lambda_d$             $\alpha_d$      1\              0.20\        $\rightarrow$       0.20\              02.00\           $\rightarrow$     **0.10**                1\               $\rightarrow$         1\                1\            $\rightarrow$        1\      
    
   $\pi_a$       Beta       $\mu$       0.50\            0.50\        $\rightarrow$       0.50\               0.50\           $\rightarrow$       0.50\                0.50\             $\rightarrow$       0.50\              0.50\          $\rightarrow$       0.50\    
                          $\sigma$      0.10\            0.10\        $\rightarrow$       0.10\               0.10\           $\rightarrow$       0.10\                0.01\             $\rightarrow$     **0.289**            0.10\          $\rightarrow$       0.10\    
     
   $\pi_b$       Beta       $\mu$       0.50\            0.50\        $\rightarrow$       0.50\               0.50\           $\rightarrow$       0.50\                0.50\             $\rightarrow$       0.50\              0.50\          $\rightarrow$       0.50\    
                          $\sigma$      0.10\            0.10\        $\rightarrow$       0.10\               0.10\           $\rightarrow$       0.10\                0.01\             $\rightarrow$     **0.289**            0.10\          $\rightarrow$       0.10\    
     
   $\pi_c$       Beta       $\mu$       0.50\            0.50\        $\rightarrow$       0.50\               0.50\           $\rightarrow$       0.50\                0.50\             $\rightarrow$       0.50\              0.50\          $\rightarrow$       0.50\    
                          $\sigma$      0.10\            0.10\        $\rightarrow$       0.10\               0.10\           $\rightarrow$       0.10\                0.01\             $\rightarrow$     **0.289**            0.10\          $\rightarrow$       0.10\    
     
   $\pi_d$       Beta       $\mu$       0.50\            0.50\        $\rightarrow$       0.50\               0.50\           $\rightarrow$       0.50\                0.50\             $\rightarrow$       0.50\              0.50\          $\rightarrow$       0.50\    
                          $\sigma$      0.10\            0.10\        $\rightarrow$       0.10\               0.10\           $\rightarrow$       0.10\                0.01\             $\rightarrow$     **0.289**            0.10\          $\rightarrow$       0.10\    
     
 $\phi_{a0}$     Beta       $\mu$       0.01\            0.50\        $\rightarrow$     **0.01**              0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\              0.01\          $\rightarrow$       0.25\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{a1}$     Beta       $\mu$       0.99\            0.50\        $\rightarrow$     **0.99**              0.99\           $\rightarrow$       0.99\                0.99\             $\rightarrow$       0.99\              0.75\          $\rightarrow$       0.75\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{b0}$     Beta       $\mu$       0.01\            0.50\        $\rightarrow$     **0.01**              0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\              0.25\          $\rightarrow$       0.25\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{b1}$     Beta       $\mu$       0.99\            0.50\        $\rightarrow$     **0.99**              0.99\           $\rightarrow$       0.99\                0.99\             $\rightarrow$       0.99\              0.75\          $\rightarrow$       0.75\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{c0}$     Beta       $\mu$       0.99\            0.50\        $\rightarrow$     **0.99**              0.99\           $\rightarrow$       0.99\                0.99\             $\rightarrow$       0.99\              0.75\          $\rightarrow$       0.75\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{c1}$     Beta       $\mu$       0.01\            0.50\        $\rightarrow$     **0.01**              0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\              0.25\          $\rightarrow$       0.25\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{d0}$     Beta       $\mu$       0.99\            0.50\        $\rightarrow$     **0.99**              0.99\           $\rightarrow$       0.99\                0.99\             $\rightarrow$       0.99\              0.75\          $\rightarrow$       0.75\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
     
 $\phi_{d1}$     Beta       $\mu$       0.01\            0.50\        $\rightarrow$     **0.01**              0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\              0.25\          $\rightarrow$       0.25\    
                          $\sigma$      0.01\            0.01\        $\rightarrow$       0.01\               0.01\           $\rightarrow$       0.01\                0.01\             $\rightarrow$       0.01\             0.001\          $\rightarrow$     **0.25**   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Table: (\#tab:sims)Simulation parameters. Each column details parameters used to generate prior distributions for one of the simulations below. The prior distribution for the full parameter vector is formed from independent draws from Beta distributions for all probabilities and the Dirichlet distribution for shares. Note that the mean and standard deviation parameterization we provide for Beta distributions can be mapped directly to the more standard $\alpha, \beta$ parameterization

\newpage


\normalsize 

\begin{table}
\footnotesize
\centering
\begin{tabular}{lccp{8cm}} 
						& $j$ steps 			& $k$ sims					&  					\\ 
Experiment 				& per exp.				& per step					& Comments 					\\ \hline
**1:**
Varying $N$ or $m$		& 29					& 5,200								& The 5,200 $k$ simulations for each $\theta_j$ were split into 26 runs of 200 $k$ sims, and then
																					  compiled through averaging. \\
																					 
&&&					Datapoints at N=3 and N=4 were added using 14,200 $k$ simulations. \\\\
**2:**
Probative Value 		& 30					& 5,200								& The 5,200 $k$ simulations for each $\theta_j$ were split into 26 batches of 200 $k$ sims, and then
																					  compiled through averaging.  \\ \\
**3:**
Effect Heterogeneity	& 30					& 5,200								& The 5,200 $k$ simulations for each $\theta_j$ were split into 26 batches of 200 $k$ sims, and then
																					  compiled through averaging.  \\ \\
**4:**
Assignment Uncertainty	& 30					& 5,200								& The 5,200 $k$ simulations for each $\theta_j$ were split into 26 batches of 200 $k$ sims, and then
																					  compiled through averaging.  \\ \\
**5:**
Clue Uncertainty    	& 30					& 10,200							& The 10,200 $k$ simulations for each $\theta_j$ were split into 26 batches of 200 $k$ sims and 
																					  10 batches of 500, then
																					  compiled through averaging.  \\
\end{tabular}
\caption{\footnotesize *Note:* 
Each experiment takes $j$ steps through different values of $\theta$. At each $\theta_j$, the data is simulated $k$ times. For each simulation,   
a call is made to the Stan model and HMC (Hamiltonian Monte Carlo) sampling is used
to approximate the posterior distribution. 
In each such call to Stan, we run 4 chains with 6000 iterations, and 
1000 warmup draws.}
\label{simdetails}
\end{table}

<!--chapter:end:12-wideordeep.Rmd-->

# Case selection as a Decision Problem {#caseselection}

```{r, include = FALSE}
source("_packages_used.R")
```

***

With a causal model in hand, together with priors over parameters, you can assess in advance what conclusions you will draw from different observations and assess what kinds of observations are most worth seeking. We draw out the implicaitons of this idea for case selection.

***


A critical decision for scholars employing mixed methods is to determine which cases are most valuable for within-case analysis.  

A host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of $X$, $Y$ data. Perhaps the most common strategy is to select cases in which $X=1$ and $Y=1$ and look to see whether in fact $X$ caused $Y$ in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases "on the regression line" or, for some purposes, cases "off the regression line" (e.g., @Lieberman2005nested). Some scholars suggest ensuring variation in $X$ (most prominently, @king1994designing), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on $X$ or $Y$ (@FL2008, @HerronQuinn). 

Which cases you should choose will likely depend on the purposes to which you want to put them. 

A matching strategy for instance---selecting cases that are comparable on many features but that differ on $X$---replicates at a small scale the kind of inference done by matching estimators with large-$n$ data. The strategy emphasize the inferences to be made from $X,Y$ variation rather than inferences drawn specifically from within case information beyond what is available in the measurement of $X$ and $Y$. (Citations needed.)

Other treatments seek to use qualitative information to check assumptions made in $X, Y$ analysis: for example, is the measurement of $X$ and $Y$ reliable in critical cases? (Citations needed) For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the $X,Y$ inferences: for example influential cases that have unusually extreme values on $X$ and $Y$.\^[Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.] Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims.

A third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman's "model-building" mode of "nested analysis" (@Lieberman2005nested). Here it may be cases off the regression line that are of interest.

Weller and Barnes (CITE article) on case selection focus on (a) X/Y relations and (b) whether the cases are useful for hypothesis generation. 

In what follows, we focus on a simpler goal: given existing $X, Y$ data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of $X$ on $Y$?

The basic insight of this chapter is simple enough: *the optimal strategy for case selection for a model-based analysis can be determined by the model and the query*, just as we saw for the optimal clue-selection strategy in Chapter \@ref(Clues). Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as:

* go where the probative value is, and
* sample from $X$ and $Y$ values in proportion to their occurrence in the population,
* invest in collections of cases that provide complementary learning. 

Beyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with $X, Y, K$ data.

For the general intuition, recall that the probative value of a process-tracing test hinges on the difference in clue likelihoods associated with the alternative hypotheses in play for a given case. Recall that for different values of $X$ and $Y$ cell, we want to use process tracing to help us distinguish between two specific types that are consistent with the $X, Y$ pattern. Which types are in question varies across $X,Y$ combinations. Table \ref{FP} illustrates.


\begin{table}[h!]
\centering
\begin{tabular}{c|cc}
           &        $Y=0$ &        $Y=1$ \\ \hline
       $X=0$ &     $b$ or $c$ &     $a$ or $d$ \\
       $X=1$ &     $a$ or $c$ &     $b$ or $d$ \\
\end{tabular}  
\caption{The ambiguity about types in each $X, Y$ cell.}
\label{FP} 
\end{table}

Thus, in the $X=0, Y-0$ cell, what would be most useful is a clue that has high probative value in distinguishing between an untreated ($X=0$) $b$ type and and an untreated $c$ type. For a case in the $X=1, Y=0$ cell, on the other hand, what matters is how well the clue can discriminate between treated ($X=1$) $a$ and $d$ types. In our notation, it is the difference in $phi_{jx}$ values for that indicates these cell-specific degrees of leverage. 

To illustrate, consider a situation in which for a given clue we have  $\phi_{b1}$=0.5^[The probability of observing the clue for a $b$ type (positive causal effect) case with $X=1$.]; $\phi_{d1}$=0.5 ^[The probability of observing the clue for a $d$ type (zero causal effect, $Y$ fixed at 1) case with $X=1$.]; $\phi_{b0}$=0.5^^[The probability of observing the clue for a $b$ type (positive causal effect) case with $X=0$.]; and $\phi_{c0}$=0.1^[The probability of observing the clue for a $c$ type (zero causal effect, $Y$ fixed at 0) case with $X=0$.]. In this situation, searching for the clue in $X=Y=1$ cases will yield no leverage since the clue does not discriminate between the two types ($b$ and $d$) that need to be distinguished given $X=Y=1$. Here there is no additional learning about $\lambda_b$ that can be gained from looking for the clue. In contrast, $X=0, Y=0$ cases will be informative since the clue is much better at distinguishing between $b$ and $c$ types---the two types in contention for this kind of case. Thus, although process tracing here does not provide information on the prevalence of positive causal effects ($b$ types) for an $X=Y=1$ case, it does provide information when $X=Y=0$. 

While it is common practice for mixed-method researchers to perform their process tracing "on the regression line," the BIQQ framework suggests that the gains to process tracing for different $X$ and $Y$ values in fact depend on the particular constellations of $\phi$ values for the potentially available clues. More generally, the framework allows one to assess the expected gains from any given case-selection strategy *ex ante* once priors have been specified. 

## Explorations
Most closely related to our analysis in this chapter is the contribution of @HerronQuinn, who build on @SeawrightGerring2008. While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches.  As we do, @HerronQuinn focus on a situation with binary $X,Y$ data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, $Z_i$  is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to "treatment", or an $X=1$ value). @HerronQuinn assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly.  

Our setup differs from that in @HerronQuinn in a few ways.  @HerronQuinn paramaterize differently, though this difference is not important.^[@HerronQuinn have a parameter $\theta$ that governs the distribution of data over $X$ and $Y$ and then, conditional on $X,Y$ values, a set of parameters $\psi_{xy}$ that describe the probability of a case's being of a given causal type. We take both $\theta$ and $\psi_{xy}$ to derive from the fundamental distribution of causal types and assignment probabilities. Thus, for example, $\psi_{00}$ from @HerronQuinn corresponds to $\frac{(1-\pi_b)\lambda_b}{(1-\pi_b)\lambda_b + (1-\pi_c)\lambda_c}$ in our notation. The difference in  paramaterization does have implications for interpretations of the priors. For example flat priors over $\theta$ and $\psi$ implies a tighter distribution that a uniform prior over the causal types. In fact @HerronQuinn use priors with greater variance than uniform in any event.] Perhaps the most important difference between our analysis and that in @HerronQuinn  is that we connect the inference strategy to process-tracing approaches. Whereas @HerronQuinn assume that causal types can be read directly, we assume that these are inferred *imperfectly* from clues. As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of $X$. And, as in the baseline model, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases.

<!-- Are Herron and Quinn's priors Jeffrey priors? -->

Here we assume that the case selection decision is made after observing the $XY$ distribution and we explore a range of different possible contingency tables. In  @HerronQuinn the distribution from which the contingency tables are drawn is fixed, though set to  exhibit an expected  observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large $XY$ data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases.

Another important difference, is that in many of their analyses, @HerronQuinn take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has *beliefs* about the true treatment effect that correspond to their priors, and for whom there is therefore no *expected* bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher's beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)?

Finally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by @SeawrightGerring2008) and evaluating them, we define a strategy as the particular distribution over $XY$ cells to be examined and proceed to examine *every possible strategy* given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies---those strategies that perform similarly---emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies.

Despite these various differences, our results will agree in key ways with those in @HerronQuinn.


### Procedure
To illustrate the procedure, consider a researcher who observes six data points.  Two cases lie in each of the on-diagonal cells of an $X,Y$ table, those where $X=Y$, and one case lies in each of the off-diagonal cells (where $X \neq Y$). Suppose that the researcher is considering gathering process-tracing clues on 3 cases. She has many different strategies she might pursue and each of these can give rise to different possible data and thus to different possible conclusions. Let's say that she starts with no knowledge about (flat priors over) the distribution of causal types in the population, assumes similar assignment propensities for all types (no confounding), and believes that the clues she is going to look for are all doubly decisive (either seeing the clue or not seeing the clue, when she looks for it, will fully identify the case's type).  

Suppose, now, that she is choosing between two strategies:

A. Select two $X=Y=1$ cases and one $X=Y=0$ case for investigation. She is thus choosing here cases "on the regression line" generated by the $X,Y$ data pattern. 

B. Take one case from each of the diagonal cells and one from the off-diagonal $X=1, Y=0$ cell

From which strategy should she expect to learn more about the average, population-level causal effect of $X$ on $Y$, given her prior beliefs? We answer the question by considering each data pattern that she might see given a strategy and then calculating (a) the probability with which she expects to see such an outcome, given her priors (or more accurately, given her posterior after observing the $XY$ pattern only), and (b) the uncertainty (posterior variance) that she would have upon seeing that pattern. We then calculate the expected variance of the strategy by considering all possible data patterns that could emerge from that strategy. We ignore the fact that in principle the subjective probability of observing one pattern is correlated with the subjective probability of observing another pattern.^[For example, a researcher may be uncertain regarding $\phi_b$; if it is high then the probability of observing any profile of outcomes with many clues observed is higher than if $\phi_b$ is low; this introduces a correlation between outcomes that have similar clue observations. However only one of these patterns will be observed in fact. NOTE ON RISK-NEUTRALITY HERE?]

Tables 11.1 and 11.2 present the results of our comparison of the researcher's two strategies. Table 11.1 examines strategy A, while Table 11.2 examines strategy B. In each table, the lefthand column lists all possible realizations of clue data that might be found under the strategy in question. In the notation that we use here, each place in the four-digit sequence refers to one cell in the 2-by-2 table implied by a binary $X$ and binary $Y$ variable. The ordering is: 

* $X=Y=0$
* $X=0, Y=1$
* $X=1, Y=0$
* $X=Y=1$ 

As an easy mnemonic, the outside digits are "on-the-diagonal" (on the regression line consistent with a positive causal effect); the inside digits are off-the-diagonal. Thus, for instance, the sequence $1001$ means that we have seen the clue in two on-the-diagonal cases: an $X=Y=0$ case and a $X=Y=1$ case. The sequence 0200 means that we have seen the clue in two cases in the $X=0, Y=1$ cell.

In each row, we see information about one possible clue pattern that we could potentially observe under the strategy. The second column indicates the probability that this clue pattern will arise, given the researcher's priors. The final column then indicates the uncertainty---the posterior variance---that we would be left with given the information provided by that clue pattern. In the "All" row, we have averaged across the posterior variances for all clue patterns, weighting each clue pattern by its probability of occurring. 

REDO BIQQ LOSSES WITH NEW FUNCTIONS

## In code

```{r selection_code}

# We define a model
model <- make_model("X->M->Y")  %>%
 set_restrictions(causal_type_restrict = "Y[M=1]<Y[M=0] | M[X=1]<M[X=0]") %>%
 set_parameter_matrix() %>%
 set_parameters(type = "flat")

# We imagine some preexisting data we have observed
data  <-  data.frame(X = c(0,0,0,1,1,1), M = NA, Y = c(0,0,1,0,1,1)) 
given <-  collapse_data(data, model)

# We can then imagine what data we might observe if we examine M inside some subset of data
possible_data <- 
  make_possible_data(model, given = given, vars = "M", within = TRUE, N = 1, condition = "X==1 & Y==1")
```

```{r, echo = FALSE}
if(do_diagnosis){
estimates_database <- make_estimates_database(
  model, given, possible_data = possible_data, queries = "Y[X=1]>Y[X=0]")
  write_rds(estimates_database, "saved/estimates_database_n1_X1Y1.rds")
}

estimates_database_n1_X1Y1 <- read_rds("saved/estimates_database_n1_X1Y1.rds")

if(do_diagnosis){
  reference_model <- gbiqq(model, data)
  write_rds(reference_model, "saved/reference_XMY_updated_model.rds")
}
reference_model <- read_rds("saved/reference_XMY_updated_model.rds")
```

Diagnosis:

```{r}
diagnosis_X1Y1 <- diagnose_strategy(
                  reference_model = reference_model, 
                  analysis_model  = model,
                  given = given, 
                  queries = "Y[X=1]>Y[X=0]",
                  estimates_database = estimates_database_n1_X1Y1,
                  possible_data = possible_data)

```

## Compare multiple data strategies 

We now apply the procedure to a set of strategies. 

```{r, echo = FALSE}
possible_data_X1Y0 <- 
  make_possible_data(model, given = given, vars = "M", within = TRUE, N = 1, condition = "X==1 & Y==0")

if(do_diagnosis){
estimates_database <- make_estimates_database(
  model, given, 
  possible_data = possible_data_X1Y0, 
  queries = "Y[X=1]>Y[X=0]")
  write_rds(estimates_database, "saved/estimates_database_n1_X1Y0.rds")
}

estimates_database_n1_X1Y0 <- read_rds("saved/estimates_database_n1_X1Y0.rds")

diagnosis_X1Y0 <- diagnose_strategy(
                  reference_model = reference_model, 
                  analysis_model  = model,
                  given = given, 
                  queries = "Y[X=1]>Y[X=0]",
                  estimates_database = estimates_database_n1_X1Y0,
                  possible_data = possible_data_X1Y0)

```

```{r, echo = FALSE}
kable(diagnosis_X1Y1, caption = "Expected value of a strategy in which one X=Y=1 case is examined", digits = 3)
```
```{r, echo = FALSE}
kable(diagnosis_X1Y0, caption = "Expected value of a strategy in which one X=Y=0 case is examined", digits = 3)
```

* \@all here is where we need most backup help right now --- these examples show the basic analysis --- interest is in understanding how posterior variance differs for difference data strategies given difference `givens` and different background models.

* Steps -- 

  * Do wrapper to make it a little faster to generate results from a new strategy / mode in one line (that uses saved results when possible)
  * Extend current example to all four "gather M for 1 case" strategies and all 8  "gather M for 2 cases" (could be two of the same sort or two different) 
  * Then do the same for (a) a model with monotonicity of X to M and M to Y assumed and (b) a X -> Y <- M model with complementarity of X and M assumed
  * Even better if possible to graph some of the results, even with confidnece intervals. You can see the previous graphs we had in K1.pdf - K4.pdf in "6 Book\ii\Figures"
  

* \@lily --- strategies that involve "look at M in each of teh X/Y cells" will involve using the full data strategies of  `make_possible_data` with `condition = list("X==0 & Y==0", "X==0 & Y==1")` etc




OLD TEXT: In this example, we see that the researcher would expect to be better off---in the sense of having less posterior uncertainty---by focusing her process-tracing efforts where a greater share of the population of cases lies: on the regression line. Taking one observation in each of three cells has her devoting much of her effort to a case that is relatively unrepresentative of the population she wishes to learn about. 

In the experiments that follow, we implement this kind of simulation for all possible clue strategies---for a fixed number of clues sought---and report the expected posterior variance.

## Experiments

In all of the graphs, we start with 16 "quantitative" cases: cases for which we have observed an $X$ and a $Y$ value. We are then choosing some subset of these cases for process tracing. Within each group of nine graphs, we are sampling a fixed number of cases for process tracing: 1 case in the first set, 2 cases in the second set, 3 cases in the third set, and 4 cases in the final set. We treat process tracing in a case as the search for one clue in that case, though this "one clue" could be conceived of as a collection of clues that jointly have a given probative value. 

Within each set of graphs, we see how different case selection strategies fare as we vary two features of the research situation. Moving down the rows of graphs, we vary the distribution of the 16 cases over an $XY$ table. In the first row, the 16 cases are spread evenly across the 4 $X,Y$ cells; in the second row, $X$ and $Y$ are positively correlated; in the third row, $Y=1$ is observed only in cases with $X=1$ (i.e., the $X=0, Y=1$ cell is empty). 

Moving across the columns of graphs, we vary the probative value of the clue that we are looking for in the process tracing. What is changing is for what kind of a case---in terms of its $X,Y$ values---the clue is most probative (i.e., doubly decisive). Where the clue is doubly decisive, finding the clue present or finding it absent both nail down the type of the case. Wherever the clue is not doubly decisive, it is assumed to be a "hoop test" for an $a$ type (in the $X=1, Y=0$ and $X=0, Y=1$ cells) and for a $b$ type (in the $X=0, Y=1$ cells). This means that *not* finding the clue rules out the case's being an $a$ or a $b$ type in the relevant cells; thus, the clue is still informative, but less so than if it were doubly decisive. In the first column, the clue is doubly decisive for all kinds of cases. In the second column, the clue is doubly decisive only for $X=1$ cases, and a hoop test otherwise. In the third column, the clue is doubly decisive only for $Y=1$ cases, and a hoop test otherwise. And in the final column, the clue is doubly decisive only "on the regression line" consistent with a positive effect, and a hoop test otherwise.

Each case selection strategy is indicated on each graph using the same four-digit pattern that we used to indicate data realizations in the example above. Throughout, what we seek to estimate is the average causal effect of $X$ on $Y$ (or $\lambda_b-\lambda_a$). The strategy's vertical placement on the graph indicates the "loss", i.e., expected posterior uncertainty, associated with the strategy. Thus, a lower placement indicates greater learning. Around each dot, we also provide 90 percent simulation error bars, though these are hidden by the dots themselves when the simulation error is very small, as it is in most cases.

In all simulations, we assume that there is no confounding, and we start (before seeing the $X, Y$ pattern) with flat priors over the distribution of causal types in the population.

In the graphs with more than one process tracing case, we also color-code and group together families of strategies as indicated in the figure caption.

We begin with the simplest problem, where only one case is to be chosen for process tracing. In this situation, as seen in our first set of figures, two principle emerge. First, it is better select cases in the $X,Y$ conditions where the probative value lies. More informative clues generate more learning; so if probative value varies across types of cases, this should have direct implications for case selection. Second, it is better to select cases from the largest cells. The second principle is perhaps less obvious, but it derives from a sampling logic: learning about a case drawn randomly from a cell gives you information about that cell and so the larger the cell the more cases there is learning about. 

Perhaps just as important is what does not emerge as a principle: all else equal there is no reason to focus on either the $X=Y=1$ cases or on the diagonal ($X=Y$): all four cells are symmetric (ceteris paribus) in that they all exhibit an ambiguity between $a$'s and $b$'s on the one hand and $c$'s and $d$'s on the other. There is thus nothing intrinsically informative about the cases on the diagonal. We do see that choosing on the diagonal is beneficial when $X$ is positively correlated with $Y$, but this is because the  population of cases is concentrated along the diagonal; this is an illustration of the representativeness principle, not of some special feature of the diagonal.


```{r echo=FALSE, out.width="100%", fig.align='center', caption = "Single case process tracing"}
knitr::include_graphics("./Figures/K1.pdf")
```

The figure shows gains from different strategies involving process tracing in 1 case.  In each simulation, we start with 16 "quantitative" cases. Moving down the rows of graphs, we vary the distribution of these cases over an $XY$ table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. 

More interesting patterns start emerging once we can choose two cases. We see the same two basic principles matter---go for probative value and for representativeness---but we now also see that there are complementarities in learning between different types of cases. In fact, for symmetric problems---as in the upper left panel, where the cases are evenly spread out and probative value is strong everywhere---we see a ranking between four families: first cases that fix  $Y$ (at 0 or 1) and spread on $X$, second cases that fix $X$ and spread on $Y$, third cases that are on or off the diagonal, and fourth cases that focus on a single cell. Seeking clues on the diagonal does emerge as a good strategy (see for example the left figure in the middle row), but this appears to arise because the diagonal is data dense, not because there are particular complementarities of probative value there. In the bottom left figure for example, we see that in the case where there is data for $X=1, Y=0$ but little data for $X=0, Y=1$ , selecting cases distributed over the $Y=0$ cells is about as informative as selecting on the diagonal. Selecting off the diagonal is one of the worst strategies here and is dominated by selecting all data from a dense cell. 

Skip now to our graph of results where 4 cases are selected. As in previous figures we see strong gains for strategies that select cases proportionate to the size off cells. This is mot clear in the top left figure where one case per (equally sized) cell is the best strategy. More subtly it can be seen in the bottom left where the 2-0-1-1 strategy dominates --- this is a strategy that spreads roughly proportionately even if that means leaving some cells unrepresented. Strikingly in the base case various hybrid strategies do quite well, likely reflecting the fact that are as close to optimal spreads as possible. On and off diagonal strategies do poorly unless there is a strong diagonal, in which case these can dominate spreading across cells. 

Overall relatively simple patterns emerge though these differ in some ways from text book suggestions. First focusing on probative value is key. Second seeking larger cells and balancing cases across cells appears fruitful. Third, and less intuitively, some combinations appear to gain more leverage than others. On and off diagonal strategies for example seem weaker in general than strategies that fix $X$ or that fix $Y$. Strategies that fix $Y$ and allow variation on $X$ seem strong, again, ceteris paribus.

Perhaps the most striking result from the simulations is that the optimal choice depends on many features.  A simple rule, or even these core principles, may not get identify the right strategy. Ye the right strategy can be calculated, at least if one is willing to lay out beliefs on causal structure and probative value.

![Gains from different strategies involving process tracing in 2 cases. We treat process tracing in a case as the search for one clue in that case, though this "clue" could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 "quantitative" cases. Moving down the rows of graphs, we vary the distribution of these cases over an $XY$ table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. Families of strategies are grouped and color-coded as follows: red=maximally dispersing across cells; yellow=](Figures/K2.pdf)


![Gains from different strategies involving process tracing in 3 cases. We treat process tracing in a case as the search for one clue in that case, though this "clue" could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 "quantitative" cases. Moving down the rows of graphs, we vary the distribution of these cases over an $XY$ table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. ](Figures/K3.pdf)



![Gains from different strategies involving process tracing in 4 cases. We treat process tracing in a case as the search for one clue in that case, though this "clue" could be conceived of as a collection of clues that jointly have the probative value indicated. Simulations invole 16 units distributed over an $XY$ table in three patterns (rows) and variation over the probative value of different clues (columns).](Figures/K4.pdf)


## Chapter Appendix: Accounting for case selection
### Independent case selection strategy
We have focused on cases in which  the researcher examines a fixed number of cases for clue information. An alternative strategy that produces a simpler likelihood is one in which each case is selected for within-case data gathering with some independent probability. The likelihood below introduces a case selection probability $\kappa_{xy}$ that covers this case and allows for the possibility that selection probabilities are different for different $X,Y$ combinations.

 Thus we assume that $X$, $Y$ data is observed for all cases under study, but that $K$ data may be sought for only a subset of these (we use the wildcard symbol ''$*$'' to denote that the value of the clue is unknown). We let $n_{xyk}$ denote the number of cases of each type. Then, again assuming data is independently and identically distributed, the likelihood is:

$$\Pr(\mathcal{D}|\theta)= {\text{Multinomial}}(n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*} |
w_{000}, w_{001},w_{00*},w_{010}, w_{010},w_{01*}, w_{100}, w_{101},w_{10*},w_{110},w_{111} ,w_{11*})$$


where the event probabilities are now given by:


$${\left( \begin{array}{c}
w_{000} \\ w_{001} \\  \vdots \\ w_{11*} \end{array} \right)=
\left( \begin{array}{c}
\lambda_b(1-\pi_b)\kappa_{00}(1-\phi_{b0}) + \lambda_c(1-\pi_c)\kappa_{00}(1-\phi_{c0})\\
\lambda_b(1-\pi_b)\kappa_{00}\phi_{b0} + \lambda_c(1-\pi_c)\kappa_{00}\phi_{c0}\\
\vdots \\
\lambda_b\pi_{b}(1-\kappa_{11}) + \lambda_d\pi_{d}(1-\kappa_{11})
\end{array} \right)}$$

Note we use a Greek symbol for the case selection probabilities to highlight that these may also be unknown and be an object of inquiry, entering into the vector of parameters, $\theta$.
\subsubsection{Non-random $XY$ Sample Selection}\label{nonrandomcase}
While we have assumed in the canonical model that $X,Y$ cases are selected at random, this need not be the case. Say instead that each case of type $j$ is selected into the study with probability $\rho_j$. In that case, assuming independent selection of cases for qualitative analysis, the likelihood function is now:


$$\Pr(\mathcal{D}|\theta) = {\text{Multinomial}}(n, w)$$
where: $$n = (n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*})$$

and the event probabilities, $w$, are now, given by:

$$\left( \begin{array}{c}
w_{000} \\ w_{001} \\  \vdots \\ w_{11*}
\end{array} \right)=
\left( \begin{array}{c}
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_b)\kappa_{00}(1-\phi_{b0}) +
\frac{\rho_c \lambda_c}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_c)\kappa_{00}(1-\phi_{c0})\\
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_b)\kappa_{00}\phi_{b0}+
\frac{\rho_c \lambda_c}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}(1-\pi_c)\kappa_{00}\phi_{c0}\\
\vdots \\
\frac{\rho_b \lambda_b}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_d}\pi_{b}(1-\kappa_{11})+
\frac{\rho_d \lambda_d}{\rho_a \lambda_a+\rho_b \lambda_b+\rho_c \lambda_c+\rho_d \lambda_{11}}\pi_{d}(1-\kappa_{11})
\end{array} \right)$$

Note we have used a Greek symbol for the selection probabilities to highlight that these probabilities may be unknown and could enter into the set of parameters of interest, $\theta$.

### Conditional random case selection

Finally consider the likelihood for a design in which a researcher selects to search for clues as a function of the $X,Y$ data. This is a somewhat harder case because the size of each $X,Y$ group is stochastic. Let $n_{xy} = n_{xy0}+n_{xy1}+n_{xy*}$ denote the number of cases with particular values on $X$ and $Y$, and let $n_{XY}=(n_{00},n_{01},n_{10},n_{11})$ denote the collection of $n_{xy}$ values.

Say now that conditional on the $X,Y$ observations, a researcher sets a target of $k_{xy}(n_{XY})$ cases for clue examination (note here that the number of clues sought for a particular $X,Y$ combination can be allowed to depend on what is observed across all $X$, $Y$ combinations). Then the likelihood is:
$$\text{Multinomial}(n_{XY}|w_{XY})\prod_{x\in\{0,1\},y \in\{0,1\}}\text{Binom}(n_{xy1}|k_{xy}(n_{xy}), \psi_{xy1})$$

The multinomial part of this expression gives the probability of observing the particular $X,Y$ combinations; the event probabilities for these depend on $\lambda$ and $\pi$ only | for example  $w_{11} = \lambda_b \pi_b+\lambda_d \pi_d$. The subsequent binomials give the probability of observing the clue patterns conditional on searching for a given number of clues ($k_{xy}(n_{xy})$) and given an event probability $\psi_{xy1}$ for seeing a clue given that the clue is sought for an $x,y$ combination; thus for example:
$$ \psi_{111} = \frac{\lambda_b \pi_b}{\lambda_b \pi_b+\lambda_d \pi_d} \phi_{b1} + \frac{\lambda_d \pi_d}{\lambda_b \pi_b+\lambda_d \pi_d} \phi_{d1}$$

<!--chapter:end:13-caseselection.Rmd-->

# (PART) Models in Question  {-}

# Where does probative value come from?



***

We outline strategies to reduce reliance on unfounded beliefs about the probative value of clues.

***


```{r, include = FALSE}
source("_packages_used.R")
```


Everything we have talked about so far assume that researchers are able to state priors on all the primitives entering into the analysis. 

But what if they cannot?

As we have noted, learning in the framework we have described can occur even with "flat" priors over causal effects and assignment probabilities. However priors on the probative value of clues have to be informative. We also suspect, however, that researchers usually have---or could readily formulate---a rough idea of where they believe the parameter values lie. They have approximate prior beliefs about whether a causal effect is very common or not very common; whether confounding is likely or unlikely; and whether a given feature of a process is highly likely to be observed, moderately likely to be observed or very unlikely to observed under a given causal theory. Indeed, beliefs about these last two parameters already play a central role in standard approaches to correlational analysis and process tracing. This means that researchers should be in a position to specify a limited range of plausible values for each of the model's primitives.  They can then use the model to explore the consequences of different values in this plausible range either for their findings (given a set of collected data) or for their research design choices (prior to collecting data). And they can express their conclusions explicitly as *conditional* on their priors, or report findings for a range of prior values. Even where researchers are reluctant to quantify their beliefs, we believe that the principles underlying the BIQQ approach can nonetheless offer valuable, heuristic guidance on how to interpret mixes of qualitative and quantitative evidence in ways that are internally consistent and make more complete use of the available information.

At the same time, it is consequential where researchers' priors---in particular, their priors on the probabilities of clues---come from: whether they reflect some form of empirical knowledge or derive purely from theory. If a theory $T$ suggests that clue $K$ will be observed if and only if $X$ causes $Y$, then the presence of $K$ provides evidence that $X$ causes $Y$ *only to the extent that $T$ provides a true account of the causal logic through which $X$ affects $Y$*. If $T$ captures the wrong mechanism of causation, for instance, then clue probabilities that derive from the theory may be wrong and causal inferences that are based on the observation of these clues will be wrong. To use process data to make claims about causal effects that are *not* conditional on theories, researchers need empirical support for claims on the probative value of clues. This may often be difficult to obtain. In particular, it requires that {knowledge travel}: whereas in traditional Bayesian analysis researchers often employ uninformative (flat) priors, the use of clue-based data contributes to analysis *only if* priors are informative. 

Finally, we believe that the model is useful in part *because* it places such high demands on scholars' beliefs about the probative value of within case data: that is, because it clearly identifies the required inputs into the process of drawing integrated causal inferences. Put differently, to the extent that scholars are unable to specify even approximate ranges on these parameters, this is a problem for causal inference, not a problem for the model. The framework thus has implications for research agendas in identifying the kinds of knowledge that scholars need to generate if they are to use mixed methods to provide causal accounts of the world. 


## Causal discovery

```{r, include = FALSE}
# source("http://bioconductor.org/biocLite.R")
# biocLite("RBGL")
# if (!requireNamespace("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# BiocManager::install("Rgraphviz")
# library(Rgraphviz)

library(gbiqq)
library(dplyr)
library(Rgraphviz)
library(pcalg)

recover_model <- function(model) {
    
  df <- simulate_data(model, n = 1000)
  V  <- colnames(df)
  
  suffStat <- list(dm = df, nlev = c(2,2,2), adaptDF = FALSE)
  skel.fit <- skeleton(suffStat,
                       indepTest = disCItest, 
                       alpha = 0.01, labels = V, 
                       verbose = TRUE)
  plot(skel.fit, main = "Estimated Skeleton")
  }
```

We start with a model with three variables, $X,M,Y$ where $X$ affects $Y$ directly and indirectly through $M$. We simulate data from this model -- assuming monotonicity but otherwise a flat distribution on types, and then try to recover the structure from this model.



```{r, include = FALSE}
model1 <- make_model("X -> M -> Y <- X") %>% 
  set_restrictions(causal_type_restrict = "(M[X=1]<M[X=0]) | (Y[M=1, X=.]<Y[M=0, X=.]) | (Y[X=1, M=.]<Y[X=0, M=.])")
```



In this case the data structure did not impse restrictions on the skeleton. The true graph can however be recovered with knowledge of the temporal ordering of variables. 

Next we consider the model in which X causes Y through M but not directly. In this case we have a restriction --- specifically there is no arrow pointing directly from $X$ to $Y$. Again we impose monotonicity, draw data, and try to recover the model:


```{r, include = FALSE}
model2 <- make_model("X -> M -> Y") %>% 
  set_restrictions(
    causal_type_restrict = "(M[X=1]<M[X=0]) | Y[M=1]<Y[M=0]")
```

Again we have the correct skeleton and knowldge of timing is enough to recover the graph.

Finally we consider the model in which $Y$ has two causes that do not influence each other. Again we impose monotonicity, draw data, and try to recover the model:


```{r, include = FALSE}
model3 <- make_model("X1 -> Y <- X2") %>% 
  set_restrictions(causal_type_restrict = "(Y[X1=1, X2=.]<Y[X1=0, X2=.]) | (Y[X2=1, X1=.]<Y[X2=0, X1=.])")
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
par(mfrow = c(3,2))
plot_dag(model1); title("True model")
recover_model(model1)
plot_dag(model2); title("True model")
recover_model(model2)
plot_dag(model3); title("True model")
recover_model(model3)
```

## The possibility of identification of probative value from experimental data

Imagine we had access to infinite experimental data on the effect of $X$ on $Y$ and we want to know for a case (exchangeable with any other in this population) with $X=Y=1$, whether $X=1$ caused $Y=1$--- PC. Our experimental data can be written as a transition matrix as follows: 

$$P^{u}=\left( \begin{array}{cc} 0.50 & 0.50 \\ 0.25 & 0.75 \end{array}\right)$$
If we have prior beliefs about the distribution of causal types, then PC would be simply $\frac{\lambda^u_b}{\lambda^u_b + \lambda^u_d}$. 

But let's assume we cannot justify such beliefs. In this case, from results in MURTAS REF, we can still place bounds on PC:

$$\frac13 \leq PC \leq \frac23 $$
For intuition note that  $P^{xy}$ implies a causal effect of .25 and so the lowest  value of $\lambda_b^u$ consistent with $P^{xy}$ arises when $\lambda_b^u =  .25$ and $\lambda_a^u = 0$, in which case  $\lambda_c^u = .25$ and $\lambda_d^u = .5$.    In this case  $\lambda_b^u/(\lambda_b^u+ \lambda_d^u)=\frac{1}{3}$.  The highest consistent   value of $\lambda_b^u$ arises when $\lambda_b^u =  .5$ and $\lambda_a^u = .25$, in which case  $\lambda_c^u = 0$ and $\lambda_d^u = .25$. In this case  $\lambda_b^u/(\lambda_b^u+ \lambda_d^u)=\frac{2}{3}$.

Say now we have access to auxiliary data $K$ and plan to make inferences based on $K$. 

We will suppose  first that $K$ is a moderator and second that $K$ is a mediator.

### Moderator
Consider first a situation  in which our case is drawn from a set of cases for which $X$ adn $K$ were each randomly assigned. Say then that the transition matrices, conditionl on $K$ look as follows:

$$P^{K=0}=\left( \begin{array}{cc} 0 & 1 \\ 0.5 & 0.5 \end{array}\right), P^{K=1}=\left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right)$$
In this case we can now identify PC, even before observing $K$. If $K=0$, PC is 0---there are no cases with positive effects in this condition. If $K=1$ PC = 1.  We have a prior  that $K=1$ of  .5 and after observing $X=Y=1$ we raise this to $2/3$. Thus our prior belief on $PC$ --- before seeing $K$--- is $2/3 * 1 + 1/3 * 0 = 2/3$. 

How about $\phi_{b1}$ and $\phi_{d1}$?

Here positive effects only arise when $K=1$ and so $\phi_{b1} = 1$. $Y=1$ without being cause by $X$ only if $K=0$ and so  $\phi_{b0} = 0$. Thus we have a double decisive clue.


### Mediator

Consider now a case with mediation. Say now that *in addition* we know from experimental data, that $K$ mediates the relationship between $X$ and $Y$; indeed we will assume that we have a case of complete mediation, such that, conditional on $K$, $Y$ does not depend on $X$. The transition matrices from $X$ to $K$ and $K$ to $Y$ are:

$$P^{xk}=\left( \begin{array}{cc} 1 & 0 \\ 1/2 & 1/2\end{array}\right), P^{ky}=\left( \begin{array}{cc} 1/2 & 1/2 \\ 0 & 1\end{array}\right)$$ 
Even without observing $K$, this information is sufficient to place a prior on PC of $p=\frac13$. 

To see this, note that we can calculate:

* $\lambda_a^K =0$, $\lambda_b^K = \frac{1}{2}$, $\lambda_c^K = \frac{1}{2}$, $\lambda_d^K = 0$
* $\lambda_a^Y =0$, $\lambda_b^Y=\frac{1}{2}$, $\lambda_c^Y=0$,  $\lambda_d^Y=\frac{1}{2}$

and so:

* $\lambda_b^u = \lambda_b^K\lambda_b^Y = \frac{1}4$
* $\lambda_d^u = \lambda_d^Y$ 
* $p = \frac{\lambda_b^u}{\lambda_b^u + \lambda_d^u} = \frac{1}3$.

whence:

* $\phi_{b1} = 1$
* $\phi_{d1} = \lambda_d^K + \lambda_b^K = \frac{1}{2}$

### Generally not so easy

Thus we can *in principle* calculate the $\phi$s from experimental data given a mediation process. In this case we get a strong hoop test. 

Note that this agrees with the calculation of $PC|K$ in MURTAS REF, which does not make use of a calculation  of $\phi$s.

$$PC | K = 1 = \frac{\phi_b p}{\phi_b p + \phi_d (1-p)} = \frac{1 \times\frac13}{1 \times\frac13 + \frac{1}{2} \times \frac23}  = \frac{1}{2}$$


## Bounds on causes of effects

We are interested in quantities such as $\phi_{b1}, \phi_{d1}$---the probability that $K=1$ given $X$ caused $Y$ and $X=Y=1$ in a model with mediation.  [Equation](#eq:phis_from_lambdas) gave expressions for this when the $\lambda$s were known. 

What if they are not?

In this case it can be possible to calculate the bounds on $\phi_{b1}, \phi_{d1}$. Consider a chain with transition matrices $P(\tau_1, \sigma_1), P(\tau_2, \sigma_2)$.  We are interested in:

$$\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + \lambda_{a}^K\lambda_{a}^Y}$$


Noting that $\tau_j = \lambda_{b_j} - \lambda_{a_j}$, and $\tau$ is known.

$$\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + (\lambda_{b}^K-\tau_1)(\lambda_{b}^Y - \tau_2)}$$
which we can see is decreasing in $\lambda_{b_j}$ (this may seem counterintuitive, but hte reason is that with $\tau_j$ fixed, lower $\lambda_{b_j}$ also means lower $\lambda_{a_j}$ which means less ambiguity about *how* $X$ affects $Y$ (ie through positive or negative effects on $K$).

 <!-- $$\phi_{1} = \frac{\lambda_{b}^Y}{2\lambda_{b}^Y -\tau_2 - \tau_1(\lambda_{b}^Y - \tau_2)/\lambda_{b}}^K$$ -->

The lowest permissible value of  $\lambda_{b_j}$  is $\tau_j$, yielding $\phi_1 = 1$. 

The highest value obtainable by $\lambda_{b_j}$ is when $\lambda_{a_j} = \frac{1-\tau_j+\rho_j}2$ and so $\lambda_{b_j} = \frac{1+\tau_j+\rho_j}2$. 

In this case:
$$\phi_{b1} = \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2) + (1-\tau_1+\rho_1)(1-\tau_2+\rho_2)}= \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau}$$

And so:

$$\frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau_2} \leq \phi_{b1} \leq 1$$
These are the bounds on $\phi_{b1}$. We can calculate bounds on $\phi_{d1}$ in a similar way---though of course these are not independent. 

```{r, echo = FALSE, fig.width = 8}
markov  <- function(t, r) matrix(c(t-r+1, 1-t-r, 1-t+r, 1+t+r)/2, 2, 2)

divide <- function(t, r, t1, r1) {
   if((abs(r1) > 1-t1) | (t1 < t)) return(NA)
   c(t2 = t/t1,
   r2 = r - r1*t/t1)}

types_given_matrix <- function(t=.25, r=.25, fine = 5) {
  maxa <- (1-t-abs(r))/2
  data.frame(a = seq(0, maxa, length = fine)) %>%
    mutate(b = a + t, c = (1-t-r)/2  - a, d = (1-t+r)/2  - a) 
}

types_given_matrices <- function(t1,r1,t2,r2,fine){
  
  maxa1 <- (1-t1-abs(r1))/2
  mina1 <- -min(0, t1)
  maxa2 <- (1-t2-abs(r2))/2
  mina2 <- -min(0, t2)

  data.frame(a1 = rep(seq(mina1, maxa1, length = fine), fine)) %>%
    mutate(b1 = a1 + t1, c1 = (1-t1-r1)/2  - a1, d1 = (1-t1+r1)/2  - a1,
           a2 = rep(seq(mina1, maxa2, length = fine), each = fine),
           b2 = a2 + t2, c2 = (1-t2-r2)/2  - a2, d2 = (1-t2+r2)/2  - a2,
          )
  }

bounds_given_matrices <- function(t1,r1,t2,r2,fine){
  types_given_matrices(t1,r1,t2,r2,fine) %>% 
    mutate(
  phi_b = (b1 * b2)/(a1*a2 + b1*b2),
  phi_d = (d2 * (b1 + d1) + d1*b2)/(d2 + c1*a2 + d1*b2))
 }

plot_bounds <-  function(t1,r1,t2,r2,fine, main = "bounds"){
  bounds <- bounds_given_matrices(t1,r1,t2,r2,fine)
  plot(bounds$phi_d, bounds$phi_b, xlim = c(0,1), ylim = c(0,1), cex = .5, main = main, 
       xlab = expression(phi[d]), ylab = expression(phi[b]))
  abline(0,1)
  }

par(mfrow = c(1,2))
plot_bounds(.2, -.6, .2, .6, 20, main = expression(paste("Doubly decisive: ", tau[1], "= .2, ",
                                                            rho[1], "= -.6, ",
                                                            tau[2], "= .2, ",
                                                            rho[2], "= .6")))

plot_bounds(.6, -.2, .6, .2, 20, main = expression(paste("Hoop: ", tau[1], "= .6, ",
                                                            rho[1], "= -.2, ",
                                                            tau[2], "= .6, ",
                                                            rho[2], "= .2")))

plot_bounds(0.0, -.9, .0, -.9, 100, main = expression(paste("Smoking gun: ", tau[1], "= 0, ",
                                                            rho[1], "= -.9, ",
                                                            tau[2], "= 0, ",
                                                            rho[2], "= -.9, ")))

```


```{r, eval = FALSE, echo = FALSE}
# An odd one!
plot_bounds(.02, -.5, -.10, -.5,600)
```


$$\phi_{d1} = \frac{\lambda_{b}^K\lambda_{d}^Y}{(\lambda_{a}^K + \lambda_{b}^K + \lambda_{c}^K)\lambda_{d}^Y+ \lambda_{c}^K\lambda_{a}^Y}$$


For the smoking gun,  $\phi_{b1}$ is .5 because $\lambda_a^j = \lambda_b^j$  exactly;  $\phi_{d1}$ might be low as $d$ types mostly arise because of $c$ types in the first step and $a$ types in the second.
.


this is achived with a low value of $\lambda_{d}^Y$

Whether the bounds map into useful probabitve value depends in part on whether causal effects are better identified in the first or the second stage.

```{r, echo = FALSE, fig.width = 7}

par(mfrow = c(1,2))

plot_bounds(0, 0, .25, .25, 20, main = expression(paste("Weak first stage: ", tau[1], "= 0, ",
                                                            rho[1], "= 0, ",
                                                            tau[2], "= .25 ",
                                                            rho[2], "= .25")))

plot_bounds(.25, .25, 0, 0, 20, main = expression(paste("Weak second stage: ", tau[1], "= .25, ",
                                                            rho[1], "= .25, ",
                                                            tau[2], "= 0, ",
                                                            rho[2], "= 0")))

```


## Qualitative beliefs and Sensitivity Analyses

A first strategy to address concerns around weak foundations for claims on probative value---even after attempting to introduce information on uncertainty around probative value, is to assess the extent to which conclusions depend on the details of information provided. In some cases conclusions may be very sensitive to details provided; in other cases not so much. 

For a simple illustration, say one's prior is that it is equally likely that a case is a $B$ or $D$ type. Say one thinks that $\phi_D=0$ and so seeing a clue provides a smoking gun test. Then conditional on seeing the clue, the conclusion is the same no matter what the actual value of $\phi_B$---as long as $\phi_B>0$. The details do not matter in this (albeit extreme case).  In this case if a clue is not see then the conclusions can depend quite strongly on the value of $\phi_B$ however: the posterior could range from $0.5$ in the case where $\phi_B=0$ (and so the clue is not informative at all) to $0$ in the case where $\phi_B = 1$ (and so the clue is double decisive). Thus how sensitive conclusions are to beliefs depend very much on the  data at hand.

For a larger $n$ illustration, refer back to our example of large $n$ data  in chapter 9 where we saw that when there is confidence around assignment probabilities (as for example in an RCT) then, with large $N$, conclusions do not depend strongly on clue information about a handful of cases, no matter what the probative value.  

More generally there is a literature on probabilistic causal models that assess the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see @parsons2001qualitative. Applied to the example above we might imagine a researcher willing to say that they think $\phi_B$ is not plausibly  greater than .5,  but unwilling to make a statement about their beliefs about where in the $0$ to $0.5$ range it lies. In this case, with the other parameter values outlined above the conclusions will lie in $0.5$ and $0.33$---ruling out half the possible ranges on the parameter results in a ruling out of a two thirds of the possible range of the conclusion. 

## Conditional claims
A second response is to clearly communicate the contingent nature of claims. As we outlined in Chapter 4, some causal models might reasonably reflect actual beliefs about the world---for example one might,  be convinced that a treatment was randomly assigned, that there is no interference, and that units are independently sampled from a distribution of types. All of these beliefs may be unwise. But if help, then the simple DAG in chapter 4 (REF) can be taken to represents beliefs about the world rather than a model of the world, in the sense of a simplified representation. But as we noted in Chapter 4, for an even modestly more complex situation, it seems inevitable that the model being used is truly a model and not a faithful summary of beliefs. 

Owning the model in this way results in a useful reposing of the question: the question becomes not whether the assumptions are correct but whether the model is useful [@clarke2012model]. That is the subject of Chapter 13.

## Learning about parameters within a model

The general approach outlined in the literature on probabilistic graph allows for considerable flexibility on what is fixed and what is not. As we described in Chapter 2, any seemingly fixed feature of a structural equation can become a quantity of interest in a lower level model. In our baseline model^[Some individuals are observed to have received a treatment ($X$) while others have not. Assume that, subsequently, a researcher observes outcomes ($Y$) for all units. Assume that each unit belongs to one of four unobserved ''types,'' $A$ (adverse), $B$ (beneficial), $C$ (chronic), $D$ (destined) with potential outcomes $Y(0)=1, Y(1)=0$ for type $A$; $Y(0)=0, Y(1)=1$ for type $B$; $Y(0)=0, Y(1)=0$ for type $C$; and $Y(0)=1, Y(1)=1$ for type $D$.  As in the canonical model,  researchers have access to data on a third variable, $K$ and we let $\phi_{tx}$ denote the probability of observing $K$ for type $t$ given $X=x$.] for instance the probative value of clues, as captured by terms $\phi_{tx}$, are parameters over which researchers update; they do not uniquely derive from external information.  

How much external information has to be brought in to learn from clues? Does any external information need to be brought in? We address the question by considering  situations where researcher have *uninformative* priors about treatment effects *and* over the probative value of clues. We simplify the problem by assuming that assignment probabilities are known (specifically we set these at 0.5 for all units) and we assume that there is infinite data available for analysis. Note that under these conditions the average treatment effect ($b-a$) can be estimated with certainty using information about $X$ and $Y$ only. 

We are interested in whether data on clues, $K$ can be used to make estimates of the size of the causal types, $a,b,c,d$ and whether the data allows us to estimate the probative value of clues. Let $\theta$ denote our parameter vector containing causal types, $(\lambda)$ and probative values $\phi_{tx}$.   

We find that when priors over clue probabilities do not discriminate between causal types, then learning clue values does not affect learning over other parameters when $n=1$. 


## Learning from observational and experimental mixtures

## Learning across populations
Now consider strategies to learn about clues from observing patterns in different populations under an assumption that clue patterns travel across cases even if causal types differ. We consider a  population independence assumption ("population invariant probative values"") on the $\phi$ values, we show that data from multiple populations can allow for both tighter assessment of $\phi$ values and identification of fundamental causal parameters.

A consideration of heterogeneous treatment effects communicates the basic idea. Consider a large randomized trial in some population where it is found that a treatment is effective in  subgroup $K$ of subjects but not among others. Then we might think that $K$ is a marker for a $B$ type.

As a more concrete illustration consider  first the data from "Population  1", given in table \ref{S1}. From this population we see that treatment is assigned in half of all cases and that outcomes are equally likely to be positive or negative, independent of treatment status.

\begin{table}[h!]
\centering
\begin{tabular}{c|cc}
           &        $Y=0$ &        $Y=1$ \\ \hline
       $X=0$ &     25\% of cases &     25\% of cases  \\
			 & No clues & No clues \\ \\
       $X=1$ &     25\% of cases &     25\% of cases  \\
			 & No clues & $K=1$ in 50\% \\
			 &  & of these cases\\
\end{tabular}  
\caption{Population 1}
\label{S1} 
\end{table}

Note that for or any $b$ in $[0,0.5]$ the observed $X,Y$ data are consistent with $a = b$, $c = d = .5 - b$. Thus although the average treatment effect is 0, the share of units for which there is a positive treatment effect could range anywhere between 0 and .5.

Note that $K$ is only observed when $X=Y = 1$. This fact provides a lot of information on $\phi$; in particular: 
$\phi_{j0} = 0$ for all $j$. Moreover $\phi_{a1} = 0$ and $\phi_{c1} = 0$. The only positive possibilities are $\phi_{b1}$ and $\phi_{d1}$. 

Unfortunately however, constraining $\phi$ in this way does nothing to better estimate causal quantities. 

Since the share of cases for which $K$ is observed in $1/8$, the observed data impose the following constraint on $b$, $\phi_{b1}$, $\phi_{d1}$:
\begin{equation}
.5b q_{b1}  + .5d q_{d1}  = 1/8 \label{C1}
\end{equation}
Substituting for $d$ we have:
$$2b \phi_{b1} +(1-2b) q\phi_{d1} = 1/2$$ \label{CC1}

$$b = \frac{.25 - .5\phi_{d1}}{\phi_{b1} - \phi_{d1}}$$ \label{CC2}

Note that from this condition, that if we knew $\phi_{b1}$ and $\phi_{d1}$ then we could figure out $b$ exactly.
If only one of these is known then we can figure $b$ only up to some range. If neither is known then *any* value of $b$ in $(0,.5)$ is consistent with the data.^[For example, for $b \in [1/4,1]$ set $\phi_{d1}=0$ and $\phi_{b1} = 1/(4b)$ otherwise set $\phi_{b1}=0$ and $\phi_{d1} = 1/(2(1-2b))$] 

That is the bad news: what we learn about $\phi$ does nothing to pin down causal effects of interest: for any stipulated causal effect there is a belief about the probative value of clues that is consistent with it. 

Nevertheless although the learning on $\phi$ does not rule out any values on $a,b,c,d$ for any given population, learning is possible across populations, at least under the assumption that $\phi$ is invariant to population. Assume specifically that the distribution of types varies across populations but that the values of $\phi$ conditional on type is constant. Thus for this illustration, the data table for Population 2, Table \ref{S2} is identical to that for population 1 except for the observations on $K$.

\begin{table}[h!]
\centering
\begin{tabular}{c|cc}
           &        $Y=0$ &        $Y=1$ \\ \hline
       $X=0$ &     25\% of cases &     25\% of cases  \\
			 & No clues & No clues \\ \\
       $X=1$ &     25\% of cases &     25\% of cases  \\
			 & No clues & $K=1$ in 10\% \\
			 &  & of these cases\\
\end{tabular}  
\caption{Population 2}
\label{S2} 
\end{table}



```{r, fig.cap="\\label{fig:somethingfig} Combinations of $\\phi_{b1}$, $\\phi_{d1}$ and $b$ values consistent with data from three populations. Populations are assumed to differ in the sizes of groups $A,B,C,D$ but not in the $\\phi$ values. Furthermore it is assumed in this illustration that observed data is identical across populations with respect to $X$ and $Y$ but differs with respect to $K$.", echo=FALSE, fig.align='center', fig.width=8, fig.height=8}

f = function(qb1, qd1, sb=.5)  (sb -qd1)/(qb1*2 - qd1*2)

s 	= seq(0,1,.05)
qd 	= as.vector(sapply(s, function(i) rep(i, length(s))))
qb 	= rep(s, length(s))

b = as.vector(sapply(s, function(i) f(s, i)))
bleg = b>=0 & b<=.5 


b2 = as.vector(sapply(s, function(i) f(s, i, sb = .1)))
bleg2 = b2>=0 & b2<=.5 

b3 = as.vector(sapply(s, function(i) f(s, i, sb = .9)))
bleg3 = b3>=0 & b3<=.5 

par(mfrow=c(2,2))
plot(qd,qb, type = "n", main = "Possible values for b and q given data 1")
text(qd[bleg],	qb[bleg], round(b,2)[bleg],cex=.8)

plot(qd,qb, type = "n", main = "Possible values for b and q given data 2")
text(qd[bleg2],	qb[bleg2], round(b2,2)[bleg2],cex=.8)

plot(qd,qb, type = "n", main = "Possible values for b and q given data 3")
text(qd[bleg3],	qb[bleg3], round(b3,2)[bleg3],cex=.8)

plot(qd,qb, type = "n", main = "Possible values for b and q in population 1 given data 1,2,3")
text(qd[bleg & bleg2 & bleg3],	qb[bleg & bleg2 & bleg3], round(b,2)[bleg & bleg2 & bleg3],cex=.8)
```

The top left panel of figure \ref{fig:somethingfig} summarizes the learning that is possible from the first population. The axes indicate possible values of $\phi_{b1}$ and $\phi_{d1}$; the numbers marked inside the figure are the possible values of $a$ implied by these values. Note that values are marked only when they collectively satisfy the constraint given in Equation \ref{CC1}. 


Key features of the graph are that both $\phi_{d1}$ and $\phi_{b1}$ span the whole range between 0 and 1: that is, the constraint does not limit the  range of either of these on their own. Second, values of $b$ range from 0 to 0.5: thus the constraint does not rule out any value for $b$ not already determined by $X$, $Y$ data alone.

However the combinations of possible values are clearly strongly constrained and these combinations depend on the data.

The second and third figures show the analogous set of constraints for two more populations that are identical to the first except that $K$ is observed in very few of the $X=Y=1$ cases in the second population and in very many of the $X=Y=1$ cases in the third. Under the assumption that $\phi$ is invariant to population, the feasible values of $\phi$ consists of those values that are admissible in *all* populations. These values are shown in the bottom right figure (they can in fact be identified by considering the intersection of the admissible values from any two of the populations).

From the bottom right figure we learn two things: first, although we have now greatly constrained the set of possible values of $\phi$, quite distinct values remain possible.  Secondly, whatever the true values of $\phi$ we have in the final figure, we have tightly limited the possible values of $b$, which we now believe to be approximately 0.25.

The intuition for this result is the following. From population 2 we learn that $\phi$ cannot be high for both $b$ and $d$ types, it must be low for one or the other or both. From  population 3 we learn that $\phi$ cannot be low for both types; it must high for one or the other or both. Together these imply that $\phi$ must be high for either $b$ or $d$ and low for the other. However since in population 1 there is a middling level of $K$ then there must be a middling frequency of $b$s and $d$s.

Note finally that in this example, our learning on the level of $b$ in populations 2 and 3 is less precise:  we learn only that $b$ is either very high or very low, and that it is *not* middling in these populations.



<!--chapter:end:14-somethingfromnothing.Rmd-->

# Robustness and Model-Evaluation {#evaluation}

***

Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good.

***


```{r, include = FALSE}
source("_packages_used.R")
```


Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these may be wrong. 

## Tools for evaluating models

DO ALL THIS WITH PIMD APPLICATION

Check conditional independencies

- Show for whether there's a direct effect of X on Y

Check confounding assumptions approach 2 -- say actual confound is q~=0; but model  assumes q = 0. Draw data from priors, draw data; given data type (001, 100 etc) plot (a) the posterior distribution under no confounding nad (b) the distribution of estimands that gave rise to the data.  

Change qual to quant priors in PIMD model

- A graph showing how some conclusions changes as we relax one of the restrictions.

How much do our conclusions depend on qual restrictions?

- How do conclusions differ if we drop all restrictions

Check fit

<!-- - Generate equivalent of an R^2. Using posterior, make predictions for all the cases, say, on Y. How much lower is the error from that prediction than from the  -->

Compare out of sample fit between model with posteriors and priors. 

Reality checks -- posterior fits. Predict data from the posterior. Look at whether the data you see are similar to the data you actually have. 

Compare likelihoods of the data under different models



## Evaluating the Democracy-Inequality model


## Prior check
In a second iteration of the analysis, we show what happens if we loosen the monotonicity restriction on $I$'s effect on $M$. Here we  consider negative effects of $I$ on $M$ *unlikely*, rather than impossible, and we consider null and positive effects somewhat likely. We refer to these priors as "quantitative priors" in the sense that they place a numerical value on beliefs rather than a logical restriction. Here, we set our prior on $U^M$ as: $p(U^M=t^M_{10})=0.1$, $p(U^M=t^M_{00})=0.25$, $p(U^M=t^M_{11})=0.25$, and $p(U^M=t^M_{01})=0.4$. We show the results for the inferences given different findings in tables \ref{tab:HK8cases1quant} and \ref{tab:HK8cases2quant}. The mapping into expected posterior variance associated with each strategy is shown by the numbers in parentheses in  Table \ref{CaseLearn}.

```{r, echo = FALSE, eval = FALSE}
I0D12 <- some_results(i=0, d=1, example = example_a2)
I0D12 <- round(I0D12, 3)
kable(cases_table(I0D12,  cases = cases1), caption = "\\label{tab:HK8cases1quant} Four cases with low inequality and  democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming quantitative restrictions.")
I1D12 <- some_results(i=1, d=1, example = example_b2)
I1D12 <- round(I1D12, 3)
kable(cases_table(I1D12, cases = cases2), caption = "\\label{tab:HK8cases2} Four cases with high inequality and  democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming quantitative restrictions.")

```


The results differ in various modest ways. However, the biggest difference we observe is in the degree to which the mobilization clue matters when we are looking for negative effects of inequality. As discussed, if we assumed monotonic positive effects of inequality on mobilization and monotonic positive effects of mobilization on inequality, then the mediator clue is uninformative about the indirect pathway since that pathway can only generate a positive effect. However, if we allow for the possibility of a negative effect of inequality on mobilization, we now make $M$ informative as a mediator even when the effect of inequality that we are interested in is negative: it is now possible that inequality has a negative effect on democratization via a negative effect on mobilization, followed by a positive effect of mobilization on democratization. So now, observing whether mobilization occurred adds information about whether a negative effect could have occurred via the mobilization pathway. 

Moreover, it is possible for the two effects of observing $M$ on our beliefs to work in opposite ways. What we learn from observing $M$ about the $I \rightarrow M \rightarrow D$ pathway may push in a different direction from what we learn from observing $M$ about the direct $I \rightarrow D$ pathway. We see this dynamic at work in a case with low inequality and democratization. Where we are only learning about $M$ as a moderator of $I$'s direct effect (monotonicity assumption in place), observing $M=0$ shifts our beliefs in favor of $I$'s negative effect. But where we are learning about $M$ as both mediator and moderator, observing $M=0$ shifts our beliefs *against* $I$'s negative effect. The reason for this latter result is straightforward: if $I=0$ and we then see $M=0$, then we have just learned that inequality's possible indirect negative effect, running via the mobilization pathway, has *not* in fact occurred; and this has a considerable downward effect on our beliefs in an overall negative effect of inequality. This learning outweighs the small positive impact of observing $M=0$ on our confidence that $I$ had a direct negative effect on $D$.

We see these differences most clearly in the cases of Albania (as compared to Mexico) and Nicaragua (as compared to Taiwan). Under priors fully constrained to monotonic causal effects, we saw that the mediator clue, $M$, made only a small difference to our inferences. However, if we allow for a negative effect of $I$ on $M$, even while believing it to be unlikely, observing mobilization in Albania and Nicaragua makes us substantially more confident that inequality mattered, and differentiates our conclusions about these cases more sharply from our conclusions about Mexico and Taiwan, respectively. 


## Monotonic restrictions

Compare fit between model with and without monotonic restrictions


<!--chapter:end:15-Justifying-Models.Rmd-->

# Final Words

Looking forward to this bit.


## Some conclusions


* You generally cannot conclude all that much about population quantiries from only a small number of cases

## Words of warning

* WHen DAGS seem hairy
timing not clear
dominos worry


## General and specific knowledge

Sometimes  understanding and applying teh causal model is enoug, without need for additional qualitative data.


* We started with the story of the engineer and  the philosopher.


Do DAGs actually capture causal processes that qualitative researchers see -- qualitative researchers see that the domino 2 fell *the moment it was hit* by domino 1. How do we express this in a DAG?

<!--chapter:end:16-conclusion.Rmd-->

# (PART) Appendices {-}

# Analysis of canonical models with `gbiqq`  {#examplesappendix}


```{r, include = FALSE}
source("_packages_used.R")
# do_diagnosis <- TRUE
if(do_diagnosis & !exists("fit")) fit <- gbiqq::fitted_model()
```

***

We walk through  a set of canonical models and show how to define and analyze them using `gbiqq`.

***

## $X$ causes $Y$, no confounding

In the simplest  $X$ causes $Y$ model the ATE is identified but the "probability of causation" (PC) is not: we can however generally place bounds on PC.

The model can be written:

```{r}
model <- make_model("X -> Y")
plot_dag(model)
```

This sparse definition assumes that there is no confounding and no constraints on the ways $X$ relates to $Y$.

You can see the parameter matrix, which confirms this, showing the mapping from parameters to causal types:


```{r, eval= FALSE, echo = FALSE}
get_parameter_matrix(model)
```

```{r, echo = FALSE}
kable(get_parameter_matrix(model), caption = "Parameter matrix for X causes Y model without confounding")
```

We can simulate  data using the bare bones model and assuming a "true model" in which there is a true positive effect of 0.5.

```{r}
data <- simulate_data(model, n = 1000, 
                      parameters = c(.5, .5, .2, .1, .6, .1))
```

The kinds of inferences on the probability that $X$ has a positive effect on  $Y$ given different data is calculated as follows:

```{r, message = FALSE, warning = FALSE, eval = FALSE}
updated <- gbiqq(model, data)
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq(model, data, refresh = 0), "saved/appendix_XY_model_1.rds")
  }
updated <- read_rds("saved/appendix_XY_model_1.rds")
```


We can then ask questions about particular estimands like this:

```{r}
ATE <- "Y[X=1] - Y[X=0]"
PC  <- "Y[X=1] > Y[X=0]"

results <- gbiqq::query_model(
 updated,
 queries = list(ATE = ATE, ATE = ATE, PC = PC, PC = PC),
 using = list("priors", "posteriors"))
```

```{r, echo = FALSE}
kable(results, digits = 2)
```

We see from the posterior variance on PC that PC is not identified (or more precisely they key feature is that this distribution does not tighten even with very large N).  For more intuition we graph the posteriors:



```{r, echo = FALSE} 
if(do_diagnosis){
ATE_dist <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "Y[X=1] - Y[X=0]"
                   )
PC_dist <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "Y[X=1] > Y[X=0]",
                   subset = "X==1 & Y==1"
                   )
write_rds(ATE_dist, "saved/appendix_ATE_dist.rds")
write_rds(PC_dist, "saved/appendix_PC_dist.rds")
}
ATE_dist <- read_rds("saved/appendix_ATE_dist.rds")
PC_dist  <- read_rds("saved/appendix_PC_dist.rds")

par(mfrow = c(1,2))
hist(ATE_dist, xlim = c(-1,1), main = "Posterior on ATE")  
hist(PC_dist, xlim = c(0,1), main = "Posterior on PC")  
```


We find that they do not converge but they do place positive mass in the right range. Within this range, the shape of the posterior depends on the priors only. 


## $X$ causes $Y$, with unmodelled confounding

An $X$ causes $Y$ model with confounding can be written:

```{r}
model <- make_model("X -> Y") %>%
         set_confound(list(X = "(Y[X=1]>Y[X=0])", X = "(Y[X=1]<Y[X=0])", X = "(Y[X=1] ==1)"))
plot_dag(model)
```


The parameter matrix here has more parameters than nodal types, reflecting the conditional assignment probabilities of $X$ -- $X$ can have different assignment probabilities for different nodal types for $Y$.

```{r, eval= FALSE, echo = FALSE}
get_parameter_matrix(model)
```


```{r, echo = FALSE}
kable(get_parameter_matrix(model), caption = "Parameter matrix for X causes Y model with arbitrary confounding")
```

With the possibility of any type of confounding, the best we can do is place "Mansky bounds" on the average causal effect. 

To see this, let's plot a histogram of our posterior on average causal effects, given lots of data:

```{r}
data <- simulate_data(
    model, n = 1000, 
    parameters = c(.5, .5, .5, .5, .5, .5, .5, .5, .1, .1, .7, .1))
```


```{r, message = FALSE, warning = FALSE, eval = FALSE}
updated <- gbiqq(model, data, refresh = 0)
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq(model, data, refresh = 0), "saved/appendix_XY_model_2.rds")
  }
updated <- read_rds("saved/appendix_XY_model_2.rds")
```

```{r, echo = FALSE}
prior_ate <- query_distribution(updated, "c(Y[X=1] - Y[X=0])", using = "priors")
post_ate <- query_distribution(updated, "c(Y[X=1] - Y[X=0])", using = "posteriors")
par(mfrow= c(2,1))
    hist(prior_ate, xlim = c(-1,1), main = "Prior")
    hist(post_ate, xlim = c(-1,1), main = "Posterior")
```

The key thing here is that the posterior on the ATE has shifted, as it should, but it is not tight, even with large data. In fact the distribution of the posterior covers one unit of the range between -1 and 1.

## $X$ causes $Y$, with confounding modelled

Say now we have a theory that the relationship between $X$ and $Y$ is confounded by unobserved variable $C$. Although $C$ is unobserved we can still include it in the model and observe the confounding it generates by estimating the model on data generated by the model but assuming that we cannot observe $C$.

```{r, message = FALSE}
model <- make_model("C -> X -> Y <- C") %>%
         set_restrictions(causal_type_restrict = "(Y[X=1, C=.] < Y[X=0, C=.]) | (Y[C=1, X=.] < Y[C=0, X=.]) | (X[C=1] < X[C=0])") %>%
         set_parameters(type = "prior_mean")  
```

The ATE estimand in this case is given by:

```{r, echo = FALSE}
result <- gbiqq::query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    using = "parameters")

kable(result)
```

In the first column below we run a  regression using data generated from this model but with $C$ unobserved. The second column shows what we would estimate if were able to observe $C$.

```{r, echo = FALSE, results='asis'}
data <- simulate_data(model, n = 10000)

stargazer::stargazer(lm(Y~X, data = data), lm(Y~X + C, data = data), header = FALSE, type = 'html')
```

Our posteriors over the effect of $X$ on $Y$ and the effect of the unobserved confounder ($C$) on $Y$ have a joint distributed with negative covariance. 

To illustrate we will use the same data but assume priors from  model where we do not restrict the relationship between $C$ and $Y$  and show the joint distribution of our posteriors.

```{r}
model <- make_model("C -> X -> Y <- C")  %>%
         set_restrictions(causal_type_restrict = "(X[C=1] < X[C=0])") 
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq::gbiqq(model, data, stan_model = fit, refresh = 0), "saved/appendix_modelled_confound.rds")
  }
updated <- read_rds("saved/appendix_modelled_confound.rds")

ate <- query_distribution(updated, "c(Y[X=1] - Y[X=0])", using = "posteriors")
confound <- query_distribution(updated, "c(Y[C=1] - Y[C=0])", using = "posteriors")
```

```{r, echo = FALSE}
plot(ate, confound)
abline(lm(confound~ate), col = "red")
```


## Simple mediation model

We define a simple mediation model and illustrate learning about  whether $X=1$ caused $Y=1$ from observations of $M$.

```{r}
model <- make_model("X -> M -> Y") %>%
         set_confound(confound = list(X = "M[X=1]==1")) %>%
         set_parameters(c(.5, .5, .2, .8, 
                          .2, 0, .8, 0, 
                          .2, 0, .8, 0))
```


```{r}
plot_dag(model)
```

Data and estimation:

```{r}
data <- simulate_data(model, n = 1000, using = "parameters")
```

```{r, message = FALSE, eval = FALSE}
updated <- gbiqq(model, data)
```

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq(model, data, refresh = 0), "saved/appendix_XMY_simple.rds")
  }
updated <- read_rds("saved/appendix_XMY_simple.rds")
```

```{r}
result <- gbiqq::query_model(
    updated, 
    queries = list(COE = "c(Y[X=1] > Y[X=0])"), 
    subsets = c("X==1 & Y==1", "X==1 & Y==1 & M==0", "X==1 & Y==1 & M==1"),
    using = "posteriors")

```

```{r, echo = FALSE}
kable(result)
```

Note that observation of $M=0$ results in a 0 probability for the posterior that $X$ caused $Y$, while observation of $M=1$ has only a modest positive effect. The mediator thus provides a hoop test for the proposition that $X$ caused $Y$.

## Simple moderator model

We define a simple  model with a moderator and illustrate how updating about COE is possible using the value of  a mediator as a clue.


```{r}
model <- make_model("X -> Y; Z -> Y") 
plot_dag(model)

```


```{r}
data <- simulate_data(
    model, n = 1000, 
    parameters = c(.5, .5, .5, .5, 
                   .02, .02, .02, .02, .02, .02, .02, .02,
                   .02, .70, .02, .02, .02, .02, .02, .02))
```

```{r, message = FALSE, eval  = FALSE}
posterior <- gbiqq(model, data)
```

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq(model, data), "saved/appendix_mod_simple.rds")
  }
updated <- read_rds("saved/appendix_mod_simple.rds")
```


```{r}
result <- gbiqq::query_model(
    updated, 
    queries = list(COE = "Y[X=1] > Y[X=0]"), 
    subsets = list("X==1 & Y==1", "X==1 & Y==1 & Z==0", "X==1 & Y==1 & Z==1"),
    using = "posteriors")

```

```{r, echo = FALSE}
kable(result)
```

As an exercise, define a model where, learning about a model with moderators allows you to tighten bounds on COE even without observing the value of the mediator.

## An IV model

We define a simple mediation model and illustrate learning about whether $X=1$ caused $Y=1$ from observations of $M$.

```{r}
model <- make_model("X -> M -> Y")  %>%
         set_confound(confound = list(M = "Y[M=1]==1")) 

plot_dag(model)
```


```{r, echo = FALSE}
pars <- c( .1, .2, .6, .1,
           .5, .5,
           .1, .1, .7, .1, 
           .1, .1, .7, .1)

data <- simulate_data(model, n = 1000, parameters = pars)
```

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  updated <- gbiqq(model, data)
  write_rds(updated, "saved/appendix_IV_simple.rds")
  }
updated <- read_rds("saved/appendix_IV_simple.rds")
```


```{r, echo = TRUE}
result <- gbiqq::query_model(
    updated, 
    queries = list(ATE = "c(Y[M=1] - Y[M=0])"), 
    subsets = list(TRUE, "M[X=1] > M[X=0]",  "M==0",  "M==1"),
    using = "posteriors")

```

```{r, echo = FALSE}
kable(result)
```

We calculate the average causal effect for all and for the compliers and conditional on values of $M$.

## A model that allows application of the frontdoor criterion

## A model with a violation of sequential ignorability

## Learning from a collider

Pearl describes a  model similar to the following as case for which controlling for covariate $W$ induces bias in the estimation of the effect of $X$ on $Y$, which could otherwise be esimated without bias.

```{r}
model <- make_model("X -> Y <- S; S -> W") %>%
         set_confound(list(X = "W[S=1]==1")) %>%
         set_parameters(c(.1, .9, 
                          .5, .5, 
                          .9, .1, 
                          .1,.1,.7,.1,
                          .2, 0,0,0, 0,0,0,0, .6,0,0,0, 0,0,0,.2)) 

plot_dag(model)

data <- simulate_data(model, n =  20000)

```

The true effect of $X$ on $Y$ is .3 but the PC is quite different for units with $W=0$ and W=1$/

```{r, echo = FALSE, message = FALSE}

kable(
  gbiqq::query_model(model, queries = list(ATE = "Y[X=1] - Y[X=0]"), 
              using = "parameters",
              subsets = list(
                TRUE, 
                "X==1 & Y==1",
                "X==1 & Y==1 & W==0",
                "X==1 & Y==1 & W==1")))
```

Priors are:

```{r, echo = FALSE, message= FALSE}
kable(
  gbiqq::query_model(model, queries = list(ATE = "Y[X=1] - Y[X=0]"), 
              using = "priors",
              subsets = list(
                TRUE, 
                "X==1 & Y==1",
                "X==1 & Y==1 & W==0",
                "X==1 & Y==1 & W==1")))
```

The ATE can be gotten fairly precisely in a simple regression. But controlling for $W$ introduces bias:

```{r}
summary(lm(Y~X, data = data))$coef
summary(lm(Y~X+W, data = data))$coef
```

How does the Bayesian model do, with and without data on $W$?

```{r, echo = FALSE}
data_no_W <- data
data_no_W$W <- NA
```

```{r, eval = FALSE, echo = FALSE}
updated1 <- gbiqq(model, data)
updated2 <- gbiqq(model, data_no_W)
```

Inferences that do not use $W$ get ATE  right on average, but PC is not identified and statements about PC conditional on $W$ are not possible:

```{r, echo = FALSE}
if(do_diagnosis){
  updated1 <- gbiqq(model, data)
  write_rds(updated1, "saved/appendix_collider1.rds")
    updated2 <- gbiqq(model, data_no_W)
  write_rds(updated2, "saved/appendix_collider2.rds")

  }
updated1 <- read_rds("saved/appendix_collider1.rds")
updated2 <- read_rds("saved/appendix_collider2.rds")

```


```{r, echo = FALSE}
kable(gbiqq::query_model(updated2, 
              queries = "Y[X=1] - Y[X=0]", 
              using = "posteriors",
              subsets = list(
                TRUE, 
                "X==1 & Y==1",
                "X==1 & Y==1 & W==0",
                "X==1 & Y==1 & W==1")))

```

We see  including the collider does not induce error in estimation of the ATE, even though it does in a regression framework. It provides an ability to make different PC case level claims given W, but these are nevertheless far off in this example because we still do not have identification. 


```{r, echo = FALSE}
kable(gbiqq::query_model(updated1, queries = "Y[X=1] - Y[X=0]", 
              using = "posteriors",
              subsets = list(
                TRUE, 
                "X==1 & Y==1",
                "X==1 & Y==1 & W==0",
                "X==1 & Y==1 & W==1")))
```





## A model mixing observational and experimental data


We imagine that node $R$ indicates whether a unit was assigned to be randomly assigned to treatment assignment ($X=Z$ if $R=1$) or took on its observational value ($X=O$ if $R=0$). We assume the exclusion restriction that entering the experimental sample is not related to $Y$ other than through assignment of $X$. 


```{r}
model <- make_model("R -> X; O ->X; Z -> X; X -> Y") %>%
         set_restrictions(causal_type_restrict = 
                            "X[Z=1, R=1, O=0]!=1 | X[Z=0, R=1, O=0]!=0 | X[Z=1, R=1, O=1]!=1 | X[Z=0, R=1, O=1]!=0 | 
                             X[Z=1, R=0, O=0]!=0 | X[Z=0, R=0, O=0]!=0 | X[Z=1, R=0, O=1]!=1 | X[Z=0, R=0, O=1]!=1") %>%
         set_confound(list(O = "(Y[X=1]>Y[X=0])", O = "(Y[X=1]<Y[X=0])", O = "(Y[X=1] ==1)"))

plot_dag(model)
```

```{r, echo = FALSE, include = FALSE}
P <- get_parameter_matrix(model)
kable(P[,1:4])
```


The parameter matrix has just one type for $X$ since $X$ really operates here as a kind of switch, inheriting the value of $Z$ or $O$ depending on $R$. Parameters allow for complete confounding between $O$ and $Y$ by $Z$ and $Y$ are unconfounded.


```{r}
model <- set_parameters(model, c(.2, .8, 
                                 .8, .2, 
                                 .2, .8, 
                                 .8, .2, 
                                 .5, .5, 
                                 .5, .5, 
                                 1,
                                 .2, .2, .4, .2))
```



The estimands:

```{r, echo = FALSE}
result <- gbiqq::query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    subsets = list(TRUE, "R==0", "R==1"),
    using = "parameters")
kable(result)
```


The priors:

```{r, echo = FALSE}
result <- gbiqq::query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    subsets = list(TRUE, "R==0", "R==1"),
    using = "priors")
kable(result)
```

Data:


```{r}
data <- simulate_data(model, n = 600)

# Uncomment if data on $O$ is not available for cases assigned to $R=1$.
# data$O[data$R == 1] <- NA   
```


The true effect is .2 but naive analysis on the observational data would yield a srtongly upwardly biased estimate.

The gbiqq estimates are:

```{r, eval = FALSE}
posterior <- gbiqq(model, data)
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq(model, data), "saved/appendix_exp_obs.rds")
  }
updated <- read_rds("saved/appendix_exp_obs.rds")
```

```{r, echo = FALSE}
result <- gbiqq::query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    subsets = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result)
```


Did observational data improve the estimates from the experimental data?


```{r, eval = FALSE}
posterior <- gbiqq(model, data[data$R==1,])
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(gbiqq(model, data[data$R==1,]), "saved/appendix_exp_obs_2.rds")
  }
updated <- read_rds("saved/appendix_exp_obs_2.rds")
```

```{r, echo = FALSE}
result <- gbiqq::query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    subsets = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result)
```


A key quantity of interest from this model is the average effect of treatment conditional on being in treatment in the observational group. We have:

```{r, echo = FALSE}
result2 <- gbiqq::query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    subsets = list("R==1 & X==0", "R==1 & X==1", "R==0 & X==0", "R==0 & X==1"),
    using = "posteriors")

kable(result2)
```

## Transportation of findings across contexts

We study the effect of $X$ on $Y$ in country 1 and want to make inferences to country 2, Our problem however is that countries differ in terms of some feature, $W$, that is distributed differently in the two countries and that affects $Y$ via some mechanism $K$. 

[[We assume that we have an experiment in country 1 but only observational variation in country 2.]]

For instance, $X$ is cash and $Y$ is welfare. $W$ is background levels of conflict which affects welfare via security $K$, possibly differently in both countries. 

Although they differ, we have the following encompassing theory for both countries.



```{r}
model <- make_model("W -> K -> Y <- X")
```

```{r, echo = FALSE}
hj_dag(x = c(1:3, 1),
       y = c(1,1,1,0),
       names = c("W", "K", "Y", "X"),
       arcs = cbind(c(1,2,4), c(2,3,3))  
       )

```


`r if (knitr:::is_html_output()) '# References {-}'`


<!--chapter:end:17-appendix.Rmd-->

