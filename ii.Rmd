--- 
title: "Integrated Inferences"
author: Macartan Humphreys and Alan M. Jacobs
date: "Version: 19 November 2021"
documentclass: book
fontsize: 12pt
bibliography: [bib.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Bayesian causal models for integrated inferences."
github-repo: macartan/integrated_inferences
cover-image: "images/plot.png"
site: bookdown::bookdown_site
url: https\://macartan.github.io/integrated_inferences/
header-includes:
 - \usepackage{amsmath}
 - \usepackage{color}
 - \usepackage{amssymb} 
 - \usepackage{amsfonts} 
 - \usepackage{dsfont}
always_allow_html: yes
---

```{r setup, include=FALSE}

# Remove from above (was second package)
# - \usepackage{mathspec}

options(
  htmltools.dir.version = FALSE, formatR.indent = 2,
  width = 55, digits = 4, warnPartialMatchAttr = FALSE, warnPartialMatchDollar = FALSE
)
```

# Preface {-}

```{r, include=FALSE}
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'), 'packages.bib')

```

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("_packages_used.R")
```

This book has four main parts:

* Part I introduces causal models and a Bayesian approach to learning about them and drawing inferences from them.

* Part II applies these tools to strategies that use  process tracing, mixed methods, and "model aggregation."

* Part III turns to design decisions, exploring strategies for assessing what kind of data is most useful for addressing different kinds of research questions given knowledge to date about a population or a case.

* Everything up to Part IV assumes that we have access to models we are happy with. In Part IV we turn to the difficult question of model justification and outline a range of strategies on can use to justify causal models. 

We have developed an `R` package---`CausalQueries`---to accompany this book, hosted on [Cran](https://cran.r-project.org/web/packages/CausalQueries/index.html). In addition, a supplementary [Guide to Causal Models](https://macartan.github.io/causalmodels/) serves as a guide to the package and provides the code behind many of the  models used in this book. 


<!--chapter:end:index.Rmd-->

# Introduction  {#intro}

<!--
:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We describe the book's general approach, preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences, and provide a roadmap for the rest of the book.
::::
-->


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We describe the book's general approach, preview our argument for the utility of causal models as a framework for choosing research strategies and drawing causal inferences, and provide a roadmap for the rest of the book.
::::

<br>

```{r packagesused01, include = FALSE}
source("_packages_used.R")
```

Here is the key idea of this book. 

Quantitative social scientists spend a lot of time trying to understand causal relations between variables by looking across large numbers of cases to see how outcomes differ when potential causes differ. This strategy relies on variation in causal conditions across units of analysis, and the quality of the resulting inferences depends in large part on what forces give rise to that variation.  

Qualitative social scientists, like historians, spend a lot of time looking at a smaller set of cases and seek to learn about causal relations by examining evidence of causal processes in operation within these cases. Qualitative scholars rely on theories of how things work, theories that specify what should be observable within a case if indeed an outcome were generated by a particular cause.  

These two approaches seem to differ in what they seek to explain---individual-level or population-level outcomes; in the forms of evidence they require---cross-case variation or within-case detail; and in what they need to assume---knowledge of assignment processes or knowledge of causal processes. 

The central theme of this book is that this distinction, though culturally real (@goertz2012tale), is neither epistemologically deep nor analytically helpful. Social scientists can work with causal models that simultaneously exploit cross-case variation and within-case detail, that address both case-level and population-level questions, and that both depend on, and contribute to developing, theories of how things work.

We describe an approach to doing this in which researchers *form* causal models, *update* those models using data, and then *query* the models to get answers to particular causal questions. This framework is very different from standard statistical approaches in which researchers focus on selecting the best estimator to estimate a particular estimand of interest. In a causal models framework, the model itself gets updated, not the estimate: we begin by learning about processes, and only then draw inferences about particular causal relations of interest, either at the case level or at the population level.

We do not claim that a causal-model-based approach is the best or only strategy suited to addressing causal questions. There are plenty of settings in which other approaches would likely work better. But we do think that the approach holds considerable promise --- allowing researchers to combine disparate data in a principled way to ask a vast range of causal questions, helping integrate theory and empirics in a compelling way, and providing coherent guidance on research design --- and that it should have a place in the applied researcher's toolkit. 

Our goals in this book are to motivate this approach; provide an introduction to the theory of structural causal models; provide practical guidance for setting up, updating, and querying causal models; and show how the approach can inform key research-design choices, especially case-selection and data-collection strategies.

## The Case for Causal Models

There are three closely related motivations for embracing a causal models approach.
One is is a concern over the limits of design-based inference. A second is an interest in integrating qualitative knowledge with quantitative approaches.  A third is an interest in better connecting empirical strategies to theory. 


### The limits to design-based inference 

To caricature positions a bit, consider the difference between an engineer and a skeptic. The engineer tackles problems of causal inference using models: theories of how the world works, generated from past experiences and applied to the situation at hand. They come with prior beliefs about a set of mechanisms operating in the world and, in a given situation, will ask whether the conditions are in place for a known mechanism to operate effectively. The skeptic, on the other hand, maintains a critical position, resisting the importation of beliefs that are not supported by evidence in the case at hand. 

The engineer's approach echoes what was until recently a dominant orientation among social scientists. At the turn of the current century, much analysis---both empirical and theoretical---took the form of modelling processes ("data generating processes") and then interrogating those models.  

Over the last two decades, however, skeptics have raised a set of compelling concerns about the assumption-laden nature of standard regression analysis, while also clarifying how valid inferences can be made with limited resort to models in certain research situations. The result has been a growth in the use of design-based inference techniques that, in principle, allow for model-free estimation of causal effects (see @dunning2012natural, @GerGreKap04, @druckman2011experimentation, @palfrey2009laboratory among others). These include lab, survey, and field experiments and natural-experimental methods exploiting either true or "as-if" randomization by nature. With the turn to experimental and natural-experimental methods has come a broader conceptual shift, with a growing reliance on the "potential outcomes" framework which provide a clear language for thinking about causation (see @Rubin1974, @splawa1990application among others) without having to invoke fully specfied models of data-generating processes.

The ability to estimate average effects and to characterize uncertainty---for instance  calculating $p$-values and standard errors---without resort to models is an extraordinary development. In @fisher1935design's terms, with these tools, randomization processes provide a "reasoned basis for inference," placing empirical claims on a powerful footing.

Excitement about the strengths of these approaches has been mixed with various concerns regarding how the approach shapes inquiry. We highlight two. 

The first concern---raised by many in recent years (e.g., @thelen2015comparative)---is about design-based inference's scope of application. While experimentation and natural experiments represent powerful tools, the range of research situations in which model-free inference is possible is inevitably limited. For a wide range of causal conditions of interest both to social scientists and to society, controlled experimentation is impossible, and true or "as-if" randomization is absent. Moreover, limiting our focus to those questions for which, or situations in which, exogeneity can be established "by design" would represent a dramatic narrowing of social science's ken.  To be clear, this is not an argument against experimentation or design-based inference when these can be used; rather it is an argument for why social science needs a broader set of tools.

The second concern is more subtle. The great advantage of design-based inference is that it liberates researchers from the need to rely on models to make claims about causal effects. The risk is that, in operating model-free, researchers end up learning about effect sizes but not about models. Yet often the model is the thing we  want to learn about. Our goal as social scientists is to come to grips with how the world works, not simply to collect propositions about the effects that different causes have had on different outcomes in different times and places. It is through models that we derive an understanding of how things might work in contexts and for processes and variables that we have not yet studied. Thus, our interest in models is intrinsic, not instrumental. By taking models out of the equation, as it were, we limit the potential for learning about the world. 

### Qualitative and mixed-method inference

Recent years have seen the elucidation of the inferential logic behind "process tracing" procedures used in qualitative political science and other disciplines. On our read of this literature, the logic of process tracing in these accounts depends on a particular form of model-based inference.^[As we describe in @humphreys2015mixing, the term "qualitative research" means many different things to different scholars, and there are multiple approaches to mixing qualitative and quantitative methods. There we distinguish between approaches that suggest that qualitative and quantitative approaches address distinct, if complementary, questions; those that suggest that they involve distinct measurement strategies; and those that suggest that they employ distinct inferential logics. The approach that we employ in @humphreys2015mixing connects most with the third family of approaches. Most closely related, in political science, is work in  @GlynnQuinn2011, in which researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. In the present book, however, we move toward a position in which fundamental differences between qualitative and quantitative inference tend to dissolve, with all inference drawing on what might be considered a "qualitative" logic in which the researcher's task is to confront a pattern of evidence with a theoretical logic.] While process tracing as a method has been around for more than three decades (e.g., @george1985case), its logic has been most fully laid out by qualitative methodologists in political science and sociology over the last 15 years (e.g., @bennett2014process, @george2005case, @brady2010rethinking, @Hall2003aligning, @mahoney2010after). Whereas @king1994designing sought to derive qualitative principles of causal inference within a correlational framework, qualitative methodologists writing in the wake of "KKV" have emphasized and clarified process-tracing's "within-case" inferential logic: in process tracing, explanatory hypotheses are tested based on observations of what happened within a case, rather than on observation of covariation of causes and effects across cases. 

The process-tracing literature has also advanced increasingly elaborate conceptualizations of the different kinds of probative value that within-case evidence can yield. For instance, qualitative methodologists have explicated the logic of different test types ("hoop tests", "smoking gun tests", etc.) involving varying degrees of specificity and sensitivity (@collier2011understanding, @Mahony:Logic:2012).^[A smoking-gun test is a test that seeks information that is only plausibly present if a hypothesis is true (thus, generating strong evidence for the hypothesis if passed); a hoop test seeks data that should certainly be present if a proposition is true (thus generating strong evidence against the hypothesis if failed); and a doubly decisive test is both smoking-gun and hoop (for an expanded typology, see also @rohlfing2013comparative).] Other scholars have expressed the leverage provided by process-tracing evidence in Bayesian terms, moving from a set of discrete test types to a more continuous notion of probative value (@fairfield2017explicit, @BennettAppendix, @humphreys2015mixing).^[In @humphreys2015mixing, we use a fully Bayesian structure to generalize Van Evera's four test types in two ways: first, by allowing the probative values of clues to be continuous; and, second, by allowing for researcher uncertainty (and, in turn, updating) over these values. In the Bayesian formulation, use of process-tracing information is not formally used to conduct tests that are either "passed" or "failed", but rather to update beliefs about different propositions.]

<!-- ^[Note that these statements are statements about likelihood functions and do not require a specifically Bayesian mode of inference.] -->

Yet, conceptualizing the different ways in which probative value might operate leaves a fundamental question unanswered: what gives within-case evidence its probative value with respect to causal relations? We do not see a clear answer to this question in the current process-tracing literature. Implicitly---but worth rendering explicit---*the probative value of process-tracing evidence depends on researcher beliefs that come from outside of the analysis in question.* We enter a research situation with a model of how the world works, and we use this model to make inferences given observed patterns in the data --- while at the same time updating those models based on the data. 

A key aim of this book is to demonstrate the role that models can --- and, in our view, must --- play in drawing case-level causal inferences and to clarify conditions under which these models can be defended. To do so we draw on an approach to specifying causal models developed originally in computer science and that predates this work in qualitative methodology. The broad approach, described in @cowell1999probabilistic and @pearl2009causality, is consistent with the potential outcomes framework, and provides rules for updating on population and case level causal queries from different types of data.

In addition to clarifying the logic of qualitative inference, we will argue that such causal models can also enable the systematic integration of qualitative and quantitative forms of evidence. Social scientists are increasingly developing mixed-method research designs, research strategies that combine quantitative with qualitative forms of evidence. A typical mixed-methods study includes the estimation of causal effects using data from many cases as well as a detailed examination of the processes taking place in a few cases. Now-classic examples of this approach include Lieberman's study of racial and regional dynamics in tax policy (@lieberman2003race); Swank's analysis of globalization and the welfare state (@swank2002global); and Stokes' study of neoliberal reform in Latin America (@stokes2001mandates). Major recent methodological texts provide intellectual justification of this trend toward mixing, characterizing  small-$n$  and large-$n$ analysis as drawing on a single logic of inference and/or as serving complementary functions (@king1994designing; @collier2004sources). The American Political Science Association now has an organized section devoted in part to the promotion of multi-method investigations, and the emphasis on multiple strategies of inference research is now embedded in guidelines from many research funding agencies [@CRESWELL2008]. 

However, while scholars frequently point to the benefits of mixing correlational and process-based inquiry (e.g., @collier2010sources, p.~181), and have sometimes mapped out broad strategies of multi-method research design (@Lieberman2005nested, @SeawrightGerring2008), they have rarely provided specific guidance on how the integration of inferential leverage should unfold. In particular, the literature has not supplied specific principles for aggregating findings---whether mutually reinforcing or contradictory---across different modes of analysis.^[A small number of exceptions stand out. In the approach suggested by @gordon2004quantitative, for instance, available expert (possibly imperfect) knowledge regarding the operative causal mechanisms for a small number of cases can be used to anchor the statistical estimation procedure in a large-N study. @WesternJackman1994 propose a Bayesian approach in which qualitative information shapes subjective priors which in turn affect inferences from quantitative data. Relatedly,  in @GlynnQuinn2011, researchers use knowledge about the empirical joint distribution of the treatment variable, the outcome variable, and a post-treatment variable, alongside assumptions about how causal processes operate, to tighten estimated bounds on causal effects. @seawrightbook presents an informal framework in which case studies are used to test the assumptions underlying statistical inferences, such as the assumption of no-confounding or the stable-unit treatment value assumption (SUTVA).] 
<!-- What we still lack, however, is a comprehensive framework that allows us to enter qualitative and quantitative forms of information into an integrated analysis for the purposes of answering the wide range of causal questions that are of interests to social scientists, including questions about case-level explanations and causal effects, average causal effects, and causal pathways.  -->
As we aim to demonstrate in this book, however, grounding inference in causal models provides a very natural way of combining information of the $X,Y$ variety with information about the causal processes connecting $X$ and $Y$. The approach that we develop here can be readily addressed both to the  case-oriented questions that tend to be of interest to qualitative scholars and to the population-oriented questions that tend to motivate quantitative inquiry. 

As will become clear, when we structure our inquiry in terms of causal models, the conceptual distinction between qualitative and quantitative inference becomes hard to sustain. Notably, this is not because all causal inference depends fundamentally on covariation but because in a causal-model-based inference, what matters for the informativeness of a piece of evidence is how that evidence alters beliefs about a model, and in turn, a query. While the apparatus that we present is formal, the approach---in asking how pieces of evidence drawn from different parts of a process map on to a base of theoretical knowledge---is arguably most closely connected to process tracing in its core logic.


### Connecting theory and empirics

The relationship between theory and empirics has been a surprisingly uncomfortable one in political science. In a recent intervention, for instance, @clarke2012model draw attention to and critique political scientists'  widespread reliance on the "hypothetico-deductive" (H-D) framework, in which a theory or model is elaborated, empirical predictions derived, and data sought to test these predictions and the model from which they derive. Clarke and Primo draw on decades of scholarship in the philosophy of science pointing to deep problems with the H-D framework, including with the idea that the truth of a model logically derived from first principles can be *tested* against evidence. 

In fact the relationship between theory and evidence in social inquiry is often surprisingly unclear both in qualitative and quantitative work. We can perhaps illustrate it best, however, by reference to qualitative work, where the centrality of theory to inference has been most emphasized. In process tracing, theory is what justifies inferences. In their classic text on case study approaches, @george2005case describe process tracing as the search for evidence of "the causal process that a theory hypothesizes or implies" (6). Similarly, @Hall2003aligning conceptualizes the approach as testing for the causal-process-related observable implications of a theory; @mahoney2010after indicates that the events for which process tracers go looking are those posited by theory (128); and @gerring2006case describes theory as a source of predictions that the case-study analyst tests (116). Theory, in these accounts, is supposed to help us figure out where to look for discriminating evidence. 

What is not clear, however, is a how researchers can derive within-case empirical predictions from theory and how exactly doing so provides leverage on a causal question. From what elements of a theory can scholars derive informative within-case observations? <!-- Given a set of possible things to be observed in a case, how can theory help us distinguish more from less informative observations?  -->
Of the many possible observations suggested by a theory, how can we determine which would add probative value to the evidence already at hand? How do the evidentiary requisites for drawing a causal inference, given a theory, depend on the particular causal question of interest---on whether, for instance, we are interested in identifying the cause of an outcome in a case, estimating an average causal effect, or identifying the pathway through which an effect is generated? Perhaps most confusingly, if the theory tells us what to look for to draw an inference, can the inferences be about the theory itself or are we constrained to make theory dependent inferences? 
In short, how exactly can we ground causal inferences from within-case evidence in background knowledge about how the world works?

Much quantitative work in political science features a similarly weak integration between theory and research design. The modal inferential approach in quantitative work, both observational and experimental, involves looking for correlations between causes and outcomes, with less regard for intervening or surrounding causal relationships.^[There are of course many exceptions, including work that uses structural equation modeling, and research that focuses specifically on understanding heterogeneity and mediation processes.] If a theory suggests a *set* of relations, it is common to examine these separately---does $A$ cause $B$ does $B$ cause $C$? are relations stronger or weaker here or there?---without standard procedures for bringing the disparate pieces of evidence together to form theoretical conclusions. More attention has been paid to empirical implications of theoretical models than to theoretical implications of empirical models.  

In this book, we seek to show how scholars can simultaneously make fuller and more explicit use of theoretical knowledge in designing their research projects and analyzing data and make use of data to update on theoretical models. Like Clarke and Primo, we treat models not as veridical accounts of the world but as maps: maps, based on prior theoretical knowledge, about causal relations in a domain of interest. Also, as in Clarke and Primo's approach, we do not write down a model in order to test its veracity (though, in later chapters, we do discuss ways of justifying and evaluating models). Rather, our focus is on how we can systematically *use* causal models --- in the sense of *mobilizing background knowledge of the world* --- to guide our empirical strategies and inform our inferences. Grounding our empirical strategy in a model allows us, in turn, to learn about features of the model itself as we encounter the data.


## Key contributions

This book draws on methods developed in the study of Bayesian networks, a field pioneered by scholars in computer science, statistics, and philosophy (see especially @pearl2009causality). Bayesian networks, a form of causal model, have had limited traction to date in political science. Yet the literature on Bayesian networks and their graphical counterparts, directed acyclic graphs (DAGs), is a body of work that addresses very directly the kinds of problems with which qualitative and quantitative scholars routinely grapple.^[For application to quantitative analysis strategies in  political science, @glynn2007non give a clear introduction to how these methods  can be used to motivate strategies for conditioning and adjusting for causal inference. @garcia2015graphical demonstrate how these methods can be used to assess claims of external validity. With a focus on qualitative methods, @Waldner2015completeness uses causal diagrams to lay out a "completeness standard" for good process tracing. @weller2014finding employ graphs to conceptualize the different possible pathways between causal and outcome variables among which qualitative researchers may want to distinguish. Generally, in discussions of qualitative methodology, graphs are used to capture core features of theoretical accounts, but  are not developed specifically to ensure a representation of the kind of independence relations implied by structural causal models (notably what is called in the literature the "Markov condition"). Moreover, efforts to tie these causal graphs to probative observations, as in @Waldner2015completeness, are generally limited to identifying steps in a causal chain that the researcher should seek to observe.] 

Drawing on this work, we show in the chapters that follow how a theory can be formalized as a causal model represented by a causal graph and a set of structural equations. Engaging in this modest degree of formalization yields enormous benefits. It allows us, for a wide range of causal questions, to specify causal questions clearly and assess what inferences to make about queries from new data.

For students engaging in process tracing, the benefits of this approach are multiple. In particular, the framework that we develop in this book provides:

- A grounding for assessing the "probative value" of evidence drawn from different parts of any causal network. The approach yields a principled and transparent approach to answering the question: how should the observation of a given piece of data affect my causal beliefs about a case?

- A transparent, replicable method of aggregating inferences from observations drawn from different locations in a causal network.  Having collected multiple pieces of evidence from different parts of a causal process or case context, what should I end up believing about the causal question of interest?

- A common approach for assessing a wide variety of queries (estimands). We can use the same apparatus to learn *simultaneously* about different case-level causal questions, such as "What caused the outcome in this case?" and "Through what pathway did this cause exert its effect?"

<!-- - Consistency of probative value, priors, and therefore inferences with how you think the world works.  -->

<!-- - Transparency, allowing for evaluation -->

- Guidance for research design. Given finite resources, researchers must make choices about where to look for evidence. A causal model framework can help researchers assess, a priori, the relative expected informativenss of different evidentiary and case-selection strategies, conditional on how they think the world works and the question they want to answer.


The approach also offers a range of distinctive benefits to researchers seeking to engage in mixed-method inference and to learn about general causal relations, as well as about individual cases. The framework's central payoff for multi-method research is the systematic integration of qualitative and quantitative information to answer any given causal query. We note that the form of integration that we pursue here differs from that offered in other accounts of multi-method research. In @seawrightbook's approach, for instance, one form of data --- quantitative *or* qualitative --- is always used to draw causal inferences, while the other form of data is used to test assumptions or improve measures employed in that primary inferential strategy. In the approach that we develop in this book, in contrast, we are always using *all* information available to update on causal quantities of interest. In fact, within the causal models framework, there is no fundamental difference between quantitative and qualitative data, as both enter as values of nodes in a causal graph. This formalization --- this reductive move --- may well discomfit some readers. And we acknowledge that our approach undeniably involves a loss of some of what makes qualitative research distinct and valuable. Yet, this translation of qualitative and quantitative observations into a common, causal model framework offers major advantages. Beyond the integration of different forms of information, these advantages include:

- Transparency. The framework makes manifest precisely how each form of evidence enters into the analysis and shapes conclusions.

<!-- AJ: I don't understand this one. - A way to justify the background assumptions you've used -->

- Learning across levels of analysis. In a causal model approach, we use case-level information to learn about populations and general theory. At the same time, we use what we have learned about populations to sharpen our inferences about causal relations within individual cases.

- Cumulation of knowledge. A causal model framework provides a straightforward, principled mechanism for building on what we have already learned. As we see data, we update our model; and then our updated model can inform the inferences we draw from the next set of observations. Models can, likewise, provide an explicit framework for positing and learning about the generalizability and portability of findings across research contexts.

- Guidance for research design. With a causal model in hand, we can formally assess key multi-method design choices, including the balance we should strike between breadth (the number of cases) and depth (intensiveness of analysis in individual cases) and the choice of cases for intensive analysis. 


<!-- * make systematic use of theory to figure out what kinds of evidence have probative value for our causal queries and, in turn, to design maximally informative empirical strategies; -->

<!-- * draw inferences from the evidence in a manner disciplined by our theoretical knowledge of how the world works; -->

<!-- * integrate in this analysis information conventionally considered "qualitative" (e.g., evidence about causal mechanisms) with information conventionally considered "quantitative" (e.g., evidence about causal and outcome variables); and -->

<!-- * learn about the models that we bring to the table, which allows for the cumulation of knowledge through the updating of the theoretical knowledge base on which future inferences can draw. -->

Using causal models also has substantial implications for common methodological intuitions, advice, and practice. To touch on just a few of these implications: 

  * Our elaboration and application of model-based process tracing shows that, given plausible causal models, process tracing's common focus on intervening causal chains may be much less productive than other empirical strategies, such as examining moderating conditions. 

  * Our examination of model-based case-selection indicates that for many common purposes there is nothing particularly especially about "on the regression line" cases or those in which the outcome occurred, and there is nothing necessarily damning about selecting on the dependent variables. Rather optimal case selection depends on factors that have to date received little attention, such as the population distribution of cases and the probative value of the available evidence. 
  
  * Our analysis of clue-selection as a decision problem shows that the probative value of a piece evidence cannot be assessed in isolation, but hinges critically on what we have already observed.  

<!-- **MORE TO COME HERE ELABORATING IMPLICATIONS, COMPARING TO ADVICE AND PRACTICE IN LITERATURE** -->

The basic analytical apparatus that we employ here is not new. Rather, we see the book's goals as being of three kinds. First, we aim to import insights: to introduce political scientists to an approach that has received little attention in the discipline but that can be useful for addressing the sorts of causal questions with which political scientists are commonly preoccupied. As a model-based approach, it is a framework especially well suited to a field of inquiry in which exogeneity frequently cannot be assumed by design---that is, in which we often have no choice but to be engineers. 

Second, we draw connections between the Bayesian networks approach and key concerns and challenges with which students in social sciences routinely grapple. Working with causal models and DAGs most naturally connects to concerns about confounding and identification that have been central to much quantitative methodological development. Yet we also show how causal models can address issues central to process tracing, such as how to select cases for examination, how to think about the probative value of causal process observations, and how to structure our search for evidence, given finite resources. 

Third, we provide a set of usable tools for implementing the approach. We provide intuition and software, the `CausalQueries` package, that researchers can use to make research design choices and draw inferences from the data.


## The Road Ahead

The book is divided into four main parts. 

In the first part of the book, we set out the basics. In Chapter \@ref(models), following a review of the common potential-outcomes approach to causality, we introduce the concept and key components of a causal model. Chapter \@ref(illustratemodels) illustrates how we can represent of causal beliefs in the form causal models by translating the arguments of a several prominent works of political science into causal models.  In Chapter \@ref(questions), we set out a range of causal questions that researchers might want to address --- including questions about case-level causal effects, population-level effects, and mechanisms --- and define these queries within a causal model framework. Chapter \@ref(bayeschapter) offers a primer on the key ideas in Bayesian inference that we will mobilize in later sections of the book. In Chapter \@ref(theory), we map between causal models and theories, showing how we can think of any causal model as situated within a hierarchy of complexity: within this hierarchy, any causal model can be justified by references to a "lower level", more detailed model that offers a theory of why things work the way do that the higher level. This conceptualization is crucial insofar as we use more detailed (lower-level) models to generate empirical leverage on relationships represented in simpler, higher-level models. 


<!-- art off by  describing the kinds of causal estimands of interest. The main goal here is to introduce the key ideas in the study of Bayesian nets and to argue for a focus of interest away from  average treatment effects as go-to estimands of interest and towards a focus on causal structures as the key quantity of interest. The next chapter introduces key Bayesian ideas; what Bayes' rule is and how to use it. The third chapter connects the study of Bayesian networks to theoretical claims. The key argument here is that causal models should be thought of as theories which are themselves supportable by lower-level models (theories).  -->

The second part of the book shows how we can use causal models to undertake process-tracing and mixed method inference. Chapter \@ref(pt) lays out the logic of case-level inference from causal models: the central idea here is that what we learn from evidence is always conditional on the prior beliefs embedded in our model. In Chapter \@ref(ptapp), we illustrate model-based process-tracing with an application to the substantive issue of economic inequality's effects on democratization. Chapter \@ref(mixing) moves to mixed data problems: situations in which a researcher wants to use "quantitative" (broadly, $X,Y$) data on a large set of cases and more detailed ("qualitative") data on some subset of these cases. We show how we can use any arbitrary mix of observations across a sample of any size (greater than 1) to update on all causal parameters in a model, and then use the updated model to address the full range of general and case-level queries of interest. In Chapter \@ref(mixingapp), we illustrate this integrative approach by revisiting the problem of inequality and democracy introduced in Chapter \@ref(ptapp). Finally, in Chapter \@ref(mm), we take the project of integration a step further by showing how we can use models to integrate findings across *studies* and across *settings*. We show, for instance, how we can learn jointly from the data generated by an observational study and an experimental study of the same causal domain and how models can help us reason in principled ways about the transportability of findings across contexts. 

The third part of the book unpacks what causal models can contribute to research design. Across Chapters \@ref(elements), \@ref(clue), and \@ref(caseselection), we demonstrate how researchers can mobilize their models, as well as prior observations, to determine what kind of new evidence is likely to be most informative about the query of interest, how to strike the balance between extensiveness and intensiveness of analysis, and which cases to select for in-depth process tracing. 

<!-- In this framework the problem of case selection is equivalent to the kind of problem of clue selection discussed in Chapter 6. For a canonical multicase model however we use simulation approaches to provide guidance for how cases should be selected. The broad conclusion here is that researchers should go where the probative value lies, and all else equal, should select cases approximately proportional to the size of $XY$ strata---whether or not these are "on the regression line."  -->

The fourth and final part of the book steps back to put the model-based approach into question. Until this point, we have been advocating an embrace of models to aid inference. But the dangers of doing this are demonstrably large. The key problem is that with model-based inference, the inferences are only as good as the model. In the end, while we advocate a focus on models, we know that skeptics are right to distrust them. This final part approaches this problem from two perspectives. In Chapter \@ref(justifying), we demonstrate the *possibility* of justifying models from external evidence, though we do not pretend that the conditions for doing so will arise commonly. In Chapter \@ref(evaluation), drawing on common practice in Bayesian statistics, we present a set of strategies that researchers can use to evaluate and compare the validity of models, and to investigate the degree to which findings hinge on model assumptions. 

In the concluding chapter we summarize what we see as the main advantages of a causal-model-based approach to inference, draw out a set of key concerns and limitations of the framework, and identify what we see as the key avenues for progress in model-based inference.

Here we go.


<!--chapter:end:01-intro.Rmd-->


# (PART) Foundations  {-}

# Causal Models {#models}

```{r packagesused02, include = FALSE}
source("_packages_used.R")
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We provide a lay-language primer on the logic of causal models.
::::

<br>

Causal claims are everywhere. Causal knowledge is often the end goal of  empirical social science. It is also a key *input* into causal inference.^[As nicely put by @cartwright1994nature, no causes in, no causes out, a point we return to more formally later.] 
 Causal assumptions are also hidden in seemingly descriptive statements: claims that someone is guilty, or exploited, or powerful, or weak, involve beliefs about how things would be were conditions different. Even when scholars carefully try to avoid causal claim-making, causal verbs---depends, drives, produces, influences---tend to surface.     

But while causal claims are commonplace, it is not always clear (1) what exactly is meant by a causal relation and (2) how causal knowledge about one thing can be marshaled to justify causal claims about another. For our purposes, the counterfactual view of causality addresses the first question. Causal models address the second.

<!-- In this chapter we provide a basic introduction to causal models. Subsequent chapters in Part I layer on other foundational components of the book's framework, including a causal-model-based understanding of theory, the definition of common causal estimands within causal models, and the basics of Bayesian inference.  -->
<!-- While here we focus on the formal definition of causal models, in Chapter 10 we discuss strategies for generating them.  -->


## The counterfactual model {#counterfactualmodel}

We begin with what we might think of as a meta-model, the counterfactual model of causation.
<!-- The counterfactual model is perhaps the dominant approach to causal relations in the social sciences.  -->
At its core, a counterfactual understanding of causation captures a simple notion of causation as "difference-making."^[The approach is sometimes attributed to David Hume, whose writing contains ideas both about causality as regularity and causality as counterfactual. On the latter, Hume's key formulation is, "if the first object had not been, the second never had existed" [@hume2000enquiry, Section VIII]. More recently, the counterfactual view has been set forth by @splawa1990application and @lewis1973counterfactuals. See also  @lewis1986causation.] In the counterfactual view, to say that $X$ caused $Y$ is to say: *had* $X$ been different, $Y$ *would have been* different. 

The causal effect, in this view, is the difference between two things that might have happened.  This means that *by definition, causal effects are not measurable quantities*. They are not differences between possible observations in the world, but, at best, differences between outcomes in the world and counterfactual outcomes. They  need to be inferred not measured.

Moreover, in this view, the antecedent, "had $X$ been different,"  imagines a *controlled* change in $X$---an intervention that altered $X$'s value---rather than a naturally arising difference in $X$. The counterfactual claim, then, is not that $Y$ is different in those cases in which $X$ is different; it is, rather, that if one could somehow have *made* $X$ different, $Y$ would have been different. In the terminology of @pearl2000causality, we represent this quantity using a "do" operator:  $Y(do(X=x))$ is the value of $Y$ when $x$ is *set* to $x$.

Consider a simple example.  Students with teacher A perform well without studying. Students with teacher B perform well if they study, and do not perform well if they do not study. Moreover, only students with teacher B in fact study. And all perform well. 

When we say that one of teacher B's students did well *because* they studied, we are comparing the outcome that they experienced to the outcome they would have experienced if they had had teacher B (as they did) but (counterfactually) had not studied. Notably, we are *not* comparing their realized outcome to the outcome they would have experienced if they had been among the people that in fact didn't study (i.e., if they had had teacher A). 

A second example. Consider the claim that Switzerland democratized ($D=1$) because it had a relatively low level of economic inequality ($I=0$) (drawing on the logic of @boix2003democracy). In the counterfactual view, this is equivalent to saying that, had Switzerland *not* had a high level of equality, the country would not have democratized. High economic equality made a difference. The comparison for the causal statement is with the outcome Switzerland would have experienced under an intervention that boosted its historical level of economic inequality --- *not* with how Switzerland would have performed if it had been one of the countries that *in fact* had higher levels of inequality, cases that likely differ from Switzerland in other causally relevant ways. 

<!-- AJ: see Slack thread about whether we're expressing the controlled change right. https://gbiqq.slack.com/archives/C02C1DHUJ1M/p1629990240000600 -->
<!-- MH: OK -->


Along with this notion of causation as difference-making, we also want to allow for *variability* in how $X$ acts on the world. $X$ might sometimes make a difference, for some units of interest, and sometimes not. High levels of equality might generate democratization in some countries or historical settings but not in others. Moreover, while equality might make democratization happen in some times in places, it might *prevent* that same outcome in others. We need a language to describe these different types of relations.


### Potential outcomes

The "potential outcomes" framework is useful for describing the different kinds of counterfactual causal relations that might prevail between variables @Rubin1974. In this framework we characterize how a given unit responds to a causal variable by positing the outcomes that it *would* take on at different values of the causal variable.

A setting in which it is quite natural to think about potential outcomes is medical treatment. Imagine some individuals in a diseased population are observed to have received a drug ($X=1$) while others have not ($X=0$). Assume that, subsequently, a researcher observes which individuals become healthy ($Y=1$) and which do not ($Y=0$). Given the assignments of all other individuals,^[We noted that we are conditioning on the assignments of others. If we wanted to describe outcomes as a function of the *profile* of treatments received by others we would have a more complex types space. For instance in an $X \rightarrow Y$ model with 2 individuals we would report how $(Y_1, Y_2)$ respond to $(X_1,X_0)$; each vector can take on four values producing a type space with $4^4$ types rather than $2^2$. The complex type space  could be reduced back down to four types again, however, if we  invoked the assumption that the treatment or non-treatment of one patient has no effect on the outcomes of other patients---an assumption known as the stable unit treatment value assumption (SUTVA).] we can treat each individual as belonging to one of four unobserved response "types," defined by the outcomes that the individual *would have* if they received or did not receive treatment:^[See @copas1973randomization} for an early classification of this form. The literature on probabilistic models also refers to such strata as "canonical partitions" or "equivalence classes."]

<!-- AJ: SUTVA Footnote above needs to be made clearer. -->
<!-- MH Check -->

* **a**dverse: Those individuals who would get better if and only if they do not receive the treatment
* **b**eneficial: Those who would get better if and only if they do receive the treatment
* **c**hronic: Those who will remain sick whether or not they receive treatment
* **d**estined: Those who will get better whether or not they receive treatment

Table \@ref(tab:PO) maps the four types ($a, b, c, d$) onto their respective potential outcomes. In each column, we have simply written down the outcome that a patient of a given type would experience if they are not treated, and the outcome they would experience if they are treated. In each cases we are imagining *controlled* changes in treatment: the responses if treatments are changed without changes to other background conditions about the case.  

<!-- \begin{table}[h!]
\begin{tabular}{l|cccc} \small

& Type a & Type b & Type c & Type d \\

& **a**dverse effects & **b**eneficial Effects & **c**hronic cases & **d**estined cases \\
\hline
Not treated &    Healthy &       Sick &       Sick &    Healthy \\
Treated &       Sick &    Healthy &       Sick &    Healthy \\
\end{tabular}  
\caption{Potential Outcomes: What would happen to each of four possible types of case if they were or were not treated.}
\label{tabPO}
\end{table} -->


| \small      |          Type a          |            Type b           |         Type c         |          Type d         |
|-------------|:------------------------:|:---------------------------:|:----------------------:|:-----------------------:|
|             | **a**dverse effects | **b**eneficial Effects | **c**hronic cases | **d**estined cases |
| Outcome if not treated |          Healthy         |             Sick            |          Sick          |         Healthy         |
| Outcome if treated     |           Sick           |           Healthy           |          Sick          |         Healthy         |

Table: (\#tab:PO). Potential outcomes: What would happen to each of four possible types of case if they were or were not treated.


We highlight that, in this framework, case-level causal relations are treated as deterministic. A given case has a set of potential outcomes. Any uncertainty about outcomes enters as incomplete knowledge of a case's "type," not from underlying randomness in causal relations.  This understanding of causality---as ontologically deterministic, but empirically imperfectly understood---is compatible with views of causation commonly employed by qualitative researchers (see, e.g., @mahoney2008toward), and with understandings of causal determinism going back at least to @laplace1901philosophical. 

As we will also see, we can readily express this kind of incompleteness of knowledge within a causal model framework: indeed, the way in which causal models manage uncertainty is central to how they allow us to pose questions of interest and to learn from evidence. There are certainly situations we could imagine in which one might want to conceptualize potential outcomes themselves as random (for instance, if individuals in different conditions play different lotteries). But for the vast majority of the settings we condsider, not much of importance is lost if we treat potential outcomes as deterministic but possibly unknown: at the end of the day something will occur or it will not occur, we just do not know which it is.

### Generalization

Throughout the book, we generalize from this simple setup. Whenever we have one causal variable and one outcome, and both variables are binary (i.e., each can take on two possible values, 0 or 1), there are only four sets of possible potential outcomes, or "types." More generally, for variable, $Y$, we will use $\theta^Y$ to capture the unit's "type": the way that $Y$ responds to its potential causes.^[Later, we will refer to these as "nodal types."] We, further, add subscripts to denote particular types. Where there are four possible types, for instance, we use the notation $\theta^Y_{ij}$, where $i$ represents the case's potential outcome when $X=0$ and $j$ is the case's potential outcome when $X=1$.

Adopting this notation, for a causal structure with one binary causal variable and a binary outcome, the four types can be represented as $\{\theta^Y_{10},   \theta^Y_{01},   \theta^Y_{00}, \theta^Y_{11}\}$, as shown in Table \@ref(tab:POGEN):

| \small      |          Type a          |            Type b           |         Type c         |          Type d         |
|-------------|:------------------------:|:---------------------------:|:----------------------:|:-----------------------:|
|             | $\theta^Y=\theta^Y_{10}$ |  $\theta^Y=\theta^Y_{01}$   |$\theta^Y=\theta^Y_{00}$| $\theta^Y=\theta^Y_{11}$|
| Set $X=0$   |     $Y(0)=1$             |       $Y(0)=0$              |      $Y(0)=0$          |      $Y(0)=1$           |
| Set $X=1$   |     $Y(1)=0$             |       $Y(1)=1$              |      $Y(1)=0$          |      $Y(1)=1$           |
Table: (\#tab:POGEN). Generalizing from Table \@ref(tab:PO), the table gives for each causal type the values that $Y$ would take on if $X$ is set at $0$ and if $X$ is set at 1.



<!-- Let $Y(x)$ denote the "potential" outcome (the value $Y$ would take on) when $X=x$. Then, if $X$ is a binary variable, the effect of $X$ on $Y$ is simply defined as $Y(1) -  Y(0)$.  -->

<!-- These types differ in their ''potential outcomes'' | that is on what outcomes, $Y$,  they *would* have depending on their treatment condition $X$ . More formally, we let $Y(x)$ denote a case or type's potential outcome  when $X=x$. Thus, the potential outcomes are $Y(0)=1, Y(1)=0$ for type $a$; $Y(0)=0, Y(1)=1$ for type $b$; $Y(0)=0, Y(1)=0$ for type $c$; and $Y(0)=1, Y(1)=1$ for type $d$.   -->


<!-- For any given response type, the causal effect of $X$ on $Y$ is $Y(1) -  Y(0)$. Thus, the causal effect is $-1$ for $a$ types, $1$ for $b$ types, and $0$ for both $c$ and $d$ types. -->

Returning to the matter of inequality and democratization to illustrate, let $I=1$ represent a high level of economic equality and $I=0$ its absence; let $D=1$ represent democratization and $D=0$ its absence. A $\theta^D_{10}$ (or $a$) type is a case in which a high level of equality, if it occurs, *prevents* democratization in a country that would otherwise have democratized. The causal effect of high equality in a case, $i$, of $\theta^D_{10}$ type is $\tau_i= -1$. A $\theta^D_{01}$ type (or $b$ type) is a case in which high equality, if it occurs, generates democratization in a country that would otherwise have remained non-democratic (effect $\tau_i= 1$). A $\theta^D_{00}$  type ($c$ type) is a case that will not democratize regardless of the level of equality (effect $\tau_i = 0$); and a $\theta^D_{11}$ type ($d$ type)  is one that will democratize regardless of the level of equality (again, effect $\tau_i= 0$).

In this setting, a causal *explanation* of a given case outcome amounts to a statement about its type. The claim that Switzerland's high level of equality was a cause of its democratization is equivalent to saying that Switzerland democratized and is a $\theta^D_{01}$ type. To claim that Sierra Leone democratized because of low inequality is equivalent to saying that Sierra Leone democratized and is a $\theta^D_{10}$ type. To claim, on the other hand, that Malawi democratized for reasons having nothing to do with its level of economic equality is to characterize Malawi as a $\theta^D_{11}$ type (which already specifies its outcome).

<!-- FLAG: CHECK USE OF THESE CASES -->

Now, let us consider more complex causal relations. Suppose now that there are two binary causal variables $X_1$ and $X_2$. We can specify any given case's potential outcomes for each of the different possible combinations of causal conditions---there now being four such conditions, as each causal variable may take on $0$ or $1$ when the other is at $0$ or $1$. 

As for notation, we now need to expand $\theta$'s subscript since we need to represent the value that $Y$ takes on under each of the four possible combinations of $X_1$ and $X_2$ values.  We construct the four-digit subscript with the ordering (and, in general, mapping any two parents alphabetically into $X_1$ and $X_2$). The next equation connects this notation to the "do" notation used to convey the idea that conditions are controlled.

<!-- FLAG: ADD CROSSREF -->
\begin{eqnarray} 
Y_{hijk} \left\{\begin{array}{ccc}  h& =& Y|do(X_1=0, X_2=0)\\
 i &=& Y|do(X_1=1, X_2=0)\\
 j &=& Y|do(X_1=0, X_2=1)\\
 k &=& Y|do(X_1=1, X_2=1)
 \end{array} \right.
\end{eqnarray}

Thus, for instance, $\theta^Y_{0100}$ means that $Y$ is 1 if $X_1=1$ and $X_2=0$ and is 0 otherwise. 

We now have 16 causal types: 16 different patterns that $Y$ might display in response to changes in $X_1$ and $X_2$. The full set is represented in Table \@ref(tab:PO16), which also makes clear how we read types off of four-digit subscripts. For instance, the table shows us that for nodal type  $\theta^Y_{0101}$,  $X_1$ has a positive causal effect on $Y$ but $X_2$ has no effect. On the other hand, for type $\theta^Y_{0011}$, $X_2$ has a positive effect while $X_1$ has none. 

We also capture interactions here. For instance, in a $\theta^Y_{0001}$ type, $X_2$ has a positive causal effect if and only if $X_1$ is 1. In that type, $X_1$ and $X_2$ serve as "complements." For $\theta^Y_{0111}$, $X_2$ has a positive causal effect if and only if $X_1$ is 0. In that setup, $X_1$ and $X_2$ are "substitutes."



<!-- let $Y(x_1, x_2)$ denote the outcome when $X_1=x_1$ and  $X_2=x_2$. Then the quantity $\left(Y(1, 1) - Y(0, 1)\right) - \left(Y(1, 0) - Y(0, 0)\right)$ describes the interactive effect of two treatments: it captures how the effect of $X_1$ changing from $0$ to $1$ is different between those situations in which $X_2=1$ and those situations in which $X_2=0$. -->



```{r PO16, echo = FALSE}
tab <- get_nodal_types(make_model("X1 -> Y <- X2"), collapse = FALSE)$Y

tab <- cbind(type = paste0("$\\theta^Y_{", rownames(tab), "}$"), tab)

kable(tab, 
      col.names = c("$\\theta^Y$", "if $X_1=0, X_2=0$",   "if $X_1=1,X_2=0$",  "if $X_1=0,X_2=1$",   "if $X_1=1, X_2=1$" ),
      row.names = FALSE, align=c(rep('c',ncol(tab))),
      caption = "With two binary causal variables, there are 16 causal types: 16 ways in which $Y$ might respond to changes in the other two variables.")
```



This is a rich framework in that it allows for all possible ways in which a set of multiple causes can interact with each other. Often, when seeking to explain the outcome in a case, researchers proceed as though causes are necessarily *rival*, where $X_1$ being a cause of $Y$ implies that $X_2$ was not. Did Malawi democratize because it was a relatively economically equal society *or* because of international pressure to do so? In the counterfactual model, however, causal relations can be non-rival. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if *either* supporter's vote were different. This typological, potential-outcomes conceptualization provides a straightforward way of representing this kind of complex causation.

Because of this complexity, when we say that $X$ caused $Y$ in a given case, we will generally mean that $X$ was *a* cause, not *the* (only) cause. Malawi might not have democratized if *either* a relatively high level of economic equality *or* international pressure had been absent. For most social phenomena that we study, there will be multiple, and sometimes a great many, difference-makers for any given case outcome.

We will mostly use $\theta^Y_{ij}$-style notation in this book to refer to types. We will, however, occasionally revert to the simpler $a, b, c, d$ designations when that helps ease exposition. As types play a central role in the causal-model framework, we recommend getting comfortable with both forms of notation before going further.

<!-- ## Counter-intuitive implications of the counterfactual model -->

<!-- Although the counterfactual framework is now widely employed, it contains a set of implications that might sit uncomfortably with common conceptions of how causal inference operates. -->

<!-- Second, it is often intuitive to think of causal processes as sets of transitive relations: if $A$ causes $B$ and  $B$ causes $C$, then we might think that $A$ causes $C$. Yet, in the counterfactual model, causal relations are *not* transitive. In a classic illustration, imagine a boulder that rolls down a hill, causing you to duck, and that ducking in turn saves your life. As a counterfactual matter, the boulder clearly caused the ducking and the ducking your survival. But the boulder rolling down the hill did not save your life. To consider an example from the social realm, think about situations in which action begets reaction. For instance, a rebellion causes a military crackdown, and the military crackdown causes the regime to survive; yet the rebellion did not cause the regime to survive. (For discussions see @hall2004two and @paul2013causation.) The insight has implications both for process tracing and for correlational approaches to establishing causation. Finding in a causal link in a case from $A$ to $B$ and a causal link from $B$ to $C$ is not equivalent to finding that $A$ caused $C$. Likewise, identifying a population-level causal effect of $X$ on some suspected mediator $M$, and another effect of $M$ on $Y$, does not establish an $X\rightarrow Y$ causal relationship. -->

<!-- <!-- I do not find the next point convincing. --> 

<!-- Third, notions of causality going back at least to Hume [@hume2000enquiry] treat causal relations as characterized by spatio-temporal contiguity between cause and effect, or at least of the intermediate steps between them. Yet in the counterfactural model, there is no requirement that causes be temporally or spatially connected to their effects. For instance, *potentially* intervening events that did *not* occur can have causal effects, even though they make no spatio-temporal contact with the observable events that seem to lie along the path from $X$ to $Y$. The plague that put Friar John into quarantine meant that he did not deliver the letter to Romeo to inform him that Juliet was not dead, which in turn led to Romeo's death. There is a *causal* path from the plague to Romeo's death, but no *spatio-temporal* one.  -->

<!-- Fourth, hypothesis-testing at the case level sometimes proceeds as though competing explanations amount to rival causes, where $A$ caused $B$ implies that $C$ did not. But in the counterfactual model, causal relations are neither rival nor decomposable. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if *either* supporter's vote were different. The causes are not rival. Now, imagine that all three of three voters supported an outcome. Then the three votes jointly caused the outcome. However, this joint cause is not decomposable into its component parts: none of the individual votes had *any* effect on the outcome. A change in any one vote would have made no difference. -->

<!-- <!-- Does the latter example above  satisfy minimality? Is a graph minimal that points from all three yesses to the outcome? --> 

<!-- Thus, there appear to be some tensions between the counterfactual model and some common notions of causality. These tensions largely disappear, however, once we properly specify causal models as systems of causal relations. For this work, Directed Acyclic Graphs provide a powerful tool.    -->

Using the same framework, we can generalize to structures in which a unit has any number of causes and also to cases in which causes and outcomes are non-binary. As one might imagine, the number of types increases rapidly (very rapidly) as the number of considered causal variables increases, as it also does as we allow $X$ or $Y$ to take on more than 2 possible values. For example, if there are $n$ binary causes of an outcome, then there can be $2^{\left(2^n\right)}$ types of this form: that is, $k=2^n$ combinations of values of causes to consider, and $2^k$ distinct ways to react to each combination. If causes and outcomes are ternary instead of binary, we have $3^{\left(3^n\right)}$ causal types of this form. Yet, the basic principle of representing possible causal relations as patterns of potential outcomes remains unchanged, at least as long as variables are discrete.

### Summaries of potential outcomes

So far, we have focused on causal relations at the level of an individual case. Causal relations at the level of a population are, however, simply a summary of causal relations for cases, and the same basic ideas can be used. We could, for instance, summarize our beliefs about the relationship between economic equality and democratization by saying that we think that the world is comprised of a mixture of $a$, $b$, $c$, and $d$ types, as defined above. We could get more specific and express a belief about what proportions of cases in the world are of each of the four types. For instance, we might believe that $a$ types and $d$ types are quite rare while $b$ and $c$ types are quite common. Moreover, our belief about the proportions of $b$ (positive effect) and $a$ (negative effect) cases imply a belief about equality's *average* effect on democratization as, in a binary setup, this quantity is simply the proportion of $b$ types minus the proportion of $a$ types. Such summaries allow us to move from discussion of the cause of a single outcome to discussions of average effects, a distinction that we take up again in Chapter \@ref(questions). 

## Causal Models and Directed Acyclic Graphs

So far we have discussed how a single outcome is affected by one or more possible causes. However, these same ideas can be used to describe more complex relations between collections of variables --- for example, with one variable affecting another directly as well as indirectly via its impact on some mediating variable.

Potential outcomes tables can be used to describe such complex relations. However, as causal structures become more complex---especially, as the number of variables in a domain increases---a causal model can be a powerful organizing tool. In this section, we show how causal models and their visual counterparts, directed acyclic graphs (DAGs), can represent substantive beliefs about counterfactual causal relationships in the world. The key ideas in this section can be found in many texts (see, e.g., @halpern2005causesa and @galles1998axiomatic), and we introduce here a set of basic principles that readers will need to keep in mind in order to follow the argumentation in this book. 

As we shift to talking about networks of causal relations between variables we will also shift our language. When talking about causal networks, or causal graphs, we will generally refer to variables as "nodes." And we will sometimes use familial terms to describe relations between nodes. For instance, two nodes directly connected by an arrow are known as "parent" and "child," while two nodes with a child in common (both directly affecting the same variable) are "spouses." We can also say that $I$ is an "ancestor" of $D$ (a node upstream from $D$'s parent) and conversely that $D$ is a descendant of $I$ (a node downstream from $I$'s child).

Return to our running democratization example, but suppose now that we have more fully specified beliefs about how the level of economic inequality can have an effect on whether a country democratizes. We might believe that inequality affects the likelihood of democratization by generating demands for redistribution, which in turn can cause the mobilization of lower-income citizens, which in turn can cause democratization. We might also believe that mobilization itself is not just a function of redistributive preferences but also of the degree of ethnic homogeneity, which shapes the capacities of lower-income citizens for collective action. In this model $R$ is a parent of $M$, $I$ is an ancestor of $M$ but not a parent. $R$ and $E$ are spouses and $M$ is their child. We can visualize this model as a Directed Acyclic Diagram (DAG) in Figure \@ref(fig:simpleDAG).

```{r simpleDAG, echo = FALSE, out.width='60%', fig.width = 7, fig.height = 4,  fig.align="center", out.width='80%', fig.cap = "A simple causal model in which high inequality ($I$) affects the democratization ($D$) via redistributive demands and mass mobilization ($M$), which is also a function of ethnic homogeneity ($E$). The arrows show relations of causal dependence between variables.  The graph does not capture the ranges of the variables and the functional relations between them."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 2, 1, 3, 3, 2, 1, 0, 1),
       y = c(2, 2, 2, 0, 2, 3, 3, 3, 3, -1),
       names = c(
         "I", 
         "R",
         "M",
         "E", 
         "D", 
         expression(paste(theta^D)),
         expression(paste(theta^M)),
         expression(paste(theta^R)),
         expression(paste(theta^I)),
         expression(paste(theta^E)) 
         ),
       arcs = cbind( c(1, 2, 4, 3, 6, 7, 8, 9, 10),
                     c(2, 3, 3, 5, 5, 3, 2, 1, 4)),
       title = "A Model of Inequality's Effect on Democratization",
       padding = .2, contraction = .15) 

```

Fundamentally, we treat causal models in this book as formal representations of *beliefs* about how the world works---or, more specifically, about causal relations within a given domain. We might use a causal model to capture our own beliefs, a working simplifcation of our beliefs, or a set of potential beliefs that one might hold. The formalization of *prior* beliefs in the form of a causal model is the starting point for research design and inference in this book's analytic framework. Using the democratization example, we will now walk through the three components of a causal model in which our beliefs get embedded: nodes, functions, and distributions.

### Components of a Causal Model 

The three components of a causal model are (i) the nodes---that is, the set of variables we are focused on and how are they defined (ii) the functional relations---which nodes are "explained" by which other nodes and how, and (iii) probability distributions over unexplained elements of a model. 

#### The nodes

The first component of a causal model is the set of variables (nodes) across which the model characterizes causal relations. On the graph in Figure \@ref(fig:simpleDAG), the five included variables are represented by the five lettered nodes. (In addition we mark $\theta^D$ on the graph though, as will be made clear, we will not think of this as a variable.)

In identifying the nodes, we also need to specify the *ranges* over which they can  vary. We might specify, for instance, that all nodes in the model are binary, taking on the values 0 or 1. We could, alternatively, define a set of categories across which a node ranges or allow a node to take on any real number value or any value between a set of bounds. ^[If we let \(\mathcal{R}\) denote a set of ranges for all nodes in the model, we can indicate $X$'s range, for instance, by writing \(\mathcal{R}(X)=\{0,1\}\). The nodes in a causal model together with their ranges---the triple \((\mathcal{U}, \mathcal{V}, \mathcal{R})\)---are sometimes called a \emph{signature}, \(\mathcal{S}\).]

Notice that some of these nodes have arrows pointing *into* them: $R, M$, and $D$ are endogenous nodes, meaning that their values are determined entirely by other nodes in the model.

Other nodes have arrows pointing out of them but no arrows pointing into them: $I$ and $E$.  $I$ and $E$ are "exogenous" nodes; they influence other nodes in the model but themselves have no causes specified in the model. 

The $\theta$ terms require a little more explanation since they do not describe substantive nodes. In our discussion above, we introduced $\theta$ notation for representing types. Generally we can think of  $\theta$ terms on a causal graph as unobservable and unspecified features of a causal domain that affect outcomes. These might include random processes (noise) or contextual features that we are unable to identify or do not understand. We imagine them pointing into everynode, whether indicated or not. In all cases the work they do is to characterize what the value of a node will be, given the value of its parents.

We note that our notation for representing these unobservable, unspecified influences differs from that commonly found in the literature on causal models. In many treatments, these components are themselves referred to as "exogenous" variables, and often labelled as sets $\mathcal U$, to be distinguished from the endogenous---named--variables often labelled as $\mathcal V$. We will generally use $\theta$ to denote these unobserved, unspecified influences to emphasize their particular role, as direct objects of interest in causal inquiry. As we will show, we can think of $\theta$ nodes as capturing the functional relations between endogeneous variables and as being quantities of direct interest for causal inquiry. We more fully develop this point --- returning to the notion of $\theta$ terms as receptacles for causal effects --- below.

<!-- For instance, here $\theta^D$ captures *how* and *whether* $D$'s parent produces $D$. In the present example, we believe democratization to be potentially affected by mobilization, and two of $\theta^D$'s possible values allow for such an effect: $\theta^D_{01}$ and $\theta^D_{10}$. We also know that democratization is affected by other things, even if we do not know what they are and have not represented them explicitly in the model. The values $\theta^D_{00}$ (no democratization regardless of mobilization) and $\theta^D_{11}$ (always democratization regardless of mobilization) capture all "other" factors---factors other than mobilization---that may affect democratization.  -->

<!-- Similarly, $\theta^M$ represents how $M$ responds to $R$ and $E$, while $\theta^R$ captures how $R$ responds to $I$. The $\theta^I$ and $\theta^E$ terms, pointing into the two exogenous nodes, represent all processes outside the model that assign these exogenous nodes to their values. -->

<!-- ^[Readers familiar with the literature on causal models will know that $U$ terms are typically used to capture unspecified exogenous influences on included nodes. We could have, here, included a term $U_D$ to indicate an "error" term or uncertainty regarding exactly what value $D$ will take given knowledge of $M$. We use the concept of a type and the $\theta$ notation, instead, to highlight the fact that, in non-parametric models, this "residual" component can be thought of as *the* locus of learning about the questions we are asking.] -->

<!-- Going forward, we set every node in our working example to be binary. -->

#### The functions

Next, we need to specify our beliefs about the causal relations among the nodes in our model. How is the value of one node affected by, and how does it affect, the values of others? For each endogenous node---each node influenced by others in the model---we need to express beliefs about how its value is affected by its parents, its immediate causes.

The DAG already represents a critical part of these beliefs: the arrows, or directed edges, tell us *which nodes we believe to be direct causal inputs into other nodes*. So, for instance, we believe that democratization ($D$) is determined jointly by mobilization ($M$) and some exogenous, unspecified factor (or set of factors), $\theta^D$. As we have said, we can think of $\theta^D$ as all of the other influences on democratization, besides mobilization, that we either do not know of or have decided not to explicitly include in the model. We believe, likewise, that $M$ is determined by $I$ and an unspecified exogenous factor (or set of factors), $\theta^M$. And we are conceptualizing inequality ($I$) and ethnic heterogeneity ($E$) as shaped solely by factors exogenous to the model, captured by $\theta^I$ and $\theta^E$, respectively.

<!-- (For all intents and purposes, $I$ and $E$ behave as  exogenous node here since its value is determined solely by an exogenous node.)  -->

<!-- We emphasize that an arrow indicates a *direct* relationship, given the other nodes in the graph. Likewise, the *absence* of an arrow between two nodes indicates the belief that there is no direct relationship between the them. Consider, for instance, the absence of a an arrow running direcly from $R$ to $D$. This omission represents the belief that any effect of redistributive preferences on democratization work *only* through mass mobilization; or, equivalently, that $R$ has no effect on $D$ *given* $M$. The omission of an $R \rightarrow D$ arrow means we are building quite a strong assumption into the model; the missing arrow rules out, for example, the possibility that $R$ moderates the effect of $M$ on $D$.  -->

Beyond the beliefs captured by the arrows in a DAG, we can express more specific beliefs about causal relations in the form of a causal function.^[The collection of all causal functions in the model can be denoted as $\mathcal{F}$.] Specifying a function means writing down whatever general or theoretical knowledge we have about the direct causal relations between nodes. A function specifies how the value that one node takes on is determined by the values that other nodes---its parents---take on. 

<!-- For each endogenous node, we can specify two kinds of nodes as direct causes:  (i) other endogenous nodes, which we call the node's *parents*;^[For node $V_i$, we write its parents as $PA_i$.] and (ii) an exogenous node. Thus, for instance, the node $Y$ in Figure \@ref(fig:simpleDAG) has as its direct causes the node $R$, its parent (an endogenous node itself) and the random-disturbance, $U_Y$.^[Any node with no parents in $\mathcal V$ must be a function of a member of $\mathcal U$; otherwise, we could not consider it endogenous. A node that is a function of one or more members of $\mathcal V$, however, can be modeled without a $U_i$ term if we believe that it is fully determined by nodes specified in $\mathcal V$.]  -->

We can specify this relationship in a vast variety of ways. It is useful however to distinguish broadly between parametric and non-parametric approaches. We take a non-parametric approach in this book, but it is helpful to juxtapose that approach to a parametric one.

**Parametric approaches.** A parametric approach specifies a functional form that relates parents to children. For instance, we might  model one node as a linear function of another and  write $D=\alpha + \beta M$, where $\beta$ is a parameter that we do not know the value of at the outset of a study but about which we wish to learn. If we believe $D$ to be linearly affected by $M$ but also subject to forces that we do not yet understand and have not yet specified in our theory, then we might write: $D=\alpha + \beta M+\theta^D$. In this function, $\alpha, \beta$ might be the parameters of interest, with $\theta^D$ treated merely as a random disturbance.  We can be still more agnostic by, for example, including parameters that govern how other parameters operate. Consider, for instance the function, $D=\beta M^{\theta^D}$. Here, $D$ and $M$ are linearly related if $\theta^D=1$, but not otherwise. 

Note that functions can be written to be quite specific or extremely general, depending on the state of prior knowledge about the phenomenon under investigation. The use of a structural model *does not require precise knowledge of specific causal relations*, even of the functional forms through which two nodes are related. 


<!-- ^[Here the difference between $\beta$ and $U_D$ is that $\beta$ is a parameter that we believe takes a constant value for all units, even if its value is unknown, while $U_D$ represents some unknown factor or combination of factors the value of which may vary across units, over a pre-specified range.] -->


<!-- * We may even be uncertain about whether or in what direction one node affects another. Or we may believe that their relationship varies across cases for reasons that we do not yet understand. We can capture this type of uncertainty as an interaction such as: $D=M U_D$. Here, the value of our random-disturbance term does not merely represent noise around an $M \rightarrow D$ relationship. Rather, $U_D$ now conditions, or moderates, the strength, sign, or existence of the relationship itself. Which of these $U_D$ conditions will depend on the $U_D$'s specified range. For instance, if we allow $U_D$ to take on the values $-1, 0$ or $1$, then $U_D$ will determine whether $M$ has a negative effect, no effect, or a positive effect on $D$. If instead, $U_D$ is allowed to vary continuously between $0$ and $1$, inclusive, then it conditions the strength and existence of a positive causal effect of $M$ on $D$, though not the sign of that effect.  As we discuss in later chapters, our investigation might then center on drawing inferences about $U_D$ in order to assess $M$'s causal effect. -->


**The non-parametric approach.** With discrete data, causal functions can take fully *non-parametric* form, allowing for *any possible relation* between parents and children. We use this framework for most of this book and thus spend some time developing the approach here.

We begin by returning to the concept of types. Drawing on our original four types from earlier in this chapter, we know that we can fully specify causal relations between a binary $M$ and a binary $D$ by allowing the node $\theta_D$ to range across four possible values $\{\theta^D_{10}, \theta^D_{01}, \theta^D_{00}, \theta^D_{11}\}$. For instance, $\theta^D_{10}$ represents a negative causal effect of $M$ on $D$ while $\theta^D_{00}$ represents $D$ remaining at 0 regardless of $M$. Put differently, $\theta^D$ represents the non-parametric function that relates $M$ to $D$. We can formally specify $D$'s behavior as a function of $M$ and $\theta^D$ in the following way: 

$$D(M, \theta^D_{ij}) = \left\{\begin{array}{ccx} i & \text{if} & M=0 \\ j & \text{if} & M=1 \end{array}\right.$$

Note that $\theta^D$ ranges over *all possible* functional forms between these two binary variables. 

How should we think about what kind of *thing* $\theta^D$ is, in a more substantive sense? It is probably most helpful to think of $\theta^D$ as an unknown and possibly random factor that conditions the effect of mobilization on democratization, determining whether $M$ has a negative effect, a positive effect, no effect with democratization never occurring, or no effect with democratization bound to occur regardless of mobilization.  Importantly, however, while we might think of $\theta^D$ as an unknown or random quantity, in this formulation $\theta^D$ should not be thought of as a nuisance --- as "noise" that we would like to get rid of --- but as *the quantity that we want to learn about*: we want to know whether $M$ likely had a positive, negative, or no effect on $D$. We elaborate on this point at much greater length in Chapter \@ref(questions).

We can similarly use $\theta$ terms to capture causal relations involving any number of parent nodes. Every substantively defined node, $J$, in a graph can be thought of as having a $\theta^J$ term pointing into it, and the (unobservable) value of $\theta^J$ represents the mapping from $J$'s parents (if it has any) to the value of $J$. 


<!-- With two parent nodes, for instance, we simply use causal types of the form $\theta^Y_{hijk}$, as illustrated above.  -->

<!-- We may, for instance, believe that an outcome occurs when and only when _two_ conditions are present. Redistributive demands and ethnic homogeneity may be individually necessary and jointly sufficient conditions for mobilization. We can express this belief with a slightly more complex function: $M=E R$. According to this function, $M=1$ (mobilization occurs) if _both_ $E=1$ (ethnic homogeneity is present) and $R=1$ (redistributive preferences are present), but $M=0$ (mobilization does not occur) otherwise. Note that this formulation also builds in causal heterogeneity: here, we are saying that redistributive preferences have an effect on mobilization when and only when ethnic homogeneity is present, and vice versa. -->



Applied to the binary nodes in Figure \@ref(fig:simpleDAG), $\theta^J$ ranges as follows:

* **Nodes with no parents**: For an exogenous node, like $I$ or $E$, $\theta^J$ represents an external "assignment" process can take on one of two values, $\theta^J_{0}$, meaning that $J$ is "assigned" to $0$ or $\theta^J_{1}$, meaning that $J$ is assigned to 1. For instance, $\theta^I_{0}$ typifies a case in which exogenous forces have generated low inequality.
* **Binary nodes with 1 binary parent**: For endogenous node $R$, with only one parent ($I$), $\theta^R$ takes on one of four values of the form $\theta^R_{ij}$ (our four original types, $\theta^R_{10}$, $\theta^R_{01}$, etc.).
* **Binary nodes with 2 binary parents**: $M$ has two parent nodes. Thus, $\theta^M$ will take on a possible 16 values of the form $\theta^M_{hijk}$ ($\theta^M_{0000}$, $\theta^M_{0001}$, etc.), using the syntax detailed earlier in this chapter.




**Nodal types and causal types.** For analytic applications later in the book, we will want to be able to think both about the type operating at a particular *node* and about *collections* of types operating across a model. We thus refer to $\theta^J$ as a unit's *nodal* causal type, or simply nodal type, for $J$. We refer to the collection of nodal types across all nodes for a given unit (i.e., a case) as the case's *unit causal type*, or simply *causal type*, denoted by the vector $\theta$. 

<!-- AJ: Should we not be using vector notation above -- i.e.,\Theta? -->
<!-- MH: I prefer \theta here; not shouting. $\theta = (\theta^X, \theta^Y)$ -->

If we hypothetically knew a unit's causal type---all nodal types for all nodes---then we would know everything there is to know about that unit. Since the nodal types of exogenous nodes include *values* of all exogenous nodes, and the nodal types of all endogenous nodes specify how those nodes respond to all of their parents, a unit's causal type fully specifies all nodal values as well as all *counterfactual* nodal values for a unit.

We will sometimes refer to the values of $\theta$ as a unit's *context*. This is because $\theta$ captures all exogenous forces acting on a unit. This includes the assignment process driving the model's exogenous nodes (in our example, $\theta^I$ and $\theta^E$) as well as all contextual factors that shape causal relations between nodes ($\theta^R$, $\theta^M$, and $\theta^D$). Put differently, $\theta$ captures both how a unit reacts to situations and which situations it is reacting to. One implication is that there is no *formal* distinction between a unit's type and a unit's situation---between, say, a hungry person, and a person who has had no food.



Nodal types, causal types

**term**          |**symbol** |**meaning**
:-------------:|:---------:|:-----------------------------------------:
nodal type        |$\theta^J$ | The way that node $J$ responds to the values of its parents. Example: $\theta^Y_{10}$: $Y$ takes the value 1 if $X=0$ and 0 if $X=1$.  
causal type       |$\theta$   | A causal type is a concatenation of nodal types, one for each node. Example:  $(\theta^X_0, \theta^Y_{00})$, is a causal type that has $X=0$ and that has $Y=0$ no matter what the value of $X$. 


<!-- It is thus worth dwelling for a moment on what this kind of function is doing. We have started with a graph in which mobilization can have an effect on democratization and the understanding that this effect, both its existence and its sign, may vary across cases. Cases, in other words, may be of different causal types. Further, we do not know what it is that shapes $D$'s response to $M$---what makes a case one type versus another. We thus use $\theta_D$ as a stand-in for the unknown and unspecified moderators of $M$'s effect. We might, at this stage, wonder what the point is of including $\theta_D$ in the model; are we not essentially just placing a question mark on the graph? We are, and that is precisely the point. As we will see in later chapters, non-substantive, causal-type nodes can play a key role in specifying (a) what we are uncertain about in a causal network and (b) what we would like to find out. Embedding our questions about the world directly into a model of the world, in turn, allows us to answer those questions in ways systematically and transparently guided by prior knowledge. -->


<!-- In fact, we can include factors as another node's "parents" even if we are unsure that those factors matter. Including nodes on the righthand side in a functional equation allows for the possibility that those nodes matter, and in turn sets us up to investigate their possible effect empirically.^[For instance, in $B=AU_B+C$, $A$ will only affect $B$ if $\theta^B$ takes on a non-zero value. ] -->

<!-- ### Interpretation of functional equations -->

A few important aspects of causal functions are worth highlighting. 

1. These functions express *causal* beliefs. When we write $D=\beta M$ as a function, we do not just mean that we believe the values of $M$ and $D$ in the world to be linearly related. We mean that we believe that the value of $M$ *determines* the value of $D$ through this linear function. Functions are, in this sense, *directional* statements, with causes on the righthand side and an outcome on the left.

2. The collection of simple functions that map from the values of parents of a given node to the values of that node are sufficient to represent potentially complex webs of causal relations. For each node, we do not need to think through entire sequences of causation that might precede it. We need only specify how we believe it to be affected by its parents---that is to say, those nodes pointing directly into it. Our outcome of interest, $D$, may be shaped by multiple, long chains of causality. To theorize how $D$ is generated, however, we write down how we believe $D$ is shaped by its parent---its direct cause, $M$. We then, separately, express a belief about how $M$ is shaped by _its_ parents, $R$ and $E$. A node's function must include as inputs all, and only, those nodes that point directly into that node.^[The set of a node's parents is required to be minimal in the sense that a node is not included among the parents if, given the other parents, the child does not depend on it in any state that arises with positive probability.]

3. As in the general potential-outcomes framework, all relations in a causal model are conceptualized as deterministic at the case level. Yet, there is not as much at stake here as one might think at first; by this we simply mean that a node's value is  *determined* by the values of its parents *along with* any stochastic or unknown components. We express uncertainty about causal relations, however, as unknown parameters, such as the causal types $\theta$. 

<!-- 4. *The values of the exogenous nodes*---those with no arrows pointing in to them---*are sufficient to determine the values of all other nodes in the model.*  In other words, context determines all other values. For instance, in Figure \@ref(fig:simpleDAG), knowing the values of $I$, $E$, and $\theta^D$ as well as the causal functions (including the values of any parameters they contain) would tell us the values of $R$, $M$, and $D$. -->

<!-- ]  nodes that have no parents are called *roots*.^[Thus in our usage all elements of $\mathcal{U}$ are roots, but so are nodes in $\mathcal{V}$ that depend on nodes in $\mathcal{U}$ only.]  We will say that $\mathcal{F}$ is a set of *ordered structural equations* if no node is its own descendant and if no element in $\mathcal{U}$ is parent to more than one element of \(\mathcal{V}\).^[This last condition can be achieved by shifting any parent of multiple children in $\mathcal{U}$ to $\mathcal{V}$.] -->

<!-- Do we need to define roots? -->

<!-- For notational simplicity we generally write functional equations in the form $c(a,b)$ rather than $f_c(a,b)$. -->

<!-- **The distributions**. So far, we have specified the nodes in the model and their causal relations, possibly with uncertainty. These relations imply  -->

<!-- In general, $U$ terms---capturing unspecified disturbances---represent features of the world that we are *not* able to directly observe. In some cases, we may not even have a specific conceptualization of the phenomena in the world to which a $U$ term corresponds. Nonetheless, we may be able to make *inferences* about the value of a $U$ term from observed data. Indeed, given the role of the exogenous terms in a model in determining the operation of the world that the model describes, learning about a case's context becomes central to model-based causal inquiry and, as we shall see, lies at the heart of the framework that we are elaborating. -->

#### The distributions

Putting causal structure and causal functions together gives us a *structural causal model.* In a structural causal model, all endogenous nodes are, either directly or by implication, functions of a case's context (the values of the set of exogenous nodes).^[More formally, a **structural causal model** *over* signature $\mathcal{S}=<\mathcal{U},\mathcal{V},\mathcal{R}>$ is a pair $<\mathcal{S}, \mathcal{F}>$, where $\mathcal{F}$ is a set of ordered structural equations containing a function  $f_i$  for each element $Y\in \mathcal{V}$. We say that $\mathcal{F}$ is a set of ordered structural equations if no node is its own descendant and if no element in $\mathcal{U}$ is parent to more than one element of \(\mathcal{V}\). This last condition can be achieved by shifting any parent of multiple children in $\mathcal{U}$ to $\mathcal{V}$. This definition thus includes an assumption of acyclicity, which is not found in all definitions in the literature.] What we have not yet inscribed into the model, however, is beliefs about how *likely* or *common* different kinds of contexts might be. 

Thus, for instance, a structural causal model consistent with Figure \@ref(fig:simpleDAG) stipulates which nodes may have effects on which other nodes. But it says nothing in itself about the distribution of values of either the exogenous nodes or of the causal relations between nodes.^[Thus $P(d|i,e, u_D)$ would defined by this structural model (as a degenerate distribution), but $P(i)$, $P(e)$, $P(u_D)$, and $P(i,e, u_D)$ would not be.] We have not said anything, for instance, about how common high inequality is across the relevant domain of cases or how common ethnic homogeneity is. Put differently, we have said nothing about the *distribution* of $\theta^I$ or of $\theta^E$. Similarly, we have said nothing yet about how commonly mobilization has positive, negative, or null effects of democratization---that is, the distribution of $\theta^D$---or about how commonly $I$ and $E$ have different possible joint causal effects on $M$ (the distribution of $\theta^M$). 

In many research situations, we will have or want to posit a set of prior beliefs about how the world works under different conditions and about what kinds of conditions are more likely than others. We can express these beliefs about context as probability distributions over the model's $\theta^J$ terms. For instance, our structural causal model might tell us that $E$ and $R$ can jointly affect $M$. We might, then, add to this a belief about $\theta^M$ such that, in the population of interest, redistribution rarely has a positive effect on mobilization when ethnic homogeneity is low. We would thus be putting a low probability on the nodal types for $M$ in which $R$ has a positive effect on $M$ when $E=0$, relative to $M$'s other nodal types.^[Ordering the parent nodes alphabetically, the types we would be setting to a low probability would be $\theta^M_{0010}, \theta^M_{0110}, \theta^M_{0111}, \theta^M_{0011}$.] 

We might add to this the belief that $E=1$ in only 10\% of cases in the population of interest, thus setting a 0.1 probability on $\theta^E_1$. Note that these two beliefs jointly imply that $R$ will rarely have a positive effect on $M$. 

As with functions, we can also (and typically would) build uncertainty into our beliefs about the shares of different nodal types in the population. We do this by specifying a *distribution* over possible "share" allocations.^[More strictly our uncertainty is over probabilities. However it is sometimes more intuitive to describe uncertainty over shares. The distinction is not important for the applications later in which we typically assume units are independently drawn from a large population.] For instance, we can specify a distribution over the shares of cases with ethnic homogeneity ($\theta^E_1$), and a distribution over the shares of $\theta^M$ types, with our degrees of uncertainty captured by each distribution's variance. (More on these distributions in Chapter \@ref(bayeschapter).)



In the default setup, we assume that each $\theta$ term ($\theta^I, \theta^E, \theta^R$, etc.) is generated independently of the others.
<!-- ---that each nodal type for a given case is drawn from an independent distribution. -->

<!-- FLAG: Cross ref for boxes -->

While this is not without loss of generality, it is not as constraining as it might at first appear: any graph in which two $\theta$ terms are *not* independent can be replaced by a graph in which these two terms are themselves generated by a common, third $\theta$ term.^[Operationally, in the `CausalQueries` package, we can specify nodal types as having joint distributions.] This independence feature is critical for being able to read off relations of conditional independence from a graph (see Box below). If it cannot be defended then the graph needs to be modified to communicate that $\theta$s are not independent, typically using two headed arrows. More on this in section \@ref(graphing).  
<!-- Note also that one could envision "incomplete probabilistic causal models" in which researchers claim knowledge regarding distributions over *subsets* of $\theta$.  -->

<!-- With our non-parametric representation of functional forms, we let $\lambda_j^X$ denote the probability that $\theta^X = \theta^X_j$. For instance in a simple $X \rightarrow Y$ model, $\lambda^Y_{01}$ denotes the probability that $\theta^Y = \theta^Y_{01}$.  -->


<!-- FLAG: FLESH OUT INCLUDING HOW CONFOUNDING IS TREATED -->

<!-- Am trying to render all the indented paragraphs below as a single, multi-paragraph footnote. Have read up and tried all sorts of things, no luck yet. -->

<!-- Thus, a probabilistic causal model is a structural causal model coupled with a probability distribution over the model's exogenous nodes. A corresponding probabilistic model, however, might support a stronger claim of the form: "Condition $C$ arises with frequency $\pi^C$, and so $X$ causes $Y$ with probability $\pi^C$." -->

:::: {.headerbox data-latex=""}
::: {.center data-latex=""  #markov}
:::
**Technical Note on the Markov Property**

  The assumptions that no node is its own descendant and that the $\theta$ terms are generated independently make the model *Markovian*, and the parents of a given node are Markovian parents. Knowing the set of Markovian parents allows one to write relatively simple factorizations of a joint probability distribution, exploiting the fact ("the Markov condition") that all nodes are *conditionally independent* of their nondescendants, conditional on their parents: nodes $A$ and $B$ are "conditionally independent" given $C$ if $P(a|b,c) = P(a|c)$ for all values of $a, b$ and $c$.  
  To see how this Markovian property allows for simple factorization of $P$ for Figure \@ref(fig:simpleDAG), note that $P(X, R, Y)$ can always be written as: 
  $$P(X, R, Y) = P(X)P(R|X)P(Y|R, X)$$ 
  If we believe, as in the figure, that $X$ causes $Y$ only through $R$ then we have the slightly simpler factorization:
  $$P(X, R, Y) = P(X)P(R|X)P(Y|R)$$ 
  Or, more generally:

\begin{equation} 
P(v_1,v_2,\dots v_n) = \prod P(v_i|pa_i)
(\#eq:markov)
\end{equation} 

  The distribution $P$ on $\theta$ induces a joint probability distribution on $\mathcal{V}$ that captures not just information about how likely different states are to arise but also the relations of conditional independence between nodes that are implied by the underlying causal process. For example, if we thought that $X$ caused $Y$ via $R$ (and only via $R$), we would then hold that $P(Y | R) = P(Y | X, R)$: in other words if $X$ matters for $Y$ only via $R$ then, conditional on $R$, $X$ should not be informative about $Y$.   
  In this way, a probability distribution $P$ over a set of nodes can be consistent with some causal models but not others. This does not, however, mean that a specific causal model can be extracted from $P$. To demonstrate with a simple example for two nodes, any probability distribution on $(X,Y)$ with $P(x)\neq P(x|y)$ is consistent both with a model in which $X$ is a parent of $Y$ and with a model in which $Y$ is a parent of $X$.
::::

<br>


Once we introduce beliefs about the distribution of values of the exogenous terms (in our setup, the $\theta$ terms) in a model, we have specified a *probabilistic causal model.* We need not say much more, for the moment, about the probabilistic components of causal models. But to foreshadow the argument to come, our prior beliefs about the likelihoods of different contexts play a central role in the framework that we develop here. We will see how the encoding of contextual knowledge---beliefs that some kinds of conditions and causal effects are more common than others---forms a key foundation for causal inference. At the same time, our expressions of *uncertainty* about context represent scope for learning: it is the very things that we are, at a study's outset, uncertain about that we can update our beliefs about as we encounter evidence.


## Graphing models and using graphs

While we have been speaking to causal graphs throughout this chapter, we want to take some time to unpack their core features and uses. A key benefit of causal models is that they lend themselves to graphical representations; in turn, graphs constructed according to particular rules can aid causal analysis. In the next subsection we discuss a set of rules for representing a model in graphical form. The following subsection then demonstrates how access to a graph facilitates causal inference.

### Rules for graphing causal models {#graphing}

The diagram in Figure \@ref(fig:simpleDAG) is a causal DAG [@hernan2006instruments]. We endow it with the interpretation that an arrow from a parent to a child means that a change in the parent can, under some circumstances, induce a change in the child. Though we have already been making use of this causal graph to help us visualize elements of a causal model, we now explicitly point out a number of general features of causal graphs as we will be using them throughout this book. Causal graphs have their own distinctive "grammar," a set of rules that give them important analytic features.

**Directed, acyclic.** A causal graph represents elements of a causal model as a set of nodes (or vertices), representing nodes, connected by a collection of single-headed arrows (or directed edges).  We draw an arrow from node $A$ to node $B$ if and only if we believe that $A$ can have a direct effect on $B$. The resulting diagram is  a *directed acyclic* graph (DAG) if there are no paths along directed edges that lead from any node back to itself---i.e., if the graph contains no causal cycles. The absence of cycles (or "feedback loops") is less constraining than it might appear at first. In particular if one thinks that $A$ today causes $B$ tomorrow which in turn causes $A$ today, we can represent this as $A_1 \rightarrow B \rightarrow A_2$ rather than $A \leftrightarrow B$. That is, we timestamp the nodes, turning what might informally appear as feedback into a non-cyclical chain.

**Meaning of missing arrows.**  The *absence* of an arrow between $A$ and $B$ means that $A$ is not a direct cause of $B$.^[By "direct" we mean that the $A$ is a parent of $B$: i.e., the effect of $A$ on $B$ is not fully mediated by one or more other nodes in the model.] Here lies an important asymmetry: drawing an $A \rightarrow B$ arrow does not mean that we know that $A$ *does* directly cause $B$; but omitting such an arrow implies that we know that $A$ does *not* directly cause $B$. We say more, in other words, with the arrows we omit than with the arrows that we include.

Returning to Figure \@ref(fig:simpleDAG), we have here expressed the belief that redistributive preferences exert no direct effect on democratization; we have done so by *not* drawing an arrow directly from $R$ to $D$. In the context of this model, saying that redistributive preferences have no direct effect on democratization is to say that any effect of redistributive preferences on democratization *must* run through mobilization; there is no other pathway through which such an effect can operate. This might be a way of encoding the knowledge that mass preferences for redistribution cannot induce autocratic elites to liberalize the regime absent collective action in pursuit of those preferences. 

The same goes for the effects of $I$ on $M$, $I$ on $D$, and $E$ on $D$: the graph in Figure \@ref(fig:simpleDAG) implies that we believe that these effects also do not operate directly, but only along the indicated, mediated paths.

**Sometimes-causes.** The existence of an arrow from $A$ to $B$ does not imply that $A$ always has a direct effect on $B$. Consider, for instance, the arrow running from $R$ to $M$. The existence of this arrow requires that $R$ appears somewhere in $M$'s functional equation, as a node's functional equation must include all nodes pointing directly into it. Imagine, though, that $M$'s causal function were specified as: $M = RE$. This function would allow for the *possibility* that $R$ affects $M$, as it will whenever $E=1$. However, it would also allow that $R$ will have no effect, as it will when $E=0$. 

<!-- This example also, incidentally, demonstrates another important consequence of context, the values of the exogenous nodes: a case's context determines not just the settings on the endogenous nodes, but also the causal *effects* that prevail among the nodes. Under the functional equation $M=RE$, a case's ethnic-compositional context determines whether or not redistributive preferences will have an effect on mobilization.  -->

<!-- **Representing $U$ on the graph.** As a matter of convention, explicitly including $U$ terms is optional. In practice, $U$'s are often excluded from the visual representation of a model on the understanding that every node on the graph is subject to some unaccounted-for influence and thus, implicitly, has a $U$ term pointing into it. In this book, we will generally draw the $U$ terms where they are of particular theoretical or analytic interest but will otherwise omit them. Whether we include or omit $U$ terms, we will generally treat those nodes in a graph that have no arrows pointing into them as the exogenous nodes that define the context. -->

<!-- AJ: Have commented out above because I think we've sufficiently dealt with the $U$'s and it's going to be very confusing to say U's are implicit when we've said we're using theta's instead of U;s. -->
<!-- MH OK -->

**No excluded common causes.** Any cause common to multiple nodes on a graph must itself be represented on the graph. If $A$ and $B$ on a graph are both affected by some third node, $C$, then we must represent this common cause. Thus, for instance, the graph in Figure \@ref(fig:simpleDAG) implies that $I$ and $E$ have no common cause. If in fact we believed that a country's level of inequality and its ethnic composition were both shaped by, say, its colonial heritage, then this DAG would *not* be an accurate representation of our beliefs about the world. To make it accurate, we would need to add to the graph a node capturing that colonial heritage and include arrows running from colonial heritage to both $I$ and $E$. 

This rule ensures that the graph captures all potential correlations among nodes that are implied by our beliefs. If $I$ and $E$ are in fact driven by some common cause, then this means not just that these two nodes will be correlated but also that each will be correlated with any consequences of the other. For instance, a common cause of $I$ and $E$ would also imply a correlation between $R$ and $E$. $R$ and $E$ are implied to be independent in the current graph but would be implied to be correlated if a common node pointed into both $I$ and $E$.

Of particular interest in Figure \@ref(fig:simpleDAG) is the implied independence of $\theta^J$'s from one another. Imagine, for instance, that the distribution of $\theta^D$ were different if $I=0$ or $I=0$. This would represent a classic form of confounding: the assignment of cases to values on the explanatory node would be correlated with the case's potential outcomes on $D$. The omission of any such pathway is precisely equivalent to expressing the belief that $I$ is exogenous, i.e., (as if) randomly assigned.

**Representing unobserved confounding.**  It may be however that there are common causes for nodes that we simply do not understand. We might believe that some unknown factor (partially) determines both $I$ and $D$, which is the same as saying that $\theta^I$ and $\theta^D$ are not independently distributed. If we were to represent the $\theta$ terms on the graph we might then want to represent a single term $(\theta^I, \theta^D)$ that points into both $I$ and $D$. Usually however the $\theta$ terms are omitted from graphs and in this case we would  represent the unobserved confounding by adding a dotted line, or a two headed arrow, connecting nodes whose unknown components are not independent. Figure \@ref(fig:simpleDAGb) illustrates. We address this kind of unobserved confounding later in the book and show how we can seek to learn about the joint distribution of nodal types in such situations. 

```{r simpleDAGb, echo = FALSE, fig.cap="A DAG with unobserved confounding", out.width = "60%", fig.height = 4, fig.width = 6, fig.align="center"}
make_model("I -> D; I <-> D") %>% plot
```


**Licence to exclude nodes.** The flip side of the "no excluded common causes" rule is that a causal graph, to do the work it must do, does not need to include everything we know about a substantive domain of interest. We may know quite a lot about the causes of economic inequality, for example. But we can safely omit any other factor from the graph as long *as it does not affect multiple nodes in the model.* Indeed, $\theta^I$ in Figure \@ref(fig:simpleDAG) already implicitly captures all factors that affect $I$, just as $\theta^D$ captures all factors *other than* mobilization that affect democratization. We may be aware of a vast range of forces shaping whether countries democratize, but choose to bracket them for the purposes of an examination of the role of economic inequality. This bracketing is permissible as long as none of these unspecified factors also act on  other nodes included in the model. 


**We can't read functional equations from a graph.** As should be clear, a DAG does not represent all features of a causal model. What it does record is which nodes enter into the structural equation for every other node: what can directly cause what. But the DAG contains no other information about the form of those causal relations. Thus, for instance, the DAG in Figure \@ref(fig:simpleDAG) tells us that $M$ is function of both $R$ and $E$, but it does not tell us whether that joint effect is additive ($R$ and $E$ separately increase mobilization) or interactive (the effect of each depends on the value of the other), or whether either effect is linear, concave, or something else. This lack of information about functional forms often puzzles those encountering causal graphs for the first time: surely it would be convenient to visually differentiate, say, additive from interactive effects. As one thinks about the variety of possible causal functions, however, it quickly becomes clear that there would be no simple visual way of capturing all possible functional relations. Moreover, causal graphs do not require functional statements to perform their main analytic purpose---a purpose to which we now turn.


### Conditional independence from DAGs

If we encode our prior knowledge using the grammar of a causal graph, we can put that knowledge to work for us in powerful ways. In particular, the rules of DAG-construction allow for an easy reading of the *conditional independencies* implied by our beliefs. (For another, somewhat more extended treatment of the ideas in this section, see @rohrer2018thinking.)

To begin thinking about conditional independence, it can be helpful to conceptualize dependencies between nodes as generating *flows of information*. Let us first consider a simple relationship of dependence. Returning to Figure \@ref(fig:simpleDAG), the arrow running from $I$ to $R$, implying a direct causal dependency, means that we expect $I$ and $R$ to be correlated. Put differently, observing the value of one of these nodes also gives us information about the value of the other. If we measured redistributive preferences, the graph implies that we would also be in a better position to infer the level of inequality, and vice versa. Likewise, $I$ and $M$ are also linked in a relationship of dependence: since inequality can affect mobilization (through $R$), knowing the the level of inequality would allow us to improve our estimate of the level of mobilization and vice versa.

In contrast, consider $I$ and $E$, which are in this graph indicated as being *independent* of one another. Learning the level of inequality, according to this graph, would give us no information whatsoever about the degree of ethnic homogeneity, and vice-versa.

Moreover, sometimes what we learn depends on *what we already know.* Suppose that we already knew the level of redistributive preferences. Would we then be in a position to learn about the level of inequality by observing the level of mobilization? According to this graph we would not: since the causal link---and, hence, flow of information between $I$ and $M$---runs through $R$, and we already know $R$, there is nothing left to be learned about $I$ by also observing $M$. Anything we could have learned about inequality by observing mobilization is already captured by the level of redistributive preferences, which we have already seen. In other words, if we were not to include $R$ in the causal model, then $I$ and $M$ would be dependent and informative about each other. When we do include $R$ in the causal graph, $I$ and $M$ are independent of one another and, hence, uninformative about each other. We can express this idea by saying that $I$ and $M$ are *conditionally independent given $R$*. 

We say that two nodes, $A$ and $C$, are "conditionally independent" given a set of nodes $\mathcal B$ if, once we have knowledge of the values in $\mathcal B$, knowledge of $A$ provides no information about $C$ and vice-versa. Taking $\mathcal B$ into account thus "breaks" any relationship that might exist unconditionally between $A$ and $C$. 

To take up another example, suppose that war is a cause of both military casualties and price inflation, as depicted in Figure \@ref(fig:warDAG). Casualties and inflation will then be (unconditionally) correlated with one another because of their shared cause. If we learn that there have been military casualties, this information will lead us to think it more likely that there is also war and, in turn, price inflation (and vice versa). However, assuming that war is their only common cause, we would say that military casualties and price inflation are *conditionally independent given war.* If we already know that there is war, then we can learn nothing further about the level of casualties (price inflation) by learning about price inflation (casualties). We can think of war, when observed, as blocking the flow of information between its two consequences; everything we would learn about inflation from casualties is already contained in the observation that there is war. Put differently, if we were just to look at cases where war is present (i.e., if we hold war constant), we should find no correlation between military casualties and price inflation; likewise, for cases in which war is absent.  


```{r warDAG, echo = FALSE, fig.width = 8, fig.height = 4,  fig.align="center", out.width = "60%", fig.cap = "This graph represents a simple causal model in which war ($W$) affects both military casualties ($C$) and price inflation ($P$)."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 1),
       y = c(1, 2, 0),
       names = c(
         "W", 
         "C",
         "P"
         ),
       arcs = cbind( c(1, 1),
                     c(2, 3)),
       title = "A Model of War's Effect on Casualties and Prices",
       padding = .2, contraction = .15) 

```

Relations of conditional independence are central to the strategy of statistical control, or covariate adjustment, in correlation-based forms of causal inference, such as regression. In a regression framework, identifying the causal effect of an explanatory node, $X$, on a dependent node, $Y$, requires the assumption that $X$'s value is conditionally independent of $Y$'s potential outcomes (over values of $X$) given the model's covariates. To draw a causal inference from a regression coefficient, in other words, we have to believe that including the covariates in the model "breaks" any biasing correlation between the value of the causal node and its unit-level effect.

As we will explore, however, relations of conditional independence are also of more general interest in that they tell us, given a model, *when information about one feature of the world may be informative about another feature of the world, given what we already know*. By identifying the possibilities for learning, relations of conditional independence can thus guide research design. We discuss these research-design implications in Chapter \@ref(pt), but focus here on showing how relations of conditional independence operate on a DAG.


<!-- Some possibilities are excluded by the framework, however: for example, one cannot represent uncertainty regarding whether $A$ causes $B$ or $B$ causes $A$. -->

<!-- It would be nice to make a less general statement than the above as it sounds like none of this has any relevance if we think there's reciprocal causation in the causal system of interest; and that sounds like it excludes A LOT of problems political scientists are interested in. Pearl has a bit of discussion of the fact that one can compute the effect of interventions for models with cyclic features as well. So can we put this point more narrowly? Have been looking into this a bit but not yet sure exactly how to do that. -->




<!-- The above point is something I want to get more clarity around. Also, why we are defining roots as we are. -->

<!-- In Figure \@ref(fig:simpleDAG) we show a simple DAG that represents a situation in which $X$ is a parent of $M$, and $M$ is a parent of $Y$. In this example, the three nodes $\theta^X$, $\theta^M$, and $\theta^Y$ are all exogenous and thus elements of \(\mathcal{U}\). $X$, $M$, and $Y$ are endogenous and members of  \(\mathcal{V}\). If these three nodes were binary, then there would be eight possible realizations of outcomes, i.e., of \(\mathcal{V}\). In the underlying model,  $\theta^X$ is an ancestor of $X$, $M$, and $Y$ which are all descendants of $\theta^X$. The elements of $\mathcal{U}$ are all roots, though $X$ is also  a root as it has no parent in $\mathcal{V}$.   -->

 
<!-- In addition we will usually omit nodes from a graph only if they are single parents---this has the advantage of clarifying that all uncertainty is over the value of roots, and not over functional forms given roots; this is without loss of generality as parameters for functional equations can themselves be represented as roots.      -->

<!-- As a very simple example one might imagine that $A$ an $B$ are independently generated binary nodes; $C$ is an indicator for whether $A$ and $B$ have the same value. Then obviously if you know $C$, then knowing $A$ tells you everything about $B$. -->

To see more systematically how a DAG can reveal conditional independencies, it is useful spell out three elemental structures according to which which information can flow across a causal graph:


```{r pathsexp, echo = FALSE, fig.width = 6, fig.height = 4,  fig.align="center", out.width='70%', fig.cap = "\\label{fig:CI} Three elemental relations of conditional independence."}
par(mfrow = c(3,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(1, 2),
                     c(2, 3)),
       title = "(a) A path of arrows pointing in the same direction", padding = .2, contraction = .15) 
hj_dag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(2, 2),
                     c(1, 3)),
       title = "(b) A forked path", padding = .2, contraction = .15) 
hj_dag(x = c(-1, 0, 1), y = c(1,1,1), names = c( "A", "B", "C" ),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "(c) An inverted fork (collision)", padding = .2, contraction = .15) 

```


(1a) Information can flow unconditionally along a path of arrows pointing in the same direction. In Panel 1 of Figure \@ref(fig:CI), information flows across all three nodes. Learning about any one will tell us something about the other two. 

(1b) Learning the value of a node along a path of arrows pointing in the same direction *blocks* flows of information across that node. Knowing the value of $B$ in Panel 1 renders $A$ no longer informative about $C$, and vice versa: anything that $A$ might tell us about $C$ is already captured by the information about $B$.

(2a) Information can flow unconditionally across the branches of any forked path. In Panel 2 learning only $A$ can provide information about $C$ and vice-versa.

(2b) Learning the value of the node at the forking point blocks *flows* of information across the branches of a forked path. In Panel 2, learning $A$ provides no information about $C$ if we already know the value of $B$.^[Readers may recognize this statement as the logic of adjusting for a confound that is a cause of both an explanatory node and a dependent node in order to achieve conditional independence.]

(3a) When two or more arrowheads collide, generating an inverted fork, there is no unconditional flow of information between the incoming sequences of arrows. In Panel 3, learning only $A$ provides no  information about $C$, and vice-versa. 

(3b) Collisions can be sites of *conditional* flows of information. In the jargon of causal graphs, $B$ in Panel 3 is a "collider" for $A$ and $C$.^[In the familial language of causal models, a collider is a child of two or more parents.] Although information does not flow unconditionally across colliding sequences, it does flow across them *conditional* on knowing the value of the collider node or any of its downstream consequences. In Panel 3, learning $A$ *does* provide new information about $C$, and vice-versa, *if* we also know the value of $B$ (or, in principle, the value of anything that $B$ causes). 

The last point is somewhat counter-intuitive and warrants further discussion. It is easy enough to see that, for two nodes that are correlated unconditionally, that correlation can be "broken" by controlling for a third node. In the case of collision, two nodes that are *not* correlated when taken by themselves *become* correlated when we condition on (i.e., learn the value of) a third node, the collider. The reason is in fact quite straightforward once one sees it: if an outcome is a joint function of two inputs, then if we know the outcome, information about one of the inputs can provide information about the other input. For example, if I know that you have brown eyes, then learning that your mother has blue eyes makes me more confident that your father has brown eyes. 

Looking back at our democratization DAG in Figure \@ref(fig:simpleDAG), $M$ is a collider for $R$ and $E$, its two inputs. Suppose that we again have the functional equation $M=RE$. Knowing about redistributive preferences alone provides no information whatsoever about ethnic homogeneity since the two are determined independently of one another. On the other hand, imagine that we already know that there was no mobilization. Now, if we observe that there *were* redistributive preferences, we can figure out the level of ethnic homogeneity: it must be 0. (And likewise in going from homogeneity to preferences.)

Using these basic principles, conditional independencies can be read off of any DAG. We do so by checking every path connecting two nodes of interest and ask whether, along those paths, the flow of information is open or blocked, given any other nodes whose values are already observed. Conditional independence is established when *all* paths are blocked given what we already know; otherwise, conditional independence is absent.

Following @pearl2000causality, we will sometimes refer to relations of conditional independence using the concept of *d-separation.* We say that variable set $\mathcal C$ $d-$separates variable set $\mathcal A$ from variable set $\mathcal B$ if $\mathcal A$ and $\mathcal B$ are conditionally independent given $\mathcal C$. We say that $\mathcal A$ and $\mathcal B$ are $d-$connected given $\mathcal C$ if $\mathcal A$ and $\mathcal B$ are *not* conditionally independent given $\mathcal C$. 


### Simplifying models

It is very easy to write down a model that is too complex to use effectively. In such cases we often seek simpler models that are consistent with models we have in mind but contain fewer nodes or more limited variation. In general this is possible but caution has to be taken to ensure that simplified models are indeed consistent with the original model. 

Fortunately the mapping between graphs and relations of conditional independence give guidance for determining when and how it is possible to simplify models.  We focus discussion on simplifications that involve node elimination and conditioning on nodes.

#### Eliminating nodes

If we want to eliminate a node the key rule is that the new model (and graph) must take into account: 

(a) all *dependencies* among remaining nodes and 
(b) all *variation* generated by the eliminated node. 

We can work out what this means, separately, for eliminating *endogenous* nodes and for eliminating *exogenous* nodes.

*Eliminating endogenous nodes*

Eliminating an endogenous node means removing a node with parents (direct causes) represented on the graph. If the node also has one or more children, then the node captures a dependency: it links its parents to its children. When we eliminate this node, preserving these dependencies requires that all of the eliminated node's parents adopt---become parents of---all of the eliminated node's children. Thus, for instance if we had a model in which $A \rightarrow M \leftarrow B$ and $M \rightarrow Y$, if we were to eliminate $M$, $M$'s parents ($A$ and $B$) would need to adopt $M$'s child, $Y$. 

<!-- As for $\theta^M$, it too must now point directly into $Y$---though we can use a bit of shorthand to make this happen. Recall that $\theta^M$ represents the part of $M$ that is randomly determined. Rather than drawing two separate disturbance ($\theta$) terms pointing into $Y$, however, we more simply represent the combined disturbance term as $\theta^Y_{\text{higher}}$, with the ''higher'' signaling the aggregation of roots. (This is, of course, simply reversing the disaggregation that we undertook earlier to move from the higher- to the lower-level model.) -->

More intuitively, when we simplify away a mediator, we need to make sure that we preserve the causal relationships being mediated---both those among substantive variables and any random shocks at the mediating causal steps.^[Eliminating endogenous nodes may also operate via "encapsulated conditional probability distributions" [@koller2009probabilistic] wherein a system of nodes, $\{Z_i\}$  is represented by a single node, $Z$,  that takes the parents of  $\{Z_i\}$ not in $\{Z_i\}$ as parents to $Z$ and issues the children of $(Z_i)$ that are not  in $(Z_i)$ as children. However, this is not a fundamental alteration of the graph.]

*Eliminating exogenous nodes*

What about eliminating exogenous nodes---nodes with no parents? For the most part, exogenous nodes cannot be eliminated, but must either be replaced by or incorporated into $U$ (or $\theta$) terms. The reason is that we need preserve any dependencies or variation generated by the exogenous node.  Figure \@ref(fig:elimrules) walks through four different situations in which we might want to simplify away the exogenous node, $X$. (Here we use the more generic $U$ notation, though the same principles apply if these are type-receptacles $\theta$.)

```{r elimrules, echo = FALSE, fig.width = 10, fig.height = 10,  fig.align="center", out.width='.5\\textwidth', fig.cap = "Here we represent the basic principles for eliminating exogenous nodes."}


par(mfrow = c(4,2))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2),
       y = c(2,3,1),
       names = c(
         expression(paste(X)),
         expression(paste(W)),
         expression(paste("Y"))
         ),  
       arcs = cbind( c(1, 1),
                     c(3, 2)),
       title = "(a1) Original model: X has two children",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2,2),
       y = c(2,3,1),
       names = c(
         expression(paste(theta^{WY})),
         expression(paste(W)),
         expression(paste(Y))
       ),
       arcs = cbind( c(1, 1),
                     c(3, 2)),
       title = "(a2) Simplification: Dependency between W and Y must be preserved",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,1,2),
       y = c(3,1,2),
       names = c(
         expression(paste(X)),
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 2),
                     c(3, 3)),
       title = "(b1) Original model: X has a substantive spouse",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,1,2),
       y = c(3,1,2),
       names = c(
         expression(paste(theta^Y)),
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 2),
                     c(3, 3)),
       title = "(b2) Simplification: X must be replaced with U term",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,1,2),
       y = c(3,1,2),
       names = c(
         expression(paste(X)),
         expression(paste(theta^Y)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 2),
                     c(3, 3)),
       title = "(c1) Original model: X has a random spouse",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,2),
       y = c(1,1),
       names = c(
         expression(paste(theta^Y[mod])),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1),
                     c(2)),
       title = "(c2) Simplification: X absorbed into $\theta$ term",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,3,2),
       y = c(1,1,1),
       names = c(
         expression(paste(X)),
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(1, 3),
                     c(3, 2)),
       title = "(d1) Original model: X has one child and no spouse",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(3,2),
       y = c(1,1),
       names = c(
         expression(paste(W)),
         expression(paste("Y"))
       ),
       arcs = cbind( c(2),
                     c(1)),
       title = "(d2) Simplification: X can be safely removed",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```



*  *Multiple children.* In (a1), we start with a model in which $X$ has two children, thus generating a dependency between $W$ and $Y$. If we eliminate $X$, we must preserve this dependency. We can do so, as pictured in (a2), by replacing $X$ with a $\theta$ term that  points into both $W$ and $Y$. By convention, we could, alternatively, convey the same information with a dashed, undirected line between $W$ and $Y$. Though we are no longer specifying what it is that connects $W$ and $Y$, the correlation itself is retained.
*  *Substantive spouse.* In (b1), $X$ has a spouse that is substantively specified, $W$. If we eliminate $X$, we have to preserve the fact that $Y$ is not fully determined by $W$; *something* else also generates variation in $Y$. We thus need to replace $X$ with a $\theta$ term, $\theta^Y$, to capture the variation in $Y$ that is not accounted for by $W$.
* *$\theta$-term spouse.* In (c1), $X$ has a spouse that is *not* substantively specified, $U^{Y}$. Eliminating $X$ requires, again, capturing the variance that it generates as a random input. As we already have a $\theta$ term pointing only into $Y$, we can substitute in $\theta^{Y}_\text{mod}$, which represents both $U^{Y}$ and the variance generated by $X$.^[This aggregation cannot occur if $\theta^{Y}$ also has another child, $W$, that is not a child of $X$ since then we would be representing $Y$'s and $W$'s random components as identical, which they are not in the original graph.]
*  *One child, no spouse.* In (d1), $X$ has only one child and no spouse. Here we can safely eliminate $X$ with no loss of information. It is always understood that every exogenous node has some cause, and there is no loss of information in simply eliminating a node's causes if those causes are exogenous and do not affect  other endogenous nodes in the model. In (d2) we are simply not specifying $Y$'s cause, but we have not lost any dependencies or sources of variance that had been expressed in (d1).


One interesting effect of eliminating a substantive exogenous node can be to render seemingly deterministic relations effectively probabilistic. In moving from (b1) to (b2), we have taken a component of $Y$ that was determined by $X$ and converting it into a random disturbance. Just as we can explain a more probabilistic claim with a less probabilistic theory, we can derive claims from simplified models with greater probabilism from theories with greater determinism.



```{r lowercomplexdem, echo = FALSE, fig.width = 10, fig.height = 6,  fig.align="center", out.width='.5\\textwidth', fig.cap = "A model  from which multiple simpler models can be derived."}

x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)

# names = c("S:\nSensitive\ngovernment\n\n", "\nX:\nFree Press", "C:\n Corruption", "R:\n Media report", "Y:\nGovernment\nreplaced")

names = c(
         expression(paste("I: Inequality")),
         expression(paste("E: Ethnic\nhomogeneity")),
         expression(paste("R: Redistributive\npreferences")),
         expression(paste("M: Mobilization")),
         expression(paste("D: Democratization"))
         )

hj_dag(x =  x,
       y = y,
       names = c(names),
       arcs = cbind( c(1,2,2, 3, 4, 3),
                     c(3,3,4, 5, 5, 4)),
       title = "A Model of Inequality and Ethnic Homogeneity's Effects on Democratization",
       add_functions = 0,
       contraction = .15,
       add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS",
       padding = .2)

# hj_dag(x =  c(x, 0, 0),
#        y = c(y, 0.25, 1.75),
#        names = c(names),
#        arcs = cbind( c(1,2,2, 3, 4, 3),
#                      c(3,3,4, 5, 5, 4)),
#        title = "Free Press and Government Survival",
#        add_functions = 0,
#        contraction = .15,
#        add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS",
#        padding = .2)

# text(c(0,0), c(.25, 1.75), c(expression(paste(theta^X[1])), expression(paste(theta^S[1]))))

```

<!-- ```{r lowercomplexdem, echo = FALSE, fig.width = 10, fig.height = 6,  fig.align="center", out.width='.5\\textwidth', fig.cap = "A lower-level model  from which multiple higher level models can be derived."} -->


<!-- par(mfrow = c(1,1)) -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(1,1,2,2,3), -->
<!--        y = c(2,1,2,1,1.5), -->
<!--        names = c( -->
<!--          expression(paste("I: Inequality")), -->
<!--          expression(paste("E: Ethnic\nhomogeneity")), -->
<!--          expression(paste("M: Mobilization")), -->
<!--          expression(paste("R: Redistributive\npreferences")), -->
<!--          expression(paste("D: Democratization")) -->
<!--          ),   -->
<!--        arcs = cbind( c(1, 3, 2, 2, 3, 4), -->
<!--                      c(3, 4, 4, 3, 5, 5)), -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .2 -->
<!-- ) -->
<!-- ``` -->



```{r runningsubs, echo = FALSE, fig.width = 12, fig.height = 13.5, fig.cap = "Simplifications of the model of Figure X. Nodes that are eliminated are marked in grey; circles denote exogenous nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two other nodes could equivalently be indicated as an undirected edge connecting the two.)"}

par(mfrow = c(5,5))
par(mar=c(1,1,3.5,1))
x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)
# names = c("S", "X", "C", "R", "Y")
names = c("I", "E", "R", "M", "D")

M <- matrix(0, 5, 5)
M[1, c(3)] <-1
M[2, c(3,4)] <-1
M[3, c(4,5)] <-1
M[4, 5] <-1

matrix_remove <- function(M, remove = NULL){
  M2 <- M
if(!is.null(remove)) {
  for(j in remove) {M2 <- (M2 + outer(M[, j], M[j,]))}  # connect parents to children
  for(j in remove) {if(sum(M2[,j]>0)) {M2[j,] <- 0; M2[,j] <- 0}}  # disconnect non-roots (roots may have to be renamed)
}
M2[M2>1] <- 1
M2
}

removes <- perm_bb(c(2,2,2,2,2))[-1,]
removes <- removes[rowSums(removes)<=3,]
removes <- removes==1
removes <- removes[order(rowSums(removes) - (1:nrow(removes))/100),]
for(j in 1:nrow(removes)){
GO <- (1:5)[removes[j,]]
    hj_dag(
       x = x,
       y = y,
       names = names,
       arcs = which(matrix_remove(M, GO)==1, arr.ind = TRUE),
       add_points = FALSE,
       solids = c(mysolids[[j]]),
       )
text(x[GO],y[GO],names[GO], col = "grey")
title(j, adj=0)
for(i in 1:ncol(M)) if((sum(M[,i])==0) & (i%in% GO)) points(x[i], y[i], cex = 3, col = "grey")
}
```


We can apply these principles to a model of any complexity. We illustrate a wider range of simplifications by starting with Figure \@ref(fig:lowercomplexdem). In Figure \@ref(fig:runningsubs), we show all permissible reductions of the more elaborate model. We can think of these reductions as the full set of simpler claims (involving at least two nodes) that can be derived from the original model. In each subgraph, 

* we mark eliminated nodes in grey; 
* those nodes that are circled must be replaced with $\theta$ terms; and
* arrows represent the causal dependencies that must be preserved. 

Note, for instance, that neither $S$ (because it has a spouse) nor $X$ (because it has multiple children) can be simply eliminated; each must be replaced with a $\theta$ term. Also, the simplified graph with nodes missing can contain arrows that do not appear at all in the graph: eliminating $C$, for instance, forces an arrow running from $X$ to $R$ (though that is there already) and another running from $X$ to $Y$, as $X$ must adopt $M$'s children. The simplest elimination is of $Y$ itself since it does not encode any dependencies between other variables.
<!-- ^[Put differently, and in language that we introduce below, colliding arrowheads do not represent a path in DAG analysis.]  -->



#### Conditioning on nodes

Another way to simpify a model is to condition on the value of a node. When we condition on a node, we are restricting the model in scope to situations in which that node's value is held constant. Doing so allows us to eliminate the node as well as all arrows pointing into it or out of it. Consider three different situations in which we might condition on a node:


* *Exogenous, with multiple children.* In simplifying (a1) in Figure \@ref(fig:elimrules), we need to be sure we retain any dependence that $X$ generates between $W$ and $Y$. However, recalling the rules of conditional independence on a graph (see Chapter \@ref(models)), we know that $W$ and $Y$ are *independent* conditional on $X$. Put differently, if we restrict the analysis to contexts in which $X$ takes on a constant value, the model implies that $Y$ and $W$ will be uncorrelated across cases. As fixing $X$'s value breaks the dependence between $Y$ and $W$, we can drop $X$ (and the arrows pointing out of it) without having to represent that dependence. 
*  *Exogenous, with spouse.* In simplifying (b1) or (c1) in Figure \@ref(fig:elimrules), we need to account for the variation generated by $X$. If we fix $X$'s value, however, then we eliminate this variation by assumption and do not need to continue to represent it (or the arrow pointing out of it) on the graph.
*  *Endogenous.* When we condition on an endogenous node, we can eliminate the node as well the arrows pointing into and out of it. We, again, leverage relations of conditional independence here. If we start with model $X \rightarrow M \rightarrow Y$ and we condition on the mediator, $M$, we sever the link between $Y$ and $X$, rendering them conditionally independent of one another. We can thus remove $M$, the arrow from $X$ to $M$, and the arrow from $M$ to $Y$. In the new model, with $M$ fixed, $Y$ will be entirely determined by the random disturbance $\theta^{Y}$.

### Retaining probabilistic relations

We have highlighted the graphical implications of node elimination or node conditioning but importantly the distribution over $\theta$ also needs to be preserved faithfully in a move to a simpler model.


In sum, we can work with models that are simpler than our causal beliefs: we may believe a model to be true, but we can derive from it a sparer set of claims. There may be intervening causal steps or features of context that we believe matter, but that are not of  interest for a particular line of inquiry. While these can be removed, we nonetheless have to make sure that their *implications* for the relations remaining in the model are not lost. Understanding the rules of reduction allow us to undertake an important task: checking which simpler claims are and are not consistent with our full belief set.


## Conclusion {#conc2}

In this chapter, we have shown how we can inscribe causal beliefs, rooted in the potential outcomes framework, into a causal model. In doing so, we have now set out the foundations of the book's analytic framework. Causal models are both the starting point for analysis in this framework and the object about which we seek to learn. Before moving on to build on this foundation, we aim in the next chapter to offer further guidance by example on the construction of causal models, by illustrating how a set of substantive social scientific arguments from the literature can be represented in causal model form. 


<!-- ### A simple running example -->

<!-- We can illustrate these core ideas with a simple example of a model of government corruption and survival, one that we will return to in future chapters. -->

<!-- We begin with two binary features of context. Consider, first, that a country may or may not have a free press ($X$). Second, the country's government may or may not be sensitive to public opinion ($S$).^[Government sensitivity here can be thought of as government sophistication (does it take the actions of others into account when making decisions?) or as a matter of preferences (does it have a dominant strategy to engage in corruption?).] Let us then stipulate what follows from these conditions. The government will engage in corruption ($C=1$) unless it is sensitive to public opinion and there is a free press. Moreover, if and only if there is both government corruption and a free press, the press will report on the corruption ($R=1$). Finally, the government will be removed from office ($Y=1$) if it has acted corruptly and this gets reported in the press; otherwise, the government remains in office. -->

<!-- As a set of equations, this simple causal model may be written as follows: -->

<!-- $\begin{array}{ll} -->
<!-- C = 1-X\times S &  \mbox{Whether the government is corrupt}\\ -->
<!-- R = C\times X &  \mbox{Whether the press reports on corruption}\\ -->
<!-- Y = C\times R & \mbox{Whether the government is removed from office} -->
<!-- \end{array}$ -->

<!-- One thing that these equations make clear is that the nodes in our model function in various places as causal-type nodes for one another. For instance, we can see from equation for $C$ that the causal effect of a free press ($X$) on corruption ($C$) depends on whether the government is sensitive to public opinion ($S$): $S$ determines $C$'s response to $X$ (as does $X$ for $S$'s effect on $C$). A similar relationship holds for $C$ and $X$ in their effect on $R$ and for $C$ and $R$ in their effect on $Y$. As we will see below, the model also implies more complex causal-type relationships. We can, further, substitute through the causal processes to write down the functional equation for the outcome in terms of the two initial causal nodes: $Y=(1-S)X$.^[In Boolean notation (but preserving a structural equation interpretation), where $Y$ stands for the occurrence of government removal, $Y= \neg S \land X$; and the function for the outcome "government retained" can be written  $\neg Y = (S\land X) \lor (S\land\neg X) \lor (\neg S \land \neg X)$ or, equivalently, $\neg Y = S + \neg S \neg X$.] -->

<!-- Let us, further, allow the two primary causal nodes---the existence of a free press and the existence of a sensitive government---to vary probabilistically. In particular, we represent the probability of a free press with the population parameter $\lambda^X_1$ and the probability of a sensitive government with the parameter $\lambda^S_1$.  -->

<!-- Note that in this model, only the most "senior" specified nodes, $X$ and $S$, have a stochastic component (i.e., $\lambda^X_1$ and $\lambda^S_1$ lie between 0 and 1). All other endogenous nodes are deterministic functions of other specified nodes (put differently: each node has only a single nodal type). -->


<!-- ```{r running, echo = FALSE, fig.width = 11, fig.height = 9, fig.align="center", out.width = "70%", fig.cap = "The figure shows a simple causal model. $S$ and $X$ are stochastic, other nodes fully determined by their parents, as shown in bottom right panel.", fig.align="center", warning = FALSE, eval = FALSE} -->


<!-- x = c(0,0, 1, 1, 2) -->
<!-- y = c(2,0, 2, 0, 1) -->

<!-- names = c("S:\nSensitive\ngovernment\n\n", "\nX:\nFree Press", "C:\n Corruption", "R:\n Media report", "Y:\nGovernment\nreplaced") -->

<!-- hj_dag(x =  x, -->
<!--        y = y, -->
<!--        names = c(names), -->
<!--        arcs = cbind( c(1,2,2, 3, 4, 3), -->
<!--                      c(3,3,4, 5, 5, 4)), -->
<!--        title = "Free Press and Government Survival", -->
<!--        add_functions = 0, -->
<!--        contraction = .15, -->
<!--        add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS", -->
<!--        padding = .2) -->

<!-- # hj_dag(x =  c(x, 0, 0), -->
<!-- #        y = c(y, 0.25, 1.75), -->
<!-- #        names = c(names), -->
<!-- #        arcs = cbind( c(1,2,2, 3, 4, 3), -->
<!-- #                      c(3,3,4, 5, 5, 4)), -->
<!-- #        title = "Free Press and Government Survival", -->
<!-- #        add_functions = 0, -->
<!-- #        contraction = .15, -->
<!-- #        add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS", -->
<!-- #        padding = .2) -->

<!-- # text(c(0,0), c(.25, 1.75), c(expression(paste(theta^X[1])), expression(paste(theta^S[1])))) -->

<!-- ``` -->


<!-- The corresponding causal diagram for this model is shown in Figure \@ref(fig:running). -->

<!-- In later chapters we will develop this model and use it to illustrate different estimands and different strategies for case level inference. -->




## Chapter Appendix

### Steps for constructing causal models

:::: {.headerbox data-latex=""}
::: {.center data-latex=""  #markov}
:::
**Steps for constructing causal models**

1. Identify a set of variables in a domain of interest. These become the nodes of the model.

  * You should specify the range of each node: is it continuous or discrete?
  * May include $\theta$ terms representing unspecified, random influences

2. Draw a causal graph (DAG) representing beliefs about causal dependencies among these nodes

  * Capture direct effects only
  * Arrows indicate *possible*, not constant or certain, causal effects
  * The absence of an arrow between two nodes indicates a belief of *no* direct causal relationship between them
  * Ensure that the graph captures all correlations among nodes. This means that either (a) any common cause of two or more nodes is included on the graph (with implications for Step 1) or (b) correlated nodes are connected with a dashed, undirected edge.
  
3. Write down one causal function for each endogenous node

  * Each node's function must include all nodes directly pointing into it on the graph
  * Functions may take any form, as long as each set of possible causal values maps onto a single outcome value
  * Functions may express arbitrary amounts of uncertainty about causal relations
  
4. State probabilistic beliefs about the distributions of the exogenous nodes

  * How common or likely to do we think different values of the exogenous nodes are?
  * Are they independently distributed? If in step 2 you drew an undirected edge between nodes then you believe that the connected nodes are not independently distributed.
::::


### Model construction in code

Our `gbiqq` package provides a set of functions to implement all of these steps concisely for *binary* models -- models in which all nodes are dichotomous.

```{r modelconstr, message = FALSE, eval = FALSE}

# Steps 1 and 2 
# We define a model with three binary nodes and specified edges between them:
model <- make_model("X -> M -> Y")

# Unrestricted functional forms are allowed by default, though these can 
# also be reduced. Here we impose monotonicity at each step 
# by removing one type for M and one for Y
model <- set_restrictions(model, labels = list(M = "10", Y="10"))

# Step 4
# We set priors over the distribution of (remaining) causal types.
# Here we set "jeffreys priors"
model <- set_priors(model, distribution = "jeffreys")

# We now have a model defined as an R object. 
# You might plot it like this:
plot(model)

# Later we will ask questions of this model and update it using data.

```

These steps are enough to fully describe a binary causal model. Later in this book we will see how we can ask questions of a model like this but also how to use data to train it. 

<!-- If we want to know whether two nodes, $A$ and $C$, are conditionally independent given some set of nodes, $\mathcal B$, we need to find out whether any paths between $A$ and $C$ are "active" given $\mathcal B$. Put slightly differently, the question is whether nodes in $\mathcal{B}$ block information flows from $A$ to $C$---or rather allow, or even create, such flows. This can be assessed as follows. For each possible path between $A$ and $C$, we check whether there are three connected nodes $X, Y, Z$^[Where $X$ and/or $Y$ may be $A$ and/or $B$.] on that path that either: -->

<!-- (a) form a "chain" $X\rightarrow Y \rightarrow Z$ (going either direction) or "fork" $X\leftarrow Y\rightarrow Z$, with $Y \subset \mathcal{B}$,^[For instance, under this criterion, if we have $A\rightarrow W\rightarrow Y \rightarrow B$, and $Y$ is an element of the set $\mathcal{B}$, then this path is not active given $\mathcal{B}$. Similarly, if we have $A \leftarrow X\leftarrow Y\rightarrow Z \rightarrow B$, and $Y$ is in $\mathcal{B}$, then the path is not active given $\mathcal{B}$.] or  -->

<!-- (b) form an "inverted fork" $X \rightarrow Y \leftarrow Z$, with neither $Y$ nor its descendants in $\mathcal C$.^[For instance, if we have $A \rightarrow W \rightarrow Y \leftarrow Z \leftarrow B$, and $Y$ is *not* in $\mathcal{C}$, then the path is not active given $\mathcal{C}$. In other words, "inverted fork" paths are not unconditionally active. Knowing only $A$, in this setup, tells us nothing about $B$, and vice-versa.]  -->

<!-- If either of these conditions holds, then the path is blocked (not active) given $\mathcal C$. In the first case, any possible information flows along the path are *blocked* by a node in $\mathcal C$. In the second case, *no* node in $\mathcal C$ is *creating* an information flow that would not otherwise be present. If there are no active paths, then $A$ and $B$ are conditionally independent given $\mathcal C$. In the graph-analytic language of Pearl and others, $A$ and $B$ are said to be "$d$-separated" by $\mathcal{C}$.^[There are multiple techniques for establishing $d-$separation. Pearl's guide "$d-$separation without tears" appears in an appendix to @pearl2009causality.]   -->

<!-- Thus, in Figure \@ref(fig:simpleDAG), we can readily see that $X$ and $Y$ are conditionally independent given $R$: $X$ and $Y$ are *d-*separated by $R$.  Conversely, $R$ and $\theta^Y$ are unconditionally independent. However, conditioning on $Y$ $d-$*connects* $R$ and $\theta^Y$, generating a dependency between them. If and only if we know the outcome, $Y$, then learning $R$ yields information about $\theta^Y$. To foreshadow a point that we develop further later in the book, this analysis reveals how we can, and cannot, learn empirically about elements of our models. For instance, if $\theta^Y$ is a node of interest but not directly observable, then information on $R$ is unhelpful if we have not observed the outcome, $Y$, but *is* informative if we have. -->


<!-- ###Interventions in causal graphs -->

<!-- A second advantage of causal graphs is that they provide a useful structure for thinking through causal effects. In a causal-model framework, we think of a node's causal effect as being the effect of an imagined *intervention*: a manipulation of some node, $X$, that provides $X$ with a value that is *not* determined by its parents.  In an intervention, it is as if the causal function for $X$ is replaced by the function $X=x$, with $x$ being the constant value to which we have set $X$. If we take $X$ to be binary, to keep things simple, then the causal effect of $X$ on $Y$ is the difference between the value $Y$ would take on under the intervention $X=0$ (which can be written as $do (X)=0$) and the value $Y$ would take on under the intervention $X=1$ ($do(X)=1$).  -->

<!-- As this manipulative account of causation makes clear, analyzing the effect of $X$ requires us we to set aside the "natural causes" of $X$ itself in a particular way. We can readily see how this works graphically by considering a modified version of our free press/government survival model as displayed in Figure \ref{fig:DAGdirect}. As compared with Figure \ref {fig:simpleDAG}, this DAG represents a model in which $X$ has effects on $Y$ both indirectly through $R$ and directly. We might imagine, for instance, that part of the effect of a free press ($X$) on government turnover ($Y$) runs via media reports of official corruption ($R$) while part of the effect runs through a deterrent effect of a free press that reduces graft and thus leaves greater public resources for investment in public goods (neither of which mediating nodes are represented on the graph). -->

<!-- Suppose that we want to estimate the effect of an increase in media reports of corruption, $R$, on government survival, $Y$. We thus want to know the difference between the value that $Y$ would take on if the number of media reports were set at a  high level and the value $Y$ would take on if media reports were set to low. As should be clear from the graph, we cannot estimate this difference by comparing the observed value of $Y$ when media report levels are high to the observed value of $Y$ when media report levels are low. The reason is that $R$ has an ancestor, $X$, that affects $Y$ via a path that does not run through $R$. Thus, the value that $Y$ takes on when $R$ is observed to be high (or low) will be determined not just by $R$ but also by $X$, which is itself systematically correlated with $R$ by being its ancestor. The difference between $Y$'s values at two values of $R$ will thus itself be a combination of $R$'s effect and of $X$'s effect. -->

<!-- Instead, the query, "What is the effect of $R$ on $Y$?", must be conceived of in a way that separates out the effect of $X$ on $R$. We can represent the empirical conditions that we would need to estimate this effect in Figure \ref{fig:DAGdirectmut}. We have "mutilated" the original DAG by removing all arrows pointing into our causal node, $R$ (here, simply the arrow running from $X$ to $R$). This graph represents a world in which $R$ has been manipulated, rather than naturally caused. (Equivalently, it is a world in which $R$ is purely exogenous.) And what the mutilated graph tells us is that, in an empirical situation in which the  dependency of $R$ on $X$ could be removed, $R$'s causal effect on $Y$ *could* be estimated by comparing $Y$'s values at high and low levels of $R$.^[In the formulation used in  @pearl2009causality, an intervention involving an endogenous node $V_i$ can be written as $do(V_i)=v_i'$, or for notational simplicity $\hat{v}_i'$ (meaning $V_i$ is forced to take the particular value $v_i'$). The resulting distribution can be written as a modified version of Equation \@ref(eq:markov): -->

<!-- \begin{equation}  -->
<!-- P(v_1,v_2,\dots v_n|\hat{v}_i) = \prod_{-i}P(v_j|pa_j)\mathbb{1}(v_i = v_i')(\#eq:eqdo) -->
<!-- \end{equation} -->

<!-- where $-i$ indicates that the product is formed over all nodes $V_j$ other than $V_i$, and the indicator function ensures that probability mass is only placed on vectors (or worlds) with $v_i = v_i'$. This new distribution has a graphical interpretation, representing the probability distribution over a graph in which all arrows into $V_i$ are removed.  The difference between Equation \@ref(eq:markov) and \@ref(eq:eqdo) is the difference between an *observed* probability distribution and the effect of an *intervention*. The key differences is that, when we intervene to manipulate a node, we break the link between the node and its "natural" causes.] -->

<!-- It is a separate question how such an empirical situation might be generated. But it is not hard to see from Figure \ref{fig:DAGdirect} that the  dependency of $R$ on $X$ could be removed either via (i) directly manipulating $R$ in a manner orthogonal to $X$ or (ii) controlling for $X$, since any variation in $R$ conditional on $X$ would itself be independent of $X$. Equivalently, using graph-analytic logic, we can also see that conditioning on $X$ blocks the path between $R$ and $Y$ that generates the confound (called a "backdoor"), thus expunging the confounding effect from the observed correlation between $R$ and $Y$. -->


<!-- ```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:DAGdirect} As compared with Figure \ref {fig:simpleDAG}, this DAG represents a model in which $X$ has effects on $Y$ both indirectly through $R$ and directly. We might imagine, for instance, that part of the effect of a free press ($X$) on government turnover ($Y$) runs via media reports of official corruption ($R$) while part of the effect runs through a deterrent effect of a free press that reduces graft and thus leaves greater public resources for investment in public goods (neither of which mediating nodes are represented on the graph)."} -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(0, 1, 2), -->
<!--        y = c(2, 3, 2), -->
<!--        names = c("X", "R", "Y"), -->
<!--        arcs = cbind( c(1, 2, 1), -->
<!--                      c(2, 3, 3)), -->
<!--        title = "A DAG with Indirect and Direct Effects", -->
<!--        padding = .4, contraction = .15)  -->

<!-- ``` -->


<!-- ```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:DAGdirectmut} This DAG represents a 'mutilated' version of the previous graph in which the causes of $R$ have been removed, and thus captures the empirical relations that would be required to hold to estimate the effect of $R$ on $Y$."} -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(0, 1, 2), -->
<!--        y = c(2, 3, 2), -->
<!--        names = c("X", "R", "Y"), -->
<!--        arcs = cbind( c(2, 1), -->
<!--                      c(3, 3)), -->
<!--        title = "A 'Mutilated' DAG for Estimating the Effect of R on Y", -->
<!--        padding = .4, contraction = .15)  -->

<!-- ``` -->





<!-- Suppose, for instance, that democracy ($D$) causes higher levels of private investment ($I$) and greater public-goods provision ($P$), and that both of these cause faster economic growth ($G$). If we want to know the joint distribution of public-goods provision and democracy, we would use Equation \ref{eqmarkov}, conditioning on the values of the parents of these two nodes. If, however, we want to know what level of growth would be produced by an increase public-goods provision---a causal question---we have to ask  -->



<!-- ```{r, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align="center", out.width='\\textwidth', fig.cap = "\\label{fig:intervention} The main panel shows a simple causal model. $S$ and $X$ are stochastic, other nodes determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator nodes, which take a value of 1 whenever the $u$ value is less than the $\\pi$ value.", fig.align="center", warning = FALSE} -->

<!-- hj_dag(x = c(1, 2, 2, 3), -->
<!--        y = c(1, 2, 0, 1), -->
<!--        names = c("D","I", "P", "G"), -->
<!--        arcs = cbind( c(1, 1, 3, 2), -->
<!--                      c(2, 3, 4, 4)), -->
<!--        add_functions = 0, -->
<!--        contraction = .2 -->
<!--        ) -->
<!-- title("A causal model") -->
<!-- ``` -->



<!-- Not completely following the logic this next paragraph or sure if it is helpful here -->

<!-- So far, although not completely general, the focus on causal DAGs is consistent with most approaches used in qualitative work on process tracing, in qualitative case analysis, and in econometric approaches. Some of these approaches commonly assume simple functional forms but these impositions are not implied by the general approach. For example econometric models often impose linear assumptions---for example in work on linear structural equations. Qualitative case analysis often assume all units are binary and that outcomes are deterministic. Under some representations the latter assumption implies conditional independencies that cannot be read from the graph, and thus violate stability conditions commonly assumed of the probability distributions that graphs are meant to represent (though it is still always that case that one can tell from the graph whenever two sets of nodes are not conditionally independent given some other set). In our running example described below we give an example of such deterministic relations. -->


### Rules for moving between levels

:::: {.headerbox data-latex=""}
::: {.center data-latex=""  #markov}
:::
**Rules for moving between levels**

*Moving down levels*: 

All (conditional) independencies represented in a higher-level model must be preserved in the lower-level model. 

When we disaggregate or add nodes to a model, new conditional independencies can be generated. But any variables that are independent or conditionally independent (given a third variable) in the higher-level model must also be independent or conditionally independent in the lower-level model.

*Moving up levels*: 

We can move up levels by eliminating an exogenous node, eliminating an endogenous node, or conditioning on a node. When we eliminate a node from a model, we must preserve any variation and dependencies that it generates: 

1. When eliminating an endogenous node, that node’s parents adopt (become direct causes of) that node’s children. 
2. When eliminating an exogenous node, we must usually replace it with a $\theta$ term. If the node has more than one child, it must be replaced with a $\theta$ term pointing into both children (or an undirected edge connecting them) to preserve the dependency between its children. If the node has a spouse, the eliminated node’s variation must also be preserved using a $\theta$ term. Where the spouse is (already) a $\theta$ term with no other children, $\theta$ terms can be combined.  
3. Since conditioning on a node “blocks” the path through which it connects its children, we can simply eliminate the node and the arrows between it and its children.
4. An exogenous node with no spouse and only one child can be simply eliminated.
::::


### Reading conditional independence from a graph

We illustrate how to  identify the relations of conditional independence between $A$ and $D$ in Figure \ref{fig:exercise}. 

```{r exercise, echo = FALSE, fig.width = 5, fig.height = 2,  fig.align="center", out.width='80%', fig.cap = "\\label{fig:CItest} An exercise: $A$ and $D$ are conditionally independent, given which other node(s)?"}
par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = 1:4, y = c(1, 1,1,1), names = c( "A", "B", "C", "D" ),
       arcs = cbind( c(1, 3, 3),
                     c(2, 2, 4)),
       title = " ", padding = .2, contraction = .15) 

```

Are A and D independent:

* unconditionally?

Yes. $B$ is a collider, and information does not flow across a collider if the value of the collider node or its consequences is not known. Since no information can flow between $A$ and $C$, no information can flow between $A$ and $D$ simply because any such flow would have to run through $C$.

* if you condition on $B$?

No. Conditioning on a collider opens the flow of information across the incoming paths. Now, information flows between $A$ and $C$. And since information flows between $C$ and $D$, $A$ and $D$ are now also connected by an unbroken path. While $A$ and $D$ were independent when we conditioned on nothing, they cease to be independent when we condition on $B$.

* if you condition on $C$?

Yes. Conditioning on $C$, in fact, has no effect on the situation. Doing so cuts off $B$ from $D$, but this is irrelevant to the $A$-$D$ relationship since the flow between $A$ and $D$ was already blocked at $B$, an unobserved collider. 

* if you condition on $B$ and $C$?

Yes. Now we are doing two, countervailing things at once. While conditioning on $B$ opens the path connecting $A$ and $D$, conditioning on $C$ closes it again, leaving $A$ and $D$ conditionally independent.

Analyzing a causal graph for relations of independence represents one payoff to formally encoding our beliefs about the world in a causal model. We are, in essence, drawing out implications of those beliefs: given what we believe about a set of direct causal relations (the arrows on the graph), what must this logically imply about other dependencies and independencies on the graph, conditional on having observed some particular set of nodes? We show in a later chapter how these implications can be deployed to guide research design, by indicating which parts of a causal system are potentially informative about other parts that may be of interest.

<!--chapter:end:02-causal-models.Rmd-->

---
title: "Illustrating causal models"
output: html_document
---

```{r setup-ch2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packagesused02a, include = FALSE}
source("_packages_used.R")
```


# Illustrating Causal Models {#illustratemodels}

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We use three  arguments from published political science research to illustrate how to represent theoretical ideas as structural causal models.
::::

<br>

In this short chapter, we provide more of a sense of how we can encode prior knowledge in a causal model by asking how we might construct models in light of extant scholarly works. We undertake this exercise by drawing on three well-known publications in comparative politics and international relations: Paul Pierson's seminal book on welfare-state retrenchment [@pierson1994dismantling]; Elizabeth Saunders' research on leaders' choice of military intervention strategies [@saunders2011leaders]; and Przeworski and Limongi's work on democratic survival [@przeworski1997modernization], an instructive counterpoint to Boix's [@boix2003democracy] argument about a related dependent variable. For each, we represent the causal knowledge that we might plausibly think we take away from the work in question in the form of a causal model. 

Readers might represent these knowledge bases differently;  our aim here is only to illustrate how causal models are constructed, rather than to defend a particular representation (much less the works in question) as accurate.

For each exercise below, we focus on a specific argument in the literature in order to fix in place a relatively clear set of background causal beliefs and simplify the exposition. We emphasize, however, that *in general* a causal model should be thought of as a representation of our state of knowledge or beliefs about causal relations within a domain, rather than as a representation of a specific argument. Suppose, for instance, that we are interested in testing a specific argument in which $X$ affects $Y$ through the mediator $M$. In constructing a causal model to guide our empirical analysis, we cannot simply draw that argument in DAG form ($X \rightarrow M \rightarrow Y$) and leave it at that. In line with the principles relating to conditional independence outlined in Chapter \@ref(models), we must consult our beliefs about this causal domain in a broader sense. For instance, given what we know about the domain from prior observations or studies, is it plausible that $X$ could affect $Y$ through a pathway that does not go through $M$? If we believe it is possible, then we must also draw a direct $X \rightarrow Y$ arrow, or our causal model will steer us wrong --- even if our primary aim is to examine the pathway through $M$. Otherwise, our DAG will contain a relation of conditional independence ($X$ being conditionally independent of $Y$ given $M$) that we do not believe holds. Thus, while we draw on specific works in the illustrations in this chapter, we urge readers to remember that in practice one would want to characterize a broader prior knowledge base in relation to a causal domain in generating a causal model.

We aim to illuminate a number of features of causal models and their construction with these exercises. The examples that we work through variously illustrate how graphs capture beliefs about relations of conditional independence; the potential causal complexity embedded in the causal structures implied by common social-scientific arguments; and the elements of a causal model that cannot be read from a graph. For each work, we discuss both a parametric rendering of the causal functions and a non-parametric formulation built on nodal types. 

## Welfare state reform

The argument in Pierson's 1994 book  *Dismantling the Welfare State?* challenged prior notions of post-1980 welfare-state retrenchment in OECD countries as a process driven primarily by socioeconomic pressures (slowed growth, rising unemployment, rising deficits, aging populations) and the rise of market-conservative ideologies (embodied for instance by the ascendance of Thatcher and Reagan). Pierson argues that socioeconomic and ideological forces put retrenchment on the policy agenda, but do not ensure its enactment because retrenchment is a politically perilous process of imposing losses on large segments of the electorate. Governments will only impose such losses if they can do so in ways that allow them avoid blame for doing so---by, for instance, making the losses hard to perceive or the responsibility for them difficult to trace. These blame-avoidance opportunities are themselves conditioned by the particular social-program structures that governments inherit. 

 <!-- $C$=conservative government; $S$=socioeconomic pressures; $P$=program structure; $A$=retrenchment being on the agenda; $B$=blame-avoidance opportunities; $R$=retrenchment; $\theta^R$=random influences on retrenchment -->

<!-- ```{r pierson, echo = FALSE, fig.width = 12, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "A graphical representation of Pierson (1994).", warning = FALSE, message = FALSE} -->

<!-- par(mfrow = c(1,1)) -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(1,2,3,1,1,2,3), -->
<!--        y = c(1,1,2,3,2,2,3), -->
<!--        names = c( -->
<!--          expression(paste("P:\nProgram\nstructure")), -->
<!--          expression(paste("B:\nBlame-avoidance\nopportunity")),   -->
<!--          expression(paste("R:\nRetrenchment")), -->
<!--          expression(paste("C:\nConservative\ngovt")), -->
<!--          expression(paste("S:\nSocioeconomic\npressures")), -->
<!--          expression(paste("A:\nOn Agenda")), -->
<!--          expression(paste(theta^R: "\nRandom influence\n on retrenchment"))), -->
<!--        arcs = cbind( c(1,2, 4,5,6,7), -->
<!--                      c(2,3, 6,6,3,3)), -->
<!--        title = "Welfare-State Rentrenchment (Pierson, 1994)", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .2 -->
<!-- ) -->


<!-- ``` -->

```{r pierson, echo = FALSE, fig.width = 12, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "A graphical representation of Pierson (1994).", warning = FALSE, message = FALSE}

par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,3,1,1,2),
       y = c(1,1,2,3,2,2),
       names = c(
         expression(paste("P:\nProgram\nstructure")),
         expression(paste("B:\nBlame-avoidance\nopportunity")),  
         expression(paste("R:\nRetrenchment")),
         expression(paste("C:\nConservative\ngovt")),
         expression(paste("S:\nSocioeconomic\npressures")),
         expression(paste("A:\nOn Agenda"))
         ),
       arcs = cbind( c(1,2, 4,5,6),
                     c(2,3, 6,6,3)),
       title = "Welfare-State Rentrenchment (Pierson, 1994)",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```

While the argument has many more specific features (e.g., different program-structural factors that matter, various potential strategies of blame-avoidance), its essential components can be captured with a relatively simple causal model. We propose such a model in graphical form in Figure \@ref(fig:pierson). Here, the outcome of retrenchment ($R$) hinges on whether retrenchment makes it onto the agenda ($A$) and on whether blame-avoidance strategies are available to governments ($B$). Retrenchment emerges on the policy agenda as a consequence of both socioeconomic developments ($S$) and the ascendance of ideologically conservative political actors ($C$). Inherited program structures ($P$), meanwhile, determine the availability of blame-avoidance strategies. To avoid cluttering the graph, we do not represent the $\theta$ terms, but it is implied that every node on this graph has a $\theta$ node pointing into it.

A few features of this graph warrant attention. As we have discussed, it is the omitted arrows in any causal graph that imply the strongest statements. The graph implies that $C$, $S$, and $P$---which are neither connected along a directed path nor downstream from a common cause---are independent of one another. This implies, for instance, that whether conservatives govern is independent of whether program structures will allow for blame-free retrenchment. Thus, as Pierson argues, a Reagan or Thatcher can come to power but nonetheless run up against an opportunity structure that would make retrenchment politically perilous. Given the absence of bidirectional arrows indicating confounding, the graph similarly implies that the nodal types for all nodes are independent of one another.

Further, this graph represents the belief that any effect of program structures on retrenchment *must* run through their effects on blame-avoidance opportunities. One could imagine relaxing this restriction by, for instance, drawing an arrow from $P$ to $A$: program structures might additionally affect retrenchment in other ways, such as by conditioning the fiscal costliness of the welfare state and thus helping to determine whether reform makes it onto the agenda. If the current state of knowledge suggested that program structures could affect retrenchment via a pathway other than blame-avoidance opportunities, then we would indeed want to include a direct $P \rightarrow A$ arrow. 

Where two variables *are* connected by an arrow, moreover, this does not imply that a causal effect will always operate. Consider, for instance, the arrow pointing from $A$ to $R$. The fact that $A$ sometimes affects $R$ and sometimes does not is, in fact, central to Pierson's argument: conservatives and socioeconomic pressures forcing retrenchment on the agenda will *not* generate retrenchment if blame-avoidance opportunities are absent. 

The graph also reflects a choice about where to begin. We could, of course, construct a causal account of how conservatives come to power, how socioeconomic pressures arise, or why programs were originally designed as they were. Yet it is perfectly permissible for us to bracket these antecedents and start the model with $C$, $S$, and $T$, as long as we do not believe that these variables have any antecedents in common. If they do have common causes, then this correlation should be captured in the DAG.^[In DAG syntax, this correlation can be captured by placing the common cause(s) explicitly on the graph or by drawing a dashed line between the correlated nodes, leaving the source of the correlation unspecified.]

The DAG itself tells us about the possible direct causal dependencies but is silent on the ranges of and functional relations among the variables. How might we express these? With three endogenous variables, we need three functions indicating how their values are determined. Moreover, every variable pointing directly into another variable must be part of that second variable's function. 

Let us assume that all variables (including the implied $\theta$ terms) are binary, with each condition either absent or present. 

One option would be to take a parameteric approach and imagine specific functions connecting parents to children, with $\theta$ terms representing exogenous "noise." For instance, we can capture quite a lot of Pierson's theoretical logic with the following quite simple functional equations:

* $A=CS\theta^C$, capturing the idea that retrenchment makes it on the agenda only if conservatives are in power *and* socioeconomic pressures are high. 
* $B=P\theta^P$, implying that blame-avoidance opportunities arise only when program structures take a particular form.
* $R=AB\theta^R$, implying that retrenchment will occur only if it is on the agenda and blame-avoidance opportunities are present.

In each equation, the $\theta$ term allows for exogenous forces that might block the outcome from occurring. In the last functional equation, for instance, retrenchment will only occur if retrenchment is on the agenda and blame-avoidance opportunities are present, but even if both are present, the effect on retrenchment also hinges on the value of $\theta^R$. When $\theta^R=1$, the $AB$ combination has a positive causal effect on retrenchment. When $\theta^R=0$, $AB$ has no causal effect: retrenchment will not occur regardless of the presence of $AB$. We can think of $\theta^R$ as capturing a collection of features of a case's context that might render the case susceptible or not susceptible to an $AB$ causal effect. For instance, Pierson's analysis suggests that a polity's institutional structure might widely diffuse veto power such that stakeholders can block reform even when retrenchment is on the agenda and could be pursued without electoral losses. We could think of such a case as having a $\theta^R$ value of 0, implying that $AB$ has no causal effect. A $\theta^R=1$ case, with a positive effect, would be one in which the government has the institutional capacity to enact reforms that it has the political will to pursue.

Alternatively, we could take a non-parameteric approach, as we generally do in the remainder of this book. In a non-parametric setup, each node's $\theta$ term captures that node's nodal type. Each value of a $\theta$ term's range represents a possible way in which the node might respond to its parents. We would define $\theta^A$ as taking on one of 16 values (16 types, given 2 parent nodes); $\theta^B$ as taking on on one of four values; and $\theta^R$ as taking on one of 16 values; with $\theta^C$ and $\theta^S$ each taking on one of two values. 

Then choices on the probability distributions fully reflect the ways these variables relate to each other. Note that the parametric argument given above can be thought  of as a special case of the non-parametric representation with all probability mass placed on a small set of possible nodal types. Thus the central thrust of Pierson's argument could then be represented in nodal-type form as:

* $\theta^A=\theta^A_{0001}$ 
* $\theta^B=\theta^B_{01}$
* $\theta^R=\theta^R_{0001}$. 

In practice, however we would allow for a richer probability  *distribution* over each $\theta$, representing beliefs over the assignment process or causal relations operating at each node. Beliefs about the distribution of exogenous conditions would be captured in distributions over the values of $\theta^C, \theta^S$, and $\theta^P$. How we handle distributions over $\theta^A, \theta^B$, and $\theta^R$ depends on the degree of confidence that we want to express in Pierson's argument. To represent the belief that Pierson's argument is correct with certainty and operates in uniform, deterministic fashion across units, we would simply have degenerate distributions for $\theta^A, \theta^B$, and $\theta^R$, with a probability of 1.0 placed on the respective nodal types shown above. To capture uncertainty about the functional relations on any graph or if we believe that there is some heterogeneity of effects across units we would disperse  probability density across types for each $\theta$. For instance, for $\theta^R$ we might want to put some weight on $\theta^R_{0011}$ (blame-avoidance opportunities alone are enough to generate retrenchment),  $\theta^R_{0101}$ (conservative leaders alone are enough), $\theta^R_{0111}$ (either is enough), and $\theta^R_{0000}$ (retrenchment will not happen even when both conditions are present), while perhaps putting greatest weight on $\theta^R_{0001}$.^[In notation that we use later, these beliefs would be represented with a $\lambda^R$ vector.]


## Military Interventions

@saunders2011leaders asks why, when intervening militarily abroad, do leaders sometimes seek to transform the *domestic* political institutions of the states they target but sometimes seek only to shape the states' *external* behaviors. 

Saunders' central explanatory variable is the nature of leaders' causal beliefs about security threats. When leaders are "internally focused," they believe that threats in the international arena derive from the internal characteristics of other states. Leaders who are "externally focused," by contrast, understand threats as emerging strictly from other states' foreign and security policies. 

These basic worldviews, in turn, affect the cost-benefit calculations leaders make about intervention strategies---in particular, about whether to try to transform the internal institutions of a target state---via two mechanisms. First, an internal focus (as opposed to an external focus) affects leaders' perceptions of the likely security gains from a transformative intervention strategy. Second, internal vs. external focus affects the kinds of strategic capabilities in which leaders invest over time (do they invest in the kinds of capabilities suited to internal transformation?); and those investments in turn affect the costliness and likelihood of success of alternative intervention strategies. Calculations about the relative costs and benefits of different strategies then shape the choice between a transformative and non-transformative approach to intervention. 

At the same time, leaders can only choose a transformative strategy if they decide to intervene at all. The decision about whether to intervene depends, in turn, on at least two kinds of considerations. The first is about fit: a leader is more likely to intervene against a target when the nature of the dispute makes the leader's preferred strategy appear feasible in a given situation. Second, Saunders allows that forces outside the logic of her main argument might also affect the likelihood of intervention: in particular, leaders may be pushed to intervene by international or domestic audiences.

Figure \ref{fig:DAGSaunders} depicts the causal dependencies in Saunders' argument in DAG form (again, with all $\theta$ terms implied). Working from left to right, we see that whether or not leaders are "internally focused" ($F$) affects the expected net relative benefits of transformation ($B$), both via a direct pathway and via an indirect pathway running through investments in transformative capacities ($T$). Characteristics of a given dispute or target state ($D$) likewise influence the benefits of transformation ($B$). The decision about whether to intervene ($I$) is then a function of three factors: internal focus ($F$), the expected relative net benefits of transformation ($B$), and audience pressures ($A$). Finally, the choice of whether to pursue a transformative strategy ($S$) is a function of whether or not intervention occurs at all ($I$), and of cost-benefit comparisons between the two strategies ($B$).

<!-- ```{r saunders, echo = FALSE, fig.width = 12, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:DAGSaunders} A graphical representation of Saunders' (2011) argument.", warning = FALSE, message = FALSE} -->

<!-- par(mfrow = c(1,1)) -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(1,2,2,1, 3, 3,3,3.5), -->
<!--        y = c(1,0,2,2, 2, 1,0, .5), -->
<!--        names = c( -->
<!--          expression(paste("C: Causal beliefs")), -->
<!--          expression(paste("P: Preparedness\ninvestment")),   -->
<!--          expression(paste("B: Benefits expected\nfrom transfers")),   -->
<!--          expression(paste("T: Target\ncharacteristics")), -->
<!--          expression(paste("A: Audience pressures")), -->
<!--          expression(paste("I: Intervention")), -->
<!--          expression(paste("S: Intervention strategy")), -->
<!--          expression(paste(U[S]: "Random influence\non strategy"))  -->
<!--          ), -->
<!--        arcs = cbind( c(1,1, 2,1, 4, 3, 3,5,6,8), -->
<!--                      c(2,3, 3,6, 3, 6, 7,6,7,7)), -->
<!--        title = "(b) Military intervention strategies (Saunders, 2011)", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .2 -->
<!-- ) -->


<!-- ``` -->


```{r saunders, echo = FALSE, fig.width = 12, fig.height = 7, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:DAGSaunders} A graphical representation of Saunders' (2011) argument.", warning = FALSE, message = FALSE}

par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2,1, 3, 3,3),
       y = c(1,0,2,2, 2, 1,0),
       names = c(
         expression(paste("F: Internal focus")),
         expression(paste("T: Invest in\ntransformation\ncapablities")),  
         expression(paste("B:\nExpected benefits\nfrom transformation")),  
         expression(paste("D: Dispute\ncharacteristics")),
         expression(paste("A: Audience pressures")),
         expression(paste("I: Intervene")),
         expression(paste("S: Transformative strategy")) 
         ),
       arcs = cbind( c(1,1, 2,1, 4, 3, 3,5,6),
                     c(2,3, 3,6, 3, 6, 7,6,7)),
       title = "Military intervention strategies (Saunders, 2011)",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```


This DAG illustrates how readily causal graphs can depict the multiple pathways through which a given variable might affect another variable, as with the multiple pathways linking $F$ to $I$ and $B$ (and, thus, all of its causes) to $S$. In fact, this graphical representation of the dependencies in some ways throws the multiplicity of pathways into even sharper relief than does a narrative exposition of the argument. For instance, Saunders draws explicit attention to how causal beliefs operate on expected net benefits via both a direct and indirect pathway, both of which are parts of an indirect pathway from $F$ to the outcomes of interest, $I$ and $S$. What is a bit easier to miss without formalization is that $F$ also acts *directly* on the choice to intervene as part of the feasibility logic: when leaders assess whether their generally preferred strategy would be feasible if deployed against a particular target, the generally preferred strategy is itself a product of their causal beliefs. The DAG also makes helpfully explicit that the two main outcomes of interest---the choice about whether to intervene and the choice about how---are not just shaped by some of the same causes but are themselves causally linked, with the latter depending on the former.

Omitted links are also notable. For instance, the lack of an arrow between $D$ and $A$ suggests that features of the dispute that affect feasibility have no effect on audience pressures. If we instead believed there could be other connections---for instance, that audiences take feasibility into account in demanding intervention---then we would want to include a $D \rightarrow A$ arrow.

Turning to variable ranges and functional equations, it is not hard to see how one might readily capture Saunders' logic in a fairly straightforward set-theoretic manner. All variables except $S$ could be treated as binary with, for instance, $F=1$ representing internally focused causal beliefs, $T=1$ representing investments in transformative capabilities, $B=1$ representing expectations that transformation will be more net beneficial than non-transformation, $D=1$ meaning that a dispute has characteristics that make transformation a feasible strategy, and so on. Although there are two strategies, we in fact need three values for $S$ because it must be defined for all values of the other variables---i.e., it must take on a distinct categorical value if there is no intervention at all. We could then define functions, such as:

* $B=FTD$, implying that transformation will only be perceived to be net beneficial in a case if and only if the leader has internally focused causal beliefs, the government is prepared for a transformative strategy, and the dispute has characteristics that make transformation feasible.
* $I=(1-|B-F|)+(1-(1-|B-F|))A$, implying that intervention can occur under (and only under) either of two alternative sets of conditions: if the generally preferred strategy and the more net-beneficial strategy in a given case are the same (i.e., such that $B-F=0$) or, when this alignment is absent (i.e., such that $|B-F|=0$), where audiences pressure a leader to intervene.

As illustrated in the Pierson example, in a non-parametric framework, each parametric functional equation represents one nodal type for the relevant $\theta$. For instance, though we spare the reader the complexities of the corresponding subscript notation, there is a single value of $\theta^B$ under which the conditions $F=1, P=1,$ and $T=1$ generate $B=1$, and we get $B=0$ otherwise.  Likewise, there exists a single value of $\theta^I$ under which $B=1, F=1$ and $B=0, F=0$ produce $I=1$, for either value of $A$; and $A$ has a positive effect on $I$ whenever $B \neq F$. To work with this model, we would specify a probability distribution over all possible nodal types for each node on the graph.

This example also nicely illustrates how much potential causal complexity a moderately intricate argument and causal graph implies. The number of *possible* nodal types at each node depends on how many parents that node has. Looking at the endogenous nodes here, we have one node with one parent ($T$), implying 4 nodal types; one node with two parents ($S$), implying 16 nodal types; and two nodes with 3 parents ($B$ and $I$), implying 256 nodal types each. If we now conceptualize the set of possible "causal types" as containing all distinct combinations of nodal types---all ways in which a case might behave across all of its nodes (see Chapter \@ref(models))---then this graph implies about 4 million different ways in which the values of exogenous nodes ($D$, $F$, and $A$) might jointly produce patterns of outcomes. Saunders' argument effectively represents one of these 4 million possible sets of relations. 

The framework that we outline in this book allows for updating on arguments like Saunders': we can ask how likely the specific causal type implied by this argument is relative to other causal types. Yet, as we will see, the approach lends itself to a much broader view of causal inquiry. In general, we will use data to update beliefs over *all* causal types allowed for in a model, and then use these updated beliefs to answer any number of causal questions about relations in the model. For instance, we can use the same data and updated model to ask about the average effect of internal focus on intervention; the relative importance in this effect of the expected-benefits pathway over the direct pathway; about individual steps in the causal chain, such as the effect of expected benefits on choice of strategy; and so on.

## Development and Democratization

@przeworski1997modernization argue that democratization occurs for reasons that are, with respect to socioeconomic or macro-structural conditions, largely idiosyncratic; but once a country has democratized, a higher level of economic development makes democracy more likely to survive. Economic development thus affects whether or not a country is a democracy, but only after a democratic transition has occurred, not before. Thus, in their description---and contrary to @boix2003democracy ---democratization is "exogenous": it is not  determined by other variables in the model. The dynamic component of Przeworski and Limongi's argument---the fact that both the presence of democracy and the causal effect of development on democracy depend on whether a democratic transition occurred at a previous point in time---forces us to think about how to capture over-time processes in a causal model. 

We represent Przeworski and Limongi's argument in the DAG in Figure \@ref(fig:DAGPL). The first thing to note is that we can capture dynamics by considering democracy at different points in time as separate nodes. According to the graph, whether a country is a democracy in a given period ($D_t$) is a function, jointly, of whether it was a democracy in the previous period ($D_{t-1}$) and of the level of per capita GDP in the current period (as well as of other unspecified forces $\theta^{D_t}$, not pictured).




```{r DAGPL, echo = FALSE, fig.width = 10, fig.height = 6, fig.align="center", out.width='.7\\textwidth', fig.cap = "A graphical representation of Przeworski and Limongi's argument, where $D_{t-1}$=democracy in the previous period; $GDP_t$=per capita GDP in the current period; $D_t$=democracy in the current period.", warning = FALSE, message = FALSE}

par(mfrow = c(1,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,1,2),
       y = c(2,0,1),
       names = c(
         expression(paste(D[t-1]: "Democracy,\nlast period")),
         expression(paste(Y[t]: "GDP per capita,\nthis period")),  
         expression(paste(D[t]: "Democracy,\nthis period"))
         ),
       arcs = cbind( c(1,2),
                     c(3,3)),
       title = "Democratization (Przeworski and Limongi, 1997)",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


```


Second, the arrow running from $GDP_{t-1}$ to $D_t$ means that $GDP$ *may* affect democracy, not that it always does. Indeed, Przeworski and Limongi's argument is that development's effect depends on a regime's prior state: GDP matters for whether democracies continue to be democracies, but not for whether autocracies go on to become democracies. The absence of an arrow between $D_{t-1}$ and $GDP_{t-1}$, however, implies a (possibly incorrect) belief that democracy and $GDP$ in the last period are independent of one another.

Inspection of this figure highlights, we think, a curious feature of this argument. The key claim---that the switch *to* democracy does not depend on income---is not readable from the graph. The reason is simply that, given this argument, being a non democracy in one period given you were a democracy in a previous period does depend on income. Your  state in the second period does depend on income. Specifically there is a *counterfactual* dependence---income causes a state to be democratic even if it does not cause it to *transition to* democracy. The effect of income may well be asymmetric depending on whether you start as a democracy or start as an autocracy but this asymmetry has to be be captured by the specification of the functional relations; it is not captured by the graph. 

For a parametric representation of this asymmetric relationship we can specify a function in which  $GDP$ can reduce the likelihood of a transition *away* from democracy but does affect the probability of a transition *to* democracy, which should be exogenously determined. One possible translation of the argument into functional terms is:


$$D_t = \mathbb{1}(\theta^{D_t} + (1-D_{t-1})p + D_{t-1}Y_{t-1}q) > 0)$$

<!-- FLAG: Get proper indicator function \mathbb{1} -->

where

* $D_t$ and $D_{t-1}$ are binary, representing current and last-period democracy, respectively
* $p$ is a parameter representing the probability that an autocracy democratizes
* $q$ is a parameter representing the probability that a democracy with a GDP of 1 remains democratic
* $Y_{t-1}$ represents national per capita GDP, scaled to 0-1.
* $\theta^{D_t}$ represents a random, additional input into democracy
* the indicator function, ${1}$, evaluates the inequality and generates a value of $1$ if and only if it is true

Unpacking the equation, the likelihood that a country is a democracy in a given period rises and falls with the expression to the left of the inequality operator. This expression itself has two parts, reflecting the difference between the determinants of *transitions to* democracy (captured by the first part) and the determinants of democratic *survival* (captured by the second). The first part comes into play---i.e., is non-zero---only for non-democracies. For non-democracies, the expression evaluates simply to $p$, the exogenous probability of democratization. The second part is non-zero only for democracies, where it evaluates to $q$ times $Y_{t-1}$: thus, remaining democratic is more likely as national income rises. The inequality is then evaluated by asking whether the expression on the left passes a threshold. Thus, higher values for the expression increase the likelihood of democracy while the randomness of the $\theta^{D_t}$ threshold captures the role of other, idiosyncratic inputs. The mean and variance of $\theta^{D_t}$ capture the overall likelihood of being a democracy as well as the importance of unspecified factors.^[Note how, while the functional equation nails down certain features of the process, it leaves others up for grabs. In particular, the parameters $p$ and $q$ are assumed to be constant for all autocracies and for all democracies, respectively, but their values are left unspecified. And one could readily write down a function that left even more openness---by, for instance, including an unknown parameter that translates $y$ into a change in the probability of reversion or allowing for non-linearities, with unknown parameters, in this effect.] In a model like this it would be natural to seek to estimate parameters $p$ and $q$ as well as trying to understand the distribution of $\theta^{D_t}$.

We can also represent the asymmetry in the binary set up with causal types that we developed in the last chapter. 

Type $\theta^{D_t}_{0001}$ is a type for which the regime type *will stay as they are* if they are wealthy, but will become authoritarian if they are not wealthy.  To be clear, wealth still affects whether or not a state is a democracy, rather than an autocracy, in this period--counterfactually---but wealth does not make a non democracy become a democracy. In other words it causes a case to *be* a democracy but not to *become* a democracy. This  type can be distinguished from a $\theta^{D_t}_{0011}$  type in which a non democracy becomes a democracy when income is high. 

Although we do not engage with dynamic models in this book, it is instructive to think through the implications of a distribution of causal types for a dynamic process. Say we were to imagine that income were constant but that in each period one half of units were of type  $\theta^{D_t}_{0001}$, and one half of type $\theta^{D_t}_{1111}$ (with types drawn afresh each period).  Say that in an initial period, half the units were democracies and half had high income and there was no relation between these two features. Then in the next period we would have that 
half of cases would be democracies (regardless of income), half of which would be surviving democracies and half new democracies; of the other half, one quarter would be surviving democracies (surviving *because of* their income), and the other three quarters would be autocracies, one third of which would be "backsliders" because of their poverty. Similar transitions occur in future periods until eventually the wealthy states are all stable democracies and the  poorer states transition between democracy and autocracy back and forth randomly each period.

In this approach there are no parameters $p$ or $q$ to be estimated. Rather the focus is entirely on the distribution of nodal types. 

<!-- 	T1	A	D	T2	A	D	T3 -->
<!-- AP	16	8	8	16	8	8	16 -->
<!-- AR	16	8	8	8	4	4	4 -->
<!-- DP	16	8	8	16	8	8	16 -->
<!-- DR	16	0	16	24	0	24	28 -->

<!--chapter:end:03-illustrating-models.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Causal Queries {#questions}


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
Although a lot of empirical work focuses on identifying average causal effects, there is a rich array of other well defined causal questions that can be asked about how variables relate to each other causally. We describe major families of questions and illustrate how these can all be described as queries about the values of nodes in a causal model.
::::
<br>


```{r packagesused04, include = FALSE}
source("_packages_used.R")
```

Although scholars share a broad common interest in causality there is tremendous heterogeneity in the kinds of causal questions that scholars ask. Consider the relationship between inequality and democratization. We might seek, for instance, to know inequality's average impact on democratization across some set of cases. Alternatively, we might be interested in a particular case---say, Mongolia in 1995---and want to know whether inequality would have an effect *here*. That is a question about causal effects at the case level. Alternatively we might wonder whether the level of democracy in Mongolia in 1995 is *due* to the level of inequality in that case---this is quite a distinct question (in the same way that establishing that poison would make you sick does not imply that you are sick because of poison). Or we may be interested in _how_ causal effects unfold, inquiring about the pathway or mechanism through which inequality affects democratization---a question we can also ask at two levels. We can ask whether inequality affected democratization in Mongolia through mobilization of the masses; or we can ask how commonly inequality affects democratization through mobilization across a broad set of cases. Pushing further we might ask a counterfactual question of the form: would inequality have produced democratization had mobilization been prevented from occurring.  

Distinct methodological literatures have been devoted to the study of average causal effects, the analysis of case-level causal effects and explanations, and the identification of causal pathways. Fortunately each of these questions can be readily captured as specific queries asked of (and answerable from)  a causal model. As described by @judea2010introduction, the goal is to deploy an "*algorithm* that receives a model *M* as an input and delivers the desired quantity *Q(M)* as the output." More specifically, we demonstrate how, given the structure we described in Chapter \@ref(models),  causal queries can be represented as question about one or more _nodes_ on a causal graph. When we assimilate our causal questions into a causal model, we are placing what we want to know in formal relation to both what we _already_ know and what we can potentially _observe_. As we will see in later chapters, this move allows us then to deploy the model to generate strategies of inference: to determine which observations, if we made them, would be likely to yield the greatest leverage on our query, given our prior knowledge about the way the world works. And by the same logic, once we see the evidence, this integration allows us to  "update" on our query---figure out in systematic fashion what we _have_ learned---in a manner that takes background knowledge into account.

In the remainder of this chapter, we walk through the conceptualization and causal-model interpretation of five key causal queries:

* Case-level causal effects

* Case-level causal attribution

* Case-level explanation

* Average causal effects

* Causal pathways

These five are in no way exhaustive of the causal questions that can be captured in causal graphs, but they are among the more common social scientific investigations.


<!-- * What is the average effect of a given increase in inequality is on the level of democracy for a given population of cases? -->

<!-- * Case-level causal effects: What is the effect of $X$ on $Y$ in a given case? -->

<!-- * Causal attribution: Did the condition $X$, present in a case, cause the outcome, $Y$, that occurred in that case? -->

<!-- * Actual causes: Which of the antecedent conditions present in the case either was a counterfactual cause of the outcome or *could* have been a counterfactual cause given the way in which events actually played out? -->

<!-- * Average causal effects: What is the mean effect of $X$ on $Y$ across a population of cases? -->

<!-- * Causal pathways: How did $X$ exert its effect on $Y$ in a case? How does $X$ affect $Y$ in a population of cases? -->

<!-- Some causal questions involve realized values of variables only, some involve counterfactual statements, and some involve combinations of these.  -->

 <!-- In what follows we advocate an approach in which causal questions --- which we term *queries* --- can be defined as questions about the *values of exogenous nodes on a causal graph*, including unobservable $U$ terms. ^[With some abuse of notation we use $Q$ generically to refer to the query itself and the the set of variables whose values determine the query. Thus a query may be written as the random variable $Q =\mathbb{1}((u_X = 1) \& (u_Y = 0))$, which takes on a value $q=1$ if both $u_X = 1$ and $u_Y = 0$ and 0 otherwise. Assessing this query requires understanding the values of particular roots, or query nodes, $\{U_X, U_Y\}$ which we also refer to as $Q$.]  Addressing a given causal question then involves using data on observed features of a graph to make inferences about those unobserved or unobservable features of the graph that define the query. These inferences are, of course, always conditional on the graph itself.  -->

<!-- (a) uncertainty about causal questions is represented as uncertainty about  and (b)  -->

<!-- THIS ALL SEEMS LIKE NUANCE WE DON'T REALLY NEED HERE.

In this framework, inferences about causation amount to inferences about the *context* that a case is in: that is, whether conditions in the case (the relevant exogenous-variable values) are such that a given causal effect, causal pathway, etc. would have been operating. We can translate questions about causation into questions about context because, in a structural causal model, the values of all exogenous variables are sufficient to determine the value of all endogenous nodes: context determines outcomes. This further implies that, for any manipulation of an exogenous or endogenous variable, there exist one or more exogenous nodes on the graph that suffice to determine the effect on all endogenous variables in the graph: context determines *effects*. Likewise, the settings on the model's exogenous variables determine the pathway(s) through which one variable in the model will affect another. -->

<!-- It is important to note a difference between this formulation and the conceptualization of causality typically employed in the potential outcomes framework. We characterize causal inference as learning about a unit *as it is*, conditional on a causal model, rather than learning about the unit as it is and as it could be. Suppose, for instance, that in a causal model a car will start if it has gas and if the key is turned.^[A version of this example is in @darwiche1994symbolic.] In a standard potential outcomes setup, the question "Does turning the key cause the car to start?" is equivalent to asking, "Would the car start if the key is turned?" and "Would the car start if the key is not turned?" In our model-based framework, the question of the key-turning's causal effect is somewhat differently framed as a question about an exogenous variable: "Does the car have gas?" In the model-based framework, then, our query becomes a question about the state of affairs in the case---about the case's *context*---rather than a pair of factual and counterfactual questions about outcomes with and without treatment. These two framings are fully consistent with one another, and counterfactual reasoning is no less important in the model-based framework; it has simply been displaced to the causal model, which encodes all counterfactual relations. -->
<!-- <!-- moreover this can always be done formally, even if the causal model contains no additional assumptions about the causal process.      --> 


## Case-level causal effects

The simplest causal question is whether some causal effect operates in an individual case. Does $X$ have an effect on $Y$ in this case? For instance, is Yemen in 1995 a case in which a change in economic inequality would produce a change in whether or not the country democratizes? We could put the question more specifically as a query about a causal effect in a particular direction, for instance: Does inequality have a positive effect on democratization in the case of Yemen in 1995?

In counterfactual terms, a query about case-level causation is a question about what would happen if we could manipulate a variable in the case: if we could hypothetically intervene to change $X$'s value in the case, (how) would $Y$'s value change? To ask whether a positive (or negative) effect operates for a case is to ask whether a particular counterfactual relation holds in that case. If we assume a setup with binary variables for simplicity, to ask whether inequality has a positive effect on democratization is to ask: if we set $I$ to $0$ would $D$ take on a value of $0$, _and_ if we set $I$ to $1$, would $D$ take on a value of $1$? (_Both_ of these conditions must hold for $I$ to have a positive effect on $D$.)

<!-- The closely connected question of causal attribution [@yamamoto2012understanding] asks: did $X$ cause $Y$'s value in this case? -->

We can easily represent this kind of query in the context of a causal model. We show the DAG for such a model in Figure \@ref(fig:casequery). As introduced in Chapter \@ref(models), $\theta^Y$ here represents the nodal type characterizing $Y$'s response to $X$ and, if $X$ and $Y$ are binary, it can take on one of four values: $\theta^Y_{10}$, $\theta^Y_{01}$, $\theta^Y_{00}$, and $\theta^Y_{11}$ (which map onto our $a, b, c$ and $d$ types, respectively). Importantly, given that the value of nodes (or variables) is allowed to vary across cases, this setup allows for $\theta^Y$---the causal effect of $X$ on $Y$---to vary across cases. Thus, $X$ may have a positive effect on $Y$ in one case (with $\theta^Y=\theta^Y_{01}$), and a negative ($\theta^Y=\theta^Y_{10}$) or no effect ($\theta^Y=\theta^Y_{00}$ or $\theta^Y_{11}$) on $Y$ in other cases.

<!-- Consider again our four nodal types, above. In this setup, $X$'s causal effects on $Y$ can vary across cases. We can readily translate this setup, in which different cases have different causal effects, into a structural causal model. We can do so by letting $Y$ be a function both of $X$ and of a *causal-type variable* that encodes potential outcomes, a variable that we will denote as $Q$. We represent this simple model graphically in Figure \ref{fig:DAGtypes}. Here $Q$ can be thought of as variable that conditions the effect of $X$ on $Y$.  -->

<!-- We then need to specify the values that $Q$ can take on, $Q$'s range. With a binary causal variable of interest, we can write down a value of $Q$ as $q_{ij}$. The pair of subscripts simply conveys the type's potential outcomes: $i$ represents the value that $Y$ takes on if $X=0$ while $j$ represents the value that $Y$ takes on if  $X=1$. Thus, in a binary framework, $Q$ can take on four values, corresponding to our original four types: $q_{00}$ (a $c$ type), $q_{10}$ (an $a$ type), $q_{01}$ (a $b$ type) and $q_{11}$ (a $d$ type). This setup also allows us to write down a simple, closed-form functional equation for å$Y$ in terms of its parents, $X$ and $Q$: $Y(x,q_{ij}) =  i(1-x) + jx$.^[To generate this closed-form function, we decompose $q_{ij}$ into its component parts, $i$ and $j$. Note that there is no loss of generality in the functional form linking $X$ and $Q$ to $Y$. In a causal model framework, the structural equations, such as those linking $X$ and $Y$ conditional on another node, can be entirely non-parametric.] -->

```{r casequery, echo = FALSE, fig.width = 8, fig.height = 4,  fig.align="center", out.width='60%', fig.cap = "\\label{fig:casequery} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's nodal type, represented by $\\theta^Y$. With a single binary causal variable of interest, we let $\\theta^Y$ take on values $\\theta^Y_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if $X=1$. With a binary framework outcome, $\\theta^Y$ ranges over the four values: $\\theta^Y_{00}$, $\\theta^Y_{10}$, $\\theta^Y_{01}$ and $\\theta^Y_{11}$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 1),
       y = c(1, 1, 2),
       names = c(
                expression(paste(X)),
                expression(paste(Y)),
                expression(paste(theta^Y))),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "A DAG Capturing a Case-Level Causal Effect",
       padding = .2, contraction = .15)

```



<!-- Let $\lambda_1^Q$ denote a multinomial distribution over these four values and let t -->

In this model, then, the query, "What is $X$'s causal effect in this case?" simply becomes _a question about the value of the nodal type $\theta^Y$_. 

Two natural variants of this question are, "What is the expected effect of $X$ on $Y$?" and "What is the probability that $X$ matters for $Y$? Answering the question requires estimating the probability that $X$ has a positive effect minus the probability that it has a negative effect: $\Pr(\theta^Y = \theta^Y_{01}) - \Pr(\theta^Y = \theta^Y_{10})$. Answering the second involves assessing $\Pr(\theta^Y = \theta^Y_{01} \text{ OR } \theta^Y = \theta^Y_{10})$.

We can conceptualize this same question even if the  model involves more complex relations between $X$ and $Y$. The question itself does not depend on the model having a particular form.  For instance, consider a mediation model of the form $X\rightarrow M \rightarrow Y$. In this model, a positive effect of $X$ on $Y$ can emerge either from a chain of positive effects of $X$ on $M$ and of $M$ on $Y$ or from a chain of negative effects, while a negative effect of $X$ on $Y$ can emerge from a chain of opposite-signed effects. Thus, answering the question means estimating:
$$\Pr((\theta^M = \theta^M_{01} \& \theta^Y = \theta^Y_{01})  \text{ OR }  (\theta^M = \theta^M_{10} \& \theta^Y = \theta^Y_{10}))   - \Pr((\theta^M = \theta^M_{01} \& \theta^Y = \theta^Y_{10})  \text{ OR }  (\theta^M = \theta^M_{10} \& \theta^Y = \theta^Y_{01}))$$

*Answering* the  question now requires guesses about nodal types for $M$ and for $Y$, not just nodal types for $Y$. Thus the question remains the same, and answerable, under different models, but the answer to the question might involve summaries of the values of different nodes. 

<!-- Of course, these $\theta$s are  not directly observable: nodal types are intrinsically unobserved properties of cases. So, as we will see in later chapters, research design becomes a challenge of determining which _observable_ nodes in the graph are potentially informative about the unobservable nodes that constitute our causal queries.  -->

<!-- We note that, in this discussion, we are employing a more generic property of causal graphs. In a graph of the general form $X \rightarrow Y \leftarrow Z$, the effect of $X$ on $Y$ in a case will depend on the value of $Z$ in that case. $Z$ in this structure might be a random disturbance term, $U_Y$, or a variable with a substantive interpretation. Either way, where a node has multiple parents, we should generally conceive of the parents as exerting their effects interactively. There are special situations in which $X$'s effect will not depend on the value of $Z$. For instance, if $Z$ operates only additively on $Y$ (say, $Y=X+Z$) and $Y$ is not bounded, then $Z$ is irrelevant to $X$'s causal effect, which will be homogeneous across cases and fixed by the model. But, in general, the causal effect of a parent on its child will depend on the value(s) of the other parent(s) (its spouse(s)).^[Nodes that share a child are spouses.] In this sense, for a given $X \rightarrow Y$ relationship, any other parents of $Y$ can be thought of as causal-type variables; these are the variables that define $X$'s case-level causal effect. Put differently, learning about the case-level effect of a causal variable on an outcome means learning about the outcome's other cause(s). -->

<!-- Note also that, in the above illustration, the variable $Q$ is not specified in substantive terms; it is a carrier for causal information. However, social scientific theories commonly use substantive concepts as causal-type variables. The effect of fiscal stimulus on economic growth is theorized to depend on the unemployment rate; the effect of public opinion on policy is held to depend on institutional arrangements; the effect of natural resources on civil war might depend on the level of economic development. Any time one variable moderates the influence of another, the two variables operate as causal-type nodes for one another's effects on the outcome. Later in this chapter and in other chapters, we work with further examples in which the exogenous variables that define a query have a stronger substantive interpretation.    -->

<!-- More generally, work in graphical models defines the causal effect of $X$ on $Y$ in terms of the changes in $Y$ that arise from interventions on $X$. For example, using the notation for interventions given above we can describe the effect of a change in $X$ from $x'$ to $x''$ on the probability that $Y=1$ in unit $i$ as: -->


<!-- FLAG: SPELL OUT ALL ESTIMANDS AS COLLECTIONS OF nodal typeS -->

## Case-level causal attribution

A query about causal attribution is related to, but different from, a query about a case-level causal effect. When asking about $X$'s case-level effect, we are asking, "*Would* a change in $X$ cause a change in $Y$ in this case?" The question of causal attribution asks: "*Did* $X$ cause $Y$ to take on the value it did in this case?" More precisely, we are asking, "Given the values that $X$ and $Y$ _in fact_ took on in this case, would $Y$'s value have been different if $X$'s value had been different?" 

For instance, given that we know that inequality in Taiwan was relatively low and that Taiwan democratized in 1996, was low inequality a _cause_ of Taiwan's democratization in 1996? Or: given low economic inequality and democratization in Taiwan in 1996, would the outcome in this case have been different if inequality had been high?

This goes beyond simply asking whether Taiwan is a case in which inequality has a causal effect on democratization. Whereas a case-level causal effect is defined in terms of the $\theta$ nodes on endogenous variables,  we define a causal-attribution query in terms of a larger set of nodes. To attribute $Y$'s value in a case to $X$, we need to know not only whether this is the kind of case in which $X$ could have an effect on $Y$ but also whether the context is such that $X$'s value *in fact* made a difference. 

Consider, for instance, the general setup in Figure \@ref(fig:attribquery). Here, $Y$ is a function of two variables, $X_1$ and $X_2$. This means that $\theta^Y$ is somewhat more complicated than in a setup with one causal variable: $\theta^Y$ must here define $Y$'s response to all possible combinations of $X_1$ and $X_2$, including interactions between them.



```{r attribquery, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='60%', fig.cap = "\\label{fig:attribquery} This DAG is a graphical representation of the simple causal setup in which $Y$ depends on two variables $X1$ and $X2$. How $Y$ responds to X1 and X2 depnds on $\\theta^Y$, the DAG itself does not provide information on whether or how X1 and X2 interact with each other."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 0, 1, 1),
       y = c(2, 0, 1, 2),
       names = c(
                expression(paste(X[1])),
                expression(paste(X[2])),
                expression(paste(Y)),
                expression(paste(theta^Y))),
       arcs = cbind( c(1, 4, 2),
                     c(3, 3, 3)),
       title = "What caused Y = 1?",
       padding = .2, contraction = .15)

```

We examined the set of nodal types for a set up like this in Chapter 2 (see Table \@ref(tab:PO16)). In the table, there are four column headings representing the four possible combinations of $X_1$ and $X_2$ values. Each row represents one possible pattern of $Y$ values as $X1$ and $X2$ move through their four combinations. 

<!-- Labelling is a little difficult with so many types. One approach used in Chapter \@ref(models) is to represent change in $X1$ on the horizontal axis, and change in the second variable, $X2$, on the vertical axis. The value of $X1$ increases from 0 to 1 as we move to the _right_ (from $i$ to $j$ or from $g$ to $h$). And the value of $X2$ increases from 0 to 1 as we move _up_ (from $i$ to $g$ or from $j$ to $h$). -->

One way to conceptualize the size of the nodal-type "space" is to note that $X_1$ can have any of our four causal effects (the four binary types) on $Y$ when $X_2=0$; and $X_1$ can have any of four causal effects when $X_2=1$.^[This is precisely equivalent to noting that $X_2$'s effect on $Y$ can be of any of the four types when $X_1=0$ and of any of the four types when $X_1=1$.] This yields 16 possible response patterns to combinations of $X_1$ and $X_2$ values. 
<!-- Thus, for instance, $\theta_{00}^{10}$ (type 5) describes a response pattern in which $W$ has a positive effect on $Y$ when $X=0$ but has no effect, with $Y$ stuck at $0$, when $X=1$; and in which $X$ has no effect when $W=0$ and a negative effect when $W=1$. For $\theta_{11}^{00}$ (type 11), $W$ has a negative effect on $Y$ regardless of $X$'s value; and $X$ has no effect regardless of $W$'s value. -->

<!-- \begin{table}[h!] -->
<!--   \centering -->
<!--   \def\arraystretch{1.3} -->
<!--     \begin{tabular}{ccccccc} -->
<!--     \hline -->
<!--     \textbf {} & \textbf {Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $\\ -->
<!--          & & $W=0)$ & W=1)$ & $W=0)$ & $W=1)$ \\  \hline -->
<!--     1 & $\theta_{00}^{00}$ 			&  0     & 0     & 0     & 0  \\ -->
<!--     2 & $\theta_{00}^{01}$ 	& 0     & 0     & 0     & 1 \\ -->
<!--     3 & $\theta_{01}^{00}$ 	& 0     & 0     & 1     & 0 \\ -->
<!--     4 & $\theta_{01}^{01}$ 			& 0     & 0     & 1     & 1 \\ -->
<!--     5 & $\theta_{00}^{10}$ 	& 0     & 1     & 0     & 0 \\ -->
<!--     6 & $\theta_{00}^{11}$ 			& 0     & 1     & 0     & 1 \\ -->
<!--     7 & $\theta_{01}^{10}$ 	& 0     & 1     & 1     & 0 \\ -->
<!--     8 & $\theta_{01}^{11}$ 		& 0     & 1     & 1     & 1 \\ -->
<!--     9 & $\theta_{10}^{00}$			& 1     & 0     & 0     & 0 \\ -->
<!--     10 & $\theta_{10}^{01}$ 	& 1     & 0     & 0     & 1 \\ -->
<!--     11 & $\theta_{11}^{00}$			& 1     & 0     & 1     & 0 \\ -->
<!--     12 & $\theta_{11}^{01}$		& 1     & 0     & 1     & 1 \\ -->
<!--     13 & $\theta_{10}^{10}$			& 1     & 1     & 0     & 0 \\ -->
<!--     14 & $\theta_{10}^{11}$		& 1     & 1     & 0     & 1 \\ -->
<!--     15 & $\theta_{11}^{10}$		& 1     & 1     & 1     & 0 \\ -->
<!--     16 & $\theta_{11}^{11}$			& 1     & 1     & 1     & 1 \\ -->
<!--     \bottomrule -->
<!--     \end{tabular}% -->
<!--    \caption{The table defines the 16 values (nodal types) that $\theta^Y$ can take on, given a binary $X$ and $W$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies.} -->
<!--   \label{tab:types2x}% -->
<!-- \end{table} -->


A query about causal attribution---whether $X_1 = 1$ caused $Y=1$---for the model in  Figure \@ref(fig:attribquery), needs to be defined in terms of both $X_2$ and $\theta^Y$. Parallel to our Taiwan example, suppose that we have a case in which $Y=1$ and in which $X_1$ was also 1, and we want to know whether $X_1$ caused $Y$ to take on the value it did. Answering this question requires knowing whether the case's type is such that $X_1$ would have had a positive causal effect on $Y$, _given the value of $X_2$_ (which we can think of as part of the context). Thus, given that we start with knowledge of $X_1$'s and $Y$'s values, our query about causal attribution amounts to a query about two nodal types: (a) $\theta^{X_2}$ (which gives $X_2$'s value) and (b) $\theta^Y$, specifically whether its value is such that $X_1$ has a positive causal effect given $X_2$'s value.

Suppose, for instance, that we were to observe $X_2=1$. We then need to ask whether the nodal type, $\theta^Y$, is such that $X_1$ has a positive effect when $X_2=1$. Consider $\theta^Y_{0111}$ (type 8 in Table \@ref(tab:PO16)).^[A reminder that, with two-parent nodes, the nodal-type subscript ordering is $Y|(X_1=0, X_2=0); Y|(X_1=1, X_2=0); Y|(X_1=0, X_2=1); Y|(X_1=1, X_2=1)$.] This is a nodal type in which $X_1$ has a positive effect when $X_2=0$ but no effect when $X_2=1$. Put differently, $X_2=1$ is a sufficient condition for $Y=1$, meaning that $X_1$ makes no difference to the outcome when $X_2=1$. 

<!-- FLAG: Check notation above, is old.

<!-- Looking down the table, we can readily identify the nodal types that qualify by focusing on the two superscripts---which provide responses to $X$ when $W=1$---and looking for a $01$ in that upper row.  -->
In all we have four qualifying $Y$-types: $\theta^Y_{0001}$, $\theta^Y_{1001}$, $\theta^Y_{0101}$, $\theta^Y_{1101}$. In other words, we can attribute a $Y=1$ outcome to $X_1=1$ when $X_2=1$ and $Y$'s nodal type is one of these four.  

Thus, a question about causal attribution is a question about the *joint* value of a set of nodal types: about whether the _combination_ of context and the nodal type(s) governing effects is such that changing the causal factor of interest would have changed the outcome.

## Actual causes

So far we have been dealing with causes in the standard counterfactual sense: antecedent conditions a change in which would have produced a different outcome. Sometimes, however, we are interested in identifying antecedent conditions that were not counterfactual difference-makers but that nonetheless _generated_ or _produced_ the outcome. Consider, for instance, a situation in which an outcome was overdetermined: multiple conditions were present, each of which on their own, _could_ have generated the outcome. Then none of these conditions caused the outcome in the counterfactual sense; yet one or more of them may have been distinctively important in *producing* the outcome. The concept of an *actual cause* can be useful in putting a finer point on this kind of causal question.  

A motivating example used in much of the literature on actual causes [e.g.  @hall2004two] imagines two characters, Sally and Billy, simultaneously throwing stones at a bottle. Both are excellent shots and hit whatever they aim at. Sally's stone hits first, and so the bottle breaks. However, Billy's stone *would* have hit had Sally's not hit, and would have broken the bottle. Did Sally's throw cause the bottle to break? Did Billy's?

By the usual definition of causal effects, neither Sally's nor Billy's action had a causal effect: without either throw, the bottle would still have broken. We commonly encounter similar situations in the social world. We observe, for instance, the onset of an economic crisis and the breakout of war---either of which would be sufficient to cause the government's downfall---but with (say) the economic crisis occurring first and toppling the government before the war could do so. In this situation, neither economic crisis nor war in fact made a difference to the outcome: take away either one and the outcome remains the same.

To return to the bottle example, while neither Sally's nor Billy's throw is a counterfactual cause, there is an important sense in which Sally's action obviously broke the bottle, and Billy's did not. We can formalize this intuition by defining Sally's throw as the *actual cause* of the outcome. Using the definition provided by [@halpern2015modification], building on [@halpern2005causesa] and others, we say that a condition ($X$ taking on some value $x$) was an actual cause of an outcome (of $Y$ taking on some value $y$), where $x$ and $y$ may be collections of events, if:

1. $X=x$ and $Y=y$ both happened
2. there is some set of variables, $\mathcal W$, such that if they were fixed at the levels that they actually took in the case, and if $X$ were to be changed, then $Y$ would change (where $\mathcal W$ can also be an empty set)
3. no strict subset of $X$ satisfies 1 and 2 (there is no redundant part of the condition, $X=x$)

The definition thus describes a condition that *would* have been a counterfactual cause of the outcome if we were to imagine holding constant some set of events that in fact occurred (and that, in reality, might not have been constant if the actual cause had not in fact occurred).

Let us now qpply these 3 conditions to the Sally and Billy example. Conditions 1 and 3 are easily satisfied, since Sally \emph{did} throw and the bottle \emph{did} break (Condition 1), and "Sally threw" has no strict subsets (Condition 3).

Condition 2 is met if Sally's throw made a difference, counterfactually speaking --- with the important caveat that, in determining this, we are permitted to condition on (to fix in the counterfactual comparison) any event or set of events that actually happened (or on on none at all). To see why Condition 2 is satisfied, we have to think of there being three steps in the process: (1) Sally and Billy throw, (2) Sally's or Billy's rock hits the bottle, and (3) the bottle breaks. In actuality, Billy's stone did not hit the bottle, so we are allowed to condition on that fact in determining whether Sally's throw was a counterfactual cause. Conditioning on Billy's stone not hitting, the bottle would *not* have broken had Sally not thrown. 

From the perspective of counterfactual causation, it may seem odd to condition on Billy's stone not hitting the bottle when thinking about Sally not throwing the stone---since Sally's throwing the stone was the very thing that prevented Billy from hitting the bottle. Yet Halpern argues that this is an acceptable thought experiment for establishing the importance of Sally's throw since conditioning is constrained to the actual facts of the case. Moreover, the same logic shows why Billy is not an actual cause. The reason is that Billy's throw is only a cause in those conditions in which Sally did not hit the bottle. But because Sally \emph{did} actually hit the bottle, we are not permitted to condition on Sally not hitting the bottle in determining actual causation. We thus cannot---even through conditioning on actually occurring events---construct any counterfactual comparison in which Billy's throw is a counterfactual cause of the bottle's breaking.

The striking result here is that there can be grounds to claim that a condition was the actual cause of an outcome even though, under the counterfactual definition, the effect of that condition on the outcome is 0. (At the same time, all counterfactual causes are automatically actual causes; they meet Condition 2 by conditioning on nothing at all, an empty set $\mathcal W$.) One immediate methodological implication follows: since actual causes need not be causes, there are risks in research designs that seek to understand causal effects by tracing back actual causes---i.e., the way things actually happened. If we traced back from the breaking of the bottle, we might be tempted to identify Sally's throw as the cause of the outcome. We would be right only in an actual-causal sense, but wrong in the standard, counterfactual causal sense. Chains of events that appear to "generate" an outcome are not always causes in that sense.^[Perhaps more surprising, it is possible that the expected causal effect is negative but that $X$ is an actual cause in expectation. For instance, suppose that 10% of the time Sally's shot intercepts Billy's shot but without hitting the bottle. In that case the average causal effect of Sally's throw on bottle breaking is $-0.1$ yet 90% of the time Sally's throw is an actual cause of bottle breaking (and 10% of the time it is an actual cause of non-breaking). For related discussions, see @menzies1989probabilistic.]

<!-- Generally, an antecedent condition, $A$, that played a role in generating an outcome might not be a counterfactual cause because, had it not occurred, some second chain of events set in motion by $B$ would have unfolded, generating the outcome anyway. In the standard counterfactual scenario, $A$ is not a counterfactual cause: take away $A$ and the outcome still happens because of the chain of events emanating from $B$. Yet let us imagine that the fact that $A$ _did_ occur _prevented_ part of $B$'s chain of consequences from unfolding and itself producing the outcome. Then let us imagine a tweaked counterfactual comparison in which we *fix* the observed fact that $B$'s causal sequence did not fully unfold. We can then ask: *conditional on $B$'s sequence not fully unfolding*, would $A$ have been a counterfactual cause of the outcome? If so, then we say that $A$ is an "actual cause" of the outcome. We have, in a sense, identified $A$ as distinctively important in the production of the outcome, even if it was not a case-level cause in the usual sense. -->

As with other causal queries, the question "Was $X=x$ the actual cause of $Y=y$?" can be redefined as a question about which combinations of nodal types produce conditions under which $X$ could have made a difference. To see how, let us run through the Billy and Sally example again, but formally in terms of a model. Consider Figure \ref{fig:actualquery}, where we represent Sally's throw ($S$), Billy's throw ($B$), Sally's rock hitting the bottle ($H^S$), Billy's rock hitting the bottle ($H^B$), and the bottle cracking ($C$). Each endogenous variable has a $\theta$ term associated with it, capturing its nodal type. We capture the possible "preemption" effect with the arrow pointing from $H^S$ to $H^B$, allowing whether Sally's rock hits to affect whether Billy's rock hits.^[We do not need an arrow in the other direction because Sally throws first.] 

For Sally's throw to be an actual cause of the bottle's cracking, we need first to establish that Sally threw ($\theta^S=\theta^S_1$) and that the bottle cracked ($\theta^C=\theta^C_1$) (Condition 1). Condition 3 is automatically satisfied in that $\theta^S=\theta^S_1$ has no strict subsets. Turning now to Condition 2, we need Sally's throw to be a counterfactual cause of the bottle cracking if we condition on the value of some set of nodes remaining fixed at the values they in fact took on. As discussed above, we know that we can meet this criterion if we condition on Billy's throw not hitting. To make this work, we need to ensure, first, that Sally's throw hits if and only if she throws: so $\theta^{H^S}=\theta^{H^S}_{01}$. Next, we need to ensure that Billy's throw does not hit whenever Sally's does: this corresponds to any of the four nodal types for $H^B$ that take the form $\theta^{H^B}_{xx00}$, meaning that $H^B=0$ whenever $H^S=1$. Note that the effect of Billy throwing on Billy hitting when Sally has *not* thrown---the first two terms in the nodal-type's subscript---does not matter since we have already selected a value for $\theta^S$ such that Sally does indeed throw.

Finally, we need $\theta^C$ to take on a value such that $H^S$ has a positive effect on $C$ when $H^B=0$ (Billy doesn't hit) since this is the actual circumstance on which we will be conditioning. This is satisfied by any of the four nodal types of the form $\theta^C_{0x1x}$. This includes, for instance, a $\theta^C$ value in which Billy's hitting has no effect on the bottle (perhaps Billy doesn't throw hard enough!): e.g., $\theta^C_{0011}$. Here, Sally's throw is a counterfactual cause of the bottle's cracking. And, as we have said, all counterfactual causes are actual causes. They are, simply, counterfactual causes when we hold _nothing_ fixed ($\mathcal W$ in Condition 2 is just the empty set). 

Notably, we do not need to specify the nodal type for $B$: given the other nodal types identified, Sally's throw will be the actual cause regardless of whether or not Billy throws. If Billy does not throw, then Sally's throw is a simple counterfactual cause (given the other nodal types). 
 

<!-- can make this work if we being the actual cause of the bottle cracking requires, first of all, that $\theta^{H^B}$ take on a particular value. Specifically, we need (a) $H^B=0$ whenever $H^S=1$ (Sally's hit preempts Billy's) and (b) $B$ to have a positive effect on $H^B$ when $H^S=0$ (Billy's throw hits if Sally's doesn't). This describes the nodal type $\theta^{H^B}_{0100}$. Further, $S$ must have a positive effect on $H^S$ ($\theta^{H^S}=\theta^{H^S}_{01}$. Third, we need $\theta^C$ to take on a value such that $C=1$ if and only if either $H^B$ or $H^S$ equals $1$: this corresponds to the nodal type $\theta^C_{0111}$.  -->

<!-- , Billy threw ($\theta^B=\theta^B_1$), -->

The larger point is that actual cause queries can, like all other causal queries, be defined as questions about the values of nodes in a causal model. When we pose the query, was Sally's throw an actual cause of the bottle cracking, we are in effect asking whether the case's combination of nodal types (or its causal type) matches $\theta^S_1, \theta^B_x, \theta^{H^B}_{xx00}, \theta^{H^S}_{01}, \theta^C_{0x1x}$. 

Likewise, if want to ask *how often* Sally's throw is an actual cause, in a population of throwing rounds, we can address this query as a question about the joint *distribution* of nodal types. We are then asking how common the qualifying combinations of nodal types are in the population given the distribution of types at each node.

<!-- This is a set of nodal types under which the query, "Does $S$ have a causal effect on $C$?" must be answered in the negative. Similarly, this is a context in which $C=1$ cannot be causally attributed to $S=1$. If Sally had not thrown, then Sally's rock would not have hit the bottle, which means that Billy's rock would have hit, and the bottle would still have cracked---we would still get $C=1$. Now, to verify that this setup makes Sally's throw the *actual cause* of the bottle's cracking, we need to ask whether there is some node whose value we can hold fixed at the value that it _actually_ assumed in the case such that $S$ *would* have a causal effect on the outcome. Since our collection of nodal types implies $H^B=0$ --- Billy's rock does not hit --- we can hold this node's value constant, creating an "opportunity" for $S$ to matter: under $\theta^C_{0111}$, if $H^B=0$, $S$ has a positive effect on $C$.  in that $C$ is no longer forced to 1 (by Billy's rock hitting). But for $S$ to matter under his scenario, something else has to be true: $\theta^C$'s value must allow for $H^S$ to have a positive effect on $C$ when $H^B=0$.  -->

<!-- Using our two-cause notation (with $H^S$ on the horizontal axis, and $H^B$ on the vertical), and given that we have already stipulated that $C=1$ when $H^B=1$, the one permissible value for $\theta^C$ is $\theta^{11}_{01}$. This is a nodal type in which neither $H^B$ nor $H^S$ can be causal if both Billy and Sally throw: whenever one variable is 1, the other has no effect. But it is also a type in which each has a causal effect if the other is held at 0.  -->

<!-- It is also the case, as we have said, that all counterfactual causes are actual causes. They are, quite simply, counterfactual causes when we hold _nothing_ fixed ($\mathcal W$ is the empty set). Thus, in fact, any $\theta^S$, $\theta^{H^S}$ and $\theta^C$ values in which $S$ has a positive effect when $B=1$ will do. This includes, for instance, a $\theta^C$ value in which Billy's hitting has no effect on the bottle (perhaps Billy doesn't throw hard enough!): e.g., $\theta^{01}_{01}$. Here, Sally's throw is both a counterfactual cause and an actual cause of the bottle's cracking.  -->


```{r actualquery, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='60%', fig.cap = "\\label{fig:actualquery} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's nodal type for $Y$, represented by $\\theta^Y$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 0, 1, 1, 1, 1, 3, 3),
       y = c(2, 0, 2, 3, 0, -1, 1, 2),
       names = c(
                expression(paste(S)),
                expression(paste(B)),
                expression(paste(H^S)),
                expression(paste(theta^H^S)),
                expression(paste(H^B)),
                expression(paste(theta^H^B)),
                expression(paste(C)),
                expression(paste(theta^C))),
       arcs = cbind( c(1, 2, 4, 6, 3, 5, 3, 8),
                     c(3, 5, 3, 5, 7, 7, 5, 7)),
       title = "A DAG Capturing an Actual Cause",
       padding = .2, contraction = .15)

```

Actual causes are conceptually useful whenever there are two sufficient causes for an outcome, but one preempts the operation of the other. For instance, we might posit that both the United States' development of the atomic bomb was a sufficient condition for U.S. victory over Japan in World War II, and that U.S. conventional military superiority was also a sufficient condition and would have operated via a land invasion of Japan. Neither condition was a counterfactual cause of the outcome because both were present. However, holding constant the _absence_ of a land invasion, the atomic bomb was a difference-maker, rendering it an actual cause. The concept of actual cause thus helps capture the sense in which the atomic bomb distinctively contributed to the outcome, even if it was not a counterfactual cause.

<!-- Similarly, the question of how *common* it is for a condition to be an actual cause can be expressed as values of nodes, possibly including nodes that record parameter values for the relevant exogenous nodes. -->

<!-- We should try to be more specific here and for notable causes about what the nodes we'd want to learn about are. -->


An extended notion [@halpern2016actual, p 81] of actual causes restricts the imagined counterfactual deviations to states that are more likely to arise (more "normal") than the factual state. We will call this notion a "notable cause." We can say that one cause, $A$, is "more notable" than another cause, $B$, if a deviation in $A$ from its realized state is (believed to be) more likely than a deviation in $B$ from its realized state.

For intuition, we might wonder why a Republican was elected to the presidency in a given election. In looking at some minimal winning coalition of states that voted Republican, we might distinguish between a set of states that *always* vote Republican and a set of states that usually go Democratic but voted Republican this time. If the coalition is minimal winning, then every state that voted Republican is a cause of the outcome in the standard (difference-making) sense. However, only the states that usually vote Democratic are notable causes since it is only for them that the counterfactual scenario (voting Democratic) was more likely to arise than the factual scenario. In a sense, we take the "red" states' votes for the Republican as given---placing them, as it were, in the causal background---and identify as "notable" those conditions that mattered and easily could have gone differently. By the same token, we can say that, among those states that voted Republican this time, those that more commonly vote Democratic are *more* notable causes than those that less commonly vote Democratic.

How notable a counterfactual cause is can be expressed as a claim about the distribution of a set of nodal types. For instance, if we observe $R^j=1$ for state $j$ (it voted Republican), then the notability of this vote directly increases in our belief about the probability that $\theta^{R^j}=\theta_0^{R^j}$: the probability that the state's vote could have gone the other way.

<!-- Though not a focus of our applied examples we show formally how to estimate these estimands in the Appendix, section XXX. -->

<!-- So there are 2 things being defined: notable vs. not notable, and more vs. less notable. -->

<!-- The election example seems to be illustrating the first of these since it refers to the volatile states being notable.  -->

<!-- But the reasoning doesn't line up with the definition of notable given here: we haven't said that these are states that usually vote non-Republican. We've only said that their voting non-R is more likely than the other states' voting non-R. -->

<!-- Shall I fix by simply changing the volatile states to ones that usually vote D? -->

<!-- Also, we are not saying what the nodes in Q are, just that there are some. Could we say that they're the same nodes as for a plain actual cause PLUS nodes going into X (a possible notable cause) representing parameters of the distribution of X? -->

## Average causal effects

A more general query asks about an average causal effect in some population. In counterfactual terms, a question about average causal effects is: if we manipulated the value of $X$ for all cases in the population---first setting $X$ to one value for all cases, then changing it to another value for all cases---by how much would the average value of $Y$ in the population change? Like other causal queries, a query about an average causal effect can be conceptualized as learning about a node in a causal model. 

We can do this by conceiving of any given case as being a member of a population composed of different nodal types. When we seek to estimate an average causal effect, we seek information about the *shares* of these nodal types in the population. 

More formally and adapted from @humphreys2015mixing, we can use $\lambda^Y_{ij}$ to refer to the *share* of cases in a population that has nodal type $\theta^Y_{ij}$. Thus, given our four nodal types in a two-variable binary setup, $\lambda^Y_{10}$ is the proportion of cases in the population with negative effects; $\lambda_{01}$ is the proportion of cases with positive effects; and so on. One nice feature of this setup, with both $X$ and $Y$ as binary, is that the average causal effect can be simply calculated as the share of positive-effect cases less the share of negative-effect cases: $\lambda^Y_{01} - \lambda^Y_{10}$. 

Graphically, we can represent this setup by including $\lambda^Y$ in a more complex causal graph as in Figure \ref{fig:DAGace}. As in our setup for case-level causal effects, $X$'s effect on $Y$ in a case depends on (and only on) the case's nodal type, $\theta^Y$. The key difference is that we now model the case's type not as exogenously given, but as a function of two additional variables: the distribution of nodal types in a population and a random process through which the case's type is "drawn" from that distribution. We represent the type distribution as $\lambda^Y$ (a vector of values for the proportions $\lambda^Y_{10}, \lambda^Y_{01}, \lambda^Y_{00}, \lambda^Y_{11}$) and the random process drawing a $\theta^Y$ value from that distribution as $U^\theta$. 


<!-- AJ: FLAG: CLARIFY PHILOSOPHOICAL INTERPREATAION OF LAMBDA AS SHARES -->



<!-- We can also think of these shares as probabilities; that is, we can think of any given case as being ``drawn'' from a multinomial distribution with probabilities $\lambda = (\lambda^Y_{10}, \lambda^Y_{01}, \lambda^Y_{00}, \lambda^Y_{11})$. -->


<!-- We might stipulate, for instance, that $U_\theta$ has a uniform distribution, between 0 and 1. We could write down the structural equation for $\theta^Y$ as:  -->

<!-- $\theta^Y=$ -->

<!--   $\theta^Y_{10}$ if $U_\theta < \lambda^Y_{10}$ -->

<!--   $\theta^Y_{01}$ if $\lambda^Y_{10} < U_\theta < \lambda^Y_{10} + \lambda^Y_{01}$ -->

<!--   $\theta^Y_{00}$ if $\lambda^Y_{10} + \lambda^Y_{01} < U_\theta < \lambda^Y_{10} + \lambda^Y_{01} + \lambda^Y_{00}$ -->

<!--   $\theta^Y_{11}$ if $\lambda^Y_{10} + \lambda^Y_{01} + \lambda^Y_{00} < U_\theta < \lambda^Y_{10} + \lambda^Y_{01} + \lambda^Y_{00} + \lambda^Y_{11}$ -->


<!-- \begin{equation}  -->
<!-- (\#eq:Q) -->
<!-- \end{equation}  -->

<!-- *** -->

In this model, our causal query---about $X$'s average causal effect---is thus defined by the vector $\lambda^Y$, and specifically by the shares of negative- and positive-causal-effect cases, respectively, in the population. What is $X$'s average effect on $Y$ amounts to asking: what are the values of $\lambda^Y_{10}$ and $\lambda^Y_{01}$? As with $\theta^Y$, $\lambda^Y$ is not directly observable. And so the empirical challenge is to figure out what we _can_ observe that would allow us to learn about $\lambda^Y$'s component values?^[Note also that $\lambda^Y$ can be thought of as itself drawn from a distribution, such as a Dirichlet. The hyperparameters of this underlying distribution of $\lambda$ would then represent our uncertainty over $\lambda$ and hence over average causal effects in the population.]

<!-- Of course, like $\theta^Y$, $\lambda^Y$ is not directly observable. Thus, inference about average causal effects will necessarily involve using information about *observable* nodes to learn both about unobservables of interest. We might, for instance, use observations of $X$ and $Y$ to learn about a case's nodal type ($Q$) and, possibly repeating across many cases, about the share of different types in the population ($\lambda$). -->

<!-- **I have decided not to incorporate $U_\lambda$ in the graph because it would actually require a node for the distribution's hyperparameters as well and I think would in any case cloud the point we want to make here.** -->

<!-- Formally, this kind of average causal effect is also calculated using Equation \ref{ate}, though for a model that is not conditional on the case at hand. -->



```{r DAGace, echo = FALSE, fig.width = 8, fig.height = 5,  fig.align="center", out.width='60%', fig.cap = "\\label{fig:DAGace} This DAG is a graphical representation of a causal setup in which cases are drawn from a population composed of different nodal types. As before, $X$'s effect on $Y$ is a function of a causal-type variable, $\\theta^Y$. Yet here we explicitly model the process through which the case's type is drawn from a distribution of types in a population. The variable $\\lambda$ is a vector representing the multinomial distribution of nodal types in the population while $U^\\theta$ is a random variable representing the draw of each case from the distribution defined by $\\lambda$. A case's nodal type, $\\theta^Y$, is thus a joint function of $\\lambda^Y$ and $U^{\\theta^Y}$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 2, 2, 1, 3),
       y = c(1, 1, 2, 3, 3),
       names = c("X", "Y", expression(paste(theta^Y)), expression(paste(lambda^Y)), expression(paste(U^theta[Y]))),
       arcs = cbind( c(1, 3, 4, 5),
                     c(2, 2, 3, 3)),
       title = "A DAG with Nodal Type Drawn from a Population-level Distribution of Nodal Types",
       padding = .2, contraction = .15) 

```


We can, of course, likewise pose queries about other population-level causal quantities. For instance, we could ask for what proportion of cases in the population $X$ has a positive effect: this would be equivalent to asking the value of $\lambda^Y_{01}$, one element of the $\lambda^Y$ vector. Or we could ask about the proportion of cases in which $X$ has no effect, which would be asking about $\lambda^Y_{00} + \lambda^Y_{11}$.



## Causal Paths

To develop richer causal understandings, researchers often seek to describe the causal path or paths through which effects propagate. Consider the DAG in Figure \@ref(fig:DAGpaths), in which $X$ can affect $Y$ through two possible pathways: directly and via $M$. Assume again that all variables are binary, taking on values of $0$ or $1$. Here we have nodal types defining $M$'s response to $X$ ($\theta^M$) and defining $Y$'s response to both $X$ (directly) and $M$ ($\theta^Y$). 

Suppose that we observe $X=1$ and $Y=1$ in a case. Suppose, further, that we have reasonable confidence that $X$ has had a positive effect on $Y$ in this case. We may nonetheless be interested in knowing whether that causal effect ran *through* $M$. We will refer to this as a query about a causal path. Importantly, a causal path query is not satisfied simply by asking whether some mediating event along the path occurred. We cannot, for instance, establish that the top path in Figure \ref{fig:DAGpaths} was operative simply by determining the value of $M$ in this case---though that will likely be useful information. 

Rather, the question of whether the mediated (via $M$) causal path is operative is a composite question of two parts: First, does $X$ have an effect on $M$ in this case? Second, does that effect---the difference in $M$'s value caused by a change in $X$---in turn *cause* a change in $Y$'s value? In other words, what we want to know is whether the effect of $X$ on $Y$ depends on---that is, *will not operate without*---the effect of $X$ on $M$.^[A very similar question is taken up in work on  mediation where the focus goes to understanding quantities such as the "indirect effect" of $X$ on $Y$ via $M$. Formally, the indirect effect would be $$Y(X=1, M = M(X=1,\theta^M), 
\theta^Y) - Y(X = 1, M = M(X=0, \theta^M), \theta^Y))$$, which captures the difference to $Y$ if $M$ were to change in the way that it would change due to a change in $X$, but without an actual change in $X$ [@pearl2009causality p 132, @imai2010general].] Framing the query in this way makes clear that asking whether a causal effect operated via a given path is in fact asking about a specific set of causal effects lying along that path.


```{r DAGpaths, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='60%', fig.cap = "\\label{fig:DAGpaths} Here $X$ has effects on $Y$ both indirectly through $M$ and directly."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 2, 1, 2),
       y = c(2, 3, 2, 4, 3),
       names = c("X", "M", "Y", expression(paste(theta^M)), expression(paste(theta^Y))),
       arcs = cbind( c(1, 2, 1, 4, 5),
                     c(2, 3, 3, 2, 3)),
       title = "A DAG with Two Causal Paths",
       padding = .2, contraction = .15) 

```


As we can show, we can define this causal-path query as a question about specific nodes on a causal graph. In particular, a causal path can be defined in terms of the values of $\theta$ nodes: specifically, in the present example, in terms of $\theta^M$ and $\theta^Y$. To see why, let us first note that there are two combinations of effects that would allow $X$'s positive effect on $Y$ to operate via $M$: (1) $X$ has a positive effect on $M$, which in turn has a positive effect on $Y$; or (2) $X$ has a negative effect on $M$, which has a negative effect on $Y$. 

Thus, in establishing whether $X$ affects $Y$ through $M$, the first question is whether $X$ affects $M$ in this case. Whether or not it does is a question about the value of $\theta^M$. We know that $\theta^M$ can take on four possible values corresponding to the four possible responses to $X$: $\theta^M_{10}, \theta^M_{01}, \theta^M_{00}, \theta^M_{11}$. For sequence (1) to operate, $\theta^M$ must take on the value $\theta^M_{01}$, representing a positive effect of $X$ on $M$. For sequence (2) to operate, $\theta^M$ must take on the value $\theta^M_{10}$, representing a negative effect of $X$ on $M$.

$\theta^Y$ defines $Y$'s response to different combinations of two other variables---here, $X$ and $M$---since _both_ of these variables point directly into $Y$. Where $X$ can have both a mediated effect through $M$ and a direct effect, $X$ and $M$ also potentially _interact_ in affecting $Y$. Another way to think about this setup is that $M$ is not just a possible mediator of $X$'s indirect effect; $M$ is also a potential _moderator_ of $X$'s direct effect. This results in sixteeen possible values for $\theta^Y$---again as shown above in Table \@ref(tab:PO16).

<!-- AJ: This table, in Chap 2, is in terms of X1 and X2. And we don't really want people having to flip back. Think we should resurrect one of the tables below, wiht correct theta notation.  -->

<!-- We spell out potential outcomes for the 16 resulting types---each a possible value of $\theta^Y$---in Table \@ref(tab:typespaths), which is parallel to Table \@ref(tab:types2x). Within $\theta$'s sub- and superscripts, the value of $X$ increases from 0 to 1 as we move to the right while the value of $M$ increases from 0 to 1 as we move up. -->

<!-- \begin{table}[h!] -->
<!--   \centering -->
<!--   \def\arraystretch{1.3} -->
<!--     \begin{tabular}{ccccccc} -->
<!--     \hline -->
<!--     \textbf {} & \textbf {Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $\\ -->
<!--          & & $M=0)$ & $M=1)$ & $M=0)$ & $M=1)$ \\  \hline -->
<!--     1 & $\theta_{00}^{00}$ 			&  0     & 0     & 0     & 0  \\ -->
<!--     2 & $\theta_{00}^{01}$ 	& 0     & 0     & 0     & 1 \\ -->
<!--     3 & $\theta_{01}^{00}$ 	& 0     & 0     & 1     & 0 \\ -->
<!--     4 & $\theta_{01}^{01}$ 			& 0     & 0     & 1     & 1 \\ -->
<!--     5 & $\theta_{00}^{10}$ 	& 0     & 1     & 0     & 0 \\ -->
<!--     6 & $\theta_{00}^{11}$ 			& 0     & 1     & 0     & 1 \\ -->
<!--     7 & $\theta_{01}^{10}$ 	& 0     & 1     & 1     & 0 \\ -->
<!--     8 & $\theta_{01}^{11}$ 		& 0     & 1     & 1     & 1 \\ -->
<!--     9 & $\theta_{10}^{00}$			& 1     & 0     & 0     & 0 \\ -->
<!--     10 & $\theta_{10}^{01}$ 	& 1     & 0     & 0     & 1 \\ -->
<!--     11 & $\theta_{11}^{00}$			& 1     & 0     & 1     & 0 \\ -->
<!--     12 & $\theta_{11}^{01}$		& 1     & 0     & 1     & 1 \\ -->
<!--     13 & $\theta_{10}^{10}$			& 1     & 1     & 0     & 0 \\ -->
<!--     14 & $\theta_{10}^{11}$		& 1     & 1     & 0     & 1 \\ -->
<!--     15 & $\theta_{11}^{10}$		& 1     & 1     & 1     & 0 \\ -->
<!--     16 & $\theta_{11}^{11}$			& 1     & 1     & 1     & 1 \\ -->
<!--     \bottomrule -->
<!--     \end{tabular}% -->
<!--    \caption{The table defines the 16 values (nodal types) that $\theta^Y$ can take on, given a binary $X$ and $M$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies.} -->
<!--   \label{tab:typespaths}% -->
<!-- \end{table} -->

<!-- ------------------------------------------------------------------- -->
<!--    **Type**    $(Y | X=0,$  $(Y |X=0,$   $(Y | X=1,$   $(Y | X=1,$   -->
<!--                   $M=0)$       $M=1)$       $M=0)$        $M=1)$     -->
<!-- -------------  -----------  -----------  ------------  ------------ -->
<!-- $\theta_{0j}^{gh}$             0            0             0             0       -->

<!--       2             0            0             0             1       -->

<!--       3             0            0             1             0       -->

<!--       4             0            0             1             1       -->

<!--       5             0            1             0             0       -->

<!--       6             0            1             0             1       -->

<!--       7             0            1             1             0       -->

<!--       8             0            1             1             1       -->

<!--       9             1            0             0             0       -->

<!--       10            1            0             0             1       -->

<!--       11            1            0             1             0       -->

<!--       12            1            0             1             1       -->

<!--       13            1            1             0             0       -->

<!--       14            1            1             0             1       -->

<!--       15            1            1             1             0       -->

<!--       16            1            1             1             1       -->
<!-- ------------------------------------------------------------------- -->

<!-- Table: (\#tab:typespaths)$Y$'s 16 nodal types---values of $Q^Y$---given binary $X$ and $M$ as parents of $Y$ -->


What values of $\theta^Y$
<!-- , of those displayed in Table \@ref(tab:typespaths),  -->
then are compatible with the operation of the $M$ causal path? Let us first consider this question with respect to sequence (1), in which $X$ has a positive effect on $M$, and that positive effect is necessary for $X$'s positive effect on $Y$ to occur. For this sequence to operate, $\theta^M$ must take on the value of $\theta^M_{01}$. When it comes to $\theta^Y$, then, what we need to look for types in which $X$'s effect on $Y$ _depends on $M$'s taking on the value it does as a result of $X$'s positive effect on $M$_. 

We are thus looking for nodal types that capture two kinds of counterfactual causal relations operating on nodes. First, $X$ must have a positive effect on $Y$ when $M$ changes as it does as a result of $X$'s positive effect on $M$. Second, that change in $M$, generated by a change in $X$, must be *necessary* for $X$'s positive effect on $Y$ to operate. The thought experiment here thus imagines a situation in which $X$ changes from $0$ to $1$,^[This is the natural thought experiment when explaining a case with realized value of $X=1$, in which the outcome can be thought of as having been generated by a change from $X=0$. The identification of types does hinge, however, on the direction in which we imagine types changing. In other situations, we might observe $X=Y=0$ and thus conceive of the outcome as having been generated by a change from $X=1$ to $X=0$ (again, assuming a positive effect of $X$ on $Y$). When we do this, query 2 below changes: we are now looking for types in which $Y=1$ when $X=0$ but $M=1$. (Does $Y$ stay at $1$ when $X$ moves to $0$ but $M$ doesn't?) The queries are then satisfied by types $6$ and $8$, rather than $2$ and $6$.] but $M$ does *not* change to the value that it should as a result of this change in $X$. We then inspect our types to see if $Y$ would change from $0$ to $1$ in this situation. It is only if $Y$ would *not* change to $1$ in this situation that we have identified a  nodal type for which the $M$-mediated path matters. It is this thought experiment that isolates the causal significance of the path that runs through $M$. 

Assuming a positive effect of $X$ on $M$ ($\theta^M=\theta^M_{01}$), we thus need to apply the following set of queries to $\theta^Y$:^[Using standard potential outcomes notation, we can express the overall query, conditioning on a positive effect of $X$ on $M$, via the inequality $Y(1, M(1)) - Y(0, M(0)) > Y(1, M(0)) - Y(0, M(0))$. The three specific queries formulated below simply correspond to the three unique elements of this expression. We can also readily map the path query that we are defining here---does the positive effect of $X$ on $Y$ depend on $X$'s effect on $M$---onto a query posed in terms of indirect effects. For instance, in our binary setup, conditioning our path query on a positive causal effect of $X$ on $Y$, a positive effect of $X$ on $M$, and an imagined change from $X=0$ to $X=1$ generates precisely the same result (identifies the same $\theta^Y$ types) as asking which $\theta^Y$ types are consistent with a positive indirect effect of $X$ on $Y$, conditioning on a positive total effect and $X=1$.]

1. Is $X=1$ a counterfactual cause of $Y=1$, given $X$'s positive effect on $M$? Establishing this positive effect of $X$ involves two queries:

    a) Where $X=0$, does $Y=0$? As we are assuming $X$ has a positive effect on $M$, if $X=0$ then $M=0$ as well. We thus look down the $X=0, M=0$ column and eliminate those types in which we do not observe $Y=0$. This eliminates types $9$ through $16$.

    b) Where $X=1$, does $Y=1$? Again, given $X$'s assumed positive effect on $M$, $M=1$ under this condition. Looking down the $X=1, M=1$ column, we eliminate those types where we do not see $Y=1$. We retain only types $2, 4, 6,$ and $8$.

2. Is $X$'s effect on $M$ necessary for $X$'s positive effect on $Y$? That is, do we see $Y=1$ *only* if $M$ takes on the value that $X=1$ generates ($M=1$)? To determine this, we inspect the _counterfactual_ condition in which $X=1$ and $M=0$, and we ask: does $Y=0$? Of the four remaining types, only $2$ and $6$ pass this test. 




<!-- \begin{table}[h!] -->
<!--   \centering -->
<!--     \begin{tabular}{cccccc} -->
<!--     \hline -->
<!--     **Type** &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $ \\ -->
<!--          & $M=0)$ & $M=1)$ & $M=0)$ & $M=1)$ \\  \hline -->
<!--         1 			&  0     & 0     & 0     & 0 \\ -->
<!--     2 	& 0     & 0     & 0     & 1 \\ -->
<!--     3 	& 0     & 0     & 1     & 0 \\ -->
<!--     4 			& 0     & 0     & 1     & 1 \\ -->
<!--     5 	& 0     & 1     & 0     & 0 \\ -->
<!--     6 			& 0     & 1     & 0     & 1 \\ -->
<!--     7 	& 0     & 1     & 1     & 0 \\ -->
<!--     8 		& 0     & 1     & 1     & 1 \\ -->
<!--     9			& 1     & 0     & 0     & 0 \\ -->
<!--     10 	& 1     & 0     & 0     & 1 \\ -->
<!--     11			& 1     & 0     & 1     & 0 \\ -->
<!--     12		& 1     & 0     & 1     & 1 \\ -->
<!--     13			& 1     & 1     & 0     & 0 \\ -->
<!--     14		& 1     & 1     & 0     & 1 \\ -->
<!--     15		& 1     & 1     & 1     & 0 \\ -->
<!--     16			& 1     & 1     & 1     & 1 \\ -->
<!--     \bottomrule -->
<!--     \end{tabular}% -->
<!--    \caption{$Y$'s 16 nodal types---values of $\Q^Y$---given binary $X$ and $M$ as parents of $Y$} -->
<!--   \label{typespaths}% -->
<!-- \end{table}% -->

Under these and only these two values of $\theta^Y$---$\theta^Y_{0001}$ and $\theta^Y_{0101}$---we will see a positive effect of $X$ on $Y$ for which the $M$-mediated path is causally necessary, given a positve effect of $X$ on $M$. These two $\theta^Y$ values are also different from one another in an interesting way. For type $\theta^Y_{0101}$, $X$'s effect on $Y$ runs strictly through $M$: if $M$ were to change from $0$ to $1$ *without* $X$ changing, $Y$ would still change from $0$ to $1$. $X$ is causally important for $Y$ _only_ insofar as it affects $M$. In a case of type $\theta^Y_{0101}$, then, anything else that similarly affects $M$ would generate the same effect on $Y$ as $X$ does. In type $\theta^Y_{0001}$, however, both $X$'s change to $1$ *and* the resulting change in $M$ are necessary to generate $Y$'s change to $1$; $X$'s causal effect thus requires both the mediated and the unmediated pathway. And here $X$ itself matters in the counterfactual sense; for a case of type $\theta^Y_{0001}$, some other cause of $M$ would *not* generate the same effect on $Y$. 

<!-- The structural equation for $M$ will include $X$ and $Q_M$ as arguments. Thus, knowing the value of $M$ for any given value of $X$, conditional on a given structural equation for $M$, requires knowing $U_M$. The same logic operates for $U_Y$'s role in determining how $Y$ responds to a given change in $M$, conditional on $Y$'s structural equation.  -->

<!-- Note that, as we saw with causal effects, it is also possible to imagine related estimands of the form "does $X$ cause $Y$ in this case through $M$?", "did $X$ cause $Y$ in this case through $M$?" (which requires knowledge of $X$), and "how often does $X$ cause $Y$ though $M$ in a larger population?" (which requires knowledge of the parameters that give rise to $U_Y$ and $U_M$).   -->

We can undertake the same exercise for sequence (2), in which $X$ first has a negative effect on $M$, or $\theta^M=\theta^M_{10}$. Here we adjust the three queries for $\theta^Y$ to take account of this negative effect. Thus, we adjust query 1a so that we are looking for $Y=0$ when $X=0$ and $M=1$. In query 1b, we look for $Y=1$ when $X=1$ and $M=0$. And for query 2, we want types in which $Y$ fails to shift to $1$ when $X$ shifts to $1$ but $M$ stays at $1$. Types $\theta_{0010}$ and $\theta_{1010}$ pass these three tests. 

In sum, we can define a query about causal paths as a query about the value of $\theta$ terms on the causal graph. For the graph in Figure \ref{fig:DAGpaths}, asking whether $X$'s effect runs via the $M$-mediated path is asking whether one of four combinations of $\theta^M$ and $\theta^Y$ hold in case:

* $\theta^M=\theta^M_{01}$ and ($\theta^Y=\theta_{0001}$ or $\theta_{0101}$)
* $\theta^M=\theta^M_{10}$ and ($\theta^Y=\theta_{0010}$ or $\theta_{1010}$) 

It is worth noting how different this formulation of the task of identifying causal pathways is from widespread understandings of process tracing. Scholars commonly characterize process tracing as a method in which we determine whether a mechanism was operating by establishing whether the events lying along that path occurred. As a causal-model framework makes clear, finding out that $M=1$ (or $M=0$, for that matter) does not establish what was going on causally. Observing this intervening step does not by itself tell us what value $M$ *would* have taken on if $X$ had taken on a different value, or whether this would have changed $Y$'s value. We need instead to conceive of the problem of identifying pathways as one of figuring out the _counterfactual_ response patterns of the variables along the causal chain. As we will demonstrate later in the book, explicitly characterizing those response patterns as nodes in a causal model helps us think systematically about empirical strategies for drawing the relevant inferences.


## General procedure

We have been able to associate a collection of causal types to each of the causal queries we have described in this chapter. But we have not described a general method for doing so. We do that now.^[In particular we describe the algorithm used by the `CausalQueries` package. This approach is not the efficient but it is intuitive and can be used for arbitrarily complex queries.] 

The algorithm  calculates the full set of outcomes on all nodes, given each possible causal type and a collection of controlled conditions ("`do` operations"). Then each causal type is marked as satisfying the query or not. This in turn then tells us the *set* of types that satisfy a query. Quantitative queries, such as the probability of a query being satisfied, or the average treatment effect, can then be calculated by taking the measure of the set of causal types that satisfies the query.  

First some notation.

Let $n$ denote the number of nodes. Label the nodes $V_1, \dots V_n$ subject to the requirement that each node's parents precede it in the ordering. Let $pa_j$ denote the set of values of the parents of node $j$ and let $V_j(pa_j, \theta_t)$ denote the value of node $j$ given the values of its parents and the causal type $\theta_t$.
 
The primitives of a query are questions about the values of outcomes, $V$, given some set of controlled operations $x$. 

* let $x = (x_1, \dots x_n)$ denote a set of `do` operations where each $x_i$ takes on a value in $\{-1,0,1\}$.  here -1 indicates "not controlled", 0 means set to 0 and 1 means set to 1 (this set can be expanded if $V$ is not binary)
* let $V(x, \theta_t)$ denote the values $V$ takes given $\theta_t$
* a "simple query" is a function $q(V(x, \theta_t))$ which returns TRUE if $V(x, \theta_t)$ satisfies some condition and FALSE otherwise.

Queries are summaries of simple queries.  For instance, for nodes $X$ and $Y$:

* Query $Q_1:1(Y(X=1)=1))$ asks whether $Y=1$ when $X$ is set to 1. This requires evaluating one simple query.
* Query $Q_2:1(Y(X=1)=1) \& 1(Y(X=0)=0))$ is composed of two simple queries: the first returns true if $Y$ is 1 when $X$ is set to 1, the second returns true if $Y$ is 0 when $X$ is set to 0; both conditions holding corresponds to a positive effect on a unit.
* Query $Q_3:E((1(Y(X=1)=1) \& (Y(X=0)=0)) - (1(Y(X=1)=0) \& 1(Y(X=0)=1))$ asks for the average treatment effect, represented here using four simple queries:  the expected difference between positive and negative effects. This query involves weighting by the probability of the causal types.


Then to calculate $V(x, \theta_t)$:

  1. Calculate $v_1$, the realized value of the first node, $V_1$, given $\theta_t$. This is given by $v_1 = x_1$ if $x_1 \neq -1$ and by $\theta_t^{V_1}$ otherwise.
  2. For each $j \in 2...n$ calculate $v_j$ using either  $v_j = x_j$ if $x_j \neq -1$ and  $V_{j}(pa_j, \theta_t)$ otherwise, where the values in $pa_j$ are determined in the previous steps.

We now have the outcomes, $V$, for all nodes given the operations $x$ and so can determine $q(V(x))$. From there we can calculate summaries of simple queries across causal types.

A last note on conditional queries. Say we are interested in an attribution query of the form: what is the probability that $X$ causes $Y$ in a case in which $X=1$ and $Y=1$. In this case define simple query $q_1$ which assesses whether $X$ causes $Y$ for a given $\theta_t$ and simple query $q_2$ which assesses whether $X=1$ and $Y=1$ under $\theta_t$. We then calculate the conditional query by conditioning on the set of $\theta$s for which $q_2$ is true and evaluating the share of these for which $q_2$ is true (weighting by the probability of the causal types). 

<!-- FLAG: Alan read -->

## Appendix

We demonstrate how queries are calculated using the `CausalQueries` package for a chain model of the form  $X \rightarrow M \rightarrow Y$. We imagine a model of this form in which we assume no negative effects of $M$ on $X$ or $M$ on $Y$. We will also suppose that in fact $X=1$, always. Doing this keeps the parameter space a little smaller for this demonstration but also serves to demonstrate that a causal model can make use of the counterfactual possibility that a node takes on a particular value even if it never does in fact. 

We then ask two questions:

Q1. What is the probability that $X$ causes $Y$? ("POS")
Q2. What is the probability that $X$ causes $Y$ in cases in which $X=1$ and $Y=1$? ("POC")

To answer these two queries we define simple query $q_1$ which assesses whether $X$ causes $Y$ for each $\theta$ and a second simple query $q_2$ which assesses whether $X=1$ and $Y=1$ for each $\theta$. In this example the first simple query involves some `do` operations, the second does not. 

The answers to these two simple queries are shown in the table below, where each row corresponds to a causal type.

```{r}
model <- make_model("X -> M -> Y") %>%
         set_restrictions("X[]==0") %>%
         set_restrictions("M[X=1] < M[X=0]") %>%
         set_restrictions("Y[M=1] < Y[M=0]")   

q1 <- "Y[X = 1] > Y[X = 0]"
q2 <- "X == 1 & Y == 1"

df <- data.frame(
  a1 = CausalQueries:::map_query_to_causal_type(model, q1)$types,
  a2 = CausalQueries:::map_query_to_causal_type(model, q2)$types,
  p  = get_type_prob(model))
```


```{r, echo = FALSE}
kable(df, caption = "Set of causal types in the model that satisfy q1 and q2 along with the probability of the type.")
```

The answer to the overall queries are then (1) the expected value of (the answers to) $q_1$ and weights $p$and  (2) the expected value of (the answers to) $q_1$ given $q_0$ and weights $p$:

```{r, eval = FALSE}
df %>% summarize(POS = weighted.mean(a1, p),
                 POC = weighted.mean(a1[a2], p[a2]))
```

```{r, echo = FALSE}
df %>% summarize(POS = weighted.mean(a1, p),
                 POC = weighted.mean(a1[a2], p[a2])) %>% kable(caption = "Calculated answers to two queries.")
```

Given the equal weighting on causal types, these answers reflect the fact that for 5 of 9 causal types we expect to see $X=1$ and $Y=1$ but that; the causal effect is present for only 1 of 9 causal types and for 1 of the 5 causal types that exhibit  $X=1$ and $Y=1$.
<!-- Such a  focus on causal paths does not restrict attention to questions of the form "how did $X$ cause $Y$" but more generally, "what paths generated $Y$?" Such questions may have answers of the form "$Y=1$ occurred because $X=0$ led to $M=0$, which, when $Z=1$, gives rise to $Y=1$ and not because $X=1$  led to $M=1$, which, when $Z=0$ gives rise to $Y=1$." Such inquiries can focus on distinct sets of conditions that give rise to an outcome ("equifinality"), as in Qualitative Comparative Analysis (QCA). While QCA analysts sometimes refer to sets of conditions as "paths",  QCA does not generally involve explicit assessment of the causal steps linking conditions to outcomes. When examining paths in a causal-model framework, the analyst can address queries that involve drawing inferences about an entire chain linking $X$ to $Y$ or even an entire causal network. An understanding of a full causal network would, in turn, allow for any more specific estimand to be estimated. -->




<!-- ## Illustration with the Running Example -->

<!-- We can more fully illustrate the definition of causal queries in terms of exogenous nodes on a graph by thinking through their application to the simple causal model described in Chapter 2.  -->

<!-- We illustrate the model again in figure \@ref(fig:running2). -->


<!-- ```{r, echo = FALSE} -->
<!-- names = c("S", "X", "C", "R", "Y") -->
<!-- M <- matrix(0, 5, 5) -->
<!-- M[1, c(3)] <-1 -->
<!-- M[2, c(3,4)] <-1 -->
<!-- M[3, c(4,5)] <-1 -->
<!-- M[4, 5] <-1 -->
<!-- f_C <- function(V) 1- V[1]*V[2] -->
<!-- f_R <- function(V) V[2]*V[3] -->
<!-- f_Y <- function(V) V[3]*V[4] -->
<!-- # Histories and effects: -->

<!-- H <- function(do = c(0,0,NA,NA,NA)) { -->
<!--   do[3] <- ifelse(is.na(do[3]), f_C(do), do[3]) -->
<!--   do[4] <- ifelse(is.na(do[4]), f_R(do), do[4]) -->
<!--   do[5] <- ifelse(is.na(do[5]), f_Y(do), do[5]) -->
<!--   do -->
<!--   } -->

<!-- edges <- function(S,X) { -->
<!--   do0 <- c(S,X,NA, NA, NA) -->
<!--   H1   <- H(do0) -->
<!--   out <- sapply(1:5, function(i) { -->
<!--     do1 <- do0 -->
<!--     do1[i] <- 1-H1[i]  # change value for i and put into do -->
<!--     1*(H1 != H(do1)) -->
<!--   }) -->
<!--   diag(out) <- 0 -->
<!--   out} -->

<!-- ``` -->


<!-- ```{r running2, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align="center", out.width='\\textwidth', fig.cap = "The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\\lambda$ value.", fig.align="center", warning = FALSE} -->

<!-- layout(matrix(c(1,1,2, -->
<!--                 1,1,3, -->
<!--                 4, 5,6), 3, 3, byrow = TRUE)) -->
<!-- par(mar=c(1,1,3.5,1)) -->

<!-- x = c(0,0, 1, 1, 2) -->
<!-- y = c(2,0, 2, 0, 1) -->

<!-- names = c("S:\nSensitive\ngovernment\n\n", "\nX:\nFree Press", "C:\n Corruption", "R:\n Media report", "Y:\nGovernment\nreplaced") -->

<!-- hj_dag(x =  c(x, 0, 0), -->
<!--        y = c(y, 0.25, 1.75), -->
<!--        names = c(names, " ", " "), -->
<!--        arcs = cbind( c(1,2,2, 3, 4, 3, 6, 7), -->
<!--                      c(3,3,4, 5, 5, 4, 2, 1)), -->
<!--        title = "Free Press and Government Survival", -->
<!--        add_functions = 0,  -->
<!--        contraction = .15, -->
<!--        add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS", -->
<!--        padding = .2) -->

<!-- text(c(0,0), c(.25, 1.75), c(expression(paste(U[X])), expression(paste(U[S])))) -->

<!-- names = c("S", "X", "C", "R", "Y") -->

<!-- myarcs <- list( -->
<!--        which(t(edges(0,0))==1, arr.ind = TRUE), -->
<!--        which(t(edges(1,0))==1, arr.ind = TRUE), -->
<!--        which(t(edges(0,1))==1, arr.ind = TRUE), -->
<!--        which(t(edges(1,1))==1, arr.ind = TRUE)) -->

<!-- mysolids <- list(H(c(0,0,NA,NA,NA)),  -->
<!--                  H(c(1,0,NA,NA,NA)),  -->
<!--                  H(c(0,1,NA,NA,NA)),  -->
<!--                  H(c(1,1,NA,NA,NA))) -->

<!-- names = c("S", "X", "C", "R", "Y") -->


<!-- titles = c("A: No free press causes Y = 0",  -->
<!--            "B: No free press is the actual cause\nBut neither S nor X counterfactually cause Y=0", -->
<!--            "C: Both S = 0 and X = 1 cause Y = 1.\n X = 1 is the notable cause.",  -->
<!--            "D: Government sensitivity\ncauses Y = 0") -->
<!-- for(j in 1:4){ -->

<!--     hj_dag( -->
<!--        x = x, -->
<!--        y = y, -->
<!--        names = names, -->
<!--        arcs = myarcs[[j]], -->
<!--        title = titles[[j]], -->
<!--        add_points = TRUE, -->
<!--        solids = c(mysolids[[j]]), -->
<!--        contraction = .15 -->
<!--        ) -->
<!-- } -->

<!-- frame(); box(); -->
<!-- text(.1,seq(.95, .05, -.1),  -->
<!--           c("Structural Equations:", -->
<!--             "  Y = CR", -->
<!--             "  C = 1 - XS", -->
<!--             "  R = CX",  -->
<!--             expression(paste("  S = 1(", u[S]<lambda^S[1],")")), -->
<!--             expression(paste("  X = 1(", u[X]<lambda^X[1],")")), "  ", -->
<!--             "P(U):", -->
<!--             expression(paste("  ", U[S], "~Unif[01]")),  -->
<!--             expression(paste("  ", U[X], "~Unif[01]"))  -->
<!--             ), cex = 1.5, adj = 0) -->
<!-- title("Structural model") -->
<!-- ``` -->


<!-- THe main panel here is the same as in Chapter 2 but but now we add in a set of another four panels. In these panels we leave the $\lambda$ and $U$ terms implicit as they will not come into play in our analysis of these graphs. In these four panels, we show all possible ''realizations'' of the graph given the four possible contexts defined by the exogenous nodes, $S$ and $X$. We build each of the four possible by assessing outcomes and counterfactual relationships for each possible combination of $S$ and $X$ values. A hollow circle at a node indicates that the variable takes on a value of $0$ while a shaded circle indicates a value of $1$. The arrows indicate causal effects. More specifically, an arrow pointing from one variable to another indicates that a manipulation of the first variable would cause a change in the second variable, *given the values realized by all other variables that are not the first variable's descendants*. Unlike in a conventional DAG, we represent here both the direct effect of each variable on its child and each variable's indirect (mediated) causal effects on its descendants. As we can see from the various arrows in the panels, we can use a single, simple causal model to think through a wide range of causal relationships that might be of interest.^[Though similar, these graphs are not DAGS or natural beams (or submodels). The panels reflect outcomes conditional on the values of $S$ and $X$, but they are not themselves DAGs because they indicate the values taken by nodes and include arrows between two nodes when and only when one causes the other, directly or indirectly. To construct "natural beams" [@pearl2009causality 10.3], we fix a realization of root variables, $U$,  (here, $\mathcal U = (S, X)$); then for each variable, $V_i$ we  partition $pa(V_i)$ into a set of "engaged parents," $S$, and "disengaged parents," with the property that (a) $f_i(S(u), \overline{s}, u) = V_i(u)$ for *all* values of $\overline{s}$ and (b) $f_i(s', \overline{S}(u), u) \neq V_i(u)$ for *some*  $s'$. Thus a natural beam  would connect a parent to a child if, given the particular history, the parent mattered for the child's outcome.] Since the values of all variables in a model are determined by the values of the exogenous nodes, this is equivalent to saying that the arrows show the causal effects that are operating each *context.* -->

<!-- One important feature of DAGs is immediately evident from a comparison of the DAG with subgraphs $A, B, C$, and $D$ in the figure. Consistent with the rules of DAG-construction, the DAG includes arrows between all variables that are under any circumstances directly causally related; but the inclusion of an arrow does not mean that two variables are *always* causally related. For instance, while the DAG (large graph) has an arrow running from $X$ to $R$, we can see from the subgraphs (where we deviate from the standard grammar of DAGs) that the causal effect is contingent on context: it is present only when $S=0$ (panels $A$ and $C$) but not when $S=1$. The arrows in a DAG represent dependencies that exist under *some*, but not necessarily under all, values of the exogenous variables. -->

<!-- These five graphs allow us to define all causal claims of interest. The graphs illustrate, in other words, how causal queries can be represented as the value of the exogenous nodes in a causal diagram. Let us consider each causal query in turn. -->

<!-- **Case-level causal effect.** Working with the four subgraphs, we can show that the query, "What is the effect of one variable on another in this case?" is equivalent to asking about the values of the model's exogenous variables, $X$ and $S$. Consider, for instance, the query: Do media reports of corruption, $R$, have a causal effect on government removal from office, $Y$, in this case? Turning to the subgraphs, we can simply ask in which of these four graphs---in which context---$R$ has a causal effect on $Y$: where is there an arrow running from $R$ to $Y$?^[The subgraphs are derived from application of Equation EQUATION REFERENCE. We can work through the $R \rightarrow Y$ relationship to demonstrate how this is done. Consider the effect of $R$ on $Y$ given $S=0, X=0$. This is the arrow between  $R$ and $Y$ in panel $A$. Removing the arrows pointing to $R$, the distribution over nodes when $R=r'$ is: $P(c,y | \hat{r}=r', s =0, x=0)$. We are interested in $P(y=1| \hat{r}=1,  s =0, x=0) - P(y=1 | \hat{r}= 0,  s =0, x=0)$. The second term is easy as for all cases in which $r=0$, $y=0$; and so  $P(y=1|| \hat{r}= 0)=0$. We focus then on  $P(y=1| \hat{r}=1, s= 0, x= 0)$. Taking the marginal distribution, this can be written $\sum_{c=0}^1P(y=1|r=1, c)P(c|s=0,x=0)$. From the structural equations, we know that $P(c=1|s=0,x=0)=1$ and that $P(y=1|r=1, c=1)=1$. So the marginal distribution is $P(y=1| \hat{r}=1, s= 0, x= 0) = 1$; and the treatment effect of $R$ on $Y$, conditional on the characteristics of this case, is then 1. This positive effect is represented with the arrow from the $R=0$ node to the $Y=0$ node in panel $A$.] We can readily see that $R$ has an effect---a positive effect---on $Y$ in all configurations of exogenous node values (i.e., in all subgraphs) except when $X=1$ and $S=1$ (panel $D$); the absence of an arrow in panel $D$ indicates that $R$'s effect on $Y$ is 0 in that context. Thus, given our model, asking whether there is a case-level causal effect of $X$ on $Y$ is equivalent to asking whether either $S$ or $X$ or both are equal to $0$ in the case. -->

<!-- Another way to put the point is that $S$ and $X$ jointly determine a case's nodal type when it comes to the effect of $R$ on $Y$. Returning to our four nodal types, the graphs tell us that a case is a $b$ type (positive effect) with respect to the $R \rightarrow Y$ relationship whenever at least one of $S$ or $X$ is $0$. If $S=X=1$, then a case is a $c$ type (no effect, with the outcome fixed at $Y=0$).  -->

<!-- We can work through other relationships in the model similarly. For instance, does a free press have an effect on government removal in a case? See an $X$-to-$Y$ arrow only in panels $A$ and $C$, we can thus conclude that $X$ has a causal effect on $Y$ in (and only in) cases in which $S=0$.  -->

<!-- For now, we are simply using the models to *define* a query about a case-level causal effect. This definition sets the stage for our discussion of research design---how one might go about empirically addressing this query---later in the book. We can point the way toward that discussion by noting making two broad points. If the presence of a free press and government's sensitivity to public opinion are observable, then estimating case-level causal effects will simply be a matter of measuring these two exogenous nodes (or, for some queries, just one of them). However, we will often be in a situation in which the nodes defining our causal query are not observable. Our models of the world often include concepts that are theoretically central to a causal logic but cannot be directly measured. Consider, for instance, government sensitivity to public opinion. When a model's exogenous variables are unobservable, then our research design may require using information from other, *observable* nodes to draw inferences about context. This is a key strategy of process tracing that we develop in later chapters. -->


<!-- <!-- We need to resolve the contradiction between the above footnote and the previous one: one says they're not submodels, the other says they are. --> 




<!-- <!-- In this causal beam with binary variables,  whenever a unique path connects one node to another then the ancestor's node's condition is a cause of the descendant's condition. These case level causal relations cannot be read directly from the graph however if there are multiple paths or non dichotomous variables. To see why multiple paths prevent this inference, return to the boulder example of non transitivity described above; to see why inferences cannot be made along paths with non binary outcomes notice that $A$ and $B$ may be connected because some change in $A$ produces a change in $B$, though not necessarily *all* changes in $A$.    --> 

<!-- **Average causal effects.** Average causal effects are simply averages of case-level causal effects for the population. Since case-level causal effects are determined by the values of the exogenous nodes in cases, we need to average over the distribution of case-level contexts in the population. Put differently, the average causal effect of any variable on another will depends on how commonly the relevant case-level conditions---those in which the causal effect is and is not present---occur. In our current example, we have seen that the free press makes a difference to government survival if and only if the government is *non-sensitive* (panels $A$ and $C$): the non-sensitive government gets exposed as corrupt if and only if there is a free press while the sensitive government never gets replaced because it adjusts to the presence of a free press by eliminating corruption. Similarly, the sensitivity of the government (and the resulting level of corruption) matters only if there *is* a free press (panels $C$ and $D$). Without a free press, non-sensitive and, thus, corrupt governments do not get exposed and so stay on; with a free press, non-sensitive (and, thus, corrupt) governments get replaced.  -->

<!-- Thus, the *average* effect of each of these initial causes on the outcome will depend on the probability with which the other cause is absent or present. To define a query about average causal effects, we need to examine the full probabilistic causal model as graphed in the large panel in Figure \@ref(fig:running2). What is the average causal effect of a free press ($X$) on government removal ($Y$)? As we have learned from the subgraphs, this effect is fully defined by the value of $S$. In particular, the effect of $X$ on $Y$ is equal to $1$ when $S=0$, and is equal to $0$ when $S=1$. As we've noted, we calculate the average causal effect by averaging causal effects over the distribution of the relevant exogenous variables -- which, here, is only $S$. In the probabilistic model, $S$ is a function of $\lambda^S$ and $U_S$. In particular, $S=1$ whenever $u_S < \lambda^S$. Since $U_S$ has a uniform distribution, this simply means that $S=1$ with probability $\lambda_1^S$; likewise, $S=0$ with probability $1-\lambda_1^S$.  Thus, we calculate $X$'s average causal effect on $Y$ by multiplying each causal effect by the probability of $S$'s taking on the value that generates that effect: $1 \times (1-\lambda_1^S) + 0 \times \lambda_1^S = 1-\lambda_1^S$. Put differently, the causal effect of a free press on government removal is equal to the commonness of insensitive governments in the relevant population of cases.  -->

<!-- We have thus defined our causal query in terms of an exogenous variable, $\lambda_1^S$, in the probabilistic causal model. Note that, just as $S$ acts as a causal-type variable for $X$'s effect on $Y$, querying $\lambda_1^S$ is equivalent to asking about the distribution of nodal types in the population. In our four-type framework, cases with $S=0$ are $b$ (positive effect) types with respect to the $X \rightarrow Y$ relationship; cases with $S=1$ are $c$ types (no effect, $Y=0$). (There are, here, no $a$ or $d$ types.) Thus, $\lambda_1^S$ represents the share of $c$ types and $1-\lambda_1^S$ the share of $b$ types in the population, vis-a-vis $X$'s effect on $Y$.  -->

<!-- We can follow the same procedure for all causal relationships in the model. Returning, for instance, to the effect of $R$ on $Y$, we learned from the subgraphs that $R$ has a causal effect of $1$ in panels $A,B$ and $C$---that is, whenever it is not the case that $X=1$ and $S=1$---and otherwise of $0$. Thus, the $R$'s average causal effect is the weighted average $1 \times (1-\lambda_1^S \times \lambda_1^X) + 0 \times \lambda_1^S \times \lambda_1^X$ = $1-\lambda_1^S \times \lambda_1^X$. This is simply the probability of not having both $X=1$ and $S=1$. Here, then, we have defined the causal query in terms of two exogenous nodes in the probabilistic model, $\lambda_1^S$ and $\lambda_1^X$. ^[Likewise, the average causal effect of $R$ conditional on $S=1$ is $1-\lambda_1^X$ (the probability of ending up in panel B, rather than D); and the average causal effect of $R$ given $S=0$ is 1 (since it has an effect in both panels A and C).] -->

<!-- **TO BECOME A LONG FOOTNOTE...** -->
<!-- These quantities can be calculated from the distributions in the same way as we calculated the case-level effects. Removing the arrows pointing to $R$, the distribution over nodes when $R=r'$---but this time not fixing $S$ and $X$---is $P(s,x,c,y | \hat{r}=r')$. Again the key part is $P(y=1| \hat{r}=1)$, which can be written $\sum_x\sum_s\sum_c P(x)P(s)P(c|x,s)P(y|c, r= 1)$. Using the structural equations, this simplifies to $\sum_x\sum_s P(x)P(s)P(c=1|x,s) = P(x=0)P(s=0) + P(x=0)P(s=1) + P(x=1)P(s=0)$, or, $1-\lambda_1^S\lambda_1^X$. -->

<!-- In the same way, we can construct the average treatment effect for each of the exogenous variables: -->

<!-- * $\tau_X = E_S(Y(X=1|S)-Y(X=0|S)) = -(1-\lambda_1^S)$ -->
<!-- * $\tau_S = E_X(Y(S=1|X)-Y(S=0|X)) = \lambda_1^X$] -->
<!-- **LONG FOOTNOTE ENDING HERE** -->

<!-- In general, then, we can define queries about average causal effects as queries about the exogenous nodes that represent a causal model's probabilistic components. In the present example, probabilistic components enter only as determinants of the initial substantive causal variables. In other models, variables further downstream might also have stochastic components, a query about average causal effects might include thus further exogenous terms representing population-level distributions. Estimating average causal effects thus amounts drawing inferences about these nodes.^[Given the model, data will be useful for estimating average effects only if one is uncertain about the distributions of $S$ and $X$, which are a function of $U_S$ and $\lambda_1^S$ and $U_X$ and $\lambda_1^X$, respectively. In this example $\lambda_1^S$ and $\lambda_1^X$ are fixed in the model and so we do not learn anything about them from data. If however $\lambda_1^S$ and $\lambda_1^X$ are represented as nodes that are themselves produced by some other distribution -- such as a Beta distribution --- then the question of understanding average effects is the question of making inferences about these nodes.] -->

<!-- <!-- I find the previous paragraph quite confusing in terms of what all of this says about learning about nodes. I would think we would want to set up the example so that we *can* use data to learn about the ATE and so that this runs through learning about root nodes.  --> 

<!-- **Actual cause.** Returning to a case-level query, the concept of an actual cause becomes useful when outcomes are overdetermined. Suppose there is a case with a sensitive government ($S=1$) and no free press ($X=0$), as in panel B. Then the *survival* of the government is over-determined: neither government sensitivity nor the free press is a counterfactual cause. (A lack of a free press is enough for even a corrupt government to survive; and sensitivity ensures non-corruption and, thus, survival even in the presence of a free press.)  -->

<!-- Nevertheless, we can distinguish between the causes in terms of which one was an actual cause. Conditioning on there being corruption, which there actually was, the lack of a free press *is* a counterfactual cause of government survival: if there had been a free press, holding corruption constant, then the government would have been removed. This makes the lack of a free press an actual cause---that is, a counterfactual cause when some (or no) feature of what actually happened is kept fixed. However, there is no set of realized variable values that we can condition on to make the presence of a sensitive government a counterfactual cause; thus, it is not an actual cause. -->

<!-- The context---the values of the exogenous nodes in the subgraphs, $S$ and $X$---determines which variables will be actual causes through setting the realized values of all endogenous variables in the model and thus restricting the values on which conditioning is permitted for the determination of actual causes. Since corruption *is* present whenever $S=1$ and $X=0$, we are permitted in this context to condition on its presence, and the free press is an actual cause of government retention. In contrast, the sensitivity of the government is not an actual cause in this same context. Given no free press, there will always be corruption but no reporting on corruption, which makes government removal impossible, regardless of government sensitivity; thus, there is no subset of actual events that, when kept fixed, would make a change to a non-sensitive government result in the government's fall. In sum, asking whether a variable in a model was the actual cause of an outcome can equivalently be understood as asking about the values of the model's exogenous nodes. Answering that question will consist either of directly observing those exogenous conditions or drawing inferences about them from other, observable nodes. -->

<!-- <!-- This example has the odd feature that the question of whether S or X is an actual cause is itself already conditional on the values of S and X.   --> 

<!-- **Notable cause.** In the event that that there is a non-sensitive government ($S=0$) and a free press ($X=1$), as in panel $C$, the government gets replaced and *both* of the two causes matter counterfactually for government replacement. (Absent either one, the government would survive.) Again, however, we can distinguish between them, this time on the basis of notable causation. The question, for identifying a notable cause, is how commonly the causal variable in question takes on its realized, as opposed to a counterfactual, value. Thus, like average causal effects, notable causation depends on *population-level* distributions---in the present example, on the parameters $\lambda_1^S$ and $\lambda_1^X$. If, for instance, governments are more frequently sensitive (the counterfactual) than non-sensitive (the actual value)---i.e., $\lambda_1^S > 0.5$---then the non-sensitive government is a notable cause. However, if free presses are rarer than non-sensitive governments---i.e.  $\lambda_1^X < 1-\lambda_1^S$---then the free press is a *more* notable cause than the non-sensitive government. -->

<!-- <!-- Again, would be more consistent with out queries-as-root-nodes to show the pi's as nodes. --> 

<!-- **Causal Paths.** Note finally that different causal paths can give rise to the same outcome, where the different paths can be distinguished based on values of root nodes $S$ and $X$. For example, the government may be retained ($Y=0$) because there is no free press ($X=0$) and so no negative reporting on the government, regardless of the value of $S$; or because, there is a free press ($X=1$) and a sensitive government ($S=1$) takes account of this and does not engage in corruption. **\color{red} To be improved to link more closely to our abstract discussion of paths as estimands.** -->



<!-- What still bugs me a bit is that we don't have a way of showing, in this example, different causal paths for the same *causes* and outcomes. -->

<!--chapter:end:04-causal-questions.Rmd-->

# Bayesian Answers {#bayeschapter}

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We run through the logic of Bayesian updating and show how it is used for answering causal queries. We illustrate with applications to correlational and process tracing inferences.
::::
<br>

```{r packagesused05, include = FALSE}
source("_packages_used.R")
```


```{r bayes, include = FALSE}

pv = function(k0,k1) k1 - k0

gt.slope.text = function(text, f, xl=0, xh=1, vshift=.05, col="black", fixer=1){
	TX = c(" ", strsplit(text, "")[[1]]," ")
	k = length(TX)
	z = xl+ ((0:(k-1))/k)*(xh-xl)
	angles = 	c(0,(atan2(f(z)[3:k]  - f(z)[1:(k-2)],fixer*2*(xh-xl)/(k))*180/pi),0)
	for(i in 1:k){text(z[i], f(z[i])+vshift, TX[i], col = col, srt=angles[i])}
	}
		
k0 = .1
k1 = .9

# posterior = Prob(data|A)Prior(A)/Prob(Data)
posterior = function(prior, observed, k0, k1) observed*k1*prior/(k1*prior+k0*(1-prior)) +  (1-observed)*(1-k1)*prior/(1-(k1*prior+k0*(1-prior)))
plotit = function(k0, k1, main="", xl=.25, xh=.75){
	prior = seq(0,1,.02)
	plot(prior, posterior(prior, 1, k0, k1), type="l",      
       main = bquote( atop(.(main), 
                           phi[0]~'='~ .(k0)~','~ phi[1]~'='~ .(k1))), 
       
             ylab="Posterior")
	lines(prior, posterior(prior, 0, k0, k1), type="l")
	abline(a=0, b=1)
	gt.slope.text("posterior if clue present", f= function(x) posterior(x, 1, k0, k1), xl = xl, xh=xh) 
	gt.slope.text("posterior if clue absent", f= function(x) posterior(x, 0, k0, k1), xl = xl, xh=xh) 
	gt.slope.text("prior", f= function(x) x, xl = xl, xh=xh) 
	}

```		


Bayesian methods are just sets of procedures to figure out how to update beliefs in light of new information. 

We begin with a prior belief about the probability that a hypothesis is true. New data then allow us to form a posterior belief about the probability of the hypothesis. Bayesian inference takes into account the consistency of the evidence with a hypothesis, the uniqueness of the evidence to that hypothesis, and background knowledge about the problem. 

In the next section we review the basic idea of Bayesian updating. The following section applies it to the problem of updating on causal queries given a causal model and data.  

## Bayes Basics

For simple problems, Bayesian inference accords well with our intuitions. Once problems get slightly more complex however, our intuitions often fail us. 


### Simple instances

Say I draw a card from a deck. The chances it is a  Jack of Spades is just 1 in 52. If I tell you that the card is indeed a spade and asked you now what are the chances it is a Jack of Spades, you should guess 1 in 13. If I told you it was a heart you should guess there is no chance it is a  Jack of Spades. If I said it was a face card and a spade you should say  1 in 3. 

All those answers are applications of Bayes' rule.  In each case the answer is derived by assessing what is possible, given the new information, and then assessing how likely the outcome of interest among the states that are  possible. In all the cases you calculate:

$$\text{Probability Jack of Spades | Information} = \frac{\text{Is Jack of Spades Consistent with Information?}}{\text{How many cards are consistent with Information?}} $$

The same logic goes through when things are not quite so black and white.

Now consider two slightly trickier examples. 

**Interpreting Your Test Results**. Say that you take a test to see whether you suffer from a disease that affects 1 in 100 people. The test is good in the sense that if you have the disease it will say you have it with a 99% probability. If you do not have it, then with a 99% probability, it will say that you do not have it. The test result says that you have the disease. What are the chances you have it? You might think the answer is 99%, but that would be to mix up the probability of the result given the disease with the probability of the disease given the result. In fact the right answer is 50%, which you can think of as the share of people that have the disease among all those that test positive. For example if there were 10,000 people, then 100 would have the disease and 99 of these would test positive. But 9,900 would not have the disease and 99 of these would test positive. So the people with the disease that test positive are half of the total number testing positive.

As an equation this might be written:

$$\text{Probability You have the Disease | Test} = \frac{\text{How many have the disease and test positive?}}{\text{How many test positive?}} $$

**Two-Child Problem** Consider last an old puzzle found described @gardner1961second.  *Mr Smith has two children, $A$ and $B$. At least one of them is a boy. What are the chances they are both boys?* 
To be explicit about the puzzle, we will assume that the information that one child is a boy is given as a truthful  answer to the question "is at least one of the children a boy?" Assuming that there is a 50% probability that a given child is a boy, people often assume the answer is 50%. But surprisingly the answer is 1 in 3. The information provided rules out the possibility that both children are girls and so the right answer is found by readjusting the probability that two children are boys based on this information. As an equation:


$$\text{Probability both are boys | Not both girls} = \frac{\text{Probability  both boys}}{\text{Probability they are not both girls}} = \frac{\text{1 in 4}}{\text{3 in 4}}$$


### Bayes' Rule for Discrete Hypotheses

Formally, all of these equations are applications of Bayes' rule which is a simple and powerful formula for deriving updated beliefs from new data. 

The formula is given as:
\begin{eqnarray}
\Pr(H|\mathcal{D})&=&\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\Pr(\mathcal{D})}\\
                  &=&\frac{\Pr(\mathcal{D}|H)\Pr(H)}{\sum_{H'}\Pr(\mathcal{D}|H')\Pr(H'))}
\end{eqnarray}


where $H$ represents a hypothesis and $\mathcal{D}$ represents a particular realization of new data (e.g., a particular piece of evidence that we might observe). 

Looking at the formula we see that the  posterior belief derives from three considerations. First, the likelihood: how likely are we to have observed these data if the hypothesis were true, $\Pr(\mathcal{D}|H$)? Second, how likely were we to have observed these data regardless of whether the hypothesis is true or false, $\Pr(\mathcal{D})$? These first two questions, then, capture how consistent the data are with our hypothesis and how specific the data are to our hypothesis. As shown in the equation above the second question can usefully be reposed as one about all the different ways (alternative Hypotheses, $H'$) that could give rise to the data. 

Note, that contrary to some claims, the denominator does not require a listing of all possible hypotheses, just an exhaustive collection of hypotheses. For example we might have the notion of the probability that the accused's fingerprints would be on the door if she were or were guilty without having to decompose the "not guilty" into a set of hypotheses regarding who else might be guilty.

Our posterior belief is further conditioned by the strength of our prior level of confidence in the hypothesis, $\Pr(H)$. The greater the prior likelihood that our hypothesis is true, the greater the chance that new data consistent with the hypothesis has *in fact* been generated by a state of the world implied by the hypothesis.

### The Dirichlet family and Bayes' Rule for Continuous Parameters

This basic formula extends in a simple way to collections of continuous variables. For example, say we are interested in the value of some parameter vector $\theta$ (as a vector, $\theta$ can contain many quantities we are uncertain about), we can calculate this, given a prior probability distribution over possible values of $\theta$, $p$, and given data $D$ as:

$$p(\theta|\mathcal{D})=\frac{p(\mathcal{D}|\theta)p(\theta)}{\int_{\theta'}p(\mathcal{D|\theta'})p(\theta')d\theta}$$


Bayes rule requires the ability to express a prior distribution but it does not require that the prior have any particular properties other than being probability distributions. 

In practice however when we are dealing with continuous parameters, it  can be useful to make use of "off the shelf" distributions. 

In the context of the framework developed in this book, we will often be interested in forming beliefs about the share of units that are of a particular type. For this type of question we will make quite heavy use of "Dirichlet" distributions: a family of distributions that capture beliefs about shares. 

Consider, for example, the share of people in a population that voted; this is a quantity between 0 and 1. Two people might both believe that the turnout was around 50\% but may differ in how certain they are about this claim. One might claim to have no information and to believe that any turnout rate between 0 and 100% is equally likely, giving an expected turnout of 50%; another might be completely confident that the number is 50% and entertain no other possibilities.

We can capture such beliefs quite well by using the Beta distribution---a special case of the Dirichlet. The Beta is a distribution over the $[0,1]$ that is governed by two parameters , $\alpha$ and $\beta$. In the case in which both $\alpha$ and $\beta$ are 1, the distribution is uniform -- all values are seen as equally likely. As $\alpha$ rises large outcomes are seen as more likely and as $\beta$ rises, lower outcomes are seen as more likely. If both rise proportionately the expected outcome does not change but the distribution becomes tighter. 

An attractive feature of the Beta distribution is that if one has a prior Beta($\alpha$, $\beta$) over the probability of some event (e.g.  that a coin comes up heads), and then one observes a positive case, the Bayesian posterior distribution is also a Beta with with parameters $\alpha+1, \beta$. Thus in a sense if people start with uniform priors and build up knowledge on seeing outcomes, their posterior beliefs should be Beta distributions.

Figure \@ref(fig:Betas) shows a set of such distributions, starting with one that has greater variance than uniform (this corresponds to the non informative "Jeffrey's prior"), then uniform, then for a case in which multiple negative and positive outcomes are seen, in equal number, and finally a set of priors with mean of 3/4. 

```{r Betas, echo = FALSE, fig.cap="Beta distributions"}
par(mfrow = c(2,2))

x <- seq(0,1,.01)
plot(x, dbeta(x, .5, .5), ylab = "density", type = "l", main = expression(paste("Beta distribution: ", alpha, ", ", beta, " = 0.5")))

plot(x, dbeta(x, 1, 1), ylab = "density", type = "l", main = expression(paste("Beta distribution: ", alpha, ", ", beta, " = 1")))

plot(x, dbeta(x, 20, 20), ylab = "density", type = "l", main = expression(paste("Beta distribution: ", alpha, " =20, ", beta, " = 20")))

plot(x, dbeta(x, 30, 10), ylab = "density", type = "l", main = expression(paste("Beta distribution: ", alpha, "=30, ", beta, " = 10")))

```

Dirichlet distributions generalize the Beta to the situation in which there are beliefs not just over a proportion, or a probability, but over collections of probabilities. For example, if four outcomes are possible and each is likely to occur with probability $\theta_k$, $k=1,2,3,4$, then beliefs about these probabilities are distributions over a three-dimensional unit simplex---that is, all 4 element vectors of probabilities that add up to 1. The distribution has as many parameters as there are outcomes and these are traditionally recorded in a vector, $\alpha$. Similar to the Beta distribution, an uninformative prior (Jeffrey's prior) has $\alpha$ parameters of  $(.5,.5,.5, \dots)$ and a uniform ("flat") distribution has $\alpha = (1,1,1,,\dots)$.

As with the Beta distribution, the Dirichlet updates in a simple way. If you have a Dirichlet prior with parameter $\alpha = (\alpha_1, \alpha_2, \dots)$ and you observe outcome $1$, for example, then then posterior distribution is also Dirichlet with parameter vector $\alpha' = (\alpha_1+1, \alpha_2,\dots)$. 

### Moments

In what follows we often refer to the "posterior mean" or the "posterior variance." These are simply summary statistics of the posterior distribution, or moments, and can be calculated easily once the posterior is known. For example the posterior mean of a parameter $\theta_1$---just one in a collection of parameters stored in $\theta$---is simply $\overline{\theta}_1 | \mathcal{D} = \int \theta_1 p(\theta | \mathcal{D}) d\theta$. Note importantly that this is calculated using the posterior over the entire vector $\theta$; there is no notion of updating parameter $\theta_1$ on its own. Similarly the posterior variance is $\int (\theta_1 - (\overline{\theta}_1 | \mathcal{D})^2  p(\theta | \mathcal{D})) d\theta$. 

### Bayes estimation in practice

Although the principle of Bayesian inference is quite simple, in practice calculating posteriors for continuous parameters is computationally complex. 

In principle, with continuous parameters there is an infinity of possible parameter values. Analytic solutions are not, in general, easy to come by and so researchers use some form of sampling.

Imagine for instance you were interested in forming a posterior on the share of U.S. voters intending to vote Democrat, given polling data. (This is not truly continuous, but with large elections it might as well be.)s   

One approach is to coarsen the parameter space: we calculate the probability of observing the polling data given possible values $\theta = 0, \theta = .1, \theta = .2, \dots, \theta = 1$, and, apply Bayes rule to form a posterior for each of these  these possibilities.  The downside of the this approach is that, for a decent level of precision, it becomes computationally expensive with large parameter spaces, and parameter spaces get large quickly. For instance, if we are interested in vote shares, we might find .4, .5, and .6 too coarse and want posteriors for 0.51 or even 0.505; the latter would require calculations for 200 parameter values. If we had *two* parameters that we wanted to slice up each into 200 possible values, we would then have 40,000 parameter pairs to worry about. What's more, *most* of those calculations would not be very informative if the real uncertainty all lies in some small (though possibly unknown) range -- such as between 40% and 60%.  

An alternative approach is to use variants of Markov Chain Monte Carlo sampling. Under these approaches, parameter vectors are sampled and their likelihood is evaluated. If a sampled parameter vector is found to have high likelihood, then new parameter vectors near it are drawn with a high probability. Based on the likelihood associated with these new draws, additional draws are made. The result is a chain of draws that build up to approximate the posterior distribution. The output from these procedures is not a set of probabilities for each possible parameter vector but rather a a set of draws of parameter vectors from the  posterior distribution. 

Many algorithms have been developed to achieve these tasks efficiently; in all of our applications we rely on the `stan` procedures which use MCMC methods, specifically the Hamiltonian Monte Carlo (HMC) algorithm and the no-U-turn sampler (NUTS). Details on these approaches are given in the [Stan Reference Manual](https://mc-stan.org/docs/2_18/reference-manual/hmc-chapter.html) [@stan2020stan].  


## Bayes applied

### Simple Bayesian Process Tracing

Process tracing in its most basic form seeks to use within-case evidence to draw inferences about a case. For example, with a focus on whether $X$ caused $Y$, data on a within-case "clue," $K$, is used to make inference about whether or not the outcome in that case was generated by the case's treatment status on $X$. We refer to the within-case evidence gathered during process tracing as *clues* in order to underline their probabilistic relationship to the causal relationship of interest. Readers familiar with the framework in @collier2004sources   can usefully think of our "clues" as akin to causal process observations, although we highlight that there is no requirement that the clues be generated by the causal process. 

To make inferences, the analyst looks for clues that will be observed with some probability if the case is of a given type and that will *not* be observed with some probability if the case is *not* of that type.

It is relatively straightforward to express the logic of process tracing in Bayesian terms, a step that will aid the integration of qualitative with quantitative causal inferences. As noted by others (e.g. @BennettBayes, @beachpedersen2013process,  @rohlfing2012case), there is an evident connection between the use of evidence in process tracing and Bayesian inference. See @fairfield2017explicit for a detailed treatment of Bayesian approach in qualitative research.

To illustrate, suppose we are interested in regime collapse. We already have $X,Y$ data on one authoritarian regime: we know that it suffered economic crisis ($X=1$) and collapsed ($Y=1$). We want to know what caused the collapse. To make progress we will try to draw inferences given a "clue." Beliefs about the probabilities of observing clues for cases with different causal effects  derive from theories of, or evidence about, the causal process connecting $X$ and $Y$. Suppose we theorize that the mechanism through which economic crisis generates collapse runs via diminished regime capacity to reward its supporters during an economic downturn. A possible clue to the operation of a causal effect, then, might be the observation of diminishing rents flowing to regime supporters shortly after the crisis. If we believe the theory, then this is a clue that we might believe to be highly probable for cases of type $b$ that have experienced economic crisis (where the crisis in fact caused the collapse) but of low probability for cases of type $d$ that have experienced crisis (where the collapse occurred for other reasons). 

To make use of Bayes rule we need to:

1. define our parameters, which are the key quantities of interest
2. provide prior beliefs about the parameters of interest
3. define a likelihood function
4. provide the probability of the data
5. plug these into Bayes' rule to calculate a posterior on the parameters of interest

We discuss each of these in turn, using our $a, b, c, d$ type notation for simplicity.

**Parameters.** The inferential challenge is to determine whether the regime collapsed *because* of the crisis (it is $b$ type) or whether it would have collapsed even without it ($d$ type). We do so using further information from the case---one or more clues. We use the variable $K$ to register the outcome of the search for a clue, with $K$=1 indicating that a specific clue is searched for and found, and $K$=0 indicating that the clue is searched for and not found.

Let $j\in \{a,b,c,d\}$ refer to the type of an individual case. Our hypothesis, in this initial setup, consists simply of a belief about $j$ for the case under examination: specifically whether the case is a $b$ type ($j=b)$. The parameter of interest is the causal type.

**Prior.** We then assign a prior degree of confidence to the hypothesis ($p = Pr(H)$). This is, here, our prior belief that an authoritarian regime that has experienced economic crisis is a $b$.

**Likelihood.** The likelihood, $\Pr(K=1|H)$ is the probability of observing the clue, when we look for it in our case, if the hypothesis is true---i.e., here, if the case is a $b$ type. The key feature of a clue is that the probability of observing the clue is believed to depend on the case's causal type. In order to calculate the probability of the data we will in fact need two such probabilities: we let $\phi_b$ denote the probability of observing the clue for a case of $b$ type ($\Pr(K=1|j=b)$), and $\phi_d$ the probability of observing the clue for a case of $d$ type ($\Pr(K=1|j=d)$). The key idea in many accounts of process tracing is that the *differences* between these probabilities provides clues with ''probative value,'' that is, the ability to generate learning about causal types. The likelihood, $\Pr(K=1|H)$, is simply $\phi_b$.

**Probability of the data.** This is the probability of observing the clue when we look for it in a case, *regardless* of its type, $(\Pr(K=1))$. More specifically, it is the probability of the clue in a treated case with a positive outcome. As such a case can only be a $b$ or a $d$ type, this probability can be calculated simply from $\phi_b$ and $\phi_d$, together with our beliefs about how likely an $X=1, Y=1$ case is to be a $b$ or a $d$ type. 
This probability aligns (inversely) with Van Evera's concept of ''uniqueness.''

**Inference.**  We can now apply Bayes' rule to describe the learning that results from process tracing. If we observe the clue when we look for it in the case, then our *posterior* belief in the hypothesis that the case is of type *b* is:


\begin{eqnarray*}
\Pr(j = b |K=1, X=Y=1)=  \frac{\phi_b p }{\phi_b p+\phi_d (1-p)}
\end{eqnarray*}


In this exposition we did not make use of a causal model in a meaningful way---we simply need the priors and the clue probabilities. 

In fact, however, these numbers can be derived from a causal model. To illustrate, imagine a simple causal model in which the $X, Y$ relationship is completely mediated by $K$. In particular, suppose, from background knowledge of the conditional distribution of outcomes given their causes, we have that:

* $\Pr(K=1 | X=0) = 0$, $\Pr(K=1 | X=1) = .5$
* $\Pr(Y=1 | K=0) = .5$, $\Pr(Y=1 | K=1) = 1$

This data is consistent with a world in which half $b$ and $c$ types in the  first step and half $b$ and $d$ types in the second step. Assume that the case at hand is sampled from this world.

Then we can calculate that the prior probability, $p$, that $X$ caused $Y$ given $X=Y=1$ is $p = \frac13$.^[Given $X=1$, $Y=1$ is consistent with $b$ types at both stages, which arises with probability .25,  or with a $d$ type in the second stage, which arises with probability .5. The conditional probability is therefore $.25/.75 = 1/3$.] We can also calculate the probability that $K=1$ for a treated $b$ and $d$ case respectively as $\phi_b=1$ and  $\phi_d=0.5$ (convince yourself of these numbers!). We then get:

\begin{eqnarray*}
\Pr(j = b |K=1, X=Y=1)&=&\frac{1\times \frac13}{1 \times \frac13 + 0.5 \times \frac23}=0.5
\end{eqnarray*}

We thus shift our beliefs from a prior of $\frac13$ to a posterior of $\frac12$. In contrast had we *not* observed the clue our posterior would have been 0. 

As should be clear from the above, the inferential leverage in process tracing comes from differences in the probability of observing $K=1$ for different causal types. Thus, the logic described here generalizes Van Evera's familiar typology of tests by conceiving of the certainty and uniqueness of clues as lying along a continuum. 

Van Evera's four tests ("smoking gun," "hoop," "straw in the wind," and "doubly decisive") represent, in this sense, special cases---particular regions that lie on the boundaries of a "probative-value space."  To illustrate the idea, we represent the range of combinations of possible probabilities for $\phi_b$ and $\phi_d$ as a square in Figure \ref(CluesInferences1) and mark the spaces inhabited by Van Evera's tests. As can be seen, the type of test involved depends on both the relative *and* absolute magnitudes of $\phi_b$ and $\phi_d$. The probative value of a test depends on the difference between them. Thus, a clue acts as a smoking gun for proposition "$b$" (the proposition that the case is a $b$ type)  if it is highly unlikely to be observed if proposition $b$ is false, and more likely to be observed if the proposition is true (bottom left, above diagonal). A clue acts as a "hoop" test if it is highly likely to be found if $b$ is true, even if it still quite likely to be found if it is false. Doubly decisive tests arise when a clue is very likely if $b$ and very unlikely if not. It is, however, also easy to imagine clues with probative qualities lying in the large space amidst these extremes.^[We thank Tasha Fairfield for discussions around this graph which differs from that in @humphreys2015mixing by placing tests more consistently on common rays originating from (0,0) and (1,1).] 



```{r cluesinferences, fig.cap="\\label{CluesInferences1} A mapping from the probability of observing a clue if the proposition that a case is a $b$ type is true ($\\phi_b$) or false ($\\phi_d$) to a generalization of the tests described in Van-Evera (1997).", echo = FALSE, fig.height=12, fig.width=12}


cex = 1
plot(c(0,1), c(0,1), type="l", col="grey", xlab=expression(paste(phi[d], " (Probability of observing ", italic(K), " given d)")),
     ylab=expression(paste(phi[b], " (Probability of observing  ", italic(K), " given b)")),
     main="Classification of tests")

polygon(c(0,0,.5, 1/3, 0), c(.5, 1, 1, 2/3, .5), col = "grey", , border = FALSE)
polygon(c(0, 1/3, 1,  0), c(0,2/3,1,0), col = "lightgrey", border = FALSE)
polygon(c(0,2/3,1, 0), c(0, 1/3, 1, 0), col = "grey", border = FALSE)
polygon(c(.5, 2/3, 1,  1, .5), c(0,1/3,.5,0,0), col = "lightgrey", border = FALSE)

text(.15,.85, "K present: \n doubly decisive for b  \n K absent: \n doubly decisive for d  ", cex = cex)
text(.05,.2833, "K present: \n smoking gun for b \n K absent \n hoop test for d", cex = cex)
text(.7177,.95, "K present: \n hoop test for b \n K absent: \n smoking gun for d", cex = cex)
text(.43,.57, "K present: \n straw in the wind for b \n K absent: \n straw in the wind for d", cex = cex)

text(.85,.15, "K present: \n doubly decisive for d  \n K absent: \n doubly decisive for b", cex = cex)
text(.95,.7177, "K present: \n hoop test for d \n K absent: \n smoking gun for b", cex = cex)
text(.2833,.05, "K present: \n smoking gun test for d \n K absent: \n hoop test for b", cex = cex)
text(.57,.43, "K present: \n straw in the wind for d \n K absent: \n straw in the wind for b ", cex = cex)



arrows(.25,.7, .25, .85, col="red")
arrows(.25,.7, .1, .7, col="red")

text(.35, .775, "More sensitive \n for b", col="red", cex = cex)
text(.1775, .65, "More specific \n for b", col="red", cex = cex)


arrows(.75,.3, .9, .3, col="red")
arrows(.75,.3, .75, .15, col="red")

text(.65, .225, "More specific \n for d", col="red", cex = cex)
text(.825, .35, "More sensitive \n for d", col="red", cex = cex)


```


In this illustration, we note that we draw both the priors and the probative value from a causal model.  If we altered the model---for example, if we had a stronger first stage and so  a larger value for $\Pr(K=1|X=0)$---this would alter both our prior, $p$, and our calculations of $\phi_d$.  An implication of this is that, although one might be tempted to think of the priors and the probative values as independent quantities, and contemplate how inferences change as priors change (as we did for example in the treatment in @humphreys2015mixing), keeping probative value fixed, that kind of thought experiment may assume values that are not justified by an underlying model.

### A Generalization: Bayesian Inference on Queries

<!-- ## FLAG -- MAKE CONSISTENT WITH QUERY SECTION -->
In Chapter \@ref(questions), we described queries of interest as queries over nodal types in causal models. 

Once queries are defined in terms of nodal types, the formation of beliefs, given data $W$, about queries follows immediately from application of Bayes rule. 

Let $Q(\theta)$ define the value of the query in context $\theta$. The updated beliefs about the query are given by the distribution:

$$P(q | W) = \int_{\theta:Q(\theta) = q} P(\theta|W)d\theta =  \int_{\theta:Q(\theta) = q} \frac{P(W|\theta)P(\theta)}{\int_{\theta'}P(W|\theta')P(\theta')d\theta'}d\theta$$

This expression gathers together all the causal types (combinations of nodal types) that satisfy a query and assesses how likely these are, collectively, given the data.^[Learning about roots from observed data is sometimes termed *abduction*; see @pearl2009causality, p 206.] For an abstract representation of the relations between  assumptions, queries, data, and conclusions, see Figure 1 in @pearl2012causal.  


Return now to Mr Smith's puzzle. We can think of the two "nodal types" here as the sexes of the two children, child $A$ and child $B$. The query here is $Q$: "Are both boys?" The statement "$Q=1$" is equivalent to the statement, $A$ is a boy \& $B$ is a boy. Thus it takes the value $q=1$ under just one causal type, when both nodes have been assigned to the value "boy." Statement $q=0$ is  the statement ("$A$ is a boy \& $B$ is a girl" or "$A$ is a girl \& $B$ is a boy" or "$A$ is a girl \& $B$ is a girl"). Thus $q=0$ in three contexts.  If we assume that each of the two children is equally likely to be a boy or a girl with independent probabilities, then  each of the four contexts is equally likely. 
 The  result can then be figured out as $P(Q=1) = \frac{1\times \frac{1}{4}}{1\times \frac{1}{4} + 1\times \frac{1}{4}+1\times \frac{1}{4}+0\times \frac{1}{4}} = \frac{1}{3}$. This answer requires summing  over only one causal type. $P(Q=0)$ is of course the complement of this, but using the Bayes formula one can see that it can be found by summing over the posterior probability of three causal types in which the statement $Q=0$ is true. 
 
We will often want to think about our causal queries being collections of states of the world --- i.e., of causal types. Returning to our discussion of queries in Chapter \@ref(questions), suppose we start with the model $X \rightarrow M \rightarrow Y$, and our query is whether $X$ has a positive effect on $Y$. This is a query that is satisfied by four sets of causal types: those in which $X$ has a positive effect on $M$ and $M$ has a positive effect on $Y$, with $X$ being either 0 or 1; and those in which $X$ has a negative effect on $M$ and $M$ has a negative effect on $Y$, with $X$ being either 0 or 1. Our inferences on the query will thus involve gathering these different causal types, and their associated posterior probabilities, together.

One way to think intuitively about Bayesian updating is to say that we update more strongly in favor of the hypothesis for which the evidence is *least* damaging to the *most-likely ways* in which the hypothesis could be true. Suppose our prior belief was that it was much more unlikely that $M$ had a negative effect on $Y$, than that $M$ had a positive effect on $Y$. This makes one of the ways in which $X$ could have a positive effect on $Y$ (the chain of negative effects) much less likely than the other way in which $X$ could have a positive effect on $Y$ (the chain of positive effects). This means that evidence against a chain of negative effects and evidence against a chain of positive effects will not be equally consequential for our query: in particular, we will update more strongly against the query if we find evidence against a chain of positive effects than if we find evidence against a chain of negative effects. Evidence against a chain of positive effects speaks against the *most* likely way in which the query could be true, whereas evidence against a chain of negative effects speaks against a way the query could be true that we did not think was very likely to begin with.



<!-- ### Bayesian correlational inference -->


<!-- In all the examples describe above Bayes rule was used to update inferences about a particular case given additional information about that case. But the same logic works just as well for problems in which one tries to update about a general relation given a set of cases.  -->
<!-- The correlational solution to the fundamental problem of causal inference is to focus on *population-level* effects. Rather than seeking to identify the types of particular cases, researchers exploit covariation across cases between the treatment and the outcome variables---i.e., dataset observations---in order to assess the {average effect} of treatment on outcomes for a {population} or {sample} of cases. In the simplest, frequentist approach, under conditions described by \citep{Rubin1974} the average effect of a treatment may be estimated as the difference between the average outcome for those cases that received treatment and the average outcome for those cases that did not receive treatment. -->

<!-- Although this frequentist approach to estimating causal effects from correlational data is more familiar, the problem can also be described in Bayesian terms.^[For a fuller treatment, see for example \citet{heckman2014treatment}.] -->

<!-- <!-- % I think the below is necessary because we do not properly define the Bayesian approach generally; we only do it for a very special illustration -->



<!-- ```{r simpleXYDAG, echo = FALSE, fig.width = 7, fig.height = 4,  fig.align="center", out.width='.5\\textwidth', fig.cap = "A graph depicting a situation in which it is possible that $X$ causes $Y$; the unit level causal type is $\\theta^Y$ and the distribution of causal types is $\\lambda^Y$."} -->
<!-- hj_dag(x = c(0, 0, 1, 1, 2, -1), -->
<!--        y = c(1, 0, 0, 1, 1, 1), -->
<!--        names = c( -->
<!--          expression(paste(theta^X)), -->
<!--          "X", -->
<!--          "Y", -->
<!--          expression(paste(theta^Y)),  -->
<!--          expression(paste(lambda^Y)),  -->
<!--          expression(paste(lambda^X))  -->
<!--          ), -->
<!--        arcs = cbind( c(1, 2, 4, 5, 6), -->
<!--                      c(2, 3, 3, 4, 1)), -->
<!--        title = "", -->
<!--        padding = .2, contraction = .15, box = FALSE)  -->

<!-- ``` -->


<!-- Suppose we are interested in determining the *distribution* of causal types in a population. We again need to specify our parameters, priors, likelihood, the probability of the data, and the inference strategy. -->

<!-- In turn we have: -->

<!-- **Parameters.** Our hypothesis consists of a set of $\lambda$ values: i.e., the proportion of the population of authoritarian regimes for which economic crisis would generate or has generated collapse ($\lambda_b$); the proportion for which collapse is inevitable ($\lambda_d$); and so on. -->

<!-- We can now define our hypothesis as a vector, $\lambda = (\lambda^X_0,\lambda^X_1,\lambda^Y_{00},\lambda^Y_{10},\lambda^Y_{01}, \lambda^Y_{11})$, that registers a possible set of values for the parameters over which we will update: type proportions in the population and assignment propensities by type.  -->

<!-- **Prior.** We next need to assign a prior probability to $\lambda$. In the general case, we will do so by defining a prior probability distribution, $p(\lambda^j)$, over possible values of the elements of $\lambda^j$. Here $\lambda^Y$ has four possible values and we use a Dirichlet distribution on a 3-simplex.  $\lambda^X$ has has only two possible values and in this case the  Dirichlet distribution reduces to a Beta distribution.   -->

<!-- **Likelihood.** Our data, $\mathcal{D}$, consist of $X$ and $Y$ observations for a sample of cases. With binary $X$ and $Y$, there are four possible data realizations (combinations of $X$ and $Y$ values) for a given case. For a single case, it is straightforward to calculate an event probability $w_{xy}$ | that is, the likelihood of observing the particular combination of $X$ and $Y$ given the type shares and assignment probabilities in $\theta$. For instance: -->

<!-- $$w_{00}=\Pr(X=0, Y=0|\theta)=\lambda^X_0(\lambda^Y_{00} + \lambda^Y_{01})$$ -->


<!-- More generally, let $w_{XY}$ denote the vector of these event probabilities for each combination of $X$ and $Y$ values, conditional on $\lambda$. Further,  let $n_{XY}$ denote vector containing the number of cases observed with each $X,Y$ combination. Under an assumption of independence (data are independently and identically distributed), the full likelihood is then given by the multinomial distribution: -->
<!-- $$\Pr(\mathcal{D}|\lambda)= \text{Multinomial}(n_{XY}  | w_{XY})$$ -->

<!-- Note again that here we have assumed that data is randomly drawn. More general functions can allow for more complex data gathering processes. -->

<!-- **Probability of the data.** We calculate the unconditional probability of the data, $Pr(\mathcal{D})$, by integrating the likelihood function above over all parameter values, weighted by their prior probabilities. -->


<!-- **Inference.** After observing our data, $\mathcal{D}$, we then form posterior beliefs over  $\lambda$ by direct application of Bayes' rule, above: -->

<!-- $$p(\lambda|\mathcal{D}) = \frac{\Pr(\mathcal{D}|\lambda)p(\lambda)}{\int\Pr(\mathcal{D}|\lambda')p(\lambda')d\lambda'}$$ -->


<!-- This posterior distribution reflects our updated beliefs about which sets of parameter values are most likely, given the data. Critically, note that, upon observing $X$ and $Y$ data, we  simultaneously update beliefs about all parameters in $\lambda$: both beliefs about causal effects (type shares) in the population *and* beliefs about the assignment propensities for $X$. -->


<!-- <!-- Intuitively, we treat each set of possible values of our parameters of interest---each $\lambda$ vector, that is---as a hypothesis and apply Bayes' rule to assess its probability given the data, that is, the posterior.^[More generally we might think of a hypothesis as being a subset of values of $\theta$ | e.g. "there is a positive treatment effect" corresponds to the set of values for which $b>a$.]  We use three quantities to calculate the posterior. -->

<!-- <!-- First, we ask, if this set of parameter values is true, how likely were the observed $X, Y$ values to have emerged? {This calculation in our binary framework is simple. For example, the probability of observing the event $X=1, Y=1$ for a single randomly selected case is given by event probability $w_{11}=b\pi_b+d\pi_d$. Note that we assume in this example that each *type* is drawn independently as would be the case if cases under study were randomly sampled from a large population.} Consider a hypothesis (a specific value of $\theta$) in which most authoritarian countries are assumed to be either susceptible to a regime-collapsing effect of economic crisis or destined to collapse anyway---i.e., a $\theta$ in which $\lambda_b$ and $\lambda_d$ are very high and $\lambda_a$ and $\lambda_c$ very low. Suppose we then observe data in which a large proportion of countries display values $X=1$ and $Y=0$---they experienced crisis and did not collapse---which pegs them as either $a$ or $c$ types. The probability of these data under the hypothesized $\theta$--- $\Pr(\mathcal{D}|\theta)$ | will then be low, reducing our confidence in this hypothesis. On the other hand, such data are far more likely under any $\theta$ vector in which $\lambda_a$ or $\lambda_c$ is high, boosting our confidence in such hypotheses. -->

<!-- <!-- Second, we ask, how likely were we to observe these data, $\mathcal{D}$, regardless of whether this particular $\theta$ is true? This value appears in the denominator, where we take into account the likelihood of observing these data for *all* of the possible values of $\theta$, weighted by their prior probabilities. More formally, under the assumption of independence, the probability of observing $\mathcal{D}$, that is, a particular collection of $X,Y$ data, is given by the corresponding value of the multinomial distribution given the event probabilities implied by $\theta$.   -->

<!-- <!-- The more likely the data are in general|whether the hypothesis is true or not|the smaller the effect of these data on our beliefs. On the other hand, if the observation of lots of crisis-suffering, collapsing regimes was generally *unlikely* across all $\theta$s, then observing these data will generate a larger shift in our confidence toward any particular $\theta$ vector with which the data are relatively consistent. -->

<!-- <!-- Third, we multiply the ratio of these first two quantities by our confidence in the values in this $\theta$ prior to seeing the data ($p(\theta)$). The more prior confidence we have in a hypothesis, the greater the probability that evidence consistent with and unique to the hypothesis in fact indicates that the hypothesis is true. Thus, for instance, suppose that prior evidence and logic suggest that a high proportion of authoritarian regimes in the world are susceptible to a regime-collapsing effect of crisis (are $b$ types). This strong prior belief in a high $\lambda_b$ increases the likelihood that any data pattern consistent with a high $\lambda_b$---say, many $X=1, Y=1$ cases---has *in fact* been generated by a large set of $b$ cases. -->

<!-- We illustrate Bayesian correlational inference with a simple case. Suppose we observe for all postwar authoritarian regimes, whether they did or did not suffer economic crisis and did or did not collapse. Say for simplicity we know that all authoritarian regimes were "assigned" to economic crisis with a 0.5 probability during the period under analysis (thus assignment is known to be *as if* random). And assume that, prior to observing $X$, $Y$ data we believe that each of two propositions are true with 0.5 probability. Under proposition **($\theta_1$)** all regimes are of type $b$ (and so the average treatment effect is 1); under proposition **($\theta_2$)** 50\% of regimes are of type $c$ and 50\% are of type $d$ (and so the average treatment effect is 0).^[In this simple case we can think of $\theta$ as being constrained to take on only one of two possible values: $\theta \in  \{\theta_1=\{a=0,b=1, c=0, d=0, \pi_a=0.5,\pi_b=0.5,\pi_c=0.5,\pi_d=0.5\},\{\theta_2=\{a=0,b=0, c=.5, d=.5, \pi_a=0.5,\pi_b=0.5,\pi_c=0.5,\pi_d=0.5\} \}$.]  -->


<!-- <!-- %Suppose that we now randomly draw a set of authoritarian regimes from the population and observe the values on $X$ and $Y$. How should our observation of this data | $\mathcal{D}$ | shift our beliefs about the value $\lambda_b$? --> 
<!-- <!-- % --> 
<!-- <!-- %A Bayesian analysis draws on our prior beliefs about three quantities: -->
<!-- <!-- % --> 
<!-- <!-- %\begin{itemize} --> 
<!-- <!-- % --> 
<!-- <!-- %\item **$\Pr(\mathcal{D**|b=1$)}: The probability of observing this collection of $X$ and $Y$ values under $H_1$, that is, if all cases are susceptible to a positive treatment effect. Each case should either have values $X=Y=0$ or $X=Y=1$ if indeed $b=1$, and the distribution across these values should follow a binomial distribution with $p=.5$ (since cases of all types were assigned to treatment with 0.5 probability). -->
<!-- <!-- % -->
<!-- <!-- %\item **$\Pr(b=1$)**: The likelihood that $H_1$ is correct. This belief represents our prior level of confidence in $H_1$, before we observe the new evidence. We have set this belief in the present illustration to 0.5. -->
<!-- <!-- % -->
<!-- <!-- %\item **$\Pr(\mathcal{D**)$}: The probability of observing this collection of $X$ and $Y$ values *without* conditioning on $H_1$. The expression is an average of the probabilities of observing the data under the two hypotheses, weighted by our prior belief for each hypothesis that it is correct. That is, $\Pr(\mathcal{D}) = \Pr(\mathcal{D}|b=1)\Pr(b=1)+\Pr(\mathcal{D}|b=0)\Pr(b=0)$  -->
<!-- <!-- %\end{itemize} --> 

<!-- <!-- %Thus, the observation of $X$ and $Y$ values in the sample allows us to update our beliefs %on the correlation between treatment and outcomes in the population and, hence,  -->
<!-- <!-- %on the average treatment effect. In this simple case  -->
<!-- <!-- %%the only data consis: do we observe data in which $X$ and $Y$ are perfectly correlated or not? In this case,  -->
<!-- <!-- %if we see a single case that has values $(X=0, Y=1)$, then we will know for certain that $H_1$ is false since this data structure could never arise under $H_1$. If we observe data in which $X$ and $Y$ are perfectly correlated, we may still think it possible that $H_2$ is true. However, such a pattern is {*less*} likely to emerge if $H_2$ is true than if $H_1$ is true.  -->
<!-- <!-- % -->
<!-- <!-- Suppose we draw a random sample of $n=2$ cases and observe one case in which $X=Y=0$ and one case in which $X=Y=1$. That is, we observe a perfect correlation between $X$ and $Y$ but only two cases. What then should we infer? -->

<!-- Applying Bayes' rule, our posterior probability on proposition $\theta_1$, having observed the data, is: -->

<!-- \begin{eqnarray*} -->
<!-- \Pr(\theta_1|\mathcal{D})  -->
<!-- =\frac{\Pr(\mathcal{D}|\theta_1)\Pr(\theta_1)}{\Pr(\mathcal{D}|\theta_1)\Pr(\theta_1)+\Pr(\mathcal{D}|\theta_2)\Pr(\theta_2)} -->
<!-- \end{eqnarray*} -->

<!-- or equivalently:  -->

<!-- \begin{eqnarray*} -->
<!-- \Pr(b=1|\mathcal{D})  -->
<!-- %=\frac{\Pr(\mathcal{D}|b=1) \Pr(b=1)}{\Pr(\mathcal{D})} -->
<!-- =\frac{\Pr(\mathcal{D}|\lambda_b=1)\Pr(\lambda_b=1)}{\Pr(\mathcal{D}|\lambda_b=1)\Pr(\lambda_b=1)+\Pr(\mathcal{D}|\lambda_b=0)\Pr(\lambda_b=0)} -->
<!-- \end{eqnarray*} -->

<!-- The event probabilities of each of the observed events is $0.5$ under $\theta_1$ but just $0.25$ under $\theta_2$.  Using the binomial distribution (a special case of the multinomial for this simple case) we know that the chances of such data arising are 1 in 2 under $\theta_1$ but only 1 in 8 under $\theta_2$. Our posterior would then be: -->

<!-- \begin{eqnarray*} -->
<!-- \Pr(\lambda_b=1|\mathcal{D}) =\frac{\frac{1}{2} \times \frac{1}{2}}{\frac{1}{2} \times \frac{1}{2} + \frac{1}{8}\times \frac{1}{2}} = \frac{4}{5}  -->
<!-- \end{eqnarray*} -->

<!-- The key difference between this example and more general applications is simply that in the general case we allow for uncertainty --- and updating --- not simply over whether $\lambda_b$ is 0 or 1, but over a range of possible values for multiple parameters of interest. Though this adds complexity, it does not change the fundamental logic of updating.   -->




<!-- FLAG: RETURN TO THESE -->

## Features of Bayesian updating

Bayesian updating has implications that may not be obvious at first glance. These will matter for all forms of inference we examine in this book, but they can all be illustrated in simple settings. 

### Priors matter {#AppPriors}

The amount of learning that results from a given piece of new data depends strongly on prior beliefs. We have already seen this with the example of interpreting our test results above.  Figure \@ref(fig:CluesInferences2) illustrates the point for process tracing inferences. 

In each subgraph of Figure \@ref(fig:CluesInferences2) , we show how much learning occurs under different scenarios. The horizontal axis indicates the level of prior confidence in the hypothesis and the curve indicates the posterior belief that arises if we do (or do not) observe the clue. As can be seen, the amount of learning that occurs---the shift in beliefs from prior to posterior---depends a good deal on what prior we start out with. For a smoking gun test, the amount of learning is highest for values roughly in the 0.2 to 0.4 range---and then declines as we have more and more prior confidence in our hypothesis. For a hoop test, the amount of learning when the clue is *not* observed is greatest for hypotheses in which we have middling-high confidence (around 0.6 to 0.8), and minimal for hypotheses in which we have a very high or a very low level of confidence.


```{r CluesInferences2, fig.cap = "Figure shows how the learning from different types of tests depends on priors regarding the proposition. A smoking gun test has the greatest impact on beliefs when priors are middling low and the clue is observed; a ''hoop test'' has the greatest effect when priors are middling high and the clue is not observed.", echo = FALSE, fig.height=10, fig.width=10}
par(mfrow = c(2,2))
	plotit(.4, .6, main="Straw in the Wind")
	plotit(.6, .95, main="Hoop")
	plotit(.05, .4, main="Smoking Gun")
	plotit(.05, .95, main="Doubly Decisive")
```


The implication here is that our inferences with respect to a hypothesis must be based not just on the search for a clue predicted by the hypothesis but also on the *plausibility* of the hypothesis, based on other things we know. Suppose, for instance, that we fail to observe evidence that we are 90 percent sure we *should* observe if a hypothesized causal effect has occurred: a strong hoop test is failed. But suppose that the existing literature has given us a very high level of confidence that the hypothesis *is* right. This high prior confidence, sometimes referred to as a "base rate," is equivalent to believing that the causal effect exists in a very high proportion of cases. Thus, while any given case with a causal effect has only a 0.1 chance of not generating the clue, the high base rate means that the vast majority of cases that we observe without the clue will nonetheless be cases with causal effects. Thus, the failure of even a strong hoop test, involving a highly certain prediction, should only marginally reduce our confidence in a hypothesis that we strongly expect to be true. 

A similar line of reasoning applies to smoking gun tests involving hypotheses that prior evidence suggests are very unlikely to be true. Innocent people may be very unlikely to be seen holding smoking guns after a murder. But if a very high proportion of people observed are known to be innocent, then a very high proportion of those holding smoking guns will in fact be innocent---and a smoking-gun clue will be far from decisive. 

We emphasize two respects in which these implications depart from common intuitions. First, we cannot make *general* statements about how decisive different categories of test, in Van Evera's framework, will be. It is commonly stated that hoop tests are devastating to a theory when they are failed, while smoking gun tests provide powerful evidence in favor of a hypothesis. But, in fact the amount learned depends not just on features of the clues but also on prior beliefs. 

Second, although scholars frequently treat evidence that goes against the grain of the existing literature as especially enlightening, in the Bayesian framework the contribution of such evidence may sometimes be modest, precisely because received wisdom carries weight. Thus, although the discovery of *disconfirming* evidence---an observation thought to be strongly inconsistent with the hypothesis---for a hypothesis commonly believed to be true is more informative (has a larger impact on beliefs) than *confirming* evidence, this does not mean that we learn more than we would have if the prior were weaker. But it is not true as a general proposition that we learn more the bigger the "surprise" a piece of evidence is. The effect of disconfirming evidence on a hypothesis about which we are highly confident will be *smaller* than it would be for a hypothesis about which we are only somewhat confident. When it comes to very strong hypotheses, the "discovery" of disconfirming evidence is very likely to be a false negative; likewise, the discovery of supporting evidence for a very implausible hypothesis is very likely to be a false positive. The Bayesian approach takes account of these features naturally.^[We note, however, that one common intuition---that little is learned from disconfirming evidence on a low-plausibility hypothesis or from confirming evidence on a high-plausibility one---*is* correct.] 


### Simultaneous, joint updating

When we update we often update over multiple quantities. When we see a smoking gun, for instance, we might update our beliefs that the butler did it, but we might also update our beliefs about how likely we are to see smoking guns -- maybe they are not as rare as we thought! 

Intuitively we might think of this updating as happening sequentially -- first of all, we update over the general proposition, then we update over the particular claim. But in fact we update over both quantities at once. 

Here we elaborate on the intuition using an example of Bayesian process tracing, in which updating occurs over both the nodal type ($j$) and beliefs about the probabilities with which clues are observed for each type ($\phi$ values). 

Suppose that we observe a case with values $X=1, Y=1$. We begin by defining a prior probability distribution over each parameter. Suppose that we establish a prior categorical distribution reflecting uncertainty over whether the case is a $b$ type (e.g., setting a probability of 0.5 that it is a $b$ and 0.5 that is a $d$ type). We also start with priors on $\phi_b$ and $\phi_d$. For concreteness, suppose that we are certain that the clue is unlikely for a $d$ type ($\phi_d=.1$), but we are very uncertain about  $\phi_b$; in particular, we have a  uniform prior distribution over $[0,1]$ for $\phi_b$. Note that, even though we are very uncertain about $\phi_b$, the clue still has probative value, arising from the fact that the expected value of $\phi_b$ is higher than that of $\phi_d$. 

Suppose that we then look for the clue in the case and observe it. This observation shifts posterior weight away from a belief that the case is a $b$. See Figure \@ref(fig:correlation) for an illustration. Yet it *simultaneously* shifts weight toward a higher value for $\phi_b$ and a lower value for $\phi_d$. The reason is that the observed clue has a relatively high likelihood *both* for combinations of parameter values in which the case is a $d$ and $\phi_b$ is low *and* for combinations in which the case is a $b$ and $\phi_b$ is *high* (or, equivalently, in this example, where $\phi_d$ is low). The marginal posterior distribution of $\phi_b$ will thus be shifted upward relative to its prior marginal distribution. The joint posterior distribution will also reflect a dependency between the probability that the case is a $b$ vs. a $d$, on the one hand, and $\phi_b$ and $\phi_d$ on the other. 




```{r correlation, echo = FALSE, fig.cap = "\\label{fig:correlation} Joint posteriors distribution on whether a case is a $b$ or $d$ and on the probability of seeing a clue for a $b$ type ($\\phi_b$).", fig.height=6, fig.align="center", fig.width=10}

# X=1 Y=1 case chose. Is it b or d. Say phi_d = .1 and phi_b~ uniform[0,1]. Say prior on b is .5.

k      <- 4000
phi_b  <- seq(0.001,.999, length = k)
theta  <- data.frame(type = c(rep(0,k), rep(1,k)), phi_b = c(phi_b, phi_b), phi_d  <- rep(.1,2*k)) 

posterior_k_seen <- (theta$type*theta$phi_b + (1-theta$type)*theta$phi_d)
posterior_k_seen <-  posterior_k_seen/sum(posterior_k_seen)

posterior_k_not_seen <- (theta$type*(1-theta$phi_b) + (1-theta$type)*(1-theta$phi_d))

posterior_k_not_seen <- posterior_k_not_seen/sum(posterior_k_not_seen)

select1 <- sample(1:(2*k), k, replace = FALSE, prob = posterior_k_seen)
select2 <- sample(1:(2*k), k, replace = FALSE, prob = posterior_k_not_seen)

  par(mfrow=c(1,2))
  plot(theta$type[select2]+(rnorm(2*k)/10)[select2], theta$phi_b[select2], col = rgb(.8,.1,.2,.4), pch =16, cex=.4, xlab = "Is it a b?", ylab = expression(phi[b]), main="Beliefs | K not seen", axes=FALSE) 
  axis(1, at=c(0,1), labels=c("d", "b"));   axis(2)
  box()
  
  plot(theta$type[select1]+(rnorm(2*k)/10)[select1], theta$phi_b[select1], col = rgb(.8,.1,.2,.4), pch =16, cex=.4, xlab = "Is it a b?", ylab = expression(phi[b]), main="Beliefs | K seen", axes=FALSE) 
  axis(1, at=c(0,1), labels=c("d", "b"));   axis(2)
  box()

```


Figure \@ref(fig:ch5joint) provides a second example, this time showing the joint distribution for the belief that $X$ causes $Y$ in cases in which $X=1, Y=1$ and the relative likelihood that  $K=1$  when $X$ causes $Y$ (and  $X=1, Y=1$) relative to the likelihood that $K=1$ when $X$ does not cause $Y$ (and  $X=1, Y=1$). In this case the joint posterior is derived from a model in which probative value of $K$ is inferred from data, rather than assumed by researchers. We develop more models of this form in later chapters but for now highlight only that in simple setups the distribution of beliefs over queries and over the informativeness of clues are likely not independent of each other.   

```{r ch5joint, echo = FALSE, fig.cap="Correlated beliefs on queries and probative value", fig.width = 10, fig.height = 5}

if(do_diagnosis){

model <- make_model("X -> Y <- K") %>%
  set_parameters(statement = decreasing("X", "Y"), 0) %>%
  set_parameters(statement = substitutes("X", "K", "Y"), 0)

data <- make_data(model, n = 1000)

updated <- update_model(model, data)

P1 <- query_distribution(updated, "Y[X=1] > Y[X=0]", given = "X==1 & Y==1", using = "posterior")
K1 <- query_distribution(updated, "K==1", given = "(Y[X=1] > Y[X=0]) & X==1 & Y==1", using = "posterior")
K2 <- query_distribution(updated, "K==1", given = "(Y[X=1] <= Y[X=0]) & X==1 & Y==1", using = "posterior")
write_rds(list(P1=P1, K1=K1, K2=K2), "saved/ch5_posteriors.rds")
}
L <- read_rds("saved/ch5_posteriors.rds")

plot(log(L$K1 / L$K2),  L$P1, col = rgb(.8,.1,.2,.4), pch =16, cex=.4, ylab = "Belief that Y is due to X when X=Y=1", xlab = "log(Pr(K=1|X=1 caused Y=1)/Pr(K=1 | not X=1 caused Y=1))")
```



### Posteriors are independent of the ordering of data

We often think of learning as a process in which we start off with some set of beliefs---our priors---we gather data, $D_1$, and update our beliefs, forming a posterior; we then observe new data and we update again, forming a new posterior, having treated the previous posterior as a new prior. In such cases it might seem natural that it would matter which data we saw first and which later. 

<!-- For instance we are interested in learning whether natural resources caused conflict. We first observe that rebels are concentrated in a natural resource rich area. We update. We then learn that these rebels are trading in resources. We update again.  -->


In fact, however, Bayesian updating is blind to ordering. If we learn first that a card is a face card and second that it is black, our posteriors that the card is a Jack of Spades go from 1 in 52 to 1 in 12 to 1 in 6.  If we learn first that the card is black and second that it is a face card, our posteriors that it is a Jack of Spades go from 1 in 52 to 1 in 26 to 1 in 6. We end up in the same place in both cases. And we would have had the same conclusion if we learned in one go that the card is a black face card.

The math here is easy enough. Our posterior given two sets of data $D_1, D_2$ can be written:

$$p(\theta | D_1, D_2) = \frac{p(\theta, D_1, D_2)}{p(D_1, D_2)} = \frac{p(\theta, D_1 | D_2)p(D_2)}{p(D_1 | D_2)p(D_2)}= \frac{p(\theta, D_1 | D_2)}{p(D_1 | D_2)}$$

or, equivalently:

$$p(\theta | D_1, D_2) = \frac{p(\theta, D_1, D_2)}{p(D_1, D_2)} = \frac{p(\theta, D_2 | D_1)p(D_1)}{p(D_2 | D_1)p(D_1)}= \frac{p(\theta, D_2 | D_1)}{p(D_2 | D_1)}$$

In other words our posteriors given both $D_1$ and $D_2$ can be thought of as the result of updating on $D_2$ given we already know $D_1$ or the result of updating on $D_1$ given we already know $D_2$. 

This fact will be useful in applications. In practice we might assume that we have beliefs based on background data $D_1$, for example regarding general relations between $X$ and $Y$ and a flat prior, and we then update again with new data on $K$. Rather than updating twice, the fact that updating is invariant to order means that we can assume a flat prior and update once given data on $X$, $Y$, and $K$.

<!--chapter:end:05-being-Bayesian.Rmd-->

# Theories as causal models {#theory}

```{r packagesused03, include = FALSE}
source("_packages_used.R")
```


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
In this short conceptual chapter we describe an approach in which theoretical claims are thought of as model justifications within a  hierarchy of causal models. Lower-level models serve as a theory for a higher level model if the higher level model can be deduced from the lower level model---possibly in combination with data. Consistency requirements determine what new theory emerges from the combination of prior theory and data, but also constrain what prior theories can be invoked to justify a given claim. 
::::
<br>

<!-- 6.1 Models imply models -->
<!-- 6.1.1 Structure implies structures -->
<!-- 6.1.2 Data implies updated probabilistic causal models -->

<!-- 6.2 Three uses -->
<!-- 6.2.1 Justification -->
<!-- 6.2.2 Explanation -->
<!-- 6.2.3 Integrated inferences -->

<!-- 6.3 The rules -->
<!-- 6.3.1 Not one to one: show many to 1 -->
<!-- 6.3.2 Structural consistency (both ways) -->
<!-- 6.3.3 Distributional consistency -->

<!-- FLAG: AJ GO OVER PROBABILISTIC SECTION TO CONNECT TO OTHER VERSIONS OF THEORY -->
<!-- FLAG: MH CHECK CONNECTION BETWEE EVALUATION AND JUSTIFICATION -->

In Chapter \@ref(illustratemodels), we described a set of theories and represented them as causal models. But so far we haven't been very explicit in what we mean by a theory or how  theory maps onto a causal-model framework. 

In this book, we will think of theory as a type of *explanation*: a theory provides an account of how or under what conditions a set of causal relationships operate. We generally express both a theory and the claims being theorized as causal models. The theory is then is a model that *implies* another model---possibly with the help of some data. 

To fix ideas: a simple claim might be that "*A* caused *B* in case $j$". This claim is itself a model, albeit a very simple one. The theory that supports this model might be of the form "*A* always causes *B*",  "*A* always causes *B* whenever *C* (and *C* holds in case *j*)", or  "*A* invariably causes *B* and  invariably *B* causes *C*". These all have in common that they are arguments that could be provided to support the simple claim; in each case, if you believe the theory you believe the implication.


<!-- We begin by elaborating upon our working definition of "theory," showing how it operates within a conception of models in "levels." This requires thinking through how a more detailed, "lower-level" causal structure can imply, and thus *explain*, a less-detailed "higher-level" structure. We also work through how the nodal and causal types in a more detailed model, and beliefs about those types, map onto types (and beliefs about types) in a simpler model. Further, we outline what kinds of elaborations of a simple model are possible (as we outlined in Chapter \@ref(models) what kinds of simplifications of models are permissible). We then turn to the relationship between theory and evidence in a causal-model framework, as compared to the way that relationship operates in a more conventional theory-testing approach.  -->


The rest of this short chapter builds out this idea and uses it to provide a way of characterizing when a theory is useful or not. 
In the first section, we consider multiple senses in which one model might imply, and thus serve as a *theory of*, another model. For one thing, we consider how one causal structure can imply (theorize) another causal structure, by including additional new nodes and nodal types that explain how or when causal effects in the original model will unfold. Next, we consider how the causal-type *ranges* of models can relate to one another: one model can imply another model when the former's causal types constitute a subset of the latter's. In this situation, the theory represents a more specific, stronger claim about the kinds of causal effects that are operating. We then turn to logical relations between probabilistic models. We show how the distributions over nodal types in a simpler model structure can be underwritten by distributions over nodal types in a more detailed model structure. Here, a claim about the prevalence (or probability) of causal effects in a causal network is justified by claims about the prevalence or probability of causal effects in a more granular rendering of that network. Finally, we show how a probabilistic model plus *data* can provide a theoretical underpinning for a new, stronger model.

Second, we consider how theories-as-models can be useful. In embedding theorization within the world of causal models, we ultimately have an empirical objective in mind. Theorizing a causal relationship of interest, in our framework, means elaborating our causal beliefs about the world in greater detail. As we show in later chapters, theorizing in the form of a causal model allows us to generate research designs: to identify sources of inferential leverage and to explicitly and systematically link observations of components of a causal system to the causal questions we seek to answer. In the second section of this chapter, however, we provide a high-level conceptualization of the empirical gains from theory.
<!-- We discuss toward the end of the chapter how this definition of theory relates to common understandings of theory in the social sciences.  -->

In the chapter's third and final section, we show how our formalization of theory maps onto *formal* theory as usually understood, showing how we can generate a causal model from a game-theoretic model.


## Models as *theories of* 

Let us say that a causal model, $M^\prime$, is a *theory of* $M$ if $M$ is implied by $M^\prime$. It is a theory *because* it has implications. Otherwise it is a conclusion, an inference, or claim.  A theory, $M^\prime$, might itself sit atop---be supported by---another theory, $M^{\prime\prime}$, that implies $M^\prime$. To help fix the idea of theory as "supporting" or "underlying" the model(s) it theorizes, we refer to the theory, $M^\prime$, as a *lower*-level model relative to $M$ and refer to $M$ as a *higher*-level model relative to its theorization, $M^\prime$.^[We note that our definition of theory differs somewhat from that given in @pearl2009causality (p207): there a theory is a (functional) causal model and a restriction over $\times_j \mathcal{R}(U_j)$, that is, over the collection of contexts envisionable. Our definition also considers probabilistic models as theories, allowing statements such as "the average effect of $X$ on $Y$ is 0.5."] 

<!-- Higher-level models can be generated from lower-level models in two ways, both of which are consistent with common understandings of what it is for a set of claims to constitute or, conversely, derive from a ''theory.'' -->

<!-- We can distill two ways in which lower-level models can relate to, or support, higher-level models: -->


<!-- ### Disaggregating nodes -->

Both structural models and probabilistic models---possibly in combination with data---imply other models. We discuss each in turn.

<!-- FLAG: DISCUSS UP FRON DIFFERENT WAYS WE USE THE TERM JIMPLICAITON -->

### Implications of structural causal models

Structural models can imply multiple other simpler structural models. Similarly structural models can be implied by multiple more involved models. 

We imagine two forms of lower level model, those that involve "type splintering" and those that involve "type reduction."

**Type splintering theorization.** Theorization often involves a refinement of causal types, implemented through the addition of nodes. 
Take the very simple model, $M$, represented in Figure \@ref(fig:Highlow)(a). The model simply states that $X$ has (or *can* have) a causal effect on $Y$. 

What theories might justify $M$? This question can be rephrased as "what models imply model $M$?" The figure points to two possibilities. Both models $M^\prime$ and  $M^{\prime\prime}$ imply model $M$. They can be thought of as  *theories*, or lower-level model, of $M$. 

Model $M^\prime$ differs by the addition of a node, $K$, in the causal chain between $X$ and $Y$. We can say that  $M^\prime$ is a *theory* of $M$ for two reasons. First it provides a *justification*---if you believe  $M^\prime$ you should believe $M$: if $X$ affects $Y$ through $K$, then $X$ affects $Y$. But as well as a justification it also provides an *explanation* of $M$. Suppose we already *know* that $X$ affects $Y$ but want to know *why*. If we ask, "why does $X$ affect $Y$?", $M^\prime$ provides an answer: $X$ affects $Y$ *because* $X$ affects $K$, and $K$ affects $Y$.

Model $M^{\prime\prime}$ differs by the addition of a node, $C$, that moderates the effect of  $X$ on  $Y$. $M^{\prime\prime}$ justifies $M$ in the sense that if you believe  $M^{\prime\prime}$  you should believe  $M$. It provides an explanation of a kind also: if you believe model  $M^{\prime\prime}$ you likely believe that the relation between $X$ and $Y$ is what it is because of $C$. Had $C$ been different the causal relation between $X$ and $Y$ might have been also.  


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
**Two kinds of type-splintering theories.**

Theories are “lower-level” causal models that explain or provide an account of a “higher-level”, simpler model. Two common forms of theorization involve articulating  *mediating* and *moderating* relationships:

  * *Mediation*: A mediator, $M$, can be introduced between $X$ and $Y$, thus splintering $\theta^Y$ into $\theta^M$ and $\theta^{Y_\text{lower}}$. The mediation theory thus explains the $X \rightarrow Y$ relationship.
  * *Moderation*: A component of $\theta^Y$ can be extracted and specified as a substantive variable. This variable is now a substantively conceptualized moderator of the $X \rightarrow Y$ relationship. The moderation theory thus provides a fuller explanation of why $X$ has different effects on $Y$ in different contexts.

::::
<br>


A key idea is that both $M'$ and $M''$ involve a redefinition of $\theta^Y$. That is we see a change in the endogenous nodes but these in turn imply a change in the interpretation of the exogenous nodes pointing into existing endogenous nodes (such as $Y$ in this example). We can think of part of $\theta^Y$ being splintered off and captured by $\theta^K$ or $C$.


Return to models $M$ and $M'$ in  Figure \@ref(fig:Highlow)(a).
Importantly, in moving from the higher- to the lower-level model, we have effectively *split* the nodal-type term $\theta^Y$ into two parts: $\theta^{Y_\text{lower}}$ and $\theta^K$. Intuitively, in the higher-level model, (a), $Y$ is a function of $X$ and $\theta^Y$, the latter representing all things other than $X$ than can affect $Y$. Or, in the language of our nodal-type setup, $\theta^Y$ represents all of the (unspecified) sources of variation in $X$'s effect on $Y$. When we insert $K$ into the model, however, $X$ now does not directly affect $Y$ but only does so via $K$. Further, we model $X$ as acting on $K$ in a manner conditioned by $\theta^K$, which represents all of the (unspecified) factors determining $X$'s effect on $K$. The key thing to notice here is that $\theta^K$ now represents *a portion of the variance that $\theta^Y$ represented in the higher-level graph*: some of the variation in $X$'s effect on $Y$ now arises from variation in $X$'s effect on $K$, which is captured by $\theta^K$. So, for instance, $X$ might have no effect on $Y$ because $\theta^K$ takes on the value $\theta^K_{00}$, so that $X$ has no effect on $K$. Put differently, any effect of $X$ on $Y$ must arise from an effect of $X$ on $K$; so $\theta^K$'s value must be either $\theta^K_{01}$ or $\theta^K_{10}$ for $X$ to affect $Y$. ^[As we emphasize further below, it is in fact only the random, unknown component of the $X\rightarrow K$ link that makes the addition of $K$ potentially informative as a matter of research design: if $K$ were a deterministic function of $X$ only, then knowledge of $X$ would provide full knowledge of $K$, and nothing could be learned from observing $K$.] What $\theta^K$ represents, then, is that part of the original $\theta^Y$ that arose from some force other than $X$ operating at the *first* step of the causal chain from $X$ to $Y$. 
So now, $\theta^Y$ in the lower-level graph is not quite the same entity as it was in the higher-level graph. In the original graph, $\theta^Y$ represented *all* sources of variation in $X$'s effect on $Y$. In the lower-level model, with $K$ as mediator, $\theta^Y$ represents only the variation in $K$'s effect on $Y$. Put differently, $\theta^Y$ has been expunged of any factors shaping the first stage of the causal process, which now reside in $\theta^K$. We highlight this change in $\theta^Y$'s meaning by referring in the second model to $\theta^{Y_\text{lower}}$. 


<!-- Put differently, when we construct the lower-level model in (b), we are taking that part of $Y$ not determined by $X$ and splintering it in two: a non-$X$ input into $K$ and a non-$K$ (and thus also non-$X$) input into $Y$. -->

Consider next model $M^{\prime\prime}$ panel (c) in Figure \@ref(fig:Highlow), which also supports (implies) the higher-level model in panel $(a)$. The logical relationship between models $(a)$ and $(c)$, however, is somewhat different. Here the lower-level model *specifies* one of the conditions that comprised $\theta^Y$ in the higher-level model. In specifying a moderator, $C$, we have extracted $C$ from $\theta^Y$, leaving $\theta^{Y_\text{lower}}$ to represent all factors *other than $C$* that condition $Y$'s response to its parents. More precisely, $\theta^{Y_\text{lower}}$ now represents the set of nodal types defining how $Y$ responds jointly to $X$ and $C$. Again, the relabeling as $\theta^{Y_\text{lower}}$ reflects this change in the term's meaning. Whereas in Model $M^{\prime}$ we have extracted $\theta^K$ from $\theta^Y$, in Model $M^{\prime\prime}$, it is $C$ itself that we have extracted from $\theta^Y$, substantively specifying what had been just a random disturbance.


<!-- We return to the quantitative implications of this redefinition of endogeneous nodes below.  -->

<!-- In model $M$, $\theta^Y$ captures all the features of units that result in them responding in one or another way to $X$. However in model $M'$, $\theta^{Y_{lower}}$  only needs to capture the way that $Y$ responds to $K$ --- that part of the effect of $X$ on $Y$ that operates via $X$'s impact on $K$ can be partitioned off. in model $M''$, $\theta^{Y_{lower}}$  only needs to capture the way that $Y$ responds to $X$  goven $K$, uncertainty about the value of $K$ can be partialed out.  -->


```{r Highlow, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='80%', fig.cap = "Here we represent the simple claim that one variable causes another, and two theories --- lower-level models --- that could explain this claim. Both model (b) and model (c) involve theorization via disaggregation of nodes."}

par(mfrow = c(3,1))
par(mar=c(1,1,3,1))
hj_dag(x = c(1,2,2),
       y = c(1,1,2),
       names = c(
         expression(paste(X)),
         expression(paste("Y")),  
         expression(paste(theta^Y))),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "(a) A Higher-Level Model, M",
       add_functions = 0, 
       contraction = .16, 
       padding = .1
)

hj_dag(x = c(1,2,2, 1.5, 1.5),
       y = c(1,1,2, 1  , 2),
       names = c(
         expression(paste(X)),
         expression(paste("Y")),  
         expression(paste(theta^Y[lower])),
         expression(paste(K)),
         expression(paste(theta[K])) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(4, 2, 4, 2)),
       title = "(b) Lower-Level Model, M': Disaggregating via Mediation",
       add_functions = 0, 
       contraction = .16, 
       padding = .1
)

hj_dag(x = c(1,2, 2, 1.5),
       y = c(1,1, 2, 2),
       names = c(
         expression(paste(X)),
         expression(paste("Y")),  
         expression(paste(theta^Y[lower])),
         expression(paste(C)) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(2, 2, 4, 2)),
       title = "(c) Lower-Level Model M'': Disaggregating via Moderation",
       add_functions = 0, 
       contraction = .16, 
       padding = .1
)

```



**Type-reducing theorization.** There is a second way in which we might imagine a model being implied by another model that does not involve a change in nodes. Let $\Theta(\mathcal M_1)$ denote the set of causal types in model $\mathcal M_1$. Then we can say that $\mathcal M_0$ implies $\mathcal M_1$ if $\Theta(\mathcal M_0)\subseteq \Theta(\mathcal M_1)$. Informally this means that any relation admitted by theory $\mathcal M_0$ is representable in model $\mathcal M_1$, though the converse may not be true. We can think of a theory of $\mathcal M_1$ as a *restriction* of ranges of $\Theta(M_1)$. 

``` {r, eval  = FALSE, echo = FALSE}
mm <- make_model("X -> M -> Y <- X") %>% 
  set_restrictions("Y[X=1, M = .] != Y[X=0, M = .]")


mm$nodal_types$Y


# "0000" "1100" "0011" "1111"
```

We illustrate the idea in Figure \@ref(fig:Highlowreduce). In panel (a) of the figure, we have a model, $\mathcal M_1$ in which $Z$ can have both a direct and an indirect effect (via $X$) on $Y$. Suppose that we believed that $\mathcal M_1$ was technically true but overly permissive, in the sense that it allowed for causal relations that we do not in fact believe are operating. We might believe, for instance, that $Z$ has no direct effect on $Y$ and that $Z$ has no negative effects on $X$ --- the beliefs we would need to hold to treat $Z$ as an instrument for $X$. We could thus write down a lower-level model, $\mathcal M_0$,in which we have *reduced* the type space accordingly. Specifically, in $\mathcal M_0$, we would restrict the nodal types at $Y$ to only the $\theta^Y_{0000}$, $\theta^Y_{1100}$, $\theta^Y_{0011}$, and $\theta^Y_{1111}$; and we would reduce the nodal types at $X$ to $\theta^X_{00}$, $\theta^X_{01}$, and $\theta^X_{11}$. In panel (b), we (somewhat loosely) represent $\mathcal M_0$. We have now eliminated the arrow from $Z$ to $Y$ to represent the dropping of all nodal types involving a direct effect of $Z$ on $Y$; not pictured is the montonicity assumption at $X$. However, we have relabeled the nodal-type nodes for both $X$ and $Y$ to represent the fact that these are different objects from the nodal type nodes in the higher-level model.^[We drop the arrow in Figure \@ref(fig:Highlowreduce), however, in order to help visually convey the difference between the two models. In fact, we would construct $\mathcal M_0$ by placing restrictions at nodes in $\mathcal M_1$, rather than by changing the model's structure, so that the allowed types in $\mathcal M_0$ form a subset of those in $\mathcal M_1$.]

Thus, while we can theorize by adding substantive nodes to a model and thus splitting types, we can also theorize by maintaining existing nodes but constraining relations among them. In both forms of theorization, we start with a model that allows for a broad, and possibly unknown, range of possibilities: for instance, a broad range of paths through which or conditions under which $X$ might affect $Y$ or a broad range of causal effects operating at each node. Theorization of both forms then involves making a *stronger* claim: for instance, a claim about *how* or *when* $X$ affects $Y$ (via type-splintering) or a claim about the *particular* causal effects operating at a given node (via type-reduction). In both forms of theory, believing the stronger claim in the lower-level model implies believing the weaker claim in the higher-level model. Further, both modes of theorization also map nicely onto common ways in which we think about theory-development in the social sciences: we theorize mechanisms, sources of heterogeneity, and directions of effects (starting with a belief that $X$ affects $Y$, for instance, and moving to a more constrained belief about whether that effect is positive or negative). 

Finally, as we speak to below, theorization of both forms can generate gains for causal inference, by allowing us to use data in ways that we are unable to use it in the higher-level model.

**Preserving (conditional) independencies**

Not all potential mappings from higher- to lower-levels are permitted. In particular, when theorizing, we may *add* but may not *remove* independencies implied by the original model. If two variables are independent--- or conditionally independent given a third variable---in one model, then this same relation of independence (or conditional independence) must be captured in any theory of that model. For instance, if we start with a model of the form $X \rightarrow Y \leftarrow W$, where $W$ and $X$ are independent, we could not theorize this model by adding an arrow from $X$ to $W$. A theory can have *additional* conditional independencies not present in the higher-level model, as in the example in Figure \@ref(fig:Highlowreduce).  But we may not theorize *away* (conditional) independencies insisted on by our higher-level claim. 

This is a key part of what it means for the lower-level model to *justify* the higher-level model. A model makes claims about what is (conditionally) independent of what. The claims about conditional independence implied by the higher-level model must therefore be warranted by (conditional) independencies operating in the lower-level model. If we introduce new dependencies via theorization, then our higher-level model (which excludes these dependencies) would no longer be justified by the lower-level model.  

```{r Highlowreduce, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='80%', fig.cap = "Here we represent theorization via type-reduction. Though we show the removal of an arrow to help convey the idea, we would in fact reduce types by imposing restrictions on the nodal types at Y within the same DAG."}

par(mfrow = c(2,1))
par(mar=c(1,1,3,1))

hj_dag(x = c(1,2,2, 1.5, 1.5),
       y = c(1,1,2, 1.5  , 2.5),
       names = c(
         expression(paste(Z)),
         expression(paste("Y")),  
         expression(paste(theta^Y)),
         expression(paste(X)),
         expression(paste(theta[X])) 
         ),
       arcs = cbind( c(1, 3, 5, 4, 1),
                     c(4, 2, 4, 2, 2)),
       title = "(a) A higher-Level Model, M_1",
       add_functions = 0, 
       contraction = .16, 
       padding = .1)
       
       
hj_dag(x = c(1,2,2, 1.5, 1.5),
       y = c(1,1,2, 1.5  , 2.5),
       names = c(
         expression(paste(Z)),
         expression(paste("Y")),  
         expression(paste(theta^Y[lower])),
         expression(paste(X)),
         expression(paste(theta^X[lower])) 
         ),
       arcs = cbind( c(1, 3, 5, 4),
                     c(4, 2, 4, 2)),
       title = "(b) A lower-Level Model, M_0, with types reduced",
       add_functions = 0, 
       contraction = .16, 
       padding = .1
)
```


### Probabilistic causal models

At the structural level, then, there are two types of theory, or two types of relations between levels of model: those defined by type-splintering and those defined by type-reduction. In general, we will want to be working with probabilistic causal models --- i.e., those that include distributions over nodal types. We can describe straightforwardly how distributions in a higher-level model relate to --- and must change with --- distributions at the lower level. Indeed, it is these relations that unlock the opportunity for reaping empirical gains from theory.

**Theoretical implications of probabilistic models.** Suppose we start with the mediation model in panel (b) of Figure \@ref(fig:Highlow). Wen then add to it a distribution over $\theta^K$ and $\theta^Y_{lower}$, giving us a probabilistic causal model that we will denote $\mathcal M^p_{lower}$. $\mathcal M^p_{lower}$, in turn, implies a higher-level probabilistic model, $\mathcal M^p_{higher}$, formed from the structure of Model (a) in Figure \@ref(fig:Highlow), and a *particular* distribution over $\theta^Y$: specifically, $\theta^Y$ will have the distribution that preserves the causal relations implied by the beliefs in $\mathcal M^p_{lower}$. Thus, for instance, the probability that $X$ has a positive effect on $Y$ in $\mathcal M^p_{higher}$ is $\theta^{Y}_{01}$; the probability that  $X$ has a positive effect on $Y$ in $\mathcal M^p_{lower}$ is  $\theta^{K_{lower}}_{01}\theta^{Y_{lower}}_{01}  + \theta^{K_{lower}}_{10}\theta^{Y_{lower}}_{10}$.  Consistency then requires that $\theta^{M_{lower}}_{01}\theta^{Y_{lower}}_{01}  + \theta^{K_{lower}}_{10}\theta^{Y_{lower}}_{10} = \theta^{Y_{higher}}_{01}$. So the value of  $\theta^{Y_{higher}}_{01}$ is *implied* by $\theta^{K_{lower}}_{01},\theta^{Y_{lower}}_{01}, \theta^{K_{lower}}_{10},\theta^{Y_{lower}}_{10}$, but not vice-versa. 

**Deducing models from theory and data.** Now we can see what happens if we bring data to the lower-level model. A probabilistic causal model coupled with data implies another probabilistic causal model via Bayes rule.  For this reason, we can fruitfully think of an initial model as being a *theory of* an updated model, coupled with data. Thought of in this way we have clarity over what is meant when we turn to theory to support a claim, but also what is meant when we seek to justify a theory. We might imagine a scholar arguing: "$\mathcal M_1$: $X$ caused $Y$ in country $j$." When pushed for a justification for the claim they provide the lower level model: "$\mathcal M_0:$ the average effect of $X$ on $Y$ in countries with feature $C=1$ is 0.95, making it likely that $X$ caused $Y$ in this case." Here $\mathcal M_1$ is implied by $\mathcal M_0$ coupled with data $C=1$. If pushed further as to why that theory is itself credible they might point to a lower level model consisting of structural model $X\rightarrow Y \leftarrow C$ plus flat priors coupled with data on $X,Y$ and $C$.  At each stage, as more justification is provided, the researcher formally provides lower-level models.  

Moving up, as more data is provided, more "specific" higher level models emerge, justified by lower models plus data. These models are more specific in the sense that they are implied by the higher level models, plus data,  but not vice versa.  But they are also  (generally) more specific in a second sense: that they make stronger statements about how causal processes operate.^[This is not universally true, a point we return to below.]  They place greater weight on a smaller set of causal claims.^[In frequentist frameworks we often think of analysis as implementing up-or-down empirical tests against data to parse between theories that should be maintained and theories that should be rejected. In a Bayesian framework we think more continuously of shifting our beliefs across causal possibilities within a multi-dimensional theoretical space.] 

<!-- FLAG:  -- see use of specific here -->

As the simplest illustration, we might imagine beginning with an $X\rightarrow Y$ model, $\mathcal M_1$, in which, $X$ and $Y$ are binary, and we we believe that $Y$ possibly responds to $X$. If we have "flat" priors over causal types, in the sense described in Chapter \@ref(bayeschapter), then our prior uncertainty over the proposition that $X$ causes $Y$, under this model, is large; as is our uncertainty that $Y=1$ is due to $X=1$ in a given case. In other words, given our theory, we are uncertain about the proposition $\mathcal M_3$: $X$ caused $Y$. However, if we then receive a lot of data, $\mathcal D$, showing strong relations between $X$ and $Y$, then our updated model $\mathcal M_2$, formed from combining $\mathcal D$ and $\mathcal M_1$ allows us to infer that $X$ caused $Y$ in this case with greater certainty. 

Thus our new theory $\mathcal M_2$ is (a) formally similar to $\mathcal M_1$, (b) formed as a product of past theory plus evidence, here justified by $\mathcal M_1$ given data $\mathcal D$, and (c) capable of providing sharper implications than past theory.^[As a general matter an updated theory may not provider sharper claims for all queries. That is, in practice, posterior variance over queries can increase with more data. As a simple illustration: say, we start out thinking that the probability that an outcome is due to conditions A, B, or C is .9, .05, and .05, respectively. If I find evidence that convinces me that A is not the cause, then I shift (a) to greater certainty about whether A was the cause but (b) greater uncertainty about whether B was the cause.]


In this way, Bayesian updating provides a simple and coherent way of thinking about the integration of theory and data.  






<!-- ### Generalizing a model -->

<!-- A lower-level model, $M^\prime$, can also be a representation of $M$ in which a node has been introduced that permits variation in a feature of context that is fixed and taken-for-granted in $M$. Here, $M^\prime$ theorizes $M$ in the sense of embedding $M$ within a *more general* set of beliefs about how the world works. $M$ then becomes a special case of the theorized relations, one that holds when we condition on some data, specifying some particularity of context. -->

<!-- To illustrate this approach to theorization, consider again graphs (a) and (c) in Figure \ref{fig:Highlow}. We have discussed how the graph in panel (c) can represent a disaggregation of $\theta^Y$ from panel (a) into $\theta^{Y_\text{lower}}$ and $C$. An alternative possibility, however, is to employ a moderation model that represents a more general claim than the higher-level model that it supports. For instance, the graph in panel (a) might represent the causal function $Y=X+\theta^Y$. In this model, $X$ always has an effect on $Y$. The graph in panel (c), in turn, might represent the more general function, $Y=XC+\theta^Y$ (where $C$ is binary). Now, whether $X$ has an effect depends on the value of $C$. In particular, model (c) combined with the observation $C=1$ directly implies model (a). Model (a) is a special case of model (c) that holds in and only in the context $C=1$. In answer to the question, "Why do you believe model (a)?" one could respond with model (c) plus the observation $C=1$.  -->

<!-- Note the difference in how theorization has proceeded for the two moderation models (both graphically represented in panel (c)). When we introduce moderation via the disaggregation of nodes, we pull content out of $\theta^Y$ and specify it substantively. When we generalize, on the other hand, we *add* a node, a source of variation not factored into the original model at all. We can think of that variation as implicitly conditioned-on in the higher-level model. -->

<!-- As a secondary matter, note that the $\theta$ term pointing into $Y$ may or may not be altered by the addition. In particular, we will explore later, if $\theta^Y$ is a vessel for causal types, then the number of possible causal types---and, thus, $\theta^Y$'s range---must expand as we add nodes pointing into $Y$. But in the functions we have used in our illustration, $\theta^{Y_\text{lower}}$ remains unchanged when we add the $C$ node. In both the higher- and lower-level models, $\theta^Y$ represents precisely the same random disturbance.  -->



<!-- $M^\prime$ is a *theory* of $M$ in that it, in a sense, helps explain the dependencies of $Y$ on $X$ more fully than does $M$. -->


<!-- Since lower-level models imply higher level models, we think of theories as implying the models they are theorizing. If we believe structural causal model $M'$, then we also must believe the structure of model $M$. If it is possible that $X$ can affect $K$ and possible that $K$ can affect $Y$, then it is possible that $X$ can affect $Y$. Similarly, if we believe Model $M^{\prime\prime}$, then we must also believe Model $M$: if it is true that $X$ can affect $Y$, possibly in ways that are moderated by $C$, then it is trivially true that $X$ can affect $Y$.  -->


<!-- AJ: I do not understand this line that was in the paragraph above: "The converse is not true, however. It is not possible to still believe that $X$ can effect $Y$ if you do not think that $X$ can affect $K$." Of course, it's possible, if you don't believe M' is the right lower-level model, right? I think I'm missing what we're getting at here. -->


<!-- ##  Consistency  -->

<!-- Given model $M_1$, what kinds of lower level models can serve as theories for $M_1$? -->

<!-- In general the mappings between higher-level claims and theories may not be one-to-one. A single theory can support multiple higher-level models. Moreover, a single higher-level relation can be supported by multiple, possibly incompatible lower-level theories.  -->

<!-- To illustrate, consider two "lower level" theories of democratization: -->

<!-- * ($L_1$): $Inequality \rightarrow Democratization  \leftarrow Mobilization$   -->
<!-- * ($L_2$): $Inequality \rightarrow Mobilization \rightarrow Democratization$ -->


<!-- Note how these theories are incompatible with one another. While  $Inequality$ and $Democratization$ are independent in $L_1$, they are causally related in $L_2$. Moreover, in $L_2$, $Inequality$ and $Democratization$ are related only through $Mobilization$, while in $L_1$, $Democratization$ is directly affected by $Inequality$.^[Put differently, these two theories record different relations of conditional independence: in $L_1$, $Inequality$ and $Mobilization$ are unconditionally independent, but they are not unconditionally independent in $L_2$. Also, in $L_2$, $Inequality$ is independent of $Democratization$ conditional on $Mobilization$; but this is not the case in $L_1$.] -->

<!-- Now, consider the following three higher-level claims: -->

<!-- * ($H_1$): $Inequality \rightarrow Mobilization$ -->
<!-- * ($H_2$): $Inequality \rightarrow Democratization$ -->
<!-- * ($H_3$): $Mobilization \rightarrow Democratization$ -->


<!-- $H_1$ can be supported only by one of these theories: only in $L_2$, and not in $L_1$, does $Inequality$ cause $Mobilization$.^[In addition, a *conditional* higher-level model $((Inequality \rightarrow Democratization)|Mobilization=1)$ can be supported by model $L_1$ but not by model $L_2$, where holding $Mobilization$ constant would sever the dependence of $Democratization$ on $Inequality$.] -->

<!-- However, our other two hypotheses could each rest on *both* of the lower-level theories, even though those two theories are incompatible with one another. $H_1$ could be derived from (explained by) either theory: though the two theories differ on whether mobilization is a mediator or a moderator, they agree that inequality can affect democratization. Similarly, both theories imply $H_2$, in which $Mobilization$ affects $Democratization$, even though the two theories disagree on whether inequality is an antecedent to mobilization or a moderator of its effect. -->

<!-- Thus, multiple theories can usually be proposed to explain any given causal effect, and those theories need not be consistent with *each other*.  When seeking an explanation for, say, $H_1$, the choice between $L_1$ and $L_2$ cannot be dictated by logical mappings between models; it must be drawn from a substantive belief about which set of causal dependencies operates in the world. On the other hand, $L_2$ *is* logically ruled out as an explanation of $H_3$.  -->

<!-- It is also true that any given theory logically implies multiple *higher-level* claims about causal relations. $L_2$ implies both $H_2$ and $H_3$.  -->

<!-- Note, however, that the multiple higher-level claims that follow from a theory *must* be compatible with one another.  -->

<!-- What, more precisely are these consistency requirements? -->

<!-- So far we have been discussing the *structural* components of theories: we have seen how a given causal structure can be justified in terms of a more detailed causal structure. But theories also involve claims about what *kinds of effects* operate between variables. For instance, the belief that inequality *generates* democratization, of course, represents a different kind of theory from the belief that inequality *prevents* democratization, although the two might share a DAG structure. As discussed in Chapter \@ref(models), a *probabilistic* causal model comprises both a causal structure and beliefs about distributions over the exogenous nodes --- i.e., in our setup, over a model's nodal types. In an $I \rightarrow D$ model, for instance, probabilistic beliefs include beliefs about the probability that a given case has the type $\theta^D_{01}$ types and the probability that it has the type $\theta^D_{10}$. Thus, as we have with structure, we can think about how a set of beliefs about the distribution of nodal types at a higher level can be supported by beliefs about nodal-type distributions in a lower-level model. -->


<!-- ### Elaboration -->

<!-- Theorization starts with the proliferation of substantive variables---adding beliefs about intervening steps in a causal process. But, critically, it also involves an accompanying disaggregation of unexplained variation. Addition and splintering thus go hand-in-hand: the *insertion* of a mediator between $X$ and $Y$ also involves the *splintering* of $Y$'s type node ($\theta_Y$).    -->


<!-- Other possible ways of elaborating a modelincluding adding *antecedent conditions*---causes of nodes that were exogenous in the higher-level model----and adding *downstream effects*: outcomes of nodes that were terminal in the higher-level model. -->


<!-- ```{r incompat, echo = FALSE, fig.width = 11, fig.height = 7, fig.align="center", out.width='80%', fig.cap = "A higher-level model and a lower-level model that is impermissible."} -->

<!-- par(mfrow = c(2,1)) -->
<!-- par(mar=c(1.5,1.5,3.5,1.5)) -->
<!-- hj_dag(x = c(1,2,1.5,2), -->
<!--        y = c(1,1,1,2), -->
<!--        names = c( -->
<!--          expression(paste(Inequality)), -->
<!--          expression(paste("Democratization")), -->
<!--          expression(paste("Mobilization")), -->
<!--          expression(paste(theta^D[higher]))), -->
<!--        arcs = cbind( c(1,3,4), -->
<!--                      c(3,2,2)), -->
<!--        title = "(a) Higher-level model", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .1 -->
<!-- ) -->

<!-- hj_dag(x = c(1,2,1.5, 2, 1.5), -->
<!--        y = c(1,1,1, 2, 1.5), -->
<!--        names = c( -->
<!--          expression(paste(Inequality)), -->
<!--          expression(paste("Democratization")), -->
<!--          expression(paste("Mobilization")), -->
<!--          expression(paste(theta^D[lower])), -->
<!--          expression(paste("Ethnic homogeneity")) -->
<!--          ), -->
<!--        arcs = cbind( c(1,3, 4, 5, 5), -->
<!--                      c(3,2, 2, 1, 2)), -->
<!--        title = "(b) An incompatible lower-level model", -->
<!--        add_functions = 0, -->
<!--        contraction = .16, -->
<!--        padding = .1 -->
<!-- ) -->

<!-- ``` -->

<!-- The central principle governing allowable elaborations is that a lower-level model *must not introduce dependencies between variables that were omitted in the higher-level model.* We provide an example of a violation of this principle in Figure \@ref(fig:incompat).  -->

<!-- We start with a higher-level model, in panel (a), in which inequality affects democratization through mobilization. We then elaborate the model in panel (b) by adding ethnic homogeneity as a moderator of mobilization's effect. However, because ethnic homogeneity is also modeled here as affecting inequality, we have now introduced a source of dependence between inequality and democratization that was omitted from the higher-level model. In panel (a), democratization and inequality were dependent only via mobilization; and so they are conditionally independent given mobilization. In panel (b), democratization and mobilization are additionally dependent via their common cause, ethnic homogeneity. By the rules governing causal graphs (see Chapter \ref{models}), the higher-level model specifically *prohibited* this second source of dependency---since all dependencies between variables must be represented.  -->


<!-- TEXT -->

<!-- We can also read Figure \@ref(fig:runningsubs) as telling us the set of claims for which the lower-level graph in Figure \ref{fig:running} can serve as a theory. As we can see, the range of claims that a moderately complex model can theorize is vast. For each simpler claim, moreover, there may be other possible lower-level graphs---theories besides ---consistent with it. -->





<!-- Nodes with no parents in $\mathcal{U}\cup\mathcal{V}$ cannot be eliminated as this would entail a loss of information. The graph in Figure \ref{fig:K}(d) illustrates the importance of this. Here $K$ is a cause of both $X$ and $Y$, in other words it is a possible confounder. A higher-level graph that does not include $K$ still requires a $U_K$ node pointing into both $K$ and $Y$ to capture the fact that there is a confounder. -->


<!-- ^[The conditioning approach can also handle theoretical propositions in the form of structural causal models that make no immediate empirical claims but still have "empirical content" in the sense of being able to inform *conditional* claims. The claim "if $X$ then $Y$" says nothing about $P(Y)$ by itself. However, it says a lot about $P(Y)$ if $P(X)$ is known.] -->


<!-- One  effect of elimination is to render seemingly deterministic relations effectively probabilistic. For example, in the lower level graph $C$ is a deterministic function of $X$ and $S$. But in higher level graphs it can depend probabilistically on one of these: in submodel 21, $C$  depends probabilistically on  $X$ since $S$ is now a stochastic disturbance; in 34 $C$ depends probabilistically on $S$. This illustrates how unobserved or unidentified  features render a model "as-if" stochastic. Conversely, models that exclude this form of uncertainty implicitly claim model-completeness. -->



<!-- Consider a second manner in which a higher level model  can be deduced from a lower level model, this time in conjunction with data (or more broadly, ancillary claims): -->

<!-- 2. A higher level model may be formed by conditioning on values of nodes in a lower level model. Conversely, a higher-level functional model, $M$, can be theorized via a lower-level $M^\prime$ in which conditions shaping the operation of the causal effect in $M$, unspecified in $M$, are now specified. -->

<!-- To illustrate this approach, consider again the graphs in Figure \ref{fig:K}. Above we described how the graph in  panel (a) can be produced by aggregating $U_Y^{\text{lower}}$ and $U_K$ from panel (c). An alternative possibility is to simplify by conditioning: we derive a higher-level graph from $M^\prime$ by fixing the value of $K$. For instance, if $Y=XK+U_Y^{\text{lower}}$ in $M'$, then at $K=1$, we have the submodel $M_k$ in which $Y=X+U_Y^{\text{lower}}$. Note that, in generating a submodel by conditioning on $K$, we retain the term $U_Y^{\text{lower}}$ as we have not added causal force into $Y$'s unspecified parent. -->




<!-- Perhaps surprisingly, in this treatment, the theoretical support for a causal model is itself just another causal model: a set of beliefs about structural relations between variables. Thus, a theory is an object that is formally similar to an empirical claim.  -->

<!-- Would like to clarify last sentence above. -->



<!-- This next paragraph is hard to follow. The u's come out of nowhere. What's a mapping from R1 to R1? Generally seems like the points could be made more simply. -->

<!-- I THINK THE UNIVERSALITY AND PRECISION MATERIAL IS INTERESTING BUT NOT SOMETHING WE NEED TO DO ON OUR ROAD TO USING CAUSAL MODELS FOR CAUSAL INFERENCE. AND IT'S NOT EASY. SO I SUGGEST WE CUT.

We can, however, use the approach to assessment of two features sometimes considered important to assess empirical content of a theory: the level of *universality* of a theory and the degree of *precision* of a theory [@popper2005logic, @glockner2011empirical]. ***DEFINE THESE HERE.*** For instance, consider a theory over $X_1, X_2, A, B, Y$ that specified $X_1, X_2 \rightarrow Y \leftarrow A, B, g$ with functional equations: -->

<!-- $$Y = \left\{ \begin{array}{ccc}  -->
<!-- A + BX_1 & \text{ if } & X_2 = 1\\    -->
<!--   g(X_1) &\text{ if } & X_2 = 0 \end{array} \right.$$  -->

<!-- where the domain of $g$, $\mathcal{R}(g)$, is the set of all functions that map from $\mathbb{R}^1$ to $\mathbb{R}^1$, and the ranges of $A$ and $B$ are the real number line. Say the distributions over $A, B, X_1, X_2$, and  $g$ are not specified. Then the theory makes a precise claim conditional on $u_1, u_2, X_1, X_2$, and  $g$. But since the distribution over $\mathcal{R}(g)$ is not provided by the theory, the theory only claims knowledge of a functional form for $Y$ for those cases in which $X_2=1$. Thus in this case the *universality* of the theory for the claim "$Y$ is a linear function of $X$," is  $P(X_2=1)$. This is the domain over which the theory has something to say about this proposition. Note that in this case the universality is not  provided by the theory, but is rather an external proposition that depends on additional data. The *precision* of the theory depends both on the claim of interest and the distribution of root variables. For example, the precision of the theory for the causal effect of $X_1$ on $Y$ when $X_2=1$ depends on the distribution of $B$: the theory is more precise about this causal effect the less uncertainty there is about the value of $B$. Moreover, a theory that specified that $B$ has large variance would be making a precise claim about causal *heterogeneity*, even if it was imprecise about the causal effect. Again this feature cannot be read from the theory without access to ancillary information that the theory itself does not provide. -->

<!-- AJ comments: -->

<!-- - Not clear what the "this approach" is that this is illustrating a feature of. Expressing theory as structural equations?  -->

<!-- - There seems to be a more specific point here than just that we can assess universality and precision. In fact, it's actually that we *can't* assess these things purely from structural models. We need probabilistic models or information on variable values, no? -->

<!-- - Universality seems an odd term for a concept that is continuous. Generality? Coverage? -->

<!-- - There was switching between precision and specificity. I've gone with precision, but could see either. -->



<!-- Better to replace type with variables.  -->

<!-- For example rather thatn $X \rightarrow Y \leftarrow Q$ where $Q$ takes on values of the four tyes.  -->
<!-- ROUGH NOTES -->
<!-- Say $Y = AX +B(1- X)$ in which case  uncertainty over functional forms, or  uncertainty about undefuned causal types  can be reconceptualized as uncertainty around positive and negative drivers -->


<!-- Drawing on different theories for subcomponents; eg theory of human decision making.  -->
<!-- More or less specified theories.  -->

<!-- Encapsulated CPDs are one way to wrap subtheories. Two identical DAGS could have two distinct theoretical underpinnings in the sense of having different encapsulated CPDs.    -->

<!-- When is one theory more general than another? When is one more specified than another? -->

<!-- If you specify a more detailed theory, the theory has more testable implications, but it is less general.  -->

<!-- Theory T'' is implied by theory T' if V'' is a subset of V' and the relations in T'' are implied by the relations in the reduced set T'' -->

<!-- For example:  -->
<!-- T'': A = 1 with prob .5, B = 1 with prob .5, if A = 1; 0 otherwise, C = 1 if B = 1 -->
<!-- T'': A = 1 with prob .5, C = 1 with prob .5, if A = 1; 0 otherwise -->
<!-- Different  subparts of theories may be implied by distinct theories provided implication relations satisfied -->
<!-- eg T'' part 1 may be implied by T, and T' implied by T'''', but T'' and T'''' not mutually consistent -->

<!-- Theory T'' explains more than T' if it identifies more causal relations -->
<!-- Theory T'' is more fertile  than T' if it implies more theories (same?) -->
<!-- Theory T'' has more observable implications that Theory T' if.... -->
<!-- Theory T'' is more falsifiable than theory T'.... -->
<!-- Theory T'' is more general (wide scope) than theory T' if its conditioning set is a  subset of theory T''s conditioning set -->
<!-- Theory T'' is more parsimonious  than theory T' if it predicts the same or more causal relations  with fewer nodes -->
<!-- Theory T'' is more complete (fully specified) than theory T' if it has lower prior variance (?) -->

<!-- Key -- there is no distinction between aleatory and epistemic uncertainty. It is all epistemic.  -->
<!-- eg the goalie might well randomize, but if we do not know which way she will go it is because we do not know  -->
<!-- what randomizing device she is using -->
<!-- Key: priors specified over all relations -->
<!-- Possibly need that nodes have classes -- eg utility -- that are used for some lo level theories -->
<!-- Question; ultimately is htere only one net and the universe is a realization of it? -->


<!-- SAME THING HERE. AN INTERESTING POINT, BUT A SEPARATE ENDEAVOR.

Functional (but not probabilistic)  causal models allow for the representation of logically derived relations between nodes without implying any unconditional empirical claims; that is, all claims may be of the  *if-then* variety, as is typical for example of propositions derived from game theoretic models. The process of connecting such models to the empirical claims can be thought of as the embedding of these incomplete models within larger structures.  -->

<!-- Consider for example the claim that in normal form games,  players play Nash equilibrium.  This claim in itself is not a tautology; that is, it is not a result. It can be contrasted for example with the *analytic result* that when rational players play a game and players have common knowledge of the game structure and of player rationality they will only play ''rationalizable'' strategies. Even still, the Nash claim does provide a set of analytically derived functional equations that relate nodes that describe game forms to actions  taken, and from actions to utilities. Representation as a causal graph can make explicit what conditional independencies are assumed in the move from analytic results to empirical claims. For example, are actions independent of the game form conditional on beliefs about the game form; are utilities independent of expectations conditional on actions, and so on. -->

<!-- We give an example of one such model below when we turn to extensive-form games for a lower-level theory that supports our running example.   -->


<!-- #### Relation to technical literature -->

<!-- Formally moving from lower level DAG to a higher level DAG requires *marginalization*: assessing the joint marginal distribution of observed nodes in the higher level graph over the distribution of unobserved nodes in the lower level graph.  -->
<!-- <!-- In Verma and Pearl 1990 (Equivalence and Synthesis of Causal Models) these higher level models are called "embedded causal models." --> 

<!-- Unfortunately there is no guarantee that the  margin of a distribution that is consistent with a lower level DAG will be consistent with any higher level DAG (techically, "DAGS are not closed under marginalization").  -->

<!-- In response, various types of richer graphs have been developed, such as "acyclic directed mixed graphs" (ADMGs) Maximal Ancestral Graphs (Spirtes and Richardson, Ancestral graph Markov models, 2002), or mDAGs (Evans 2015 (Graphs for Margins)). See also (Wermuth 2011) -->

<!-- ADMGs for example have directed and bidirected edges but no directed cycles and are closed under marginalization. -->

<!-- We use two approaches in our applications to engage with this problem. First we generally allow for *unobserved confounding* in models.  Second we will allow for the estimation of lower level models with unobserved nodes. -->



<!-- ### Integration of theory and empirics -->

<!-- The understanding of theory as a causal model has important implications for how we think about the relationship between theory and evidence and about theoretical development.  -->

<!-- In a common hypothetico-deductive framework, we formulate a theory *a priori*, derive its empirical predictions, and then test the theory by examining whether those predictions are borne out by the data. If the evidence does not line up with the predictions, the theory is falsified. We might then respond by amending the theory, but the amended theory must then be tested against new data. -->

<!-- As we will see in chapters to come, when we confront a causal model with data, we do not seek to confirm or falsify the model, but to update beliefs over its parameters. In particular, we can update beliefs about the (possibly joint) distribution of nodal types in the population of interest, potentially arriving a new theory.  -->

<!-- To illustrate the difference, consider @saunders2011leaders' argument about transformative foreign military intervention, discussed in Chapter \@ref(illustratemodels). As we noted, Saunders' argument---the particular causal effects operating across the causal linkages---represents one of roughly 4 million possible combinations of nodal types that could operate within the causal structure that the argument (on our reading, at least) assumes. For instance, her argument implies that the nodal type $\theta^B$ takes on a value such that the conditions $F=1, P=1,$ and $T=1$ generate $B=1$, and we get $B=0$ otherwise; that $\theta^I$ takes on a value such that $B=1, F=1$ and $B=0, F=0$ produce $I=1$, for either value of $A$; and $A$ has a positive effect on $I$ whenever $B \neq F$; and so on, for all endogenous nodes. It would make little sense to read Saunders' argument as implying that this combination of nodal types is the only one operating in the world. But we might think of the argument as implying that this combination appears with some frequency---it implies a belief that this combination of nodal types characertizes some substantial proportion of cases in the relevant population.  -->

<!-- If we were then to confront this model with data (observations of some set of nodes for some set of cases), we would be seeking not to *falsify* the theory but to learn about the distribution of nodal types, relative to some prior belief about this distribution. We could, for instance, start with only the causal structure and flat priors over all nodal types. Or we might start with Saunders' theoretical argument itself as a basis for placing greater prior weight on the nodal types implied by that argument. Updating based on new data then has the potential to shift our beliefs within this model's parameter space, toward beliefs in the prevalence of some types at each node and away from others. Our posterior estimates might suggest that the set of effects that Saunders' theorizes are more or less prevalent than we had *ex ante* believed. And, equally importantly, we can expect the very same kind of learning about all *other* nodal types that possible under this causal structure. That is, we can learn about the prevalence of effects across this causal structure that are *not* anticipated by Saunders' argument at all---such as, say, the prevalence of negative effects of $A$ on $I$ or of positive effects of $B \neq F$ on $I$s. -->

<!-- Of course, in updating over nodal types we are evaluating the relevance of Saunders' argument; if the nodal types implied by her theory turn out to have greater weight in our posteriors, her argument has in a sense been more successful in explaining the world than if those nodal types end up with less weight. But what we come away with is more than the assessment of "a" theory, understood as a single set of propositions about the way the world works. We learn about the world of the model as a whole---about all of the causal effects that it allows. On this view, we do not proceed via pure deduction, from *a priori* theory to empirical test. While our models might be informed by deductive reasoning (see Chapter appendix), we also *develop* theory through engagement with the data.  -->




<!-- If a lower level model is invoked to justify a higher level model then the relations implied by the lower level models must obtain in the lower level model. The lower level model must be consistent with the higher level model an vice versa. This consistency requirement relates both to structural and probabilistic relations.  -->

<!-- For instance, suppose we start with an $I \rightarrow D$ model and the belief that $Pr(\theta^D=\theta^D_{01}) = 0.5$ -- i.e., effects are positive $50\%$ of the time. On a structural level, as we have seen, this model could be supported by the model $I \rightarrow M \rightarrow D$. But what beliefs about causal effects in this lower-level model might support the belief that effects of $I$ on $D$ are positive $50\%$ of the time?  -->

<!-- As with structural mappings, we can imagine a whole range of probabilistic beliefs at the lower level that would be consistent with a given higher-level belief. In a $I \rightarrow M \rightarrow D$ model, we can arrive at a positive effect of $I$ on $D$ either through a combination of the nodal types $\theta_{01}^M$ and $\theta_{01}^{D_{\text lower}}$ (linked positive effects) or of the nodal types $\theta_{10}^M$ and $\theta_{10}^{D_{\text lower}}$ (linked negative effects). So, for instance, a lower-level model in which $Pr(\theta^M=\theta_{01}^M)=0.5$, $Pr(\theta^{D_{\text lower}}=\theta_{01}^{D_{\text lower}})=0.5$, $Pr(\theta^M=\theta_{10}^M)=0.5$, and $Pr(\theta^{D_{\text lower}}=\theta_{10}^{D_{\text lower}})=0.5$ would be consistent with the belief that $Pr(\theta^D=\theta^D_{01}) = 0.5$. So too, however, would a lower-level model in which $Pr(\theta^M=\theta_{01}^M)=0.707$, $Pr(\theta^{D_{\text lower}}=\theta_{01}^{D_{\text lower}})=0.707$, $Pr(\theta^M=\theta_{10}^M)=0$, and $Pr(\theta^{D_{\text lower}}=\theta_{10}^{D_{\text lower}})=0$ equally implies $Pr(\theta^D=\theta^D_{01}) = 0.5$ in the higher-level model.  -->

<!-- Meanwhile, there is a whole range of beliefs about effects in the lower-level model that do not justify the higher-level belief. For instance,  any lower-level model in which $(Pr(\theta^D=\theta^D_{00}) + Pr(\theta^D=\theta^D_{11})) > 0.5$ is inconsistent with the belief in a 0.5 probability of positive $I \rightarrow D$ effects and cannot serve as a theory of the higher-level model. -->

<!-- We will generally want to think of lower-level models with quite different distributional beliefs as representing different theories, as they will typically capture quite different substantive beliefs about how the world works. We can think of the first lower-level model, above, as one in which inequality can cause democratization via two mechanisms, either by causing mass-mobilization which causes democratizaiton or by preventing mass-mobilization which prevents democratization --- while the second model is a theory in which only the former mechanism operates. -->

<!-- #### Type consistency in a mediation model -->

<!-- We might then wonder *how* inequality might exert its effect on democratization. One possible answer, drawing on our model in Chapter \@ref(models) is that inequality can affect mass-mobilization, which in turn can affect democratization. This explanatory claim is visually represented in Panel (b) of the figure. Here, we can see that any effect of $I$ on $D$ runs through $M$. There are details of this graph that we will delve into later. But for now, it is sufficient to see that we have partly explained variation left unexplained by model (a). Model (b) allows us to say, for instance, that whether inequality has an effect on democratization has to depend on whether inequality has an effect on mobilization. Model (b) thus theorizes, in one important sense, a part of inequality's effect that is left untheorized in model (a).  -->

<!-- Whereas Model (a) has nodal types defined for $D$,^[All models also have a type defined for node $I$, but $\theta^I$ is unaffected by the movement between these models.] Model (b) has nodal types defined both for node $M$ and for node $D$. We thus allow $I$ to have a positive, negative, or no effect on $M$, with $\theta^M$ taking on four possible values, $\theta_{10}^M,\theta_{01}^M,\theta_{00}^M$,and $\theta_{11}^M$. Further, we allow for $M$ to have a positive, negative, or no effect on $D$, with $\theta^D_{\text{lower}}$'s possible values again being one of $\theta_{10}^{D_{\text lower}}$, $\theta_{01}^{D_{\text lower}}$, $\theta_{00}^{D_{\text lower}}$, $\theta_{11}^{D_{\text lower}}$. -->

<!-- We can now think about _combinations_ of nodal types in the lower-level model as mapping onto nodal types in the higher-level model. Table \@ref(tab:highlowmapping) illustrates. -->

<!-- |                    | $\theta_{10}^{D_{lower}}$  | $\theta_{01}^{D_{lower}}$  | $\theta_{00}^{D_{lower}}$  | $\theta_{11}^{D_{lower}}$  | -->
<!-- |:-------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:---------------------------:| -->
<!-- | $\theta_{10}^{M}$  | $\theta_{01}^{D_{higher}}$ | $\theta_{10}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- | $\theta_{01}^{M}$  | $\theta_{10}^{D_{higher}}$ | $\theta_{01}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- | $\theta_{00}^{M}$  | $\theta_{11}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- | $\theta_{11}^{M}$  | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | $\theta_{00}^{D_{higher}}$ | $\theta_{11}^{D_{higher}}$ | -->
<!-- Table: (\#tab:highlowmapping) Mapping from lower-level nodal types on $M$ and $D$ to higher-level nodal types on $D$.  -->

<!-- For instance, in a case in which both $\theta^M=\theta^M_{01}$ (there is a positive effect of $I$ on $M$) and $\theta^{D_{\text{lower}}}=\theta_{01}^{D_{lower}}$ (there is a positive effect of $M$ on $D$), we have a positive effect of $I$ on $D$---meaning that, in the _higher-level_ model, $\theta^{D_{higher}}=\theta^{D_{higher}}_{01}$. Two linked *negative* effects at the lower level likewise generate a positive effect of $I$ on $D$---and so map onto the same higher-level nodal type, $\theta^{D_{higher}}=\theta^{D_{higher}}_{01}$.  -->

<!-- Further, it is easy to see that if there is no causal effect at _either_ the $I \rightarrow M$ step _or_ the $M \rightarrow D$ step, we will have one of the null effect types at the higher level. This is because, in Model (b), $I$ cannot affect $D$ unless there are causal effects at both constituent steps. In other words, in Model (b), $I$ can affect $D$ _only_ through $M$ in this model; there are no direct effects or other pathways permitted. -->

<!-- To foreshadow the discussion in later chapters, these mappings of nodal types between levels are critical. They allow us to draw inferences about causal relations at a lower level and then *use* those inferences to answer questions posed at a higher level. For instance, if we can learn about the effect of inequality on mass mobilization (a question posed at the lower level), then we can apply that learning to answering a question about whether inequality affects democratization (a higher-level question). -->

<!-- The lower-level functional equations are formally similar though now each unit's outcome (given $X$) depends on two event probabilities: one that determines type with respect to the effect of $X$ on $K$ ($t_{ij}^{K}$), and one with respect to the effect of $K$ on $Y$ ($u_{ij}^{Y}$): -->

<!-- $$Y(K, u_{ij}^{Y}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } K=0 \\ j & \text{ if } K=1 \end{array}  \right.$$ -->
<!-- $$K(X, u_{ij}^{K}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } X=0 \\ j & \text{ if } X=1 \end{array}  \right.$$ -->

<!-- Thus, in the lower-level model, there are sixteen types that derive from the cross product of two independent random terms. -->

<!-- Critically, one can derive the higher-level types from the lower level types, and beliefs about the higher level types from beliefs about the lower level types. For example, using the nomenclature in @humphreys2015mixing: -->

<!-- \begin{eqnarray*} -->
<!-- \text{adverse: }u_{10}^{high} &=& u_{01}^{K}\&u_{10}^{Y} \text{ or } u_{10}^{K}\&u_{01}^{Y} \\ -->
<!-- \text{beneficial: }u_{01}^{high} &=& u_{01}^{K}\&u_{01}^{Y} \text{ or }  u_{10}^{K}\&u_{10}^{Y} \\ -->
<!-- \text{chronic: } u_{00}^{high} &=& u_{00}^{Y} \text{ or }  u_{00}^{K}\&u_{01}^{Y} \text{ or }  u_{11}^{K}\&u_{10}^{Y}\\ -->
<!-- \text{destined: }u_{11}^{high} &=& u_{11}^{Y} \text{ or }  u_{00}^{K}\&u_{10}^{Y} \text{ or }  u_{11}^{K}\&u_{01}^{Y} -->
<!-- \end{eqnarray*} -->

<!-- In the same way, the higher-level probabilities are implied by the lower level probabilities. -->

<!-- \begin{eqnarray*} -->
<!-- \text{adverse: }\lambda_{10}^{high} &=& \lambda_{01}^{K}\lambda_{10}^{Y} + \lambda_{10}^{K}\lambda_{01}^{Y} \\ -->
<!-- \text{beneficial: }\lambda_{01}^{high} &=& \lambda_{01}^{K}\lambda_{01}^{Y} + \lambda_{10}^{K}\lambda_{10}^{Y} \\ -->
<!-- \text{chronic: } \lambda_{00}^{high} &=& \lambda_{00}^{Y} + \lambda_{00}^{K}\lambda_{01}^{Y} + \lambda_{11}^{K}\lambda_{10}^{Y}\\ -->
<!-- \text{destined: }\lambda_{11}^{high} &=& \lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y} -->
<!-- \end{eqnarray*} -->

<!-- Importantly, even without specifying a distribution over $U_K$ or $U_Y^{\text{lower}}$, a lower-level structural model could be informative by restricting the *ranges* of  $U_K$ or $U_Y^{\text{lower}}$. For instance, a lower level theory that imposed a monotonicity condition (no adverse effects) might exclude $t^K_{10}$ and $t^y_{10}$---that is, increasing $X$ never reduces $K$, and increasing $K$ never reduces $Y$.  -->

<!-- We return  to this example below and show how observation  of $K$ can yield inference on causal estimands when  the theory places this kind of a structure on relationships. -->

<!-- #### Type consistency in a moderation model -->

<!-- Alternatively, we might wonder *why* or under what conditions inequality causes democratization. Our simple claim, in panel (a), allows that $I$ *can* cause $D$, but provides no information about the conditions under which it does so. Those conditions are implicitly embedded within $\theta^D$, where they are left unspecified. We could, however, theorize some of what is left unsaid in in panel (a). We do this in panel (c), where we posit ethnic homogeneity ($E$) as a moderator of  inequality's effect on democratization. Panel (c) represents a theory of panel (a) in that it can help account for variation in causal effects that is unaccounted for in Model (a). -->


<!-- Put differently, Model (c) gives substantive meaning to an aspect of the phenomenon that is merely residual variation in Model (a). Model (a) provides no account of why inequality has the effects it does, relying fully on $\theta^D$ as a placeholder for this uncertainty. In Model (c), $\theta^D$ plays a more modest role, with the substantive variable of ethnic homogeneity now doing some of the work of determining inequality's possible effects.  -->


<!-- We note one final possibility. Imagine that we started with the quite *specific* claim that inequality sometimes has a positive effect on democratization and sometimes has no effect (with democratization happening for other reasons). Suppose we believed this claim to be true for some, possibly not well defined, domain of cases.^[This claim could be graphically represented by panel (a), but would involve a more restricted range for $\theta^D$ and simpler functional equation, involving only two types.] Model (c) could serve as a theory of this more specific claim in that Model (c), paired with some data, could explain the claim. In particular, Model (c) paired with the data $E=1$---we are in an ethnically homogeneous context---produces the more specific claim. Here, it is the theory *plus an observation* of context that accounts for the specific claim.  -->

<!-- Similarly, take the functional equation $f_1: Y=X_1X_2$. Coupled with data $X_2=1$, $f_1$ implies the functional equation $f_2: Y=X_1$.  -->


<!-- ### Causal types in lower level models  -->

<!-- We have discussed theorization largely from a graphical perspective, showing how features of causal graphs change (nodes get split, combined, added, or removed) as we move down or up levels. But there is more that happens beneath the surface of a graphical structure when we theorize a claim: the space of causal types itself changes. We walk through how this works for the mediation and moderation theories described above.  -->

<!-- ### Mediation {#medtheory} -->

<!-- We begin with a simple claim: there are two binary variables, $X$ and $Y$, and $X$ may have an effect on $Y$. This claim is represented in Figure \@ref(fig:Highlow)(a) above. In this graph, $X$ is independent of $\theta^Y$, which means that it is as if $X$ is randomly assigned. -->

<!-- We will let $\theta^Y$ be a variable that ranges across our four different causal types, conditioning how $Y$ responds to $X$. While  $a, b, c$, and $d$ were heuristically useful as a way of introducing the  idea of a causal type, things will soon get more complicated, so it will be useful to have more flexible notation. Going forward, we will usually refer to causal types using $\theta$ notation, with subscripts and superscripts used to denote potential outcomes and outcome variables. In our binary $X \rightarrow Y$ setup, we can indicate the causal type governing $Y$'s response with notation of the form $\theta^Y_{ij}$, where $i$ and $j$ represent $Y$'s potential outcomes. Specifically, $i$ represents the value $Y$ takes on when $X=0$, while $j$ represents the value $Y$ takes on when $X=1$.^[The functional equation for $Y$ is then given by:  -->
<!-- $$Y(x, \theta_{ij}^{Y_\text{higher}}) = \left\{ \begin{array}{cc}   -->
<!-- i & \text{ if } x=0 \\ j & \text{ if } x=1 \end{array}  \right.$$] Thus, the translation from $a, b, c$ and $d$ notation is: -->


<!-- * *a*: $\theta_{10}^Y$. A negative effect implies that $Y$ is $1$ when $X=0$ and $0$ when $X=1$. -->
<!-- * *b*: $\theta_{01}^Y$. A positive effect implies that $Y$ is $0$ when $X=0$ and $1$ when $X=1$. -->
<!-- * *c*: $\theta_{00}^Y$. A null "chronic" effect implies that $Y$ is $0$ regardless of $X$'s value. -->
<!-- * *d*: $\theta_{11}^Y$. A null "destined" effect implies that $Y$ is $1$ regardless of $X$'s value. -->


<!-- To be clear, these $\theta_{ij}^Y$ terms are not random variables; they are the four _values_ (types) that the type-variable $\theta^Y$ can take on. -->

<!-- represented with the notation $u_{ij}$: we read the subscripts to mean that a unit of type $u_{ij}$ has outcome $i$ when $X=0$ and $j$ when $X=1$. Then let $u_Y^{higher}$ have a multinomial distribution over the four values of  $u_{ij}$ with event probabilities  $\lambda_{ij}^{higher}$. ; for example, let $u_X\sim \text{Unif}[0,1]$ and $X = \mathbb{1}(u_K<\pi^K)$. -->


<!-- For example if $U_Y^{higher}$ is distributed normally and $Y$ takes on the value 1 if $bX+u_Y^{higher}$ is above some threshold, we have a probit model.  -->


<!-- Now consider a theory that specifies a variable intervening between $X$ and $Y$. This theory is depicted in Figure \@ref(fig:Highlow)(b) above, where $M$ mediates the relationship. We see that there are now two $\theta$ terms, each representing a set of causal types for a different step in the causal chain. While $\theta^Y$ represented $Y$'s response to its parent $X$, $\theta^Y_{\text{lower}}$ represents $Y$'s response to its "new" parent, $M$. We now also need to conceive of a causal type capturing $M$'s response to $X$, and we let $\theta^M$ represent this type.^[This graph assumes no confounding in the mediating relationship either as the two $\theta$ terms and $X$'s assignment are all independent of one another.] -->



<!-- Now consider the alternative lower-level theory in which  $E$ is posited as a second parent of $D$. This graph contains the substantive assumptions that $E$'s value is determined independently of $I$'s, as well as the assumption that $I$ and $E$ are both as-if randomly assigned. -->

<!-- In this graph, we again have a $\theta_D^{\text{lower}}$ term, but it is a different object from $\theta_D^{\text{lower}}$ in the mediation graph. In this moderation model, $\theta_D^{\text{lower}}$ is more complex as it determines the mapping from two binary variables into $D$. $D$'s nodal type in this setup now represents how a case's outcome will respond to four different possible combinations of $I$ and $E$ values. Rather than four nodal types for $D$, we now have 16, as there are 16 possible ways in which one binary variable might respond to two binary parent variables (see Table  \@ref(tab:PO16) in Chapter \@ref(models)). -->

<!-- In Table \@ref(tab:PO16b) we give a mapping from a subset of these lower-level types to the higher-level types corresponding to Model (a).  -->



<!-- ----------------------------------------------------------------------------------------------------------------------------------- -->
<!--    Lower Type                  $I=0,E=0$     $I=0,E=1$     $I=1,E=0$      $I=1, E=1$    Higher Type -->
<!-- -------------------------  ------------  -------------  -------------  -------------  ------------------------------------------------ -->
<!-- $\theta^{D}_{0000}$            0           0             0             0              $\theta^{D}_{00}$ -->

<!-- $\theta^{D}_{0001}$            0           0             0             1              $\theta^{D}_{01}$ if $E=1$, else $\theta^D_{00}$  -->
<!-- $\theta^{D}_{0010}$            0           0             1             0              $\theta^D_{00}$ if $E=1$, else $\theta^D_{01}$ -->

<!-- $\theta^{D}_{0011}$            0           0             1             1              $\theta^{D}_{01}$ -->

<!-- $\vdots$                       $\vdots$    $\vdots$      $\vdots$      $\vdots$       $\vdots$      -->

<!-- $\theta^{D}_{1110}$            1           1             1             0              $\theta^D_{11}$ if $E=0$, else $\theta^D_{10}$ -->

<!-- $\theta^{D}_{1111}$            1           1             1             1              $\theta^D_{11}$ -->
<!-- ----------------------------------------------------------------------------------------------------------------------------- -->
<!-- Table: (\#tab:PO16b) Values for $D$ given $E$ and $I$. With two binary causal variables, there are 16 nodal types: 16 ways in which $D$ depends on $I$ and $E$. These lower level types map into higher level types for a model in which $D$ depends on $I$ only, as shown in the final column.  -->


<!-- To illustrate, $\theta_Y^{\text{lower}}=$: -->


<!-- * $\theta_{00}^{11}$ means that $I$ has no effect under any value of $E$, and $E$ has a positive effect under any value of $I$.  -->
<!-- * $\theta_{10}^{10}$ implies that $I$ always has a negative effect, and $E$ never has an effect.  -->
<!-- * $\theta_{01}^{11}$ represents one kind of conditional effect: $I$ has a positive effect only when $E=0$, and $E$ has a positive effect only when $I=0$. -->


<!-- Importantly we see that the mapping between lower- and higher-level types can depend on the value of the moderator. More generally, since we can think of the value of exogeneous nodes, $E$ and $I$, as being nodal types for those nodes,^[In other words, saying $E=1$ in this model is the same as saying $\theta^{E}=\theta^{E}_{1}$. An exogenous nodes nodal type *is* the value to which it has been exogenously "assigned."] we can think of the lower level nodal type as a concatenation of the higher-level nodal types for $E$ and $D$. Thus, we can think of the the higher-level type as depending uniquely on the fully specified lower level type. -->

<!-- For instance, a case can have type $\theta^D_{01}$ in the higher-level model if it is of type $\theta^D_{0010}$ _and_ c in the lower-level model. This is a case for which $I$ has a positive effect on $D$ when $E=0$ _and_ in which $E$ _is in fact_ 0. On the other hand, the same lower-level $D$ type  in combination with $\theta^{E}_1$ maps onto the type $\theta_{10}$ in the higher-level model---a type in which $D$ responds *negatively* to $I$.  -->

<!-- In later chapters, we represent all lower- to higher-level mappings relevant to a question of interest with the use of "type-reduction" tables that allow one to readily see how inferences drawn at one level inform causal questions posed at another level. -->

<!-- We let $u_Y^{\text{lower}}$ in this graph denote a multinomial distribution over the sixteen values of  $u_{ij}^{gh}$ with event probabilities  $\lambda_{ij}^{gh}$. -->

<!-- I changed abcd scripts above to ghij and made corresponding (I think) changes below. I don't care what it is but abcd obviously could be confusing in this context. -->

<!-- The sixteen types are illustrated in Table \@ref(tab:types2X) in the appendices. -->

<!-- Again, the types in the higher level mapping are functions of the types in the lower-level mapping. For example,  a unit has type $u_{01}$ in the higher level model if $K=1$ and it is of type $u_{00}^{01},u_{10}^{01},u_{01}^{01}$, or $u_{11}^{01}$, or if $K=0$ and it is of type $\lambda_{01}^{00},\lambda_{01}^{10},\lambda_{01}^{01}$, or $\lambda_{01}^{11}$.  -->

<!-- We write this as: -->

<!-- $$u_{01} =  ((K=1) \land (t^{lower} \in \{u_{00}^{01} \cup u_{10}^{01} \cup  u_{01}^{01} \cup u_{11}^{01} \}) \lor  ((K=0) \land (t^{lower} \in \{\lambda_{01}^{00} \cup \lambda_{01}^{10} \cup \lambda_{01}^{01} \cup \lambda_{01}^{11}\})$$ -->

<!-- In the same way, the probability of type $u_{01}$ can be written in terms of the parameters of the lower-level graph.  Importantly, the parameters of the higher-level distribution  $u_Y^{higher}$ depend on both $u_K$ and $u_Y^{\text{lower}}$. Thus, unlike the mediation case above, the probative value depends on the likelihood of an *observable* event occurring. Specifically, the share of a given higher-level type is given by: -->

<!-- $$\lambda_{ij} = P(u_Y^{higher} = u_{ij}) = \pi^K\left(\lambda_{00}^{gh}+\lambda_{10}^{gh}+\lambda_{01}^{gh}+\lambda_{11}^{gh}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{ij}^{00}+\lambda_{ij}^{10}+\lambda_{ij}^{01}+\lambda_{ij}^{11}\right)$$ -->

<!-- For example: -->

<!-- $$\lambda_{00} = P(u_Y^{higher} = u_{00}) = \pi^K\left(\lambda_{00}^{00}+\lambda_{10}^{00}+\lambda_{01}^{00}+\lambda_{11}^{00}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{00}^{00}+\lambda_{00}^{10}+\lambda_{00}^{01}+\lambda_{00}^{11}\right)$$ -->


<!-- Conditional probabilities follow in the usual way. Consider, for instance, the case where it is known that $X=Y=1$ and so the posterior probability of type $u_{01}$ is simply $P(i \in u_{01} | X=Y=1) = \frac{\lambda_{01}}{\lambda_{01}+\lambda_{11}}$. Note that $\pi^x$ does not appear here as this $X$ is orthogonal to $u_Y$. The probability of type $u_{01}$, knowing that $X=Y=1$, can be written in terms of the parameters of the $u$ distributions in the lower-level graph.  -->

<!-- $$P(i \in u_{01} | X=Y=1) = \frac{ -->
<!-- \pi^K\left(\lambda_{00}^{01}+\lambda_{10}^{01}+\lambda_{01}^{01}+\lambda_{11}^{01}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{01}^{00}+\lambda_{01}^{10}+\lambda_{01}^{01}+\lambda_{01}^{11}\right) -->
<!-- }{ -->
<!-- \sum_{i = 0}^1\left(\pi^K\left(\lambda_{00}^{i1}+\lambda_{10}^{i1}+\lambda_{01}^{i1}+\lambda_{11}^{i1}\right) -->
<!-- + -->
<!-- (1-\pi^K)\left(\lambda_{i1}^{00}+\lambda_{i1}^{10}+\lambda_{i1}^{01}+\lambda_{i1}^{11}\right) -->
<!-- \right)}$$ -->

<!-- We return below to this example and describe how the lower-level model can be used to generate inferences on relations implied by the higher level model.  -->





<!-- ## Rules for moving between higher- and lower-level models -->

<!-- Thinking about models as conditionally nested within one another can be empirically useful. It provides a way of generating empirical leverage on a causal question by plumbing more deeply our background knowledge about a domain of interest. When we more fully specify higher-level claims via a more elaborate, lower-level model, we are a making explicit unspecified conditions on which the higher-level relationships depend. In doing this, we are identifying potentially observable nodes that might be informative about our research question. -->


<!-- As we develop lower-level models to support our claims, or determine which claims are supported by our theories, what kinds of moves are we permitted to make?  -->

<!-- ## Illustration of unpacking nodal types -->

<!-- We now show more specifically how sets of nodal types in lower-level models map into nodal types in higher-level models.  -->

<!-- For concreteness, let us return to our democratization example and consider first the very basic claim that inequality can have an affect on democratization. We represent this simple claim in Figure \@ref(fig:demtheory5), Panel (a). In this simple model, $I$ may sometimes have an effect on $D$ and sometimes not; and that effect may be positive or negative. $I$'s effect will, of course, depend on the case's nodal type on $D$.  -->

<!-- ```{r demtheory5, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='80%', fig.cap = "DAG representations of three theories. DAGs only capture claims that one variable causes another, conditional on other variables. Theories (b) and (c) each imply theory (a)."} -->

<!-- par(mfrow = c(3,1)) -->
<!-- par(mar=c(1,1,3,1)) -->
<!-- hj_dag(x = c(1,2,2), -->
<!--        y = c(1,1,2), -->
<!--        names = c( -->
<!--          expression(paste(I)), -->
<!--          expression(paste("D")),   -->
<!--          expression(paste(theta[D]))), -->
<!--        arcs = cbind( c(1, 3), -->
<!--                      c(2, 2)), -->
<!--        title = "(a) A Claim: Inequality Causes Democratization", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .1 -->
<!-- ) -->

<!-- hj_dag(x = c(1,2,2, 1.5, 1.5), -->
<!--        y = c(1,1,2, 1  , 2), -->
<!--        names = c( -->
<!--          expression(paste(I)), -->
<!--          expression(paste("D")),   -->
<!--          expression(paste(theta[D])), -->
<!--          expression(paste(M)), -->
<!--          expression(paste(theta[M]))  -->
<!--          ), -->
<!--        arcs = cbind( c(1, 3, 5, 4), -->
<!--                      c(4, 2, 4, 2)), -->
<!--        title = "(b) A Theory: How Inequality Causes Democratization", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .1 -->
<!-- ) -->

<!-- hj_dag(x = c(1,2, 2, 1.5), -->
<!--        y = c(1,1, 2, 2), -->
<!--        names = c( -->
<!--          expression(paste(I)), -->
<!--          expression(paste("D")),   -->
<!--          expression(paste(theta[D])), -->
<!--          expression(paste(E))  -->
<!--          ), -->
<!--        arcs = cbind( c(1, 3, 5, 4), -->
<!--                      c(2, 2, 4, 2)), -->
<!--        title = "(c) Another theory: When Inequality Causes Democratization", -->
<!--        add_functions = 0,  -->
<!--        contraction = .16,  -->
<!--        padding = .1 -->
<!-- ) -->

<!-- ``` -->


<!-- Next, the figure shows two models that can each *explain* Model (a), though in different ways. Model (b) answers the explanatory question, "*How* does inequality affect democratization?" Model (c) answers the explanatory question, "Why does inequality's effect on democratization vary?" or "Under what conditions does $I$ have a given effect on $D$?" Both theories provide richer, more interpretable accounts of the phenomenon of interest than the simpler model that they are theorizing. -->

<!-- These lower-level models also imply a set of nodal types that are richer than that implied by (a). Recall that in Chapter \@ref(models), we considered the idea that at any node, a nodal type may be conceptualized as a case-specific exogenous disturbance that governs the mapping from input variables to outcome variables. The type node $\theta^D$ can take on the values $\theta^D_{10}, \theta^D_{01}, \theta^D_{00}, \theta^D_{11}$, and this node can take on different values in different cases. However, differences in $\theta^D$'s value are left entirely unaccounted for. We are saying nothing about *why* inequality causes democratization in some places, prevents democratization in other places, and has no effect in still other places.  -->

<!-- Let us consider how Models (b) and (c) provide answers to these questions and how these answers map onto these models' nodal types. -->


<!-- <!-- To fix this idea going forward, we make a shift in notation and use $\theta$ to indicate that a node is a receptacle for causal types. Thus, $\theta_D$ here captures the case's causal type, or $I$'s causal effect on $D$ for a given case.  --> 

<!-- In particular, if we  deploy our four-causal-type function from Chapter \@ref(models) we have:  -->

<!--   * $a$: $\theta^D=\theta^D_{10}$, then $D=1-I$ ($I$ has a negative effect on $D$) -->
<!--   * $b$: $\theta^D=\theta^D_{01}$, then $D=I$ ($I$ has a positive effect on $D$) -->
<!--   * $c$: $\theta^D=\theta^D_{00}$, then $D=0$ ($I$ has no causal effect) -->
<!--   * $d$: $\theta^D=\theta^D_{11}$, then $D=1$ ($I$ has no causal effect) -->

<!-- Knowing $\theta$ tells us how $D$ responds to $I$ and it ignores any heterogeneity between units as long they respond in the same way. For any causal type the model is *consistent* with $I$'s causal effect operating for different reasons for different units, but  -->



<!-- ### Implied consistency of priors -->

<!-- We caution that the mappings of distributional beliefs between levels is not always intuitive. Suppose, for instance, that we begin with no information about causal effects in a model and so want to set flat priors, meaning that we accord equal prior weight to all nodal types at each node. In Model (a), flat priors would mean putting an 0.25 weight on each of the following: positive effects, negative effects, zero effects with $D$ always $0$, and negative effects with $D$ always $1$. Now, suppose we engage in the same flat-prior-setting in the lower-level model, Model (b). That is, we put equal weight (i.e., 0.25 across the board) on all nodal types at node $M$ and equal weight (0.25 across the board) on all nodel types at node $D$. Surprisingly perhaps, setting flat priors at this lower level in fact implies a strong weighting *against* either positive or negative causal effects at the *higher* level. Now, we are saying that the probability of a positive effect at the higher level is $(0.25 \times 0.25) + (0.25 \times 0.25) = 0.125$; and likewise for negative effects.  -->

<!-- Put simply, the fact that the lower-level model involves more mediating steps between $I$ and $D$ means that more things have "line up"  for an $I \rightarrow D$ effect to emerge --- and so causal effects will be rarer under a flat distribution of nodal types in this model than they are in the simpler, higher-level model. Another way to think about this is that simply by spelling out the mediating steps in a causal chain, we can, perhaps indadvertently, generate beliefs that causal effects at the higher level are weaker than we might have thought. We can, of course, set priors at the lower level that would map onto flat priors at the higher level.^[Placing roughly 0.35 weight on a positive effect at each mediating step and 0.35 weight on a negative effect at each step implies approximately a 0.25 probability of a positive $I \rightarrow D$ effecta and a 0.25 probability of a negative $I \rightarrow D$ effect.] Our advice to researchers is, simply, to check for consistency of priors across levels: to ask whether the priors that we set at a lower level imply beliefs at the higher level that we are willing to live with.  -->





## Gains from theory {#theorygains}

We now turn to consider how to think about whether a theory is *useful*. We are comfortable with the idea that theories, or models more generally, are wrong. Models are not full and faithful reflections of reality; they are maps designed for a particular purpose.  We make use of them because we think that they *help* in some way.


<!-- In the case of causal models, the purpose is to capture relationships of independence and possible causal dependence. As we have shown, that is a purpose that allows for the stripping away of detail---though it also forbids certain simplifications (such as any simplification that removes a dependency between variables).  -->


But how do they actually help, and can we quantify the gains we get from using them?


We think we can. Using the notion of hierarchies of models, imagine we begin with model $\mathcal M_1$, which together with data $\mathcal D$, implies claim $\mathcal M_2$. We then posit theory $\mathcal M_0$ of $\mathcal M_1$, so $\mathcal M_0$ implies $\mathcal M_1$. But when we bring $\mathcal D$ to $\mathcal M_0$ we get a new model, $\mathcal M_2'$, that is different---and, hopefully, better---than $\mathcal M_2$. Our gain from theory $\mathcal M_0$ should be some summary of how much better $\mathcal M_2'$ is than $\mathcal M_2$.

Here is an illustration using a theory that allows use of the "front-door criterion." 


Imagine that we have data on three variables, $X$, $Y$, and $K$. We begin, however, with a model $\mathcal M_1$ with confounding: $C \rightarrow X \rightarrow Y \leftarrow C$. $\mathcal M_1$ includes nodes for two of the three variables we have data on, $X$ and $Y$, but not $K$. Assume, further, that we do not have data on node $C$, the confound. 

Suppose that we observe a strong correlation between $X$ and $Y$ and infer $\mathcal M_2$: that $X$ is a likely cause $Y$. Our inference under $\mathcal M_2$ is, however, quite uncertain because we are aware that the correlation may be due to the confound $C$.

Suppose now that we posit the lower-level model $\mathcal M_0$: $C \rightarrow X \rightarrow K \rightarrow Y \leftarrow C$. $\mathcal M_0$ now lets us make better use of data $K$. If we observe, for instance, that $X$ and $K$ are uncorrelated, then we infer with confidence that in fact $X$ did not cause $Y$, despite the correlation. 

Thus, in return for specifying a theory of $\mathcal M_1$, we have been able to make better use of data and form a more confident conclusion. In this case, stating the theory, $\mathcal M_0$, does not alter our *priors* over our query. Our prior over the effect of $X$ on $Y$ may be identical under $\mathcal M_1$ and $\mathcal M_0$---but our conclusions, given data, differ because the theory lets us make use of the data on $K$, which we could not do under $\mathcal M_1$ (which did not include $K$).


In other situations, we might imagine invoking a theory that does not necessarily involve new data but that allows us to make different, perhaps tighter inferences using the same data. An example might be the invocation of a type-reducing theory that involves a monotonicity restriction or exclusion restriction that allows for identification of a quantity that would not be identifiable without the theory.    

```{r, echo = FALSE,  eval = FALSE}

M_1 <- make_model("C -> X -> Y <- C")

M_0 <- make_model("C -> X -> M -> Y <- C")

```

Thus, one reason to theorize our models --- develop lower-level models that make stronger claims --- is to be able to reap greater inferential leverage from the more elaborate theory. 

But are we, in fact, better off? 

We might imagine answering the question in different ways: from an internal or external position, and from an *ex ante* or *ex post* perspective.

In all cases we ask how much better do we do as a result of making use of a theory. 

If we are willing to posit an external ground truth, then we can define "better" in objective terms. An *ex post*, objective way of operationalizing "better" is to assess the size of the error we make relative to the ground truth, from an inference that uses a theory, compared to an inference that does not make use of the theory. An objective *ex ante* approach might be to ask what the expected error is from conclusions one draws given a theory. For instance: how wrong are we likely to be if we base our best guess on our posterior mean? "How wrong" might be operationalized in terms of mean squared error---the square of the distance between the truth and the posterior mean.^[Other loss functions could be used, including functions that take account of the costs of collecting additional data or to the risks associated with false diagnoses. For instance, in @heckerman1991toward, an objective function is generated using  expected utility gains from diagnoses generated based on new information over diagnoses based on what is believed already. In their treatment [@heckerman1991toward, Equation 6],  the expected value of new information $K$, given existing information $W$ is: $\sum{K}P(K|W)( EU(d(Q,W,K)|W, K) - EU(d(Q, W)|W, K))$ where $EU$ is expected utility and $d$ is the optimal inference (diagnosis) given available data. Note that the diagnosis can take account of $K$ when it is observed, but the expected utility depends on $K$ whether or not it is observed, as $K$ carries information about the state of interest.] 
 
A more subjective approach would be to ask about the reductions in posterior variance. *Ex post* we can define "better" as the reduction in posterior variance from conclusions that make use of a theory compared to conclusions that do not. We might also think about  the *expected posterior variance*: how certain do you expect you will be after you make use of this new information?

More formally, imagine a situation in which there is an unknown parameter $q$ and we have a data strategy that produces a distribution over data $k$, given $q$. Let $p(q,k)$ denote the joint prior distribution over $q$ and  $k$ with marginal distributions $p(k)$ and $p(q)$. For any $k$ there is posterior estimate $q_k$.

The squared error, given $k$ is just $(q - q_k)^2$.

The *expected* squared error is:


$$ESE := \int_q\int_k \left({q}_k-q\right)^2p(k, q)dkdq $$

This takes the error one might get with respect to any true value of the parameter ($q$), given the data one might see given $q$ and the inferences one might draw.  

For any $k$ we might write the  posterior variance as $v_k$. 

The *expected* posterior variance can be written:

$$EV := \int_k v_k p(k)dk$$

This takes the posterior variance, given some data, over all the possible data one might see given marginal distribution $p(k)$.

Interestingly, if we assess expectations using the same priors as you use for for forming posteriors the expected posterior variance and expected squared error  are equivalent [@scharf1991statistical].
To see this, we take advantage of the fact that $p(q,k) = p(k)p(q|k) = p(q)p(k|q)$ and that $p(q|k)$ gives the posterior distribution of $q$ given $k$. We then have:


<!-- $$ -->
<!-- \begin{eqnarray} -->
<!-- ESE &=& \int_q\int_k \left({q}_k-q\right)^2p(q,k)dkdq \\ -->
<!--     &=& \int_k\int_q \left({q}_k-q\right)^2p(q,k)dq dk \\ -->
<!--     &=& \int_k\int_q \left({q}_k-q\right)^2p(k)p(q|k)dq dk \\ -->
<!--     &=& \int_k\int_q \left({q}_k-q\right)^2p(q|k)dq p(k)dk \\ -->
<!--     &=& \int_k\left[\int_q \left({q}_k-q\right)^2p(q|k)dq\right]p(k)dk \\ -->
<!--     &=& \int_k v_k p(k)dk \\ -->
<!--     & = & EV -->
<!-- \end{eqnarray} -->
<!-- $$ -->

$$
\begin{eqnarray}
ESE &=& \int_q\int_k \left({q}_k-q\right)^2p(q,k)dkdq \\
    &=& \int_k\int_q \left({q}_k-q\right)^2p(k)p(q|k)dq dk \\
    &=& \int_k\left[\int_q \left({q}_k-q\right)^2p(q|k)dq\right]p(k)dk \\
    &=& \int_k v_k p(k)dk  = EV
\end{eqnarray}
$$

Note that the key move is in recognizing that $p(q |k)$ is in fact the posterior distribution on $q$ given $k$. In using this we assume that the same distribution is used for assessing error and for conducing analysis---that is we take the researcher's prior to be the relevant one for assessing error.

Moreover, it is easy to see that whenever inferences are sensitive to $K$, the *expected* variance of the posterior will be lower than  the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of $K$, given what is already known from observation of $W$.^[See @raiffa1961applied. A similar expression can be given for the expected posterior variance from learning $K$ in addition to $W$ when $W$ is not yet known. See, for example, Proposition 3 in @geweke2014analysis.]  
$$Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))$$

However, although *expected* posterior variance goes down, it is still always possible that posterior variance rises. The increase in uncertainty does not, however, mean you haven't been learning. Rather, you have learned that things aren't as simple as you thought.

<!-- For illustration say that it is known that $X=1, Y=1$ and that, given this information (playing the role of $W$), the posterior probability that a unit is of type $b$---for whom $Y$ would be 0 were $X=0$ (and not type $d$, for which $Y$ would be 1 regardless) is $p$. Say then that that under some theory we have $\phi_b := \Pr(K=1 | Y(0)=0, Y(1)=1, X=1)$, $\phi_d := \Pr(K=1 | Y(0)=1, Y(1)=1, X=1)$.  -->

<!-- Then what is the value added of this theory? Define $Q$ here as the query regarding whether the unit is a $b$ type. Then the prior variance, $Var(Q|W)$, is simply $p(1-p)^2 +(1-p)p^2 = p(1-p)$.  -->

<!-- <!-- Would be best to  write down the theory as a structural equation that has phi_j as p(K=1|j) --> 

<!-- To calculate $E_{K|W}(Var(Q|K,W))$, note that the posterior if $K$ is observed is $\frac{\phi_bp}{\phi_bp+\phi_d(1-p)}$. Let us call this $\hat{q}_K$, and the belief when $K$ is not observed $\hat{q}_{\overline{K}}$. -->
<!-- In that case the  *expected error* is:  -->

<!-- $$\text{Expected Error} = p\phi_b\left(1-\hat{q}_K\right)^2+(1-p)\phi_d\hat{q}_K^2+p(1-\phi_b)\left(1-\hat{q}_{\overline{K}}\right)^2+(1-p)(1-\phi_d)\hat{q}_{\overline{K}}^2$$ -->

<!-- where the four terms are the errors when $K$ is seen for a $b$ type, when $K$ is seen for a $d$ type, when $K$ is not seen for a $b$ type, and when $K$ is not see for a $d$ type. -->


<!-- Defining $\rho_K = (p\phi_b+(1-p)\phi_d)$ as the probability of observing $K$ given the prior, we can write the posterior variance as: -->

<!-- $$\text{Expected Posterior Variance} = \rho_K\hat{q}_K(1-\hat{q}_K)+(1-\rho_K)\hat{q}_{\overline{K}}(1-\hat{q}_{\overline{K}})$$ -->


<!-- <!-- Making use of the fact that $\rho_K\hat{q}_K = ({\phi_bp+\phi_d(1-p)})\frac{\phi_bp}{\phi_bp+\phi_d(1-p)} = \phi_bp$ and similarly  --> 
<!-- <!-- $(1-\rho_K)\hat{q}_{\overline{K}} = (1-\phi_b)p$, this can be written in terms of primitives as: --> 

<!-- With a little manipulation, both of these expressions simplify to: -->

<!-- $$\text{Expected Posterior Variance} =p(1-p)\left(\frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} + \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\right)$$ -->

<!-- The gains are then: -->

<!-- $$\text{Gains} =1- \frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} - \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}$$ -->

<!-- Note that we still learn even if our posterior variance increases. For example say $p = 1/5$, $\phi_d = 1/3$, $\phi_b = 2/3$ and we observe $K=1$. Then our prior variance is $p(1-p) = 4/15$. Our posterior is $1/3$ and our posterior variance is 2/9, an increase. Even still although we are more uncertain we are wiser since we attribute a squared error to the guesses made by our former selves now of $(1/3)(1-1/5)^2 + (2/3)(0 - 1/5)^2 = 6/25$. -->

<!-- <!-- .2 2/3 / .2 2/3 + .8 1/3 ) = .4/ .4 + .8    --> 


<!-- One approach to assessing the contribution of a theory is to calculate the mean reduction in Bayes risk. Suppose we start with a baseline model with variables in the set $\mathcal W$ and want to answer a query, $Q$. We can then define the gains from theory as:  -->

<!-- $$\text{Gains from theory} = 1- \frac{E_{K|\mathcal W}(Var(Q|K,\mathcal W))}{Var(Q|\mathcal W)}$$ -->

<!-- <!-- <!-- AJ: Have tried to explain what Q and W are, and walk through the intuition behind this formulation. Make sure this is right. Also should the E not be in the denominator too? --> 

<!-- We can think of the Bayes risk as the inverse of an $R^2$ measure in a regression framework (see also @gelman2006bayesian): it is the variance in our query given what we have observed. Here we are defining the gains from theory as the degree to which we have reduced that variance by observing $K$ and $\mathcal W$, relative to just observing $\mathcal W$.  -->

One way to capture this idea that, although we are more uncertain, we think we are better off now than we were, is to ask: how much better are our guesses having observed $K$ compared to what we would have guessed before, *given* what we know having observed $K$? We will call this "Wisdom" to reflect the idea that it values appreciation of justifiable uncertainty:

<!-- <!-- AJ: Needs a sentence here introducing the terminology. Is this our term, expected wisdom? --> 

<!-- Expected wisdom. -->

$$Wisdom  = \int(q_0 - q)^2 - (q_k - q)^2 p(q | k)dq$$

This metric captures how much better off we are with the guess we have made given current data ($q_k$) compared to the guess we would have made if we had a theory that did not let us make use of it  ($q_0$), knowing what we know having observe $K$ ($p(q|k)$. 

An advantage of this conceptualization is that we can still record gains in learning even if the learning operates such that the posterior variance is larger than the prior variance.  Even so, the implications for strategy are the same since wisdom is maximized by a strategy that reduces expected squared error.  

Thus expected wisdom, is:

\begin{eqnarray}
\text{Expected Wisdom}  &=& \int_q(q_0 - q)^2dq - \int_k\int_q(q_k - q)^2 p(q, k)dqdk\\
&=& \text{Prior variance} - \text{Expected Posterior Variance}
\end{eqnarray}

We close with a reminder. Although expected reduction in variance and expected wisdom are both positive, both are are  fundamentally subjective ideas, that presuppose the theory is correct. In contrast the expected error measure can be assessed under rival theoretical propositions and so allow for  the real possibility that the gains of invoking a theory are negative.  

## Formal theories and causal models

It is relatively easy to see how the ideas above play out for what might be called empirical models. But in social sciences "theory" is a term sometimes reserved for what might be called analytic models. In this last section we work through how to use this framework when seeking to bring analytic models to data.

Let's start with analytic models. As an example we might consider the existence of "Nash equilibria." Nash considered a class of settings ("normal form games") in which each player $i$ can choose an action $\sigma_i$ from set $\Sigma_i$ and receives a payoff $u_i$ that depends on the actions of all players. A particular game, $\Gamma$ is the collection of players, action sets, and payoffs. 

Nash's theorem relates to the existence of a collection of strategies with the property that each strategy would produce the greatest utility for each player given the strategies of the other players. Such a collection of strategies is called a Nash equilibrium. 

The claim that such a collection of strategies exists in these settings is an analytic claim.  Unless there are errors in the derivation of the result, the claim is true in the sense that the conclusions follow from the assumptions. There is no evidence that we could go looking for in the world to assess the claim. The same can be said of the theoretical claims of many formal models in social sciences; they are theoretical conclusions of the if-then variety [@clarke2012model]. 

We will refer to theories of this form as "analytic theories."  

When researchers refer to a theory of populism or a theory of democratization however they generally do not have such pure theories in mind. Rather they have in mind what might be called "applied theories" (or perhaps more simply "scientific theories" or "empirical theories"): general claims about the relations between objects in the world. The distinction here corresponds to the distinction in @peressini1999applying between "pure mathematical theories" and  "mathematized scientific theories."^[Or see the distinction, for instance in in Keynes, between pure theory and applied theory.] 

<!-- In the standard ("hypothetico-deductive") model; a  theory originates in the mind and is then retained or rejected if it is inconsistent with data [@popper2014conjectures]. In practice, this  sometimes requires a step in which a pure theory is translated either wholesale or piecemeal into an aplied theory via a  set of empirical statements which are put to the test.   -->

Applied theory, in this sense, is a collection of claims with *empirical* content: an applied theory refers to a set of propositions of causal relations in the world that might or might not hold, and is susceptible to assessment using data. These theories might look formally a lot *like* analytic theories but it is better to think of them as translations at most. The relations between nodes of an applied theory are a matter of conjecture not a matter of necessity.^[@peressini1999applying distinguishes between "applied  mathematical theories" and "mathematized scientific theories" on the grounds that not all mathematized theories are an application of a pure theory.]

<!-- Using the causal modelling framework a possible approach is to embed direct empirical analogs of pure theoretical propositions into causal models. Endow nodes with physical interpretations and  let functional relations capture the ideal operation of the theory.  The causal model can naturally include uncertainty about any or all assumptions and claims of the theory, as well as a representation of the conditions under which the claims of the theory are expected to hold.  -->

Though it is not standard practice, formal models produced by game theorists can often be translated and then represented using the notation of structural causal models in this way. Moreover, doing so may be fruitful. Using the approach described above we can then assess the utility of the applied theory, if not the pure theory itself.

For two players, for instance, we might imagine a representation of a game as shown in Figure \@ref(fig:nfg).


```{r nfg, echo = FALSE, fig.width = 11, fig.height = 5, fig.align="center", out.width='80%', fig.cap = "Formal structure of a normal form game."}

par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(1, 2,  2, 3),
       y = c(1, 1.5,.5,1),
       names = c(
         expression(sigma[1]),
         expression(u[1]),
         expression(u[2]),
         expression(sigma[2])),
       arcs = cbind( c(1,1,4, 4),
                     c(2,3,3, 2)),
       add_functions = 0,
       contraction = .16,
       padding = .1
)


```


Here the only functional equations are the utility functions. The utilities, given actions, are the implications of the  theory, and so this is just a theory of how outcomes depend on social actions. It is not---yet---a behavioral theory.

In contrast to Nash's theorem regarding the existence of equilibria, a behavioral theory might claim that in problems that can be represented as normal form games, players indeed play Nash equilibrium. This is a theory about how people act in the world. We might call it Nash's theory.

How might this theory be represented as a causal model? Figure \@ref(fig:nfg2) provides one representation.


```{r nfg2, echo = FALSE, fig.width = 11, fig.height = 5, fig.align="center", out.width='80%', fig.cap = "Formal structure of a normal form game."}

par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(3, 1, 2, 3, 3,   4, 5),
       y = c(0, 1, 1, .5,1.5, 1, 1),
       names = c(
         expression(Gamma),     #1:  3 
         "" ,       #2:  1
         expression(sigma[1]),   #3:  2  
         expression(u[1]),       #4:  3
         expression(u[2]),       #5:  3
         expression(sigma[2]),   #6:  4
         ""        #7:  5
         ),
       arcs = cbind( c(3, 3,  6, 6, 1, 1),
                     c(4, 5,  4, 5, 3, 6)),
       add_functions = 0,
       contraction = .16,
       padding = .1
)


```

Here beliefs about the game form ($\Gamma$) results in strategy choices by actors. If players play according to Nash's theory, *the functional equations for the strategy choices are given by the Nash equilibrium solution itself*, with a refinement in case of multiplicity. 

This model represents what we expect to happen in a game under Nash's theory and we can indeed see if the relations between nodes in the world look like what we expect under the theory. But it does not provide much of an *explanation* for behavior. 

A lower level causal model might help. In Figure  \@ref(fig:nfg3) 
 the game form $\Gamma$ determines the beliefs about what actions the other player would make (thus $\sigma_2^e$ is 1's belief about 2's actions). The functional equations for $\sigma_2^e$ and $\sigma_1^e$ might, for instance, be the Nash equilibrium solution itself: that is, players expect other players to play according to the Nash equilibrium (or in the case of multiple, a particular equilibrium selected using some refinement). The beliefs in turn, together with the game form (which contains $u1, u_2$),  are what cause the players to select a particular action. The functional equation for $\sigma_1$ might thus be $\sigma_1 = \arg \max_\sigma u_1(\sigma, \sigma_2^e)$.

```{r nfg3, echo = FALSE, fig.width = 11, fig.height = 5, fig.align="center", out.width='80%', fig.cap = "Formal structure of a normal form game."}

par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(3, 1, 2, 3, 3,   4, 5),
       y = c(0, 1, 1, .5,1.5, 1, 1),
       names = c(
         expression(Gamma),     #1:  3 
         expression(sigma[2]^e),       #2:  1
         expression(sigma[1]),   #3:  2  
         expression(u[1]),       #4:  3
         expression(u[2]),       #5:  3
         expression(sigma[2]),   #6:  4
         expression(sigma[1]^e)        #7:  5
         ),
       arcs = cbind( c(1,1,2, 3, 3,  6, 6, 7, 1, 1),
                     c(2,7,3, 4, 5,  4, 5, 6, 3, 6)),
       add_functions = 0,
       contraction = .16,
       padding = .1
)


```

This representation implies a set of relations that can be compared against empirical patterns. Do players indeed hold these beliefs when playing a given game? are actions indeed consistent with beliefs in ways specified by the theory. It provides a theory of beliefs and a theory of individual behavior as well as an explanation for social outcomes.    

The model in Figure \@ref(fig:nfg3) provides a foundation of sorts for Nash's theory. It suggests that players play Nash equilibria *because* they expect others to and they are utility maximizers. But this is not the only explanation that can be provided; alternatively  behavior might line up with the theory without passing through beliefs at all as suggested in some accounts from evolutionary game theory that show how  processes might select for behavior that corresponds to Nash even if agents are unaware of the game they are playing.  

One might step still further back and ask *why* would actors form these beliefs, or take these actions, and answer in terms of assumptions about actor rationality. Figure \@ref(fig:nfg4) for instance is a model in which actor rationality might vary and might influence beliefs about the actions of others as well as reactions to those beliefs. Fully specified functional equations might specify not only how actors act when rational but also how they react when they are not. In this sense the model in Figure \@ref(fig:nfg4) both nests Nash's theory and provides an explanation for why actors conform to the predictions of the theory.


```{r nfg4, echo = FALSE, fig.width = 11, fig.height = 5, fig.align="center", out.width='80%', fig.cap = "Formal structure of a normal form game."}

par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(3, 1, 2, 3, 3,   4, 5, 1.5, 4.5),
       y = c(0, 1, 1, .5,1.5, 1, 1, 1.5, 1.5),
       names = c(
         expression(Gamma),     #1:  3 
         expression(sigma[2]^e),       #2:  1
         expression(sigma[1]),   #3:  2  
         expression(u[1]),       #4:  3
         expression(u[2]),       #5:  3
         expression(sigma[2]),   #6:  4
         expression(sigma[1]^e),        #7:  5
         expression(R[1]),   #8:  4
         expression(R[2])        #9:  5

         ),
       arcs = cbind( c(1,1,2, 3, 3,  6, 6, 7, 1, 1, 8, 8, 9, 9),
                     c(2,7,3, 4, 5,  4, 5, 6, 3, 6, 2, 3, 6, 7)),
       add_functions = 0,
       contraction = .16,
       padding = .1
)


```

In a final elaboration we can represent a kind of underspecification of Nash's theory that make it difficult to take the theory to data. In the above we assumption that players chose actions based on expectations that the other player would play the Nash equilibrium---or that the theory would specify which equilibrium in the case of multiplicity. But it is well known that Nash's theory often does not provide a unique solution. This indeterminacy can be captured in the Causal model as shown in  Figure \@ref(fig:nfg5) where a common shock---labelled $\nu$, and interpreted as norms---interacts with the game form to determine the expectations of other players. 

The functional equation for expectations can then allow for the possibility that (i) there is a unique equilibrium invariably chosen and played by both (ii) or a guarantee that players are  playing one or other equilibrium together but uncertainty over which one is played, or (iii)  the possibility that players are in fact out of sync, with each playing optimal strategies given beliefs but nevertheless not playing the same equilbria.

Nash's theory likely corresponds to position (ii). It can be captured by functional equations on beliefs given $\nu$ but the theory does not specify $\nu$, in the same way that it does not specify $\Gamma$.


```{r nfg5, echo = FALSE, fig.width = 11, fig.height = 5, fig.align="center", out.width='80%', fig.cap = "A normal form game with a representation of equilibrium selection norms."}

par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(3, 1, 2, 3, 3,   4, 5, 1.5, 4.5, 3),
       y = c(0, 1, 1, .5,1.5, 1, 1, 1.5, 1.5, 2),
       names = c(
         expression(Gamma),     #1:  3 
         expression(sigma[2]^e),       #2:  1
         expression(sigma[1]),   #3:  2  
         expression(u[1]),       #4:  3
         expression(u[2]),       #5:  3
         expression(sigma[2]),   #6:  4
         expression(sigma[1]^e),        #7:  5
         expression(R[1]),   #8:  4
         expression(R[2]),        #9:  5
         expression(nu)        #9:  5
         ),
       arcs = cbind( c(1,1,2, 3, 3,  6, 6, 7, 1, 1, 8, 8, 9, 9, 10, 10),
                     c(2,7,3, 4, 5,  4, 5, 6, 3, 6, 2, 3, 6, 7, 2, 7)),
       add_functions = 0,
       contraction = .16,
       padding = .1
)


```

We highlight three points from this discussion. 

First the discussion highlights that thinking of theory as causal models does not force a sharp move away from abstract analytic theories; close analogues of these can often be incorporated in the same framework. This is true even for equilibrium analysis that seems to involve a kind of simultaneity on first blush. 

Second, the discussion highlights how the causal modelling framework can make demands for specificity from formal theories. For instance specifying a functional relations from game form to actions requires a specification of a selection criterion in the event of multiple equilibria. Including agent rationality as a justification for the theory invites a specification for what would happen absent rationality.

Third the example shows a way of building a bridge from pure theory to empirical claims. One can think of Nash's theory as an entirely data free set of claims. When translate into an applied theory---a set of proposition about the ways actual players *might* behave---and represented as a causal model, we are on a path to being able to use data to refine the theory. Thus we might begin with a formal specification like that in Figure \@ref(fig:nfg5) but with initial uncertainty about player rationality, optimizing behavior, and equilibrium selection. This theory nests Nash but does not presume the theory to be a valid description of processes in the world. Combined with data, however, we shift to a  more refined theory that selects Nash from the lower level model. 

Finally, we can then apply the ideas of section \@ref(theorygains) to applied formal theories and ask: is the theory useful? For instance, does data on player rationality help us better understand the relationship between game structure and welfare?

<!-- Other possible measures of gains from theory might include the simple correlation between $K$ and $Q$, or entropy-based measures (see @zhang2003properties for many more possibilities).  -->

<!-- For this problem the correlation is given by (see appendix): -->

<!-- $$\rho_{KQ} = \frac{(\phi_b+\phi_d)(1-2p)(p(1-p))^{.5}}{ -->
<!-- (p\phi_b+(1-p)\phi_d)(1-(p\phi_b+(1-p)\phi_d)))^{.5}}$$ -->

<!-- One might also use a measure of "mutual information" from information theory: -->

<!-- $$I(Q,K) = \sum_q \sum_k P(q,k)\log\left(\frac{P(q,k)}{P(q)P(k)}\right)$$ -->

<!-- <!-- here: --> 


<!-- <!-- \begin{equation} --> 
<!-- <!-- \begin{aligned} --> 
<!-- <!-- I(Q,K) ={} & p\phi_b\log\left(\frac{\phi_b}{p\phi_b+(1-p)\phi_d}\right)+ (1-p)\phi_d\log\left(\frac{\phi_d}{p\phi_b+(1-p)\phi_d}\right) \\ --> 
<!-- <!--       & +p(1-\phi_b)\log\left(\frac{1-\phi_b}{1-p\phi_b-(1-p)\phi_d}\right)+ --> 
<!-- <!-- (1-p)(1-\phi_d)\log\left(\frac{1-\phi_d}{1-p\phi_b-(1-p)\phi_d}\right) --> 
<!-- <!-- \end{aligned} --> 
<!-- <!-- \end{equation} --> 

<!-- To express this mutual information as a share of variation explained, we could divide $I(Q,K)$ by the entropy of $Q$, $H(Q)$ where $H(Q) = -\sum_qP(q)\log(P(q))$. The resulting ratio can  be interpreted as 1 minus the ratio of the entropy of $Q$ conditional (on $K$) to the unconditional entropy of $Q$. -->

<!-- For this example, Figure \ref{fig:probative_value} shows gains as a function of $\phi_b$ given a fixed value of $\phi_d$. The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely.  -->

<!-- ```{r probativevalue, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:probative_value} The solid line shows gains in precision (reduced posterior variance) for different values of $\\phi_b$ given $\\phi_d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\\phi_b - \\phi_d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$."} -->



<!-- gains = function(p, phi_b, phi_d){ -->
<!--   1- (phi_b*phi_d)/(phi_b*p +phi_d*(1-p)) - (1-phi_b)*(1-phi_d)/((1-phi_b)*p+(1-phi_d)*(1-p)) -->
<!-- } -->

<!-- corr_qk <- function(p, phi_b, phi_d){ -->
<!--   ((phi_b-phi_d)*(p*(1-p))^{.5})/ -->
<!--   (((p*phi_b+(1-p)*phi_d)*(1-(p*phi_b+(1-p)*phi_d)))^{.5}) -->
<!--   } -->
<!-- # Mutual Information -->
<!-- mi_qk <- function(p, phi_b, phi_d, base = 2){ -->
<!--  p*phi_b*        log({phi_b}   / {p*phi_b+(1-p)*phi_d}, base = base)+ -->
<!-- (1-p)*phi_d*     log({phi_d}   / {p*phi_b+(1-p)*phi_d}, base = base)+ -->
<!-- p*(1-phi_b)*     log({1-phi_b} / {1-p*phi_b-(1-p)*phi_d}, base = base)+ -->
<!-- (1-p)*(1-phi_d)* log({1-phi_d}/ {1-p*phi_b-(1-p)*phi_d}, base = base) -->
<!--   } -->

<!-- norm_mi_qk <-  function(p, phi_b, phi_d, base = 2){ -->
<!--     -mi_qk(p, phi_b, phi_d, base = base)/(p*log(p, base = base)+(1-p)*log(1-p, base = base))} -->

<!-- phi_b = seq(0,1,.01) -->

<!-- plot(phi_b, gains(.75, phi_b, .25), type = "l", xlab = expression(paste(phi[b])), ylab = "Probative Value") -->
<!--   points(phi_b, abs(corr_qk(.75, phi_b, .25)), type = "l", lty=2) -->
<!--   points(phi_b, (norm_mi_qk(.75, phi_b, .25)), type = "l", lty = 3) -->
<!--   points(phi_b, abs(phi_b - .25), type = "l", lty = 4) -->
<!--   title("Reduced posterior variance, correlation, mutual information") -->
<!-- text(.8, c(.15, .25, .3, .62, .41), c("I(K,Q)/H(Q)","(Reduced posterior variance)", "Gains",  expression(paste(abs(phi[b]-phi[d]))), "Cor(K,Q)")) -->

<!-- #plot(abs(corr_qk(.75, phi_b, .25)), gains(.75, phi_b, .25), type = "l", xlab = "Probative value") -->
<!-- #lines(abs(phi_b - .25), gains(.75, phi_b, .25), type = "l", lty=2) -->
<!-- #lines(norm_mi_qk(.75, phi_b, .25), gains(.75, phi_b, .25), type = "l", lty=2) -->


<!-- ``` -->


<!-- Clarke and Primo see models as useful to the extent that they are similar to features of the real world in ways related to the model's purpose. Along these lines, a causal model will be useful to the extent that it posits relations of independence that are similar to those prevailing in the domain under investigation. -->




<!-- ### Connections to other writing on theory -->

<!-- We close this chapter by considering how the understanding of theory that we work with in this book compares to other prominent understandings of theory.  -->


<!-- **Theory as model.** Although @clarke2012model argue for a separation of the ideas of model and theory, it is common for social scientists to use the terms interchangeably to denote an abstract representation of some part of the world that is of interest. For instance, a model may stipulate that outcome $X$ can have a positive effect on $Y$ because $X$ can cause $M$ and $M$ can cause $Y$.  One can read from a model how things work in the context of the model: for instance, if $M$ does not obtain, then under this model, $X$ does not cause $Y$.   One can use a model to make claims about the world only by assuming a mapping from elements in the model to elements in the worlds.  In this sense a  model is best thought of as an object that may or may not be useful [@clarke2012model]; whether the model itself is true or false is, in this usage, not a coherent question.  -->

<!-- **Theory as empirical claim.** In common usage, "a theory of" a phenomenon is a direct claim about the phenomenon, in the world. The claim that natural resources cause conflict is a theoretical claim of this form. The claim is certainly not true by definition, and empirical evidence can be used to assess it. In this claim, the *theory*, as usually understood, is certainly thin; the claim is no more than an empirical proposition, and it possesses no internal logic. Yet, more elaborate collections of empirical propositions are easily constructed. For instance: natural resources cause conflict because they can finance secessionist claims in resource rich areas.^[This latter claim does seem to possess something like a logic; though it does not take much to see that the logic is just a slightly more elaborate set of empirical claims. The outcomes do not follow  *logically* from the causes----there is no logical reason why secessionist claims would cause conflict, but the theory---as a collection of claims---has implications similar to those in the model in the paragraph above: if there are no secessionist claims, then under this theory, natural resources are not causing conflict.] Theory in this sense can certainly be right or wrong. -->



<!-- In this book we take a somewhat idealist position and assume that we are permanently inhabiting a world of models.  -->

<!-- The distinction between the last two accounts is sometimes confusing, and  @clarke2012model make a case for cleaning up the language on this front. In their account, drawing on @giere2010explaining, a theory might be best thought of as a set of models accompanied by hypotheses linking the model to the question of interest in the  world.  -->

<!-- We see our approach to theory as models as following in the spirit of  @clarke2012model and  @giere2010explaining yet also as being consistent with the treatment of models in the literature on probabilistic causal models with which this book is centrally engaged. A nice feature is that it preserves a close associated between theory and explanation and it incorporates naturally the notion of deduction without requiring that models themselves are statements of the  *if-then* variety. -->

<!-- **Theory as generalization** In another of the many uses of "theory," political scientists often think of theorization as generalization. For @Van-Evera:1997 and @przeworski1970logic, for instance, theories are by their nature general statements that we can use to explain specific events. In this view, "Diamond resources caused Sierra Leone's civil war" is a case-specific explanation; "Natural resource endowments cause civil war" is a theoretical formulation.  -->

<!-- In our treatment of theory as a lower-level causal model, however, there is no generic sense in which a theory is more or less general than the higher-level claim that it explains. In this book's framework, we _can_ theorize by generalizing: when we elaborate a model by building in variation in a factor that was held constant in the higher-level claim, we are making the model more general in scope. If our natural resources claim implicitly applies only to weak states, we can theorize this claim by allowing state strength to vary and articulating how the natural-resource effect hinges on that claim.  -->

<!-- However, when we theorize by disaggregating nodes---say, by adding intervening causal steps---we have in fact made a more _specific_ claim. Natural resources may cause civil war under a broad set of circumstances. Natural resources will cause civil war *through looting by rebel groups* under an almost certainly narrower set of circumstances. Here, the more elaborate argument---the theorization of *why* $X$ causes $Y$---is actually a stronger claim, with narrower scope, than the simpler one that it supports.  -->

<!-- **The value of parsimony** @Van-Evera:1997 and @przeworski1970logic also express a common view in characterizing _parsimony_ as a quality of good theory. While they recognize that parsimony must often be traded off against other goods, such as accuracy and generality, _ceteris paribus_ a more parsimonious theory---one that uses fewer causal variables to explain variation in a given outcome---is commonly understood to be a better theory.  -->

<!-- We do not take issue with the idea that simpler models and explanations are, all else equal, better. But the succeeding chapters also demonstrate a distinctive and important way in which all else will often not be equal when we seek to use theory to guide research design and support causal inference. To foreshadow the argument to come, the elaboration of more detailed, lower-level models can direct us to new opportunities for learning. As we unpack a higher-level claim, we will often be identifying additional features of a phenomenon the observation of which can shed light on causal questions of interest. Moreover, our background beliefs---the prior knowledge on which causal inference must usually rest---are often more informative at lower levels than at higher levels: it will, for instance, often be easier for us express beliefs about causal effects for smaller steps along a causal chain than about an overarching $X \rightarrow Y$ effect.  -->

<!-- Making things more complicated, of course, still makes things more complicated. And we should avoid doing so when the payoff is small, as it will sometimes be. But in the pages to come, we will also see a distinct set of benefits that arise from drilling more deeply into our basis of prior knowledge when formulating inferential strategies. -->

<!-- , with these claims,  perhaps derived or inspired from some model via a statement that the model represents the world faithfully for some purpose.  -->

<!-- The key difference as we see it is between representations of a system, a model, and claims that the model itself represents another system---the world---in some ways. The difference betw  -->



<!-- ### Illustration of a Mapping from a Game to a DAG -->

<!-- Our running example supports a set of higher level models, but it can also  be *implied* by a lower level models. Here we illustrate with an example in which the lower level model is a game theoretic model, together with a solution.^[Such representations have been discussed as multi agent influence diagrams, for example in @koller2003multi or @white2009settable on "settable systems"--- an extension of the "influence diagrams" described by @dawid2002influence.]  -->

<!-- In Figure \@ref(fig:tree) we show a game in which nature first decides on the type of the media and the politician -- is it a media that values reporting on corruption or not? Is the politician one who has a dominant strategy to engage in corruption or one who is sensitive to the risks of media exposure? In the example the payoffs to all players are fully specified, though for illustration we include parameter $b$ in the voter's payoffs which captures utility gains from sacking a politician that has had a negative story written about them *whether or not they actually engaged in corruption*. A somewhat less specific, though more easily defended, theory would not specify particular numbers as in the figure, but rather assume ranges on payoffs that have the same strategic implications.   -->

<!-- The theory is then the  game plus a solution to the game. Here for a solution the theory specifies subgame perfect equilibrium. -->

<!-- In the subgame perfect  equilibrium of the game; marked out on the game tree (for the case  $b=0$) the sensitive politicians do not engage in corruption when there is a free press -- otherwise they do; a free press writes up any acts of corruption, voters throw out the politician if indeed she is corrupt and this corruption is reported by the press.   -->

<!-- As with any structural model, the theory says what will happen but also what *would* happen if things that should not happen indeed happened.  -->

<!-- ```{r tree, echo=FALSE, fig.width = 15, fig.height = 12, fig.cap = "\\label{fig:tree} A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which  $b=0$."} -->

<!-- H <-  matrix(c(rep("O", 32),  -->
<!-- rep("X=1, S=1", 8), rep("X=1, S=0", 8), rep("X=0, S=1", 8), rep("X=0, S=0",8 ), -->
<!-- rep(rep(c("C","NC"), each = 4 ), 4), -->
<!-- rep(rep(c("R","NR"), each = 2 ), 8), -->
<!-- rep(c("Y","NY"), 16)),  -->
<!--  32)[32:1,] -->

<!-- in.history = function(action) rowSums(H==action)>0 -->

<!-- P <- cbind(rep(1, 32),  -->
<!--            rep(2, 32),  -->
<!--            rep(3, 32),  -->
<!--            rep(4, 32))[32:1,] -->
<!-- U <- matrix(NA, 32, 4) -->
<!-- U[,2] <- in.history("C") -    -->
<!--           2*in.history("Y") +  -->
<!--           2*(in.history("X=0, S=0")+ in.history("X=1, S=0"))*in.history("C")  -->


<!-- # Media gains only when it does reliable story  -->
<!-- U[,3] <- in.history("NR") +    -->
<!--           2*in.history("R")*in.history("C")*(in.history("X=1, S=0")+ in.history("X=1, S=1"))  -->


<!-- # Voters prefer firing if reports on corrupt politician -->
<!-- U[,4] <- in.history("NY") +    -->
<!--           2*in.history("Y")*in.history("C")*in.history("R")  -->


<!-- gt_tree(H,U,P, player.names = c("Nature", "Gov", "Media", "Voters"),          -->
<!--   mark.branches=((ncol(H)-1):2), -->
<!--   print.utilities = c(FALSE, TRUE, TRUE, TRUE), -->
<!--   force_solution = TRUE, warnings = FALSE) -->

<!-- text(6.6, (1:32)[in.history("Y") & in.history("R")]- .02, expression(italic(+b)) , cex = 1.2)  -->

<!-- ``` -->



<!-- To draw this  equilibrium as a DAG we include nodes for every action taken, nodes for features that determine the game being played, and the utilities at the end of the game.  -->

<!-- If equilibrium claims are justified by claims about the beliefs of actors then these could also appear as nodes. To be clear however these are not required to represent the game  or the equilibrium, though they can capture assumed logics underlying the equilibrium choice. For instance a theorist might claim that humans are wired so that whenever they are playing a "Stag Hunt" game they play "defect." The game and this solution can be represented on a DAG without reference to the  beliefs of actors about the action of other players. However, if the *justification* for the equilibrium involves optimization given the beliefs of other players, a lower level DAG could represent this by having a node for the  game description that points to beliefs about the actions of others, that then points to choices. In a game with dominant strategies, in contrast, there would be no arrows from these beliefs to actions. -->

<!-- For our running example, nodes could usefully include the politician's expectations, since the government's actions depend on expectations of the actions of others. However, given the game there is no gain from  including the media's expectations of the voter's actions since in this case the media's actions do not depend on expectations of the voters actions then these expectations should be included.   -->

<!-- In Figure \@ref(fig:gamedag) we provide two examples of DAGs that illustrate lower level models that support our running example.  -->

<!-- The upper panel gives a DAG reflecting equilibrium play in the game described in Figure \@ref(fig:tree). Note that in this game there is an arrow between $C$ and $Y$ even though $Y$ does not depend on $C$ for some values of $b$---this is because conditional independence requires that two variables are independent for *all* values of the conditioning set. For simplicity also we mark $S$ and $X$, along with $b$ as features that affect which subgame is being played---taking the subgames starting after Nature's move. Note that the government's expectations of responses by others matters, but the expectations of other players do not matter given this game and solution. Note that the utilities appear twice in a sense. They appear in the subgame node, as they are part of the definition of the game--though here they are the utilities that players expect at each terminal node; when they appear at the end of the DAG they are the utilities that actually arise (in theory at least).  -->

<!-- The lower level DAG  is very low and much more general, representing the theory that in three player games of complete information, players engage in backwards induction and choose the actions that they expect to maximize utility given their beliefs about the actions of others. The DAG assumes that players know what game is being played ("Game"), though this could also be included for more fundamental justification of behavioral predictions. Each action is taken as a function of the beliefs about the game, the expectations about the actions of others, and knowledge of play to date. The functional equations---not shown---are given by optimization and belief formation assuming optimization by others.   -->


<!-- ```{r gamedag, echo = FALSE, fig.width = 12, fig.height = 10, out.width='\\textwidth', fig.cap = "\\label{fig:gamedag} The upper panel shows a causal graph that describes  relations between nodes suggested by analysis of  the  game  in Figure \\ref{fig:tree} and which can imply the causal graph of  Figure \\ref{fig:running}. The game itself  (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors.   The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''", fig.align="center", warning = FALSE} -->

<!-- par(mfrow = c(2,1)) -->
<!-- par(mar=c(1,1,3.5,1)) -->


<!-- x = c(0, 1, 2, 2,  3,  3, 4, 5) -->
<!-- y = c(0, 0, 2, -2, 2, -2.5, -2, 0) -->

<!-- names = c("S, X, b",                                        #1  -->
<!--           "Subgame",                                           #2  -->
<!--           "E: Gov's Beliefs\nabout responses by\n Media and Voters",    #3 -->
<!--           "Corruption",                                       #4 -->
<!--           "",            #5 -->
<!--           "Report",                                       #6 -->
<!--           "Remove\nGovernment",                                       #7 -->
<!--           "Utilities"                          #8 -->
<!-- ) -->

<!-- hj_dag(x =  x, -->
<!--        y = y, -->
<!--        names = c(names, " ", " "), -->
<!--        arcs = cbind( c(1,rep(2,5)  ,3, c(4,6,7),  4, 4, 6), -->
<!--                      c(2,3:4, 6:8,      4,  rep(8, 3), 6, 7, 7)), -->
<!--        title = "Lower DAG: Backwards induction in a game with 3 players  with one  move  each", -->
<!--        contraction = .22, -->
<!--        padding = .5) -->



<!-- x = c(0, 1, 2, 2,  3,  3, 4, 5) -->
<!-- y = c(0, 0, 2, -2, 2, -2.5, -2, 0) -->

<!-- names = c("Context",                                        #1  -->
<!--           "Game",                                           #2  -->
<!--           "1's Beliefs\nabout actions \n 2|1 and 3|2,1",    #3 -->
<!--           "Action 1",                                       #4 -->
<!--           "2's Beliefs\nabout actions \n 3|2,1",            #5 -->
<!--           "Action 2",                                       #6 -->
<!--           "Action 3",                                       #7 -->
<!--           "Utilities"                          #8 -->
<!-- ) -->

<!-- hj_dag(x =  x, -->
<!--        y = y, -->
<!--        names = c(names, " ", " "), -->
<!--        arcs = cbind( c(1,rep(2,6)  ,3, 5, c(4,6,7),  4, 4, 6), -->
<!--                      c(2,3:8,      4, 6, rep(8, 3), 6, 7, 7)), -->
<!--        title = "Still lower: Backwards induction, 3 player game with one  move for each player", -->
<!--        contraction = .2, -->
<!--        padding = .5) -->




<!-- ``` -->

<!-- These lower level graphs can themselves provide clues for assessing relations in the higher level graphs. For instance, the lower level model might specify that the value of $b$ in the game affects the actions of the government only through their beliefs about the behavior of voters, $E$. These beliefs may themselves have a stochastic component, $U_E$. Thus  $b$ high  might be thought to reduce the effect of media on corruption. For instance if $b \in \mathbb{R}_+$, we have $C= 1-FG(1-\mathbb{1}(b>1))$. If $X$ is unobserved and one is interested in whether $S=0$ caused corruption, knowledge of $b$ is informative. It is a root node in the causal estimand. If $b>1$ then $S=0$ did not cause corruption. However if $b$ matters only because of its effect on $E$ then the query depends on $U_E$.  In this case, while knowing $b$ is informative about whether $S=0$ caused $C=1$, knowing $E$ from the lower level graph is more informative. -->

<!-- Note that the  model we have examined here involves no terms for $U_C$, $U_R$ and $U_Y$---that is, shocks to outcomes given action. Yet clearly any of these could exist. One could imagine a version of this game with "trembling hands," such that errors are always made with some small probability, giving rise to a much richer set of predictions.  These can be  represented in the game tree as moves by nature between actions chosen and outcomes realized. Importantly in a strategic environment such noise could give rise to different types of conditional independence. For instance say that a Free Press only published its report on corruption with  probability $\pi^R$, then with $\pi^R$ high enough the sensitive government might decide it is worth engaging in corruption even if there is a free press; in this case the arrow from $X$ to $C$ would be removed. Interestingly in this case as the error rate rises, $R$ becomes less likely, meaning that the effect of a $S$ on $Y$ becomes gradually weaker (since governments that are not sensitive become  more likely to survive) and then drops to 0 as sensitive governments start acting just like nonsensitive governments.  -->


<!--chapter:end:06-theory-as-causal-models.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
# (PART) Model-Based Causal Inference {-}

# Process Tracing with Causal Models {#pt}

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We connect the literature on causal models to qualitative inference strategies used in process tracing and describe a procedure for inference on case level queries from causal models. We show how a key result from the causal models literature provides a condition for when clues may be (or certainly will not be) informative and extract a set of implications for process tracing. 
::::

<br>


```{r packagesused06, include = FALSE}
source("_packages_used.R")
```


## Process tracing and causal models

This chapter demonstrates how we can use causal models to conduct confirmatory process tracing: that is, to draw causal inferences about a single case from a causal model with data provided at the case-level.  

### The intuition

We first walk through the basic intuition and then provide a more formal account.

When we undertake process tracing, we seek to answer a causal question about a given case.
The key insight driving our approach is that the inference about a causal question for a case is a claim about **which causal types (collections of nodal types) are both likely ex ante (given prior knowledge) and consistent with the data**.^[This differs from the task for mixed methods that  we will address in Chapter \@ref(mixing) as these concern claims about the distribution of causal types in populations.]

The question of interest can be about any number of case-level causal features, including questions about a case-level causal effect, the pathway through which an effect operates, an actual cause, or causal attribution. We use observations from the case itself to address this query. We do so via a procedure in which we first encode prior knowledge in the form of a causal model, then use data to learn about features of the model, and finally take what we have learned about the model and map it into our query.

Given a causal model, we form posteriors over queries as follows:

1. **Specify all possible causal types for a model**. A causal type, recall, is a particular combination of nodal types for all nodes in a unit. That is, a single causal type specifies both a specific set of values of all exogenous variables in a model and the values that all endogenous variables *would* potentially take on for all possible values of the exogenous variables. For a simple, binary $X \rightarrow Y$ model, the number of possible causal types will be 2 (the number of possbile values $X$, the exogenous node, can take on) times 4 (the number of possible nodal types for $Y$, the endogenous node). Three of these causal types would be:
  * Type 1: ($X=1$) *and* ($Y=1$ if $X=1$, $Y=0$ if $X=0$).
  * Type 2: ($X=0$) *and* ($Y=1$ if $X=1$, $Y=0$ if $X=0$).
  * Type 3: ($X=1$) *and* ($Y=1$ if $X=1$, $Y=1$ if $X=0$).

Whatever the model, we generate a complete set of all possible causal types.


2. **Specify priors over causal types.** We report how likely we think it is that a given unit is of a particular causal type. In the simplest situation, we might place 0 weight on some causal types (which might be ruled out by background theory, for example) and equal weight on the others. 

3. **Specify the query in terms of causal types.** For instance, for the simple $X \rightarrow Y$ model, the query "$Y$ responds positively to $X$" can be thought of as a collection of causal types: Q={Type 1, Type 2}.
<!-- , or using the notation we introduced above: Q=1(\theta^Y = {\theta^Y_{01}}).^[In general we think of queries as defined on causal types; a query defined in terms of nodal types, as here, can be reinterpreted as a query on causal types.] -->

4. **Once we observe the data, specify the set of causal types that are consistent with those data.** For instance, if we observe $X=1, Y=1$ we might specify the data-consistent set as {Type 1, Type 3}, excluding Type 2 with which these data are inconsistent.

5. **Update.** Updating is then done by adding up the prior probabilities on all causal types that are consistent with both the data and the query, and dividing this sum by the sum of prior probabilities on all causal types that are consistent with the data (whether or not they are consistent with the query).


<!-- 1. **Draw a DAG.** We begin by constructing a causal model in graphical form, a DAG, expressing which variables in the domain of interest we think can have a direct effect on which other variables. As we have discussed, the causal model we start with may be derived from theory, from data on other cases, or some combination of the two. (We show, for instance, in Chapter \@ref(mixing) how data from a larger set of cases can inform the priors we bring to single-case process tracing.)  -->

<!-- 2. **Identify causal types**. A DAG, in turn, defines a set of possible causal types: all of the different possible combinations of nodal types that any case might have.  -->

<!-- 3. **Form priors**. We draw further on background knowledge, about the population to which the case belongs, to formulate prior beliefs about the probability that the case is of different causal types. We can generate these priors by ruling out certain nodal types as inconsistent with prior knowledge. Where our prior knowledge supports doing so, we can also place differential quantitative weights on those nodal types that we believe to be more or less common in the population. -->

<!-- 4. **Observe data**. We observe data on some or all of the nodes in the graph. -->

<!-- 5. **Eliminate causal types inconsistent with the data**. Check the consistency of each causal type with the data. Eliminate from contention any causal type that could not have generated the data pattern that we observe. -->

<!-- 6. **Form posteriors**. We now scale up the probabilities on all remaining causal types, providing a posterior probability on each type. -->

<!-- 7. **Map from causal types to query**. As any causal query can be formulated as a question about causal types (see Chapter \@ref(questions)), we can now map from our posteriors on causal types to a posterior probability on the query of interest: whether a causal effect, a causal pathway, causakl attribution, or some other case-level causal quantity. -->

```{r plotrix, include = FALSE}
library(plotrix)
```

```{r ptvenn, echo = FALSE, fig.width=6, fig.height=6, fig.cap = "Logic of simple updating on arbitrary queries.", fig.align = "center", out.width = "50%"}

par(mar = c(.1,.1,.1,.1))
frame()
mycols <- c(
  rgb(0, 0, 255, max = 255, alpha = 125, names = "blue50"),
  rgb(255, 0, 0, max = 255, alpha = 125, names = "c2"),
  rgb(0, 255, 0, max = 255, alpha = 125, names = "c3"))

mycols <- c(NA, NA, NA)
draw.circle(.35,.35,.3,nv=100,border=NULL,lty=1,density=NULL,
						angle=45,lwd=1, col = mycols[1])
draw.circle(.65,.35,.3,nv=100,border=NULL,col=mycols[2],lty=1,density=NULL,
						angle=45,lwd=1)
draw.circle(.5,.65,.3,nv=100,border=NULL,col=mycols[3],lty=1,density=NULL,
						angle=45,lwd=1)
text(.5, .8, "Consistent\nwith query")
text(.2, .35, "Consistent\nwith priors")
text(.8, .35, "Consistent\nwith data")
text(.05, .95, expression(Theta))
text(.15, .95, "All\ncausal types")
text(.5, .45, "A")
text(.32, .55, "B")
text(.5, .25, "C")


box()
```

This process is represented graphically in Figure \@ref(fig:ptvenn), where we can think of probabilities as proportionate to areas. Our causal model defines the causal-type space. We then proceed by a process of elimination. Only some of the causal types in the model are consistent with prior knowledge. Only some are consistent with the data that we observe. Finally, any query itself maps onto a subset of the possible causal types. The causal types that remain in contention once we have observed the evidence are those at the intersection of consistency with priors and consistency with the data. $A$ represents those types that are *also* consistent with a given answer to the query (say, $X$ has a positive effect on $Y$).

Thus, our belief about the query before we have seen the data is the probability of all causal types consistent with our priors and with the query ($A + B$) as a proportion of all types consistent with our priors. Once we have seen the data, we have reduced the permissible types to $A + C$. Our posterior belief on the query is, then, the probabilities of those remaining types that are consistent with the query as a share of the probabilities of *all* remaining types, or $A/(A+C)$.

We now turn to a formalization of these ideas. 
<!-- What we are doing here is straightforward: assessing causal possibilities for their compatibility with both the evidence at hand and our prior knowledge of how the world works. The formalization that we will present ensures that prior knowledge and evidence are all recorded  while forcing logical consistency on the inferences that emerge from them. -->


### A formalization of the general approach

The general approach to inference draws on the components we outlined in chapters 2 to 4: graphical causal models (DAGs), queries, and priors. Coulped with data these elements provide grounds for causal inferences. We continue to focus on a situation with binary variables, though describe later how this can be extended. We walk through the procedure for simple models, though note that the approach outlined here can be applied to *any* causal model with discrete variables and to any queries defined over the model.

The process tracing procedure operates as follows.

#### The model

First we need a model.

**A DAG**

We begin with a DAG, or graphical causal model. As discussed in Chapter 2, a DAG identifies a set of variables and describes the parent-child relations between them, indicating for each variable which other variables are its direct (possible) causes. These relationship, in turn, tell us which (non-descendant) variables a given variable is *not* independent of given the other variables in the model. 

**Nodal types**

Once we have specified a DAG, we can determine the full set of possible nodal types: the types defining the value that a variable will take on given the values of its parents, which we have denoted with $\theta^j$ values for node $j$, as in $\theta^X_{0}$ or $\theta^Y_{10}$. At each node, the range and number of possible nodal types is defined by the number of parents that that node has and the number of values the variables can take on. For instance, assuming all variables to be binary, if $Y$ has parents $X$ and $W$, then there are $2^{\left(2^2\right)}=16$) possible causal types for the $Y$ node. 

<!-- There are $2^2$ possible combinations of values that two binary causal variables can take on----$(X=0,W=0), (X=0,W=1), (X=1,W=0), (X=1,W=1)$---which implies four possible causal conditions over which $Y$'s possible responses must be defined. For instance, as we have seen, with two causal variables, we can have $\theta^Y_{0000}$, where $Y$ is always 0; $\theta^Y_{0001}$, where $Y$ is 0 unless both $X$ and $W$ are 1; and so on.^[These nodal types can require many indices--$2^k$ for a node with $k$ parents---and the rule we follow is that the $i$th subscript indicates the value the node takes when parent $j \in {1, 2, ..., k}$ take values $\mod(floor((i-1)/(2^{j-1})), 2)$ For instance for `Y0111` the first index means that Y takes the value 0 where both parents are 0,  in all other cases it takes value 1.] To get the total number of nodal types, we simply raise $2$ (since $Y$ is binary) to the number of causal conditions (4), giving the number of possible patterns of $Y$ values that could be generated across these four conditions (16). (The full set of nodal types for two causal variables in a binary setup is given in \@ref(tab:PO16).)^[More generally, let us say that any node $j$ can take on $r_j$ possible values and has parents belonging to set $PA_j$ and that each parent, $i \in PA_j$, can take on $r_i$ values. Then the number of nodal types for node $j$ is equal to $r_j^{\prod_{i \in PA_j}r_i}$. Informally, the exponent in this expression simply multiplies by one another the number of values that each of $j$'s parents can take on. This product tells us the number of causal conditions across which $j$'s responses must be defined. We then raise the number of values that $j$ can take on to the power of the number of causal conditions. With all variables binary, this expression translates to $2^{\left(2^k\right)}$ nodal types for a node with $k$ parents.] -->

<!-- All variables in a model have nodal types defining the value they take on given the value of their parents, including those variables without substantive parents. Suppose that $X$ and $W$, in this model, have no substantively defined parents. We nonetheless define a nodal type for each of them, which simply captures their exogenous assignment to some value. With $X$ binary, for instance, there are two nodal types, $\theta^X_{0}$, where $X$ is set to $0$, and $\theta^X_{1}$, where $X$ is set to $1$. -->

**Causal types**

From the set of all possible nodal types for a DAG, we get the set of all possible causal types by simply elaborating all possible permutations of nodal types.

<!-- We will want to be able to conceive not just of types for individual nodes but of the full collection of nodal types across all nodes in a model. We refer to a unit's full set of nodal types as its *unit causal type* --- or, more simply, causal type --- which we represent as $\theta$.  A causal type is simply a listing that contains one nodal type for each node in the model. For instance, with a model with variable $X$, $W$, and $Y$, each unit has a *causal* type composed of its *nodal* types on each of the three nodes.^[A model in which each node $j$ has $k_j$ parents has $\prod_j2^{\left(2^{k_j}\right)}$ causal types that uniquely determine what data will be observed for a type under all possible interventions on its exogenous nodes.]  Thus, one causal type in this model could be $\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_1, \theta^Y = \theta^Y_{1101})$. Another could be $\theta = (\theta^X = \theta^X_0, \theta^W = \theta^W_1, \theta^Y = \theta^Y_{0001})$. And so on. -->

<!-- We show the mapping between nodal and causal types, for a simply $X \rightarrow Y$ model, in Table \@ref(tab:nodalcausalmatrix). The column headings represent the $8$ permissible causal types, each expressed simply as a concatenated strings of nodal types. The row headings represent the nodal types. In each interior cell, a $1$ or $0$ indicates whether or not a given nodal type is a component of a given causal type. As can be seen, each causal type has two nodal types that are its components since there are two nodes in this model. Each $X$-nodal type is part of four causal types since it can be combined with four different $Y$-nodal types, while each $Y$-nodal type is part of two causal types since it can be combined with two $X$-nodal types. -->

<!-- |             **Causal Types $\rightarrow$** | $\theta^X_0$.$\theta^Y_{00}$ | $\theta^X_1$.$\theta^Y_{00}$ | $\theta^X_0$.$\theta^Y_{10}$ | $\theta^X_1$.$\theta^Y_{10}$ | $\theta^X_0$.$\theta^Y_{01}$ | $\theta^X_1$.$\theta^Y_{01}$ | $\theta^X_0$.$\theta^Y_{11}$ | $\theta^X_1$.$\theta^Y_{11}$ | -->
<!-- |-------------------------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:| -->
<!-- |         **Nodal types $\downarrow$**        |                              |                              |                              |                              |                              |                              |                              |                              | -->
<!-- |  $\theta^X_0$ |               1              |               0              |               1              |               0              |               1              |               0              |               1              |               0              | -->
<!-- | $\theta^X_1$ |               0              |               1              |               0              |               1              |               0              |               1              |               0              |               1              | -->
<!-- |               $\theta^Y_{00}$              |               1              |               1              |               0              |               0              |               0              |               0              |               0              |               0              | -->
<!-- |               $\theta^Y_{10}$              |               0              |               0              |               1              |               1              |               0              |               0              |               0              |               0              | -->
<!-- |               $\theta^Y_{01}$              |               0              |               0              |               0              |               0              |               1              |               1              |               0              |               0              | -->
<!-- |               $\theta^Y_{11}$              |               0              |               0              |               0              |               0              |               0              |               0              |               1              |               1              | -->
<!-- Table: (\#tab:nodalcausalmatrix). A mapping between nodal types and causal types for a simple $X \rightarrow Y$ model. -->






<!-- |                                    **Causal Types $\rightarrow$** | $\theta^X_0$.$\theta^Y_{00}$ | $\theta^X_1$.$\theta^Y_{00}$ | $\theta^X_0$.$\theta^Y_{10}$ | $\theta^X_1$.$\theta^Y_{10}$ | $\theta^X_0$.$\theta^Y_{01}$ | $\theta^X_1$.$\theta^Y_{01}$ | $\theta^X_0$.$\theta^Y_{11}$ | $\theta^X_1$.$\theta^Y_{11}$ | -->
<!-- |------------------------------------------------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:|:----------------------------:| -->
<!-- |                    **Parameters $\downarrow$**                    |                              |                              |                              |                              |                              |                              |                              |                              | -->
<!-- |               $\theta^X_0 | \theta^Y= \theta^Y_{01}$              |               0              |               0              |               0              |               0              |               1              |               0              |               0              |               0              | -->
<!-- | $\theta^X_1 | \theta^Y_{01}$$\theta^X_1 | \theta^Y= \theta^Y_{01}$ |               0              |               0              |               0              |               0              |               0              |               1              |               0              |               0              | -->
<!-- |             $\theta^X_0| \theta^Y \neq \theta^Y_{01}$             |               1              |               0              |               1              |               0              |               0              |               0              |               1              |               0              | -->
<!-- |             $\theta^X_1 | \theta^Y \neq \theta^Y_{01}$            |               0              |               1              |               0              |               1              |               0              |               0              |               0              |               1              | -->
<!-- |                          $\theta^Y_{00}$                          |               1              |               1              |               0              |               0              |               0              |               0              |               0              |               0              | -->
<!-- |                          $\theta^Y_{10}$                          |               0              |               0              |               1              |               1              |               0              |               0              |               0              |               0              | -->
<!-- |                          $\theta^Y_{01}$                          |               0              |               0              |               0              |               0              |               1              |               1              |               0              |               0              | -->
<!-- |                          $\theta^Y_{11}$                          |               0              |               0              |               0              |               0              |               0              |               0              |               1              |               1              | -->





<!-- |                            | $\theta^X_0$.$\theta^Y_{00}$ | $\theta^X_1$.$\theta^Y_{00}$ | $\theta^X_0$.$\theta^Y_{10}$ | $\theta^X_1$.$\theta^Y_{10}$ | $\theta^X_0$.$\theta^Y_{01}$ | $\theta^X_1$.$\theta^Y_{01}$ | $\theta^X_0$.$\theta^Y_{11}$ | $\theta^X_1$.$\theta^Y_{11}$ | -->
<!-- |-----------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------| -->
<!-- | $\theta^X_0 | \theta^Y= \theta^Y_{01}$ | 0                            | 0                            | 0                            | 0                            | 1                            | 0                            | 0                            | 0                            | -->
<!-- | $\theta^X_1 | \theta^Y= \theta^Y_{01}$ | 0                            | 0                            | 0                            | 0                            | 0                            | 1                            | 0                            | 0                            | -->
<!-- | $\theta^X_0| \theta^Y \neq \theta^Y_{01}$                | 1                            | 0                            | 1                            | 0                            | 0                            | 0                            | 1                            | 0                            | -->
<!-- | $\theta^X_1 | \theta^Y \neq \theta^Y_{01}$                | 0                            | 1                            | 0                            | 1                            | 0                            | 0                            | 0                            | 1                            | -->
<!-- | $\theta^Y_{00}$             | 1                            | 1                            | 0                            | 0                            | 0                            | 0                            | 0                            | 0                            | -->
<!-- | $\theta^Y_{10}$             | 0                            | 0                            | 1                            | 1                            | 0                            | 0                            | 0                            | 0                            | -->
<!-- | $\theta^Y_{01}$             | 0                            | 0                            | 0                            | 0                            | 1                            | 1                            | 0                            | 0                            | -->
<!-- | $\theta^Y_{11}$             | 0                            | 0                            | 0                            | 0                            | 0                            | 0                            | 1                            | 1                            | -->

### Priors

Our background beliefs about a causal domain will usually consist of more than just beliefs about which variables have causal connections; they will also typically contain beliefs about what *kinds* of effects operate between variables. That is, they will contain beliefs about which types are possible or, more generally, are more or less common in the world. We express these beliefs over causal effects as probability distributions over the nodal types. Beliefs about causal types are implied by beliefs about nodal types. In cases with unobserved confounding beliefs are defined over the joint distributions of nodal types.

For process tracing, our beliefs over nodal type $\theta^j$, say, simply capture the subjective probability we have that the type takes on different values. We do not *need* to defend this belief to use the machinery.  We use $\lambda^j_x$ to denote the probability that $\theta^j = \theta^j_x$. Often however it helps with intuition to think of  a given case of interest -- the one we are studying and seek to learn about -- as being drawn at random from a population and to think about our beliefs for the *single* case as stemming from our beliefs about the population from which it is drawn. In that case $\lambda^j_x$ can be thought of as a *share*. So, for instance, our prior belief about the probability that inequality has a positive effect on democratization in Mexico in 1999 is our belief about how commonly inequality has a positive effect on democratization in the population of cases that are "like" Mexico in 1999.^[The reference population for a case is defined based on whatever we already know about the case. Thus, for instance, if we already know that the case has $Y=1$ before we begin process tracing, then the relevant population for the formation of prior beliefs is all cases in which $Y=1$.] 

A $\lambda^j$ is simply a vector of numbers (or proportions), one for each possible nodal type, with all numbers non negative and summing to $1$. So, for instance, $\lambda^Y$ for our current example would be a vector with four values, each of which expresses a probability on one of the four nodal types at $Y$. So we might have $\lambda^Y_{01}=0.1$, $\lambda^Y_{11}=0.05$, and so on -- with the $\lambda^Y$ values summing to $1$ because these values are defined over the full set of possible nodal types for $Y$. For the purposes of this chapter we take  $\lambda$ as given---you start with beliefs; in later chapters however, when we move beyond single cases,   $\lambda$  becomes the quantity of interest.

<!-- We can, in turn, use these beliefs about nodal-type probabilities -- to create prior probabilities over the *causal* type for the case at hand. Since causal types are merely combinations of nodal types we can take a set of probabilities on nodal types and calculate the probability that our case is of any given causal type. To do so, we need to join together $\lambda$'s across the nodes in a model.  -->

Consider now beliefs over causal types. Let's start with with a situation in which we  assume that the nodal types are independent of one another. We can think of this as a situation in which there is no confounding that is not captured in the graph -- no variable missing from the model that is a common ancestor of multiple nodes in the model. In this situation, our beliefs over causal types are simply the product of our beliefs over the component nodal types (since the joint probability of independent events is simply the product of their individual probabilities). For instance, one causal type might be "a unit in which $X=1$ and in which $Y=1$ no matter what value $X$ takes." In this case the probability that a case is of this causal type is  $\Pr(\theta^X = \theta^X_1)\Pr(\theta^Y = \theta^Y_{11}) = \lambda^X_1\lambda^Y_{11}$.

The simplest way in which we can express beliefs about the differential probabilities of different causal possibilities is by *eliminating* nodal types that we do not believe to be possible---setting their parameter values to $0$. Suppose, for instance, that we are examining the effect of ethnic diversity on civil war in a case. We might not know whether ethnic diversity causes civil war in this case, but we might have sufficient background knowledge to believe that ethnic diversity never has a *negative* effect on civil war: it never prevents a civil war from happening that would have happened in the absence of ethnic diversity. We would thus want to set the parameter value for a negative causal effect to $0$. If we then know nothing about the relative frequencies of the three remaining nodal types for $Y$, we may (following the  principle of indifference),  frequency of positive effects, null effects with civil war destined to happen, and null effects with civil war never going to happen, assigning a weight of $\frac{1}{3}$ to each of them. 

In a situation of unobserved confounding, our beliefs over causal types are still well defined, though they are no longer the simple product of beliefs over nodal types. In this situation we need to describe a joint distribution over nodal types. In practice we can do this by specifying a probability for one nodal type and a conditional probability for another.  Let us imagine for instance, in a simple $X \rightarrow Y$ model, that we believe that some unobserved factor both affects both the likelihood of $X = 1$ and also $X$'s effect on $Y$: maybe, for instance, $X$ is more likely to be assigned to 1 where $X$ has a positive effect. This is the same as saying that the probability that $\theta^X$ and $\theta^Y$ are correlated. Now, the probability of any combination of $\theta^X$ and $\theta^Y$ must be calculated using the joint probability formula, $\Pr(A, B) = \Pr(A)\Pr(B|A)$.^[In words, the probability of $A$ and $B$ occurring is equal to the probability of $A$ occurring times the probability of $B$ occurring *given* that $A$ occurs.] Thus, for instance, $\Pr(\theta^Y = \theta^Y_{01}, \theta^X = \theta^X_1) = \Pr(\theta^Y = \theta^Y_{01})\Pr(\theta^X = \theta^X_1 | \theta^Y = \theta^Y_{01})$. To form priors over causal types in this situation, we need to posit beliefs about a set of more complex, conditional probabilities for $X$'s type. Specifically, we need to posit, *for those cases* with a positive effect of $X$ on $Y$, what are the chances a case is "assigned" to $X=1$; *for those cases* with a negative effect, and similarly for other nodal types. 

Thus for instance we represent $\Pr(\theta^X_1, \theta^Y_{01}) = \Pr(\theta^X_1)\Pr(\theta^Y_{01}|\theta^X_1)$ which we write as $\lambda^X_1, \lambda^{Y|\theta^X_1}_{01}$. The notation is awkward but the key thing is that we have a well defined set of beliefs that we need to take into account to assess the probability of different causal types.  

<!-- FLAG: Check population slanguage -->

<!-- It might be difficult to form beliefs about such conditional probabilities. Doing so amounts to saying that we do not know what generates confounding, but we know the correlations it generates in the data. We may wonder how often we will be in that epistemological position. An alternative way to parse the problem, then, is to *model* the confounding by including the confounder (say, $Z$) as a new node in the graph. In the above example, $Z$ would point into both $X$ and $Y$. We would then posit probabilities (or population proportions) for a set of nodal types for $X$ -- representing $X$'s possible responses to $Z$ -- and for $Y$ -- representing $Y$'s possible responses to both $X$ and $Z$. We may find it easier to reason and form beliefs about these more complex nodal types than about the conditional proportions involved in unobserved confounding. The two approaches work out to be analytically equivalent given equivalent underlying beliefs, so the choice between them will be a matter of researcher preference.^[As we will see later in the book, another approach is to gather data on additional cases. When analyzing multiple cases, we can set up our priors to allow for the possibility of unobserved confounding and then, potentially, learn about that confounding from the data. This is not possible under our procedure for single-case process tracing, where we treat the population parameters as given and fixed.] -->

In the `CausalQueries` package we represent the relationship parameters and causal types using a "parameter matrix." This matrix has a row for each parameter, a column for each causal type, and cell entries that indicate whether the parameter forms part of the type probability.  In models without confounding we have a parameter for each nodal type.   In the $X \rightarrow Y$ model, for instance, the causal causal type ($\theta^X_1, \theta^Y_{10}$), has a 1 in the $\lambda^X_1$ row and a 1 in the  $\lambda^Y_{10}$ row only, indicating that the product of these probabilities  gives the probability of the  causal type. In models with confounding the logic is the same except that we have rows for as many conditional probabilities as we need to fully characterize joint distributions.  

<!-- Unobserved confounding in this setup takes the form of a difference in the proportions of a given $X$ type among different $Y$ types. Thus, if $\lambda^X_1, | \theta^Y_{01}$ is not the same as $\lambda^X_1 | \theta^Y \neq \theta^Y_{01}$, we have unobserved confounding. Imagine, for instance, if we are studying the effect of faster economic growth ($X$) on democratization ($Y$), and we believe that there is some unobserved factor that both makes some countries' economies grow more quickly and also makes economic growth more likely to have a positive effect on democratization. This belief amounts to a belief that the probability of a case being assigned to $X=1$ is higher if $Y$'s nodal type is $\theta^Y_{01}$ than if it is not. In other words, in terms of the rows in Table \@ref(tab:parammatrix), we believe here that $\lambda^X_1 | \theta^Y=\theta^Y_{01}$ is greater than $\lambda^X_1 | \theta^Y \neq \theta^Y_{01}$. To illustrate, we provide parameter values along these lines in the final column. -->

<!-- Again, however, a researcher might prefer to specify the confounder (say, $Z$) as a node in the model. The rows in the parameter matrix would then be a set of population parameters defined as proportions of *un*conditional nodal types, with four $X$-types representing possible responses to $Z$, and 16 $Y$ types, representing $Y$'s possible responses to $X$ and $Z$. -->

<!-- | **Causal Types $\rightarrow$** | $\theta^X_0,\theta^Y_{00}$ | $\theta^X_1,\theta^Y_{00}$ | $\theta^X_0,\theta^Y_{10}$ | $\theta^X_1,\theta^Y_{10}$ | $\theta^X_0,\theta^Y_{01}$ | $\theta^X_1,\theta^Y_{01}$ | $\theta^X_0,\theta^Y_{11}$ | $\theta^X_1,\theta^Y_{11}$ | Parameter values (population proportions) | -->
<!-- |------------------------------------------------:|:------------------------------:|:------------------------------:|:------------------------------:|:-----------------------------:|:------------------------------:|:------------------------------:|:------------------------------:|:------------------------------:| -->
<!-- |           **Population parameters $\downarrow$**           |                                |                                |                                |                               |                                |                                |                                |                                |                                           | -->
<!-- |     $\lambda^X_0 | \theta^Y= \theta^Y_{01}$     |                0               |                0               |                0               |               0               |                1               |                0               |                0               |                0               |                    0.3                    | -->
<!-- |          $\lambda^X_1 | \theta^Y=\theta^Y_{01}$         |                0               |                0               |                0               |               0               |                0               |                1               |                0               |                0               |                    0.7                    | -->
<!-- |    $\lambda^X_0| \theta^Y \neq \theta^Y_{01}$   |                1               |                0               |                1               |               0               |                0               |                0               |                1               |                0               |                    0.5                    | -->
<!-- |   $\lambda^X_1 | \theta^Y \neq \theta^Y_{01}$   |                0               |                1               |                0               |               1               |                0               |                0               |                0               |                1               |                    0.5                    | -->
<!-- |                 $\lambda^Y_{00}$                |                1               |                1               |                0               |               0               |                0               |                0               |                0               |                0               |                    0.2                    | -->
<!-- |                 $\lambda^Y_{10}$                |                0               |                0               |                1               |               1               |                0               |                0               |                0               |                0               |                    0.2                    | -->
<!-- |                 $\lambda^Y_{01}$                |                0               |                0               |                0               |               0               |                1               |                1               |                0               |                0               |                    0.4                    | -->
<!-- |                 $\lambda^Y_{11}$                |                0               |                0               |                0               |               0               |                0               |                0               |                1               |                1               |                    0.2                    | -->
<!-- Table: (\#tab:parammmatrixconf). A mapping between nodal types and causal types for a simple $X \rightarrow Y$ model *with* unobserved confounding. -->

<!-- In this case the number of parameters may exceed the number of nodal types, with, for instance parameters $\hat{\lambda}^Y_{11}$ representing $\Pr(\theta^Y = \theta^Y_{11}|\theta^X = \theta^X_1)$  and $\tilde{\lambda}^Y_{11}$ representing $\Pr(\theta^Y = \theta^Y_{11}|\theta^X = \theta^X_0)$.   -->

<!-- One special kind of prior that we might wish to set is to disallow a particular (conditional) type altogether. For instance, if studying the effect of we may believe that  -->

#### Possible data types.

A *data type* is a particular pattern of data that we could potentially observe for a given case. More specifically, a data type is a set of values, one for each node in a model. For instance, in our $X, W, Y$ setup, $X=1, W=0, Y=0$ would be one data type. 

Importantly, absent intervention, each possible causal type *maps deterministically into a single data type.* One intuitive way to think about why this is the case is that a causal type tells us (a) the values to which all exogenous variables in a model are assigned and (b) how all endogenous variables respond to their parents. Given these two components, only one set of node values is possible. For example, causal type $\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_0, \theta^Y = \theta^Y_{0100})$ imples data $X=1, W=0, Y=1$. Absent intervention, there is no other set of data that can be generated by this causal type. 

Equally importantly, however, *the mapping from causal types to data types is not one-to-one.* More than one causal type can generate the same case-level data pattern. For instance, the causal type $\theta = (\theta^X = \theta^X_1, \theta^W = \theta^W_0, \theta^Y = \theta^Y_{1101})$ will *also* generate the  data type, $X=1, W=0, Y=1$. Thus, observing this data type leaves us with ambiguity about the causal type by which it was generated.

In the `CausalQueries` package we use an *ambiguity matrix* to summarize the  mapping between causal types and data types. There is a row for each causal type and a column for each data type and an entry of 1 indicates that the causal type generates the data type. Each row has a single 1 but each column can have many 1s---an indicator of the ambguity we have regarding causal types when we observe data types. 

|  **Data types** $\rightarrow$ | X0Y0 | X1Y0 | X0Y1 | X1Y1 | Priors on causal types |
|:-----------------------------:|:----:|:----:|:----:|:----:|:----------------------:|
| **Causal types** $\downarrow$ |      |      |      |      |                        |
|   $\theta^X_0,\theta^Y_{00}$  |   1  |   0  |   0  |   0  |           0.1          |
|   $\theta^X_1,\theta^Y_{00}$  |   0  |   1  |   0  |   0  |           0.1          |
|   $\theta^X_0,\theta^Y_{10}$  |   0  |   0  |   1  |   0  |           0.1          |
|   $\theta^X_1,\theta^Y_{10}$  |   0  |   1  |   0  |   0  |           0.1          |
|   $\theta^X_0,\theta^Y_{01}$  |   1  |   0  |   0  |   0  |           0.2          |
|   $\theta^X_1,\theta^Y_{01}$  |   0  |   0  |   0  |   1  |           0.2          |
|   $\theta^X_0,\theta^Y_{11}$  |   0  |   0  |   1  |   0  |           0.1          |
|   $\theta^X_1,\theta^Y_{11}$  |   0  |   0  |   0  |   1  |           0.1          |
Table: (\#tab:ambigmatrix). An ambiguity matrix, mapping from data types to causal types for a simple $X \rightarrow Y$ model.

<!-- In the last column, we provide prior probabilities for each of the causal types. These have been calculated directly from the parameter matrix (Table \@ref(tab:parammatrix)). To see how the calculation works, start with a causal type in the parameter matrix -- say, $\theta^X_0,\theta^Y_{01}$. We go down that causal type's column and select the rows with $1$'s, representing the parameters for the included nodal types, $\lambda^X_0$ and $\lambda^Y_{01}$. As we want the joint probability of these two nodal types (and a parameter matrix is constructed such that the rows represent independent events),^[That is, when there is unobserved confounding, we express conditional proportions, making all of the proportions conditionally independent of one another.] we simply multiply together the values for these included parameters: $0.5 \times 0.4 = 0.2$. As noted, our prior belief about whether the case at hand is of a given causal type is a straightforward function of our beliefs about how prevalent each of the component nodal types is in the population.  -->

As models get more complex, the numbers of causal and data types multiply, though generally the number of causal types increases faster than the number of data types. For a simple mediation model ($X \rightarrow M \rightarrow Y$) there are $2^3 = 8$ data types---possible combinations of values for $X,M,Y$ but $2\times 4 \times 4$ causal types. 

<!-- |       **Data types** $\rightarrow$      | X0M0Y0 | X1M0Y0 | X0M1Y0 | X1M1Y0 | X0M0Y1 | X1M0Y1 | X0M1Y1 | X1M1Y1 | Priors on causal types | -->
<!-- |:---------------------------------------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:----------------------:| -->
<!-- |      **Causal types** $\downarrow$      |        |        |        |        |        |        |        |        |                        | -->
<!-- | $\theta^X_0,\theta^M_{00},\theta^Y_{00}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{00},\theta^Y_{00}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{10},\theta^Y_{00}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{10},\theta^Y_{00}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{01},\theta^Y_{00}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_1,\theta^M_{01},\theta^Y_{00}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_0,\theta^M_{11},\theta^Y_{00}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{11},\theta^Y_{00}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{00},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{00},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{10},\theta^Y_{10}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{10},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{01},\theta^Y_{10}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_1,\theta^M_{01},\theta^Y_{10}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_0,\theta^M_{11},\theta^Y_{10}$ |    0   |    0   |    1   |    0   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{11},\theta^Y_{10}$ |    0   |    0   |    0   |    1   |    0   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{00},\theta^Y_{01}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_1,\theta^M_{00},\theta^Y_{01}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_0,\theta^M_{10},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.04          | -->
<!-- | $\theta^X_1,\theta^M_{10},\theta^Y_{00}$ |    0   |    1   |    0   |    0   |    0   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_0,\theta^M_{01},\theta^Y_{01}$ |    1   |    0   |    0   |    0   |    0   |    0   |    0   |    0   |          0.08          | -->
<!-- | $\theta^X_1,\theta^M_{01},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.08          | -->
<!-- | $\theta^X_0,\theta^M_{11},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.04          | -->
<!-- | $\theta^X_1,\theta^M_{11},\theta^Y_{01}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.04          | -->
<!-- | $\theta^X_0,\theta^M_{00},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{00},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{10},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{10},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    1   |    0   |    0   |          0.02          | -->
<!-- | $\theta^X_0,\theta^M_{01},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    1   |    0   |    0   |    0   |          0.04          | -->
<!-- | $\theta^X_1,\theta^M_{01},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.04          | -->
<!-- | $\theta^X_0,\theta^M_{11},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    1   |    0   |          0.02          | -->
<!-- | $\theta^X_1,\theta^M_{11},\theta^Y_{11}$ |    0   |    0   |    0   |    0   |    0   |    0   |    0   |    1   |          0.02          | -->
<!-- Table: (\#tab:ambigmatrixmed). An ambiguity matrix, mapping from data types to causal types for a simpe mediation model, $X \rightarrow M \rightarrow Y$. -->

<!-- Again, the ambiguities arising from data patterns are apparent. For instance, if we observe $X=1, M=0, Y=0$, we see that there are four causal types that could have generated this pattern.  -->

The ambiguities matrix tells us what types are data consistent and, in doing so, shapes our inferences. 
Table \@ref(tab:ambigmatrixmed)  shows a portion of the ambiguities matrix for the $X \rightarrow M \rightarrow Y$ model, with priors on causal types appended in the final column. In this model  if we observe $X=1, M=0, Y=0$ we have ambiguities over causal types. These data tell us that $\theta^X = \theta^X_1$. But they do not tell us whether $M$'s type is such that $X$ has a negative effect on $M$ ($\theta^M_{10}$) or $X$ has no effect with $M$ fixed at $0$ ($\theta^M_{00}$). Similarly, we do not know whether $M$ has a positive effect on $Y$ ($\theta^Y_{01}$) or no effect with $Y$ fixed at $0$ ($\theta^Y_{00}$). This leaves four combinations of nodal types---four causal types---that are consistent with the data. These types are picked out by the ambiguities matrix.

<!-- Our priors here derive from a set of parameter values, much like in the previous example, in which the $X$ types are equally common (0.5 each); a positive effect of $X$ on $M$ is twice as common (0.4) as the other $M$ types (all set to 0.2); and a positive effect of $M$ on $Y$ is twice as common (0.4) as all other $Y$ types (all at 0.2). We can then easily see why we thus get priors on some causal types are higher than those on others: for instance, the two causal types with priors of 0.08 both have two positive effects (at the $X \rightarrow Y$ and $M \rightarrow Y$ stages) while the causal types with priors of 0.02 include no positive effects at either stage. -->


```{r model289, echo = FALSE}
XY <- make_model("X -> Y") %>% set_parameters(c(.5, .5, .2, .2, .4, .2))
ambiguityXY  <- get_ambiguities_matrix(XY)


XMY <- make_model("X -> M -> Y") %>% set_parameters(c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2))
ambiguityXMY  <- get_ambiguities_matrix(XMY)
```

<!-- ```{r ambigmatrix, echo = FALSE} -->
<!-- ambXY_with_priors <- data.frame(cbind(ambiguityXY, prior = draw_type_prob(XY, using = "parameters"))) -->
<!-- kable(ambXY_with_priors), caption = "Ambiguity matrix for X -> Y model. Rows are causal types, columns are data types. Last column shows possible priors over rows.") -->
<!-- ``` -->


```{r ambiguityXY, echo = FALSE}

ambXY_with_priors <- data.frame(cbind(ambiguityXY, prior = get_type_prob(XY)))

```
<!-- For an $X \rightarrow Y$ model: -->

```{r ambigmatrixmed, echo = FALSE}

ambXMY_with_priors <- data.frame(cbind(ambiguityXMY, prior = get_type_prob(XMY)))

kable(ambXMY_with_priors[1:16, c(1:6, 9)], caption = "Excerpt from the Ambiguity matrix for a chain model. Rows are causal types, columns are data types. Last column shows possible priors over rows.")

# View(x)
```

### Updating on types given the data.

Once we observe actual data in a case, we can then update on the probabilities assigned to each causal type. The logic is simple. When we observe a set of data from a case, we place $0$ probability on all causal types that could not have produced these data; we then scale up the probabilities on all causal types that could have. 

<!-- We can see how this works within an ambiguity matrix. Let's return to the ambiguity matrix in Table \@ref(tab:ambigmatrix).  -->
As a simple example imagine we have an $X\rightarrowY $ model with equal prior weights (1/8) on each of the eight possible causal types. Now, suppose that we observe the data $X=1, Y=1$, i.e., data type $X1Y1$. This data is consistent with some causal types but not others.  Only two causal types are consistent with the data: $\theta^X_1, \theta^Y_{01}$ and $\theta^X_1, \theta^Y_{11}$. We therefore put 0 weight on all other causal types and scale up the remaining probabilities so that they sum to 1 (preserving the ratio between them).  The result gives *posterior* probabilities on the causal types. We display an "updated" ambiguity matrix, with excluded data types and causal types removed, in Table \@ref(tab:ambigupdate). 

Before we see any data on the case at hand, then, we believe (based on our beliefs about the population to which the case belongs) that there is a 1/8 probability that the case is one in which $X$ is assigned to $1$ and has a positive effect on $Y$; and 1/8 probability that it's a case in which $X$ gets assigned to $1$ and has no effect on $Y$  (and so $Y$ is $1$ regardless of $X$). Seeing the $X=1, Y=1$ data, we now believe that there is a 1/2 probability that the case is of the former type, and a 1/2 probability that it is of the latter type. Had our prior beliefs on types been different from each other, the posterior beliefs would have scaled up accordingly.

|  **Data types** $\rightarrow$ | X1Y1 | Priors on causal types | Posteriors on causal types |
|:-----------------------------:|:----:|:----------------------:|:--------------------------:|
| **Causal types** $\downarrow$ |      |                        |                            |
|   $\theta^X_1,\theta^Y_{01}$  |   1  |           1/8          |          1/2            |
|   $\theta^X_1,\theta^Y_{11}$  |   1  |           1/8          |          1/2            |
Table: (\#tab:ambigupdate). Ambiguities in an $X \rightarrow Y$ model after observing $X=1, Y=1$ in a case.



<!-- ```{r, echo = FALSE} -->
<!-- ambXY_with_priors%>% -->
<!--     mutate(type = rownames(ambXY_with_priors)) %>% -->
<!--     select(type, X1Y1, prior) %>% -->
<!--     filter(X1Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable() -->
<!-- ``` -->

We now walk through how this work for the more complex  $X \rightarrow M \rightarrow Y$ model, and the ambiguity matrix in Table \@ref(tab:ambigmatrixmed). If we observe the data $X=1, M=0, Y=0$, for instance, this exercise would yield the updated ambiguity matrix in Table \ref@(tab:ambigmedupdate). Here we have eliminated all rows (causal types) with a $0$ in the relevant data-type column ($X1M0Y0$) and formed the posteriors by scaling up the priors in the retained rows. 


|       **Data types** $\rightarrow$      | X1M0Y0 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |                        |                            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{00}$ |    1   |          0.02          |          0.1667            |
| $\theta^X_1,\theta^M_{10},\theta^Y_{00}$ |    1   |          0.02          |          0.1667            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{01}$ |    1   |          0.04          |          0.3333            |
| $\theta^X_1,\theta^M_{10},\theta^Y_{01}$ |    1   |          0.04          |          0.3333            |
Table: (\#tab:ambigmedupdate). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing $X=1, M=0, Y=0$ in a case.

A notable feature of the logic of single-case process tracing is that the relative probabilities on the retained causal types never change. If we start out believing that causal type $A$ is twice as likely as causal type $B$, and both $A$ and $B$ are retained once we see the data, then $A$ will be twice as likely as $B$ in our posteriors. All updating occurs by *eliminating* causal types from consideration and zeroing in on those that remain.

```{r ambXMYwpriors, echo = FALSE}
ambXMY_with_priors%>%
    mutate(type = rownames(ambXMY_with_priors)) %>%
    select(type, X1M0Y0, prior) %>%
    filter(X1M0Y0 ==1)%>%
   mutate(posterior = prior/sum(prior)) %>%
   kable()
```



<!-- ```{r, echo = FALSE} -->
<!-- data.frame(cbind(ambiguityXMY)) %>% -->
<!--     mutate(type = rownames(ambiguityXMY), prior = draw_type_prob(XMY)) %>% -->
<!--     select(type, X1M1Y1, prior) %>% -->
<!--     filter(X1M1Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable(digits = 2) -->
<!-- ``` -->

#### Partial data

A similar logic applies if partial data are observed: that is, if we do not collect data for all nodes in the model. The one difference is that, now, rather than reducing to one column we entertain the possibility of any data *type* consistent with the *observed data*. In general, more than one data type will be consistent with partial data. For instance, suppose that we observe $X=1, Y=0$ but do not observe $M$'s value. These are data that are consistent with both the data type $X1M0Y0$ and the data type $X1M1Y0$ (since the unobserved $M$ could be either $0$ or $1$). We thus retain both of these data-type columns as well as all causal types consistent with *either* of these data types. This gives the updated ambiguity matrix in Table \@ref(tab:ambigmedupdatepartial). We note that, with these partial data, we are not able to update as strongly. For instance, for the causal type $\theta^X_1,\theta^M_{00},\theta^Y_{00}$, instead of updating to a posterior probability of 0.1667, we update to a posterior of only 0.0833 -- because there is a larger set of causal types with which these partial data are consistent.


|       **Data types** $\rightarrow$      | X1M0Y0 | X1M1Y0 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |        |                        |                            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{00}$ |    1   |    0   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{00}$ |    1   |    0   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{00}$ |    0   |    1   |          0.04          |           0.1667           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{00}$ |    0   |    1   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{10}$ |    0   |    1   |          0.04          |           0.1667           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{10}$ |    0   |    1   |          0.02          |           0.0833           |
| $\theta^X_1,\theta^M_{00},\theta^Y_{01}$ |    1   |    0   |          0.04          |           0.1667           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{01}$ |    1   |    0   |          0.04          |           0.1667           |
Table: (\#tab:ambigmedupdatepartial). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing partial data in case: $X=1, Y=0$, with $M$ unobserved.


```{r ambigmedupdatepartial, echo = FALSE, include = FALSE}
ambXMY_with_priors%>%
    mutate(type = rownames(ambXMY_with_priors)) %>%
    select(type, X1M0Y0, X1M1Y0, prior) %>%
    filter(X1M0Y0 ==1 | X1M1Y0 ==1)%>%
   mutate(posterior = prior/sum(prior)) %>%
   kable()
```


<!-- ```{r, echo = FALSE} -->
<!-- data.frame(cbind(ambiguityXMY)) %>% -->
<!--     mutate(type = rownames(ambiguityXMY), prior = draw_type_prob(XMY)) %>% -->
<!--     select(type, X1M0Y1, X1M1Y1, prior) %>%  -->
<!--     filter(X1M1Y1 ==1 | X1M0Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable(digits = 2) -->
<!-- ``` -->

**Updating on queries.**  We now have a posterior probability for each causal type for the case at hand. The causal question we are interested in answering, our query, may not be about causal types *per se.* It is about a query  that can be expressed as a *combination* of causal types, as described in Chapter \@ref(questions). 

For instance, suppose we are working with the model $X \rightarrow M \rightarrow Y$; and that our question is, "Did $X=1$ cause $Y=1$?". This question is asking both:

1. Does $X=1$ in this case? 

2. Does $X$ have a positive effect on $Y$ in this case?  

The causal types that qualify are those, and only those, in which the answer to both is "yes."  

Meeting condition (1) requires that $\theta^X=\theta^X_1$. 

Meeting condition (2) requires that $\theta^M$ and $\theta^Y$ are such that $X$ has an effect on $M$ that yields a positive effect of $X$ on $Y$. This could occur via a positive $X \rightarrow M$ effect linked to a positive $M \rightarrow Y$ effect or via a negative $X \rightarrow M$ effect linked to a negative $M \rightarrow Y$ effect. 

Thus, the qualifying causal types in this model are:

* $\theta^X_1, \theta^M_{01}, \theta^Y_{01}$
* $\theta^X_1, \theta^M_{10}, \theta^Y_{10}$

Our *prior* on the query---what we believe before we collect data on the case at hand---is given simply by summing up the prior probabilities on each of the causal types that correspond to the query. Note that we must calculate the prior from the full ambiguity matrix, before excluding types for inconsistency with the data. Returning to the full ambiguity matrix in Table \@ref(tab:ambigmatrixmed), we see that the priors on these two types (given the population parameters assumed there) are 0.08 and 0.02, respectively, giving a prior for the query of 0.1. 

The posterior on any query is, likewise, given by summing up the posterior probabilities on each of the causal types that correspond to the query, drawing of course from the updated ambiguity matrix. For instance, if we observe the data $X=1, M=1, Y=1$, we update to the ambiguity matrix in Table \@ref(tab:ambigmedupdate2).  Our posterior on the query, "Did $X=1$ cause $Y=1$?" is the sum of the posteriors on the above two causal types. Since $\theta^X_1, \theta^M_{10}, \theta^Y_{10}$ is excluded by the data, this just leaves the posterior on $\theta^X_1, \theta^M_{01}, \theta^Y_{01}$, 0.4444, which is the posterior belief on our query. 

If we observe only the partial data, $X=1, Y=1$, then we update to the ambiguity matrix in Table \@ref(tab:ambigmedupdatepartial2). Now both corresponding causal types are included, and we sum their posteriors to get the posterior on the query: $0.08 + 0.31 = 0.39$.


<!-- FLAG: Briefly discuss other query(s) one could do, though don't show in detail here. Will do pathways in Chap. 7. -->


```{r ambXMYwithpriors, echo = FALSE, include = FALSE}
ambXMY_with_priors%>%
    mutate(type = rownames(ambXMY_with_priors)) %>%
    select(type, X1M1Y1, prior) %>%
    filter(X1M1Y1 ==1)%>%
   mutate(posterior = prior/sum(prior)) %>%
   kable()
```


|       **Data types** $\rightarrow$      | X1M1Y1 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |                        |                            |
| $\theta^X_1,\theta^M_{01},\theta^Y_{01}$ |    1   |          0.08          |          0.4444            |
| $\theta^X_1,\theta^M_{11},\theta^Y_{01}$ |    1   |          0.04          |          0.2222            |
| $\theta^X_1,\theta^M_{01},\theta^Y_{11}$ |    1   |          0.04          |          0.2222            |
| $\theta^X_1,\theta^M_{11},\theta^Y_{11}$ |    1   |          0.02          |          0.1111            |
Table: (\#tab:ambigmedupdate2). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing $X=1, M=1, Y=1$ in a case.



|       **Data types** $\rightarrow$      | X1M0Y0 | X1M1Y0 | Priors on causal types | Posteriors on causal types |
|:---------------------------------------:|:------:|:------:|:----------------------:|:--------------------------:|
|      **Causal types** $\downarrow$      |        |        |                        |                            |
| $\theta^X_1,\theta^M_{00},\theta^Y_{10}$ |    1   |    0   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{10}$ |    1   |    0   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{01}$ |    0   |    1   |          0.08          |           0.3077           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{01}$ |    0   |    1   |          0.04          |           0.1538           |
| $\theta^X_1,\theta^M_{00},\theta^Y_{11}$ |    0   |    1   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{10},\theta^Y_{11}$ |    0   |    1   |          0.02          |           0.0769           |
| $\theta^X_1,\theta^M_{01},\theta^Y_{11}$ |    1   |    0   |          0.04          |           0.1538           |
| $\theta^X_1,\theta^M_{11},\theta^Y_{11}$ |    1   |    0   |          0.02          |           0.0769           |
Table: (\#tab:ambigmedupdatepartial2). An updated version of the ambiguity matrix in Table \@ref(tab:ambigmatrixmed), after observing partial data in case: $X=1, Y=0$, with $M$ unobserved.



<!-- ```{r, echo = FALSE} -->
<!-- ambXMY_with_priors%>% -->
<!--     mutate(type = rownames(ambXMY_with_priors)) %>% -->
<!--     select(type, X1M0Y1, X1M1Y1, prior) %>% -->
<!--     filter(X1M0Y1 ==1 | X1M1Y1 ==1)%>% -->
<!--    mutate(posterior = prior/sum(prior)) %>% -->
<!--    kable() -->
<!-- ``` -->


For more complex models and queries, it can be more difficult to eyeball the corresponding causal types. In practice, therefore, we use the `get_query_types` function in the `CausalQueries` package to do this for us. 


<!-- For example, supposer we want to know whether $X$ has some causal effect on $Y$ in our simple mediation model. The query, "$X$ haa a causal effect on $Y$" maps onto a relatively large, though still easily calculated, collection of types. Using `gbiqq`'s get_types function, we would define our query as a search for all causal types in which $Y$'s potential outcome when $X=1$ is different from $Y$'s potential outcome when $X=0$. The function then reports back all causal types meeting this condition: -->

<!-- ```{r, eval = FALSE} -->
<!-- get_types(XMY, "Y[X=1] != Y[X=0]") -->
<!-- ``` -->


```{r typesXMY, echo = FALSE, comment = "", include = FALSE}
types <- get_query_types(XMY, "Y[X=1] != Y[X=0]")
cat(paste0(names(types$types[types$types]), collapse = ", "))
```


<!-- This completes the abstract representation of the process tracing procedure. We now build up the intuition by walking through the procedure for simple mediation and moderation models. -->



:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::

**Illustration of Process Tracing with Code**

We illustrate process tracing for a simple mediation model. First, we define the structure of the model using the `make_model` function. We then use `set_parameters` to specify our beliefs about population-level shares of nodal types for each node, entering one proportion for each nodal type in the model. The ordering of nodal types can be viewed within the model summary (`summary(XMY)`). We then query the model to find out what we should believe about the query of interest given different possible data-realizations. Here we define the query (PC) as the probability that $X$ has a positive effect on $Y$. We then use the `given` argument to list the possible data-realizations for which we want to report an estimate for this query. Finally, we tell `CausalQueries` to use the parameter values that we have specified to generate the estimates.

```{r XMY, eval = FALSE}

XMY <- make_model("X -> M -> Y") %>% 
       set_parameters (c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2))

query_model(model = XMY, 
              queries = list(PC = "Y[X=1] > Y[X=0]"), 
              given = list(TRUE, "X==1 & Y==1", "X==1 & Y==1 & M==0", "X==1 & Y==1 & M==1"),
              using = "parameters")
```

```{r XMY2, echo = FALSE}
make_model("X -> M -> Y") %>% 
       set_parameters (c(.5, .5, .2, .2, .4, .2, .2, .2, .4, .2)) %>%
       query_model(
         query = list(PC = "Y[X=1] > Y[X=0]"), 
         given = list(TRUE, "X==1 & Y==1", "X==1 & Y==1 & M==0", "X==1 & Y==1 & M==1"),
         using = "parameters") %>%
      kable()

```

::::

<br>


## Mapping from models to classic qualitative tests 

The approach we have elaborated here appears different from that described in the literature on process-tracing tests -- such as @collier2011understanding, @BennettBayes, or @humphreys2015mixing -- in which one seeks specific evidence that is directly informative about causal propositions: "clues" that are arise with different probabilities if one proposition or another is true. In fact, however, the approaches are deeply connected. Specifically, we can think of causal models as providing a *justification* for the probative value that researchers assign to clues in the classic approach. 

To see this, let's write down the probability of observing a given clue conditional on a unit's causal type using the $\phi$ notation from @humphreys2015mixing. Here $\phi_{jx}$ refers to the probability of observing a clue, $K$, in a case of type $j$ when $X=x$. Assuming an $X\rightarrow M \rightarrow Y$ model and a  prior distribution over the lower-level causal types (the $\lambda$'s), we can derive, for an $X=1$ case, the probability of seeing the clue if the case is of type $b$ (positive effect) or of type $d$ (no effect, and $Y$ always $1$):

\begin{equation}
\begin{split}
\phi_{b1} & = \frac{\lambda_{01}^{K}\lambda_{01}^{Y}}{\lambda_{01}^{K}\lambda_{01}^{Y}+\lambda_{10}^{K}\lambda_{10}^{Y}}\\ 
\phi_{d1} & = \frac{\lambda_{11}^{Y}(\lambda_{01}^{K}+\lambda_{11}^{K})+\lambda_{11}^{K}\lambda_{01}^{Y}}{\lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y}}
\end{split}
\label{eqn:phisfromlambdas}
\end{equation}


These quantities allow for easy mapping between our prior beliefs about our causal query---as expressed in the lower-level model---and the classic process-tracing tests in @Van-Evera:1997. Figure \@ref(fig:phis) illustrates. In each panel, we manipulate a prior for one or more of the lower-level causal effects, keeping all other priors flat, and we see how probative value changes. As the curves for $\phi_b$ and $\phi_d$ diverge, probative value is increasing since there is an increasing difference between the probability of seeing the clue if $X$ has a positive effect on $Y$ and the probability of seeing the clue if $X$ has no effect. 

In the left panel, we see that as we place a lower prior probability on $K$'s being negatively affected by $X$,^[For a given value of $\lambda^K_{01}$, we hold the other $\lambda^K$ values equal by assigning a value of $(1-\lambda^K_{01})/3$ to each.] seeking $K=1$ increasingly takes on the quality of a hoop test for $X$'s having a positive effect on $Y$. The clue, that is, increasingly becomes something we must see if $X$ positively affects $Y$, with the clue remaining moderately probable if there is no effect. Why? The less likely we believe it is that $K=0$ was caused by $X=1$, the less consistent the observation of $K=0$ is with $X$ having a positive causal effect on $Y$ via $K$ (since, to have such an effect, if $X=1$ and $K=0$, would precisely have to mean that $X=1$ *caused* $K=0$). 

In the second graph, we simultaneously change the prior probabilities of zero effects at both stages in the sequence: of $K$ and $Y$ being $1$ regardless of the values of $X$ and $K$, respectively.^[For a given value of $\lambda^K_{11}$, we hold the other $\lambda^K$'s equal by assigning a value of $(1-\lambda^K_{11})/3$ to each; likewise for $\lambda^Y_{11}$ and the other $\lambda^Y$ values.] We see here that, as the probabilities of zero effects jointly diminish, seeking $K=1$ increasingly becomes a smoking-gun test for a positive effect of $X$ on $Y$: the probability of seeing the clue if the case is a $d$ type diminishes. The reason is that, as zero effects at the lower level become less likely, it becomes increasingly unlikely that $K=1$ could have occurred without a positive effect of $X$ on $K$, and that $Y=1$ could have occurred (given that we have seen $K=1$) without a positive effect of $K$ on $Y$.

<!-- This example also helps clarify the kind of theoretical knowledge required for drawing inferences from clues. As we have emphasized, the structural equations comprising a causal model can be fully non-parametric. As the example illustrates, $\theta_Y$ can be a type variable that determines different the equation for an endogenous variable in a causal model can  can take the form of beliefs about the proportions of  -->


```{r phis, echo = FALSE,  fig.align="center", out.width='.85\\textwidth', fig.width = 9, fig.height = 4, fig.cap = "The probability of observing $K$ given causal type for different beliefs on lower-level causal effects. In the left figure, priors on all lower-level causal effects are flat except for the probability that $X$ has a negative effect on $K$. If we believe that it is unlikely that $X$ has a negative effect on $K$, $K$ becomes a `hoop' test for the proposition that a case is of type $b$. The righthand figure considers simultaneous changes in $\\lambda_{11}^K$ and  $\\lambda_{11}^Y$---the probabilities that $K=1$ regardless of $X$, and that $Y=1$  regardless of $K$, with flat distributions on all other lower-level effects. With $\\lambda_{11}^K$, $\\lambda_{11}^Y$ both close to 0, $K$ becomes a 'smoking gun' test for the proposition that $X$ has a positive effect on $Y$ ($b$ type).", errors = FALSE, warning = FALSE, message = FALSE, comment = FALSE}

# sim_data <- function(sims=100, pX = 1, K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25)){
#   X <- rmultinom(sims, 1, c(1-pX, pX))
#   X <- t(X) %*% (0:1)  
#   uK <- rmultinom(sims, 1, K_eventprobs) 
#   uK <- t(uK) %*% (1:4)  
#   uY <- rmultinom(sims, 1, Y_eventprobs) 
#   uY <- t(uY) %*% (1:4)  
#   K <- (uK==1)*(X==0) +  (uK==2)*(X==1) +  (uK==4)
#   Y <- (uY==1)*(K==0) +  (uY==2)*(K==1) +  (uY==4)
#   
#   a_higher <- (uK==2)*(uY==1) + (uK==1)*(uY==2)
#   b_higher <- (uK==2)*(uY==2) + (uK==1)*(uY==1)
#   c_higher <- (uY == 3) + (uK==3)*(uY==2) + (uK==4)*(uY==1)
#   d_higher <- (uY == 4) + (uK==3)*(uY==1) + (uK==4)*(uY==2)
#   
#   data.frame(X, K, Y, a_higher, b_higher, c_higher, d_higher)
#  }
# 
# 
# pv <- function(sims=100, K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25)){
#   D <- sim_data(sims = sims, K_eventprobs = K_eventprobs, Y_eventprobs = Y_eventprobs)  
#   c(mean(D$K[D$X==1 & D$Y==1 & D$b_higher ==1]), mean(D$K[D$X==1 & D$Y==1 & D$d_higher ==1]))
# }

pv_analytic_b <- function(K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25))
  {K_eventprobs[2]*Y_eventprobs[2]}/{K_eventprobs[2]*Y_eventprobs[2]+K_eventprobs[1]*Y_eventprobs[1]}

pv_analytic_d <- function(K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25))
  {Y_eventprobs[4]*(K_eventprobs[2]+K_eventprobs[4])+K_eventprobs[4]*Y_eventprobs[2]}/{Y_eventprobs[4]+K_eventprobs[3]*Y_eventprobs[1]+K_eventprobs[4]*Y_eventprobs[2]}

par(mfrow = c(1,2))

plot(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_b(K_eventprobs = c(i,(1-i)/3,(1-i)/3,(1-i)/3), Y_eventprobs = c(1/4,1/4,1/4,1/4))), type = "l", xlab = expression(paste(lambda[10]^{K})), ylab = "", main = "A Hoop Test", ylim = c(0,1))

points(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_d(K_eventprobs = c(i,(1-i)/3,(1-i)/3,(1-i)/3), Y_eventprobs = c(1/4,1/4,1/4,1/4))), type = "l", lty = 2)
text(.1,.8, expression(phi[b]))
text(.1,.5, expression(phi[d]))


plot(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_b(K_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i), Y_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i))), ylim = c(0,1), type = "l", xlab = expression(paste(lambda[11]^{K}," and ", lambda[11]^{Y})), ylab = "", main = "A Smoking Gun Test")

points(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_d(K_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i), Y_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i))),  type = "l", lty = 2)
text(.1,.55, expression(phi[b]))
text(.1,.25, expression(phi[d]))

```

## Assessing the possibility of probative value from a graph

<!-- Rules for inferring information about one variable from another are th stuff of graphoids  [@pearl1985graphoids] see also [@geiger1987non] and [@pearl1987logic]...  -->

As we have argued, causal queries can be expressed as collections of combinations of nodal types (i.e., as collections of causal types) in a causal model. A nodal type is itself represented as an unobservable node in a model --- as a $\theta^j$ pointing into node $j$. Thus, causal inference in this framework is *the use of observable nodes on a causal graph to assess the value of one or more unobserved nodes on a causal graph.* Placing our queries on the graph together with the observable nodes has the important advantage of allowing us to graphically identify the possibilities for learning about these queries: that is, to say which observable nodes are potentially informative about a given query.

<!-- Case-level causal effects and causal paths can be defined in terms of response-type nodes; average effects and notable causes in terms of population-level parameter nodes (e.g., $\pi$ or $\lambda$ terms); and questions about actual causes in terms of exogenous conditions that yield particular endogenous values (conditioning on which makes some variable a counterfactual cause).  -->

<!-- We thus define   -->

To think through the logic of potential probative value, it is useful to distinguish among three different features of the world, as represented in our causal model: there are the things we want to learn about; the things we have already observed; and the things we could observe. As notation going forward, we let:

* $\mathcal Q$ denote the collection of $\theta^j$ nodes that define our *query*; $\mathcal Q$ cannot be directly observed so that its values must be inferred;
* $\mathcal W$ denote a set of previously observed nodes in the causal model; and 
* $\mathcal K$ denote a set of additional  variables---clues---that we have not yet observed but could observe.

Now suppose that we seek to design a research project to investigate a causal question. How should the study be designed? Given that there are some features of the world that we have already observed, which additional clues should we seek to collect to shed new light on our question? In terms of the above notation, what we need to figure out is whether a given $\mathcal K$ might be informative about---might provide additional leverage on---$\mathcal Q$ given the prior observation of $\mathcal W$. 

To ask whether one variable (or set of variables) is informative about another is to ask whether the two (sets of) variables are, on average, *correlated* with one another, given whatever we already know. Likewise, if two variables' distributions are fully *independent* of one another (conditional on what else we have observed), then knowing the value of one variable can provide no new information about the value of the other. 

Thus, asking whether a set of clues, $\mathcal K$, is informative about $\mathcal Q$ given the prior observation of $\mathcal W$, is equivalent to asking whether $\mathcal K$ and $\mathcal Q$ are conditionally independent given $\mathcal W$. That is, $\mathcal K$ can be informative about $\mathcal Q$ given $\mathcal W$ only if $\mathcal K$ and $\mathcal Q$ are *not* conditionally independent of one another given $\mathcal W$. 

As our discussion of conditional independence in Chapter \@ref(models) implies, as long as we have built $\mathcal Q$, $\mathcal K$, and $\mathcal W$ into our causal model of the phenomenon of interest, we can answer this kind of question by inspecting the structure of the model's DAG. In particular, what we need to go looking for are relationships of *$d$-separation*. The following proposition, with only the names of the variable sets altered, is from @pearl2009causality (Proposition 1.2.4): 

**Proposition 1:**  If sets $\mathcal Q$ and $\mathcal K$ are $d$-separated by $\mathcal W$ in a DAG, $\mathcal G$, then $\mathcal Q$ is independent of $\mathcal K$ conditional on $\mathcal W$ in every distribution compatible with $\mathcal G$. Conversely, if $\mathcal Q$ and $\mathcal K$ are *not* $d$-separated by $\mathcal W$ in DAG $\mathcal G$, then $\mathcal Q$ and $\mathcal K$ are dependent conditional on $\mathcal W$ in at least one distribution compatible with DAG $\mathcal G$.

We begin with a causal graph and a set of nodes on the graph ($W$) that we have already observed. Given what we have already observed, *a collection of clue nodes, $\mathcal K$, will be uninformative about the query nodes, $\mathcal Q$, if $\mathcal K$ is  $d$-separated from $\mathcal Q$ by $\mathcal W$ on the graph.* (Equivalently, $\mathcal K$, will be uninformative about $\mathcal Q$, given that we have already observed $\mathcal W$, if  $\mathcal K$ and $\mathcal Q$ are conditionally independent given $\mathcal W$.) When $\mathcal W$ $d$-separates $\mathcal K$ from $\mathcal Q$, this means that what we have already observed already captures all information that the clues might yield about our query. On the other hand, if $\mathcal K$ and $\mathcal Q$ are $d$-connected (i.e., not $d$-separated) by $W$, then $K$ is *possibly* informative about $Q$.$K$ is not  $d$-separated from $\mathcal Q$ by $\mathcal W$.^[This proposition is almost coextensive with the definition of a DAG. A DAG is a particular kind of dependency  model ("graphoid") that is a summary of a  collection of "independency statements", $(I)$, over distinct subsets of $\mathcal V$ (Pearl and Verma 1987), where $I(\mathcal Q,\mathcal W,\mathcal K)$ means  "we learn nothing about $\mathcal Q$ from $\mathcal K$ if we already know $\mathcal W$". More formally:
$I(\mathcal K, \mathcal W,\mathcal Q) \leftrightarrow P(\mathcal K,\mathcal Q|\mathcal W)=P(\mathcal K|\mathcal W)P(\mathcal Q|\mathcal W)$. A Directed Acyclic Graph Dependency model is one where the set of independencies corresponds exactly to the relations that satisfy $d$-separation  (Pearl and Verma 1987, p376).  Thus on DAG $\mathcal G$, $I(\mathcal K,\mathcal W,\mathcal Q)_{\mathcal G}$ implies that $\mathcal K$ and $\mathcal Q$ are $d$-separated by $\mathcal W$.] Note, moreover, that under quite general conditions (referred to in the literature as the *faithfulness* of a  probability distribution) then there are at least *some* values of $\mathcal W$ for which $\mathcal K$ *will* be informative about $\mathcal Q$.^[Put differently, there will not be any conditional independencies that are *not* captured in the DAG.] 


<!-- ^[In Pearl's terminonology, the graph may *represent* the probability distribution but not be *faithful* to it.] -->

<!-- This can be put another way. An $I$-map of $M$ is a model with no extra independencies; a $D$-map is a model that contains all of $M$ with,  possibly aditional independencies; a *perfect* map is a model with the same set of dependencies. Given an independency model $M$, a DAG, $G$, may be an $I$-map of $M$ in the sense that whenever $D$ separates $K$ from $Q$ then $I(K,D,Q)_M$, yet there may still be indepenencies in $M$ not captured by $G$; that is, it may also be htat $I(K,D,Q)_M$ but not $I(K,D,Q)_G$. Pearl refers to such cases as instances of a violation of *stability*, though in simple graphs with discrete variables such violations may be plausible.  -->

<!-- In the example given by Pearl with two matching pennies, $X_1$ and $X_2$ and $Y$ is 1 if the pennies match, $X_1$ adn $X_2$ are probabilisitcally independent of $Y$, yet $Y$ depends on both of them.  -->
<!-- The problem is that $d$-separation satisfies composition, that is, if $I(X_1, D, Q)$ and $I(X_2, D, Q)$ then $I(X_1X_2, D, Q)$; but since we cannot have $I(X_1X_2, D, Q)$ then we cannot have   $I(X_1, D, Q)$ and $I(X_2, D, Q)$ either (see also [@bouckaert1994conditional]). -->

<!-- Note that this example depends on infomration about the probability distribution over $V$, that is, the functional equations, and cannot be inferred from the structure of $S$ alone.   -->

<!-- [Note for us: We seek a  related proposition holds however using $d-separation$ on partially discovered submodels.] -->

Let us examine Proposition 1 in practice. We begin with the simplest case possible, and then move on to more complex models. 

The very simplest probabilistic causal graph, shown in Figure \ref{fig:d-sepsimple}, has $X$ influencing $Y$, with $X$ determined by a coin flip. If we want to know $X$'s effect on $Y$, this query is defined solely in terms of $Y$'s nodal type, $\theta^Y$. To help us conceptualize the more general point about informativeness for queries, we relabel $\theta^Y$ as $Q$ to emphasize the fact that this node represents our query. 

<!-- pointing into $Y$, as shown in . Here, $Q^Y$ determines the value of $Y$ that will be generated by $X$. Asking about the causal effect of $X$ in a case thus means learning the value of $Q^Y$ in that case. As will be recalled, in a binary setup with one causal variable, a response-type variable can take on one of four values, $q^Y_{00}$, $q^Y_{10}$, $q^Y_{01}$ and $q^Y_{11}$,^[As a reminder, we read $q^Y_{ij}$ (when $X$ is binary) as meaning that $Y$ will take on value $i$ when $X=0$ and value $j$ when $X=1$.] corresponding to the four possible causal types in this setting. -->

```{r sepsimple, echo = FALSE, fig.width = 8, fig.height = 4,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:d-sepsimple} A simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's response type for $Y$."}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 1, 1),
       y = c(1, 1, 2),
       names = c("X", "Y", "Q"),
       arcs = cbind( c(1, 3),
                     c(2, 2)),
       title = "Simplest X, Y, graph",
       padding = .1, contraction = .15) 

```

Let us assume that we have observed nothing yet in this case and then ask what clue(s) might be informative about $Q$, the node of interest. The other two nodes in the graph are $X$ and $Y$: these are thus the possible clues that we might go looking for in our effort to learn about $Q$ (i.e., they are the possible members of $\mathcal K$). 

First, can we learn about $Q$ by observing $X$? We can answer this question by asking whether $X$ is $d$-connected to $Q$ on the graph given what we have already observed (which is nothing). We can see visually that there is no active path from $X$ to $Q$: the only path between $X$ and $Q$ is blocked by colliding arrow heads. Thus, $X$ and $Q$ are $d$-separated, meaning that $X$ will not be informative about $Q$: observing the value that a causal variable takes on in a case---having seen nothing else in the case---tells us nothing whatsoever about that variable's effect on the outcome. If we want to know whether a case is of a type in which the presence of natural resources would cause civil war, for instance, observing only that the case has natural resources does not help answer the question.

<!-- **LONG FOOTNOTE STARTING HERE....** -->
<!-- In the case where we observe only $X$, the posterior on $Q^Y$ is: -->
<!-- \begin{eqnarray*} -->
<!-- P(Q^Y=q^Y | X=x) &=& \frac{\sum_{j=0}^1p(X=x)P(Q^Y=q^Y)P(Y=j|X=x, Q^Y=q^Y)}{\sum_{q^{Y'}}\sum_{j=0}^1p(X=x)P(Q^Y=q^{Y'})P(Y=j|X=x, Q^Y=q^{Y'})}\\ -->
<!-- &=&\frac{P(Q^Y=q^Y)}{\sum_{q^{Y'}}P(Q^Y=q^{Y'})} -->
<!-- \end{eqnarray*} -->
<!-- which is simply the prior on $Q^Y$. Thus, nothing is learned about $Q^Y$ from observing $X$ only.]  -->
<!-- <!-- &=& \frac{p(Q=q)\sum_{j=0}^1p(Y=j|X=x, Q=q)}{\sum_{q'}p(Q=q')\sum_{j=0}^1p(Y=j|X=x, Q=q')}\\ --> -->
<!-- **...ENDING HERE** -->

What, then, if we instead were to observe only $Y$? Is $Y$ $d$-connected to $Q$ given what we have already observed (which, again, is nothing)? It is: the arrow from $Q$ to $Y$ is an active path. Observing only the *outcome* in a case does tell us something about causal effects. Returning to the natural resources and civil war example, observing only that a country has had a civil is informative about the case's causal type (the value of $Q$). In particular, it rules out the possibility that this is a case in which nothing could cause a civil war: that is, it excludes $\theta^Y_{00}$ (i.e., $c$-type) as a possible value of $Q$.

<!-- **LONG FOOTNOTE STARTING HERE....** -->
<!-- In the case where we observe $Y$ only we have: -->
<!-- $$P(Q=q | Y=y) = \frac{\sum_{j=0}^1p(X=j)P(Q=q)P(Y=y|X=j, Q=q)}{\sum_{q'}\sum_{j=0}^1p(X=j)P(Q=q')P(Y=y|X=j, Q=q')}$$ -->
<!-- Here terms involving $Y$ and $Q$ cannot be separated, so the same kind of reduction is not possible. This implies scope for learning about $Q$ from $Y$.  For instance, if  we have $P(Q=j) = 1/4$ for type $j \in \{a,b,c,d\}$  and $P(X=j) = \frac{1}{2}$, then we have $P(Q=a | Y=1)=P(Q=b | Y=1) =\frac{1}{4}$, $P(Q=c | Y=1)=0$ and $P(Q=d | Y=1)=1$. -->
<!-- **...ENDING HERE** -->

Suppose now, having observed $Y$, that we were to consider also observing $X$. Would we learn anything further about $Q$ from doing so? We have already seen that observing $X$ alone yields no information about $Q$ because the two nodes are unconditionally $d$-separated, the path between them blocked by the colliding arrowheads at $Y$. However, as we have seen, observing a collider variable (or one of its descendants) *unblocks* the flow of information, generating relations of conditional dependence across the colliding arrowheads. Here, $X$ and $Q$ are $d$-connected by $Y$: thus, if we have *already* observed $Y$, then observing $X$ does confer additional information about $Q$. Knowing only that a country has natural resources tells us nothing about those resources' effect on civil war in that country. But if we already know that the country has a civil war, then learning that the country has natural resources helps narrow down the case's possible response types. Having already used the observation of $Y=1$ to rule out the possibility of $\theta^Y_{00}$, observing $X=1$ *together with* $Y=1$ allows us to additionally rule out the possibility that natural resources *prevent* civil war, i.e., that $Q=\theta^Y_{01}$.^[That is, we can rule out that the case is an $a$ type, or one with a negative causal effect.]

<!-- **LONG FOOTNOTE STARTING HERE....** -->
<!-- Where we observe both $Y$ and $X$,  we have: -->
<!-- $$P(Q=q | Y=y, X=x) = \frac{P(X=x)P(Q=q)P(Y=y|X=x, Q=q)}{\sum_{q'}P(X=x)P(Q=q')P(Y=y|X=x, Q=q')}$$ -->
<!-- which does not allow separation either of  $Q$ and $X$ or of $Q$ and $Y$. Thus, there is again learning from $Y$ and, given $Y$, there is *also* learning from $X$. Put differently, we have $P(Q|Y,X) \neq P(Q|Y)$.  -->

<!-- **...ENDING HERE** -->

Finally, what if we observe $X$ first and are considering whether to seek information about $Y$? Would doing so be informative? $X$ does not $d-$separate $Q$ from $Y$; thus, observing $Y$ will be informative about $Q$. In fact, observing $Y$ if we have already seen $X$ is *more* informative than observing $Y$ alone. The reasoning follows the logic of collision discussed just above. If we observe $Y$ having already seen $X$, not only do we reap the information about $Q$ provided by $Y$'s correlation with $Q$; we simultaneously open up the path between $X$ and $Q$, learning additionally from the conditional dependence between $X$ and $Q$ given $Y$. 


```{r kinformative, echo = FALSE, include = FALSE, errors = FALSE}
# Graphing all dags in some class, X,Y,K,Q

translate_matrix <- function(A, var_names = c("K", "X", "Y", "Q")){
 paste(var_names[which(A==1, arr.ind = TRUE)[,1]], "causes", var_names[which(A==1, arr.ind = TRUE)[,2]])
  }

translate_dagitty <- function(A, var_names = c("K", "X", "Y", "Q")){
  paste(var_names[which(A==1, arr.ind = TRUE)[,1]], "->", var_names[which(A==1, arr.ind = TRUE)[,2]])
  }

make_daggity     <- function(A) dagitty(paste("dag{", paste(translate_dagitty(A), collapse = ";"), "}"))

kinformative2 <- function(A){
   paste(c(
       dseparated(make_daggity(A), "Q", "K", c()),
       dseparated(make_daggity(A), "Q", "K", c("X")),
       dseparated(make_daggity(A), "Q", "K", c("Y")),
       dseparated(make_daggity(A), "Q", "K", c("X","Y"))
       ), collapse = ",")
   }

kinformative <- function(A){
  paste(c("\U2205", "X", "Y", "XY")[!c(
    dseparated(make_daggity(A), "Q", "K", c()),
    dseparated(make_daggity(A), "Q", "K", c("X")),
    dseparated(make_daggity(A), "Q", "K", c("Y")),
    dseparated(make_daggity(A), "Q", "K", c("X","Y"))
  )], collapse =",")
}


# 34 graphs
# All paths. All have Q --> Y. None have Y --> X
#   KXYQ
# K 0???
# X ?0??
# Y ?000
# Q ??10
var_names <- c("K", "X", "Y", "Q")

raise <- function(X, k){
  out <- X
  if(k ==1) return(X)
  for(j in 2:k) out <- out %*% X
  out}
  
check.A <- function(A, iterations = 4){
  x <- 1
  if( any(sapply(1:iterations, function(j) sum(diag(raise(A,j))) > 0))) x <- 0 # Not acyclic
  if( sum(sapply(1:iterations, function(j) raise(A,j)[2,3])) == 0) x <- -1 # "No path from X to Y (2 to 3)"
  if(min(rowSums(A) + colSums(A))==0) x<- -2 # "Unconnected element"
  if(sum(A[,4])>0) x <- -3 # "No causes of Q allowed in graph"
  x}

.A <-  matrix(c( 0,NA,NA,NA, 
                 NA, 0,NA,NA,
                 NA, 0, 0, 0,
                 NA,NA, 1, 0), 4, 4, byrow = TRUE)
rownames(.A) <- var_names; colnames(.A) <- var_names

possibilities <- perm_bb(rep(2,9))

new_A <- function(j) {
  A <- .A
  A[c(2:4, 5, 8:10, 13:14)] <- possibilities[j,]
  A}

As <- lapply(1:nrow(possibilities), new_A)

accept <- As[(lapply(As, check.A) %>% unlist)==1]

```

We put Proposition 1 to work in a slightly more complex set of models in Figure \ref{fig:34graphs}. Here we investigate the informativeness of a clue that is neither $X$ nor $Y$. Each graph in Figure \ref{fig:34graphs} has four variables: $X$; $Y$; a possible clue, $K$; and a node, $Q$, representing the query (which we might also naturally think of as $\theta^Y$). We draw all 34 possible graphs with variables $X$, $Y$, $K$, and $Q$ for causal models in which (a) all variables are connected to at least one other variable, (b) $X$ causes $Y$ either directly or indirectly, and (c) $Q$ is a direct cause of $Y$ but is not caused by any other variable in the model and is thus exogenous. The title of each panel reports $K$'s conditional informativeness using principles of $d$-separation: it tells us when $K$ is possibly informative about $Q$ depending on whether $X$, $Y$, both or none are observed.^[Note the "possibly" can be dropped under the assumption that the underlying probability model is "stable" (Pearl 2009, section 2.9.1) and with the interpretation that $K$ is informative about $Q$ for some, but not necessarily all, values of $W$.]

<!-- Above footnote: do we want to say "faithful" rather than stable, as we do earlier? -->

```{r 34graphs, echo = FALSE, fig.width = 11, fig.height = 12, fig.cap = "\\label{fig:34graphs} All connected directed acyclic graphs over $X,Y,K,Q$, in which $Q$ is an exogenous variable that directly causes $Y$, and $X$ is a direct or indirect cause of $Y$. The title of each graph indicates the conditions under which $K$ can be informative about (i.e., is not $d$-separated from) $Q$, given the prior observation of $X$, $Y$, both, or neither (...).", errors = FALSE, warning = FALSE, message = FALSE, comment = FALSE}


par(mfrow = c(6, 6) )
  par(mar=c(1,1,3.5,1))
  for(i in 1:length(accept)){
    kinf <- kinformative(accept[[i]])
    hj_dag(x = c(0,0,1,1), y = c(1,0,0,1), names = var_names, 
           arcs = which(accept[[i]]==1, arr.ind = TRUE), 
           title = title(ifelse(
                         length(kinf)==0, 
                         paste0(i, ". K never informative"),
                         paste0(i, ". K possibly informative\n given: ", kinf))), 
           padding = .15, cex = 1.2, length = .15) 
    }

```

The results show us not just what kinds of variables can be informative about a case's response-type but also what combinations of observations yield leverage on case-level causal effects. A number of features of the graphs are worth highlighting:

* **Clues at many stages.** Process tracing has focused a great deal on observations that lie "along the path" between suspected causes and outcomes. What we see in Figure \ref{fig:34graphs}, however, is that observations at many different locations in a causal model can be informative about causal effects. We see here that $K$ can be informative when it is pre-treatment (causally prior to $X$---e.g. panel (3)), post-treatment but pre-outcome (that is, "between" $X$ and $Y$ as, e.g., in panel (20)), an auxiliary effect of $X$ that itself has no effect on $Y$ (e.g., in panel (19)), post-outcome (after $Y$---e.g., in panel (15)), or a joint effect of both the suspected cause and the outcome (e.g., panel (31)). 

* **Mediator Clues**. While clues that lie in between $X$ and $Y$ may be informative, they can only be informative under certain conditions. For instance, when a clue serves *only* as a mediator in our model (i.e., its only linkages are being caused by $X$ and being affected by $Y$) and $Q$ only affects $Y$, as in panels (20) and (21), the clue is only informative about $Q$ if we have also observed the outcome, $Y$. Of course, this condition may commonly be met---qualitative researchers usually engage in retrospective research and learn the outcome of the cases they are studying early on---but it is nonetheless worth noting why it matters: in this setup, $K$ is unconditionally $d$-separated from $Q$ by the collision at $Y$; it is only by observing $Y$ (the collider) that the path between $K$ and $Q$ becomes unblocked. (As we saw above, the very same is true for observing $X$; it is only when we know $Y$ that $X$ is informative about $Q$.)

In short, observations along causal paths are more helpful in identifying causal effects to the extent that we have measured the outcome. Importantly, this is not the same as saying that mediator clues are *only* informative about causal effects where we have observed the outcome. Observing $Y$ is necessary for the mediator to be informative about a $Q$ term that is connected only to $Y$. Observing a mediator without the outcome, however, could still be informative about the overall effect of $X$ on $Y$ by providing leverage on how the mediator responds to $X$, which is itself informative about $X$'s effect on $Y$ via the mediator.^[In other words, the clue would then be providing leverage on a response-type variable pointing into the mediator itself.] Moreover, observing the mediator could be informative without the observation of $Y$ if, for instance, $Q$ also points into $K$ itself or into a cause of $K$. As we discuss below, the clue then is informative as a "symptom" of the case's response type, generating learning that does not hinge on observing the outcome.

* **Symptoms as clues.** Some clues may themselves be affected by $Q$: that is to say, they may be symptoms of the same conditions that determine causal effects in a case. For instance, in our illustrative model involving government survival, government sensitivity functions as a response-type variable for the effect of a free press ($X$) on government removal ($Y$): a free press only generates government removal when the government is non-sensitive to public opinion.  Sensitivity to public opinion thus represents our query variable, $Q$, if we seek to learn whether a free press causes government removal in a case. While it may not be possible to observe or otherwise measure the government's sensitivity, there may be *consequences* of government sensitivity that are observable: for instance, whether government officials regularly consult with civil-society actors on policy issues. While consultations would not be part of the causal chain generating the free press's effect, observing consultations (or the lack of them) would be informative about that effect because consultations are a symptom of the same conditions that enable the effect. 

We see that $K$ is a child or descendant of $Q$ in several of the graphs in Figure \ref{fig:34graphs}: $Q$ directly causes $K$ in panels (7) through (14), (17), (18), (25)-(30), (33), and (34); $Q$ causes (K) only indirectly through $X$ in panels (22) through (24); $Q$ causes (K) only indirectly through $Y$ in panels (15), (16), and (31); and $Q$ causes $K$ only indirectly through $X$ and through $Y$ in panel (32). We can then use the principle of $d$-separation to figure out when the symptom clue is potentially informative, given what we have already observed. It is easy to see that $K$ is potentially informative, no matter what we have already observed, if $K$ is directly affected by $Q$; there is nothing we could observe that would block the $Q \rightarrow K$ path. Thus, $Q$'s "symptom" can, in this setup, contain information about type above and beyond that contained in the $X$ and $Y$ values. However, where $Q$ affects $K$ only through some other variable, observing that other variable renders $K$ uninformative by blocking the $Q$-to-$K$ path. For instance, where $Q$ affects $K$ indirectly through $X$, once we observe $X$, we already have all the information about $Q$ that would be contained in $K$. 

* **Surrogates as clues.** Clues may be consequences of the outcome, as in graphs (15) and (16). If $K$ is a consequence *only* of $Y$, then it will contain no new information about $Q$ where $Y$ is already known. However, in situations where the outcome has not been observed, $K$ can act as a "surrogate" for the outcome and thus yield leverage on $Q$ (@frangakis2002principal). A researcher might, for instance, seek to understand causal effects on an outcome that is difficult to directly observe: consider, for instance, studies that seek to explain ideational change. Ideas themselves, the $Y$ in such studies, are not directly observable. However, their consequences---such as statements by actors or policy decisions---will be observable and can thus serve as informative surrogates for the outcome of interest.

Clues may similarly serve as surrogates of a cause, as in graphs (19) and (22). Here $X$ causes $K$, but $K$ plays no role in the causal process generating $Y$. $K$ is of no help if we can directly measure $X$ since the latter $d$-separates $K$ from $Q$. But if an explanatory variable cannot be directly measured---consider, e.g., ideas or preferences as causes---then its consequences, including those that have no relationship to the outcome of interest, can provide leverage on the case-level causal effect.

Clues can also be a consequence of both our suspected cause and the outcome of interest, thus serving as what we might call "double surrogates," as in panels (31) and (32). Here $X$ is a direct cause of $Y$, and $K$ is a joint product of $X$ and $Y$. A double surrogate can be informative as long as we have not already observed both $X$ and $Y$. Where data on either $X$ or $Y$ are missing, there is an open path between $K$ and $Q$. If we have already observed both, however, then there is nothing left to be learned from $K$.

* **Instruments as clues.** Clues that are causally prior to an explanatory variable, and have no other effect on the outcome, can sometimes be informative. Consider, for instance, graph (3). Here $K$ is the only cause of $X$. It can thus serve as a proxy. If we have seen $X$, then $X$ blocks the path between $K$ and $Q$, and so $K$ is unhelpful. $K$ can be informative, though, if we have *not* observed $X$. Note that informativeness here still requires that we observe $Y$. Since $Y$ is a collider for $Q$ and the $K \rightarrow X \rightarrow$ chain, we need to observe $Y$ in order to $d$-connect $K$ to $Q$.

A rather different setup appears in graph (5), where both $K$ and $Q$ cause $X$. Now the conditions for $K$'s informativeness are broader. Observing $X$ still makes $K$ uninformative as a proxy for $X$ itself. However, because $X$ is a collider for $K$ and $Q$, observing $X$ *opens up* a path from $K$ to $Q$, rendering a dependency between them. Still, we have to observe at least one of $X$ or $Y$ for the instrument to be informative here. This is because both of $K$'s paths to $Q$ run through a collision that we need to unblock by observing the collider. For one path, the collider is $X$; for the other path, the collider is $Y$.^[As a simple example one might imagine a system in which $X = K$ if  $q \in {a,b}$  and $X = 1-K$ if  $q \in {c,d}$. Then if we observe, say, $X=Y=K=1$, we can infer that $q = b$. Another way to think about what is happening in graph (5) is that $K$ is providing information about the *assignment process*. In this graph, the causal effect ($Y$'s potential outcomes, determined by $Q$) is also a partial determinant of the assignment of cases to values on $X$. In terms of cross-case correlational inference, then, we would think of this as a situation of confounding. Observing another cause of $X$, then, allows us to more fully characterize the process of assignment.] 

<!-- Graph (5) is similar to one discussed in [@hausman1999independence] in which there is learning from a pretreatment clue because $X$ is a collider for $K$ and $Q$.  -->

<!-- To return to our government-removal model, government sensitivity to public opinion is a response-type variable (a $Q$ term), with non-sensitivity a pre-condition for the positive effect of a free press on removal. Yet it is possible (though we did not include it in our original model) that government sensitivity also affects whether or not a government gets a free press: more sensitive governments may impose tighter media restrictions. In that case, when governments are not sensitive, we would expect to see a free press and government removal.   -->

Other patterns involving instrumentation are also imaginable, though not graphed here. For example, we might have a causal structure that combines instrumentation and surrogacy. Suppose that $X$ is affected by $Q$ and by an unobservable variable $\theta_X$; and that $\theta_X$ has an observable consequence, $K$. Then $K$, though not a cause of $X$, is a "surrogate instrument" [@hernan2006instruments] as it is a descendant of an unobserved instrument, $U$, and thus allows us to extract inferences similar to those that we could draw from a true instrument.

* **Confounders as clues.** In several of the graphs, $K$ is a confounder in that it is a direct cause of both $X$ and $Y$ (panels (4), (6), (12), and (14)). Let us focus on graph (4), which isolates $K$'s role as a confounder. Here $K$ can be informative via two possible paths. First, if $X$ is not observed but $Y$ is, then $K$ is $d$-connected to $Q$ along the path $K \rightarrow X \rightarrow Y \leftarrow Q$. $K$ is in this sense serving as a proxy for $X$, with its path to $Q$ opened up by the observation of the collider, $Y$. Second, with $Y$ observed, $K$ can provide information on $Q$ via the more direct collision, $K \rightarrow Y \leftarrow Q$. If $X$ *is* observed, then the first path is blocked, but the second still remains active. As with any pre-outcome variable, for a confounder clue to provide purchase on $Y$'s response type, $Y$ itself must be observed.

In a sense, then, the role of confounders as clues in case-level inference is the mirror image of the role of confounders as covariates in cross-case correlational inference. In a correlational inferential framework, controlling for a variable in $K$'s position in graph (5) renders the $X, Y$ correlation (which we assume to be observed) informative about $X$'s average causal effect. When we use confounders as evidence in within-case inference, it is our observations of other variables that determine how informative the confounder *itself* will be about $X$'s causal effect.


It is important to be precise about the kinds of claims that one can make from graphs like those in Figure \{fig:34graphs}. The graphs in this figure allow us to identify informativeness about an unobserved node $Q$ that is a parent of $Y$. This setup does not, however, capture all ways in which clues can be informative about the causal effect of $X$ on $Y$ or about other causal queries of interest. For instance, as noted above, even if a clue is uninformative about a $Q$ node pointing into $Y$, it may still help establish whether $X$ causes $Y$: the statement that $X$ causes $Y$ will for some graphs be a statement about a *collection* of nodes that form the set of query variables $\mathcal Q$. This is the case, for instance, in any graph of the form $X \rightarrow M  \rightarrow Y$, where we are interested not just in $Y$'s response to $M$ (the mediator) but also in $M$'s response to $X$. Of interest, thus, are not just a $Q^Y$ response-type node pointing into $Y$ but also a $Q^M$ response-type node that is a parent of $M$. Observations that provide leverage on either $Q$ term will thus aid an inference about the overall causal effect. A clue $K$ that is $d-$separated from $Q^Y$ may nevertheless be informative about $X$'s effect on $Y$ if it is not $d-$separated from $Q^M$; this opens up a broader range of variables as informative clues. 

Additionally, as our discussion in Chapter 2 makes clear, queries other than the case-level causal effect---such as average causal effects, actual causes, and causal paths---involve particular features of context: particular sets of exogenous nodes as members of our query set, $\mathcal Q$. Thus, even for the same causal model, informativeness will be defined differently for each causal question that we seek to address. The broader point is that we can identify what kinds of observations may address our query if we can place that query on a causal graph and then assess the graph for relationships of $d$-separation and -connection.

Further, we emphasize that a DAG can only tell us when a clue *may* be informative (conditional some prior observation): $d-$connectedness is necessary but not sufficient for informativeness. This fact derives directly from the rules for drawing a causal graph: the absence of an arrow between two variables implies that they are *not* directly causally related, while the presence of an arrow does not imply that they always are. As we saw in our analysis of the government-removal example in Chapter 2, whether variables connected to one another by arrows in the original DAG were in fact linked by a causal effect depended on the context. Likewise, whether a clue $K$ is in fact informative may depend on particular values of $\mathcal W$---the variables that have already been observed. As a simple example, let $q = k_1w + (1-w)k_2$, where $W$ is a variable that we have already observed and $K_1$ and $K_2$ are clues that we might choose to observe next. Here, if $w=1$ then learning $K_1$ will be informative about $Q$, and learning $K_2$ will not; but if $w=0$, then $K_1$ will be uninformative (and $K_2$ informative). 


In general, then, graphical analysis alone can help us exclude unhelpful research designs, given our prior observations and a fairly minimal set of prior beliefs about causal linkages. This is no small feat. But identifying those empirical strategies that will yield the *greatest* leverage requires engaging more deeply with our causal model, as we show in detail in our discussion of clue-selection in Chapter \@ref(clue). 


## Principles of learning

While `CausalQueries` can implement process-tracing inference for us, it is helpful for researchers to be able to reason their way through what is happening "under the hood." We provide here some core principles and intuitions for thinking through the features of models and queries that influence whether and how much we can learn from within-case observations.

### A DAG alone does not guarantee probative value for a single case 

A DAG puts qualitative structure on causal relations but quantitative implications depend on the beliefs over causal types. In general, learning from new data requires that, conditional on known data, the probability of a new data pattern is different depending on whether or not the query is true. With flat priors this condition may not hold for many queries of interest.

To illustrate, suppose that we are interested in whether $X$ caused $Y$ and we posit a simple $X \rightarrow M \rightarrow Y$ model with flat priors over $\theta^M$ and $\theta^Y$. Now we would like to conduct process tracing and observe $M$ to tell us about the effect of $X$ on $Y$ in a case with  $X=Y=1$ case. 

Does the observation of $M$ provide leverage on whether $X=1$ caused $Y=1$?

It does not. We can learn nothing about $X$'s effect on $Y$ from observing $M$. 

<!-- Observing a process is *uninformative* if all that we know is the structure of relations of conditional independence.  -->

<!-- FLAG: PROVE CLAIM -->

To see why, consider that there are two causal types that will satisfy the query, $X=1$ caused $Y=1$. Those are the types $\theta^X_1 \theta^M_{01} \theta^Y_{01}$ and $\theta^X_1 \theta^M_{10} \theta^Y_{10}$: either linked positive effects or linked negative effects could generate an overall positive effect of $X$ on $Y$. Moreover, with flat priors over nodal types, these causal types are equally likely. Now, think about what we would conclude if we collected process data and observed $M=1$ in the $X=Y=1$ case. This observation would rule out various ways in which $X$ did not cause $Y$ but it also rules out one way in which the query could be satisfied: the causal type with linked negative effects. And what if we observed, instead, $M=0$? This would have similar implucations, this time ruling out the other way in which the query could be satisfied: linked positive effects. Intuitively we would update the same way no matter what we find, which means we must not be updating at all. 

More formally, conditional on observing $X=1, Y=1$ our prior that $X$ caused $Y$ is:

$$\frac{\lambda^M_{01}\lambda^Y_{01}+ \lambda^M_{10}\lambda^Y_{10}}{(\lambda^M_{01}+\lambda^M_{11})(\lambda^Y_{01}+ \lambda^Y_{11})+ (\lambda^M_{10}+\lambda^M_{00})(\lambda^Y_{10}+ \lambda^Y_{11}) }$$

Our posterior on observing $M=1$ is:

$$\frac{\lambda^M_{01}\lambda^Y_{01}}{(\lambda^M_{01}+\lambda^M_{11})(\lambda^Y_{01} + \lambda^Y_{11})}$$
it is easy to see these are equal with flat priors ($\lambda^j_{ab}=\lambda^{j*}$ for all $a,b$). What we can see from the comparison is that when we observe data we rule out half the types consistent with the data (denominator) but also rule out half the types consistent with the query *and* data (numerator) .

It is worth noting however that informative priors on *either* $\theta^M$ or $\theta^Y$, would help here. 

<!-- If we had flat priors on $\theta^Y$ only the comparison would be between -->

<!-- $$\frac{\lambda^M_{01}+ \lambda^M_{10}}{2}$$ -->
<!-- Our posterior on observing $M=1$ is: -->

<!-- $$\frac{\lambda^M_{01}}{2(\lambda^M_{01}+\lambda^M_{11})}$$ -->
<!-- These are different as long as  -->
<!-- $$\lambda^M_{01}+ \lambda^M_{10} \neq \frac{\lambda^M_{01}}{(\lambda^M_{01}+\lambda^M_{11})}$$ -->

<!-- learning from observing a node requires informative priors about causal effects involving *that node.* For instance, in an $X \rightarrow M \rightarrow Y$ model, where we plan to observe $M$, it would not be sufficient to have an informative prior about the $X \rightarrow Y$ relationship. Such a prior would be cast at the wrong level. Rather, we need an informative prior about the $X \rightarrow M$ or $M \rightarrow Y$ link in order to learn from $M$.  -->

More generally,  what we need at the level of priors depends on the query. Suppose that we start with the model, $X \rightarrow M \rightarrow Y$, and formulate the following query: does $X$ have a positive effect on $Y$ that runs through a chain of positive effects via $M$? We can learn about this query without any informative priors over nodal types because of the way in which the query itself restricts the type space. Since the query is not satisfied if negative mediating effects are operating, we will update to probability 0 on the query for any observation that violates $X=M=Y$, and we will update upwards on the query for any observation of $X=M=Y$.


### Learning requires uncertainty  

While case-level inference from within-case evidence requires informative priors about nodal types, there is also such a thing as *too much* information -- or, put differently, as insufficient uncertainty about causal relations. Suppose, for instance, that our beliefs are such that $X$ always has a positive effect on $M$ in an $X \rightarrow M \rightarrow Y$ model. Consider, further, that we already know that $X=1$ in a case. In that situation, nothing can be learned by observing $M$ since the prior observation of $X$ already reveals $M$'s value given our prior beliefs.

To take a less extreme example, suppose that our priors put a *very high probability* on $X$'s having a positive effect on $M$ and that, again, we already know that $X=1$ in a case. In that situation, we should *expect* to learn very little from observing $M$ since we believe that we are very likely to see $M=1$, given that we already know $X=1$. It is true that our beliefs will shift *if* we look for $M$ and find the unexpected value of $M=0$. But because that data-realization is highly unlikely, we should expect the learning from observing $M$ to be minimal. 

We address the concept of expected learning more systematically in Chapters \@ref(clue) and \@ref(caseselection), but our general point here is that, we will learn more from process-tracing evidence, to the extent that (a) we know enough about causal relations in a domain to know how to make causal sense of the evidence we find, but (b) we do not know so much that that evidence can be largely predicted from what we have already observed.


<!-- FLAG: mh check -->

<!-- ### Multiple ways for queries to be satisfied -->

<!-- As we have seen, there will often be multiple causal types consistent with a given query. Back to our $X \rightarrow M \rightarrow Y$ model, for instance, $X=1$ could cause $Y=1$ through either two intermediate negative effects or two intermediate positive effects. Similarly, if we have a model in which $X$ can affect $Y$ through $M$ or through $N$ ($X \rightarrow M \rightarrow Y \leftarrow N \leftarrow X$), the same query could be satisfied via effects operating along either pathway. There are three key points to make about learning about queries that can be satisfied via multiple causal types. -->

<!-- First, evidence for or against one of those causal types -- that is, for or against one of the ways in which the query could be true -- is evidence for or against (respectively) the query as a whole. In the two-path model, for instance, if we observe an $M$ data pattern that is inconsistent with an an effect along this pathway, then this is also evidence against an overall $X \rightarrow Y$ effect --- even though that effect *could* operate via $N$. In general, finding evidence against one way the effect can happen reduces our confidence in the effect happening at all.  -->

<!-- Second, the more ways there are for a query to be satisfied, the less we learn about the query by learning about only one of those ways. Imagine a model in which $X$ can affect $Y$ only through $M$. Then compare that to a model in which $X$ can affect $Y$ through four different, equally probably (in our priors) paths: via $M$, $N$, $P$, or $Q$. In general, $M$ by itself will be much more informative about the $X \rightarrow Y$ effect in the first model than in the second since, in the second model, observing $M$ gives us leverage only on the causal types relevant to a small share of the ways in which the effect of interest could emerge.^[As we discuss in Chapter \@ref(mixing), the limits to learning from one pathway are even more severe when we have prior cross-case evidence that informs a prior about the $X \rightarrow Y$ effect.] -->

<!-- Third, evidence against the *likelier* way a query could be satisfied constitutes stronger evidence against the query than does evidence against an unlikely way. For instance, if we started out thinking that an effect via $M$ was more likely than an effect via $N$, then evidence against the effect via $M$ will have a bigger impact on our beliefs about the overall $X \rightarrow Y$ effect. Note that this is a special case of a point that we make in Chapter \@ref(bayeschapter): we update more strongly in favor of the hypothesis for which the evidence is least damaging to the most-likely ways in which the hypothesis could be true. -->


### The more specific the query the more difficult it is to gain leverage

It is difficult to get empirical leverage on very highly  queries. 

To illustrate, suppose that we start with the two-path model, $X \rightarrow M \rightarrow Y \leftarrow X$, and formulate the following query: does $X$ have a positive effect on $Y$ that runs through a chain of positive effects via $M$? Suppose that we begin with flat priors over all nodal types. Intuitively, this seems like exactly the kind of question for which an observation of $M$ is the perfect empirical strategy. And that intuition is, in a sense, correct: we can indeed learn about the query by observing $M$. Seeing $M=1$ in an $X=Y=1$ case, for instance, would be evidence consistent with the query while seeing $M=0$ in that same case would be inconsistent with the query. 

Yet, we will only learn very modestly about the query from this observation. The reason is that the query itself has a very low prior probability. It may, in fact, not be obvious at first glance just how unlikely our query is in our priors. At first glance, it looks as though all we are asking is whether there exist positive effects running through one of the two causal paths in the model. However, consider the joint nodal-type probabilities implied by the query. First, the query requires $X$ to have a positive effect on $M$, to which our priors give only a $25\%$ chance In addition, the query puts a very narrow constraint on $Y$’s possible nodal types:  to satisfy the query, $Y$ must have a nodal type in which $M$ has a positive effect on $Y$ when $X$ does not change, and in which $X$ does not have a positive effect on $Y$ unless $M$ changes from $0$ to $1$. This pair of conditions is met by only 2 of $Y$’s 16 nodal types, implying a $12.5\%$ chance. The prior on the query itself is, thus, $0.25 \times 0.125 = 0.03125$. Thus, while observing $M=0$ takes the probability of the query down to $0 \%$, we started out very close to 0%! Observing $M=1$ results in only a small uptick, to about $6\%$ because there remain many type combinations consistent with M=1 but that do not fit through the needle-eye of this query.

In sum, what seems intuitively like a simple question will sometimes be a very unlikely query. What makes this query so unlikely is the interplay between the query itself and the model. The model allows for a *wide* range of causal combinations (e.g., $Y$'s 16 nodal types), spreading prior weight thinly across those many possibilities, while the query zeroes in on a couple of *particular* combinations that, in our priors, each have very low probability.


### Population-level uncertainty does not alter case-level causal inference

In the procedure described for process tracing in this chapter (and different to what we introduce in Chapter 8) we assume that $\lambda$ is known and we do not place uncertainty around it.

This might appear somewhat heroic, but in fact for single case inference it is  without loss of generality. The expected inferences we would make for any query accounting for priors  is the same as the inferences we if we use the expectation only.  

To see this, let $\pi_j$ denote the probability of observing causal type $j$ and $p(D)$ te probability of observing data realization $D$. Say that $j \in D$ if type $j$ produces data type $D$ and say $j \in E$ if causal type $j$ is an element of the query set of interest. 
The posterior on a query $E$ given data $D$ given prior over $\pi$, $p(\pi)$ is:

$$\Pr(E | D) = \int_\pi  \frac{\sum_{j \in E \cap D}\pi_j}{\sum_{j \in D}\pi_j} f(\pi)d\pi$$

However, since for any $\pi$, $\sum_{j \in D}\pi_j = p(D)$ we have:

$$\Pr(E | D) = \int_\pi  \sum_{j \in E \cap D}\pi_j f(\pi)d\pi/p(D) = \sum_{j \in  E \cap D} \overline{\pi}_j/p(D)$$
For instance in an $X \rightarrow Y$ model, if we observe $X=Y=1$ then $D$ consists of causal types $D={(\theta^X_1, \theta^Y_{01}), (\theta^X_1, \theta^Y_{11})})$ and the query set for "$X$ has a positive effect on  $Y$" consists of  $E={(\theta^X_1, \theta^Y_{01}), (\theta^X_0, \theta^Y_{01})})$. Let $\pi_1$, $\pi_2$ denote the priors on the two elements of $D$. We then have:

$$\Pr(E | D) =  \frac{\pi_1}{\pi_1 + \pi_2}$$



<!-- In process tracing, we are focused on drawing case-level inferences and, as such, we treat the population-level parameters as given and fixed. In general, these parameters derive from our beliefs about how the world works, and those beliefs will typically be uncertain.  The key point, however, is that in process tracing, the population parameters serve as an *input* into the analysis, conditioning our inferences from the evidence: we interpret a case in light of what we know about general causal relations. But, in the process-tracing setup, we do not *update* on these population-level beliefs once we see the data from a single case. Importantly, as we show later in the book, we *do* update on population-level inferences in the more general setup that we introduce in Chapter \@ref(mixing) for analyzing mixed data in multiple cases. We also show in Chapter \@ref(evaluation) how we can test the sensitivity of conclusions to the values at which we set population parameters. Interestingly, as we also show, process-tracing inferences, including uncertainty about conclusions, are unaffected by the level of uncertainty we might have about population parameters; we thus do not specify this uncertainty for the purposes of process tracing. -->

<!--chapter:end:07-process-tracing-with-models.Rmd-->

# Process Tracing Application: Inequality and Democracy {#ptapp}


```{r packagesused07, include = FALSE}
source("_packages_used.R")
```



:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We apply the causal-model-based approach to process tracing to a major substantive issue in comparative politics: the relationship between inequality and democratization.  Drawing on case level data, we use qualitative restrictions on causal types together with flat priors to draw inferences about the probability with which inequality facilitated or hindered democratization.
::::
<br>


<!-- We show that systematically integrating theory into process tracing has substantial implications for inference. We also show how inferences depend on how exactly one specifies the causal model.** -->




In this chapter, we demonstrate how causal-model-based process-tracing works using real data. We undertake this illustration on a substantive issue that has been of central interest to students of comparative politics for decades: the causes of democratization. As the literature and range of arguments about democratization are vast, we focus on just a piece of the debate---specifically on causal claims about the relationship between economic inequality and democratization, with particular attention to the work of @boix2003democracy, @acemoglu2005economic, and @haggard2012inequality. In this chapter, we demonstrate process tracing with causal models, while in a later chapter we demonstrate the integration of process-tracing with correlational analysis. Our focus in this chapter is on using process tracing to assess *the case-level causal effect of inequality on democracy.*


## Inequality and Democratization: The Debate

Sociologists, economists, and political scientists have long theorized and empirically examined the relationship between inequality and democracy (e.g., @dahl1973polyarchy, @bollen1985political, @acemoglu2005economic, @boix2003democracy, @ansell2014inequality). In recent years, the work of @boix2003democracy,  @acemoglu2005economic, and @ansell2014inequality represent major theoretical advances in specifying when and how inequality might generate transitions to democracy (as well as its persistence, which we bracket here). The first and third of these books also provide large-n cross-national and historical tests of their theories' key correlational predictions. @haggard2012inequality, moreover, derive causal process observations from a large number of "Third Wave" cases of democratization in order to examine these theories' claims about the centrality of distributional issues to regime change. We provide a very condensed summary of the core logic of @boix2003democracy and @acemoglu2005economic before seeking to translate that logic into a causal model for the purposes of process tracing, using a transformed version of Haggard and Kaufman's causal-process data.

We briefly summarize the core logics of and differences among these three sets of arguments here, bracketing many of their moving parts to focus on the basic theorized relationship between inequality and democracy. Both Boix's and Acemoglu and Robinson's theories operate within a Meltzer-Richard (@meltzer1981rational) framework in which, in a democracy, the median voter sets the level of taxation-and-transfer and, since mean income is higher than median income, benefit from and vote for a positive tax rate, implying redistribution from rich to poor. The poorer the median voter, the more redistribution she will prefer. Democracy, with its poorer median voter, thus implies greater redistribution than (rightwing) authoritarianism---a better material position from the poor at the expense of the rich elite. Thus, in each of these approaches, struggles over political regimes are conflicts over the distribution of material resources.

In Boix's model, the poor generally prefer democracy for its material benefits. When they mobilize to demand regime change, the rich face a choice as to whether to repress or concede, and they are more likely to repress as inequality is higher since, all else equal, they have more to lose from democracy. Thus, with the poor always preferring democracy over rightwing authoritarianism, inequality reduces the prospects for democratization. 

In Acemoglu and Robinson's model, inequality simultaneously affects the expected net gains to democracy for both rich and poor. At low levels of inequality, democracy is relatively unthreatening to the elite, as in Boix, but likewise of little benefit to the poor. Since regime change is costly, the poor do not mobilize for democracy when inequality is low, and democratization does not occur. At high levels of inequality, democracy is of great benefit to the poor but has high expected costs for the elite; thus, democratization does not occur because the elite repress popular demands for regime change. In Acemoglu and Robinson's model, democracy emerges only when inequality is at middling levels: high enough for the poor to demand it and low enough for the rich to be willing to concede it.

Ansell and Samuels, finally, extend the distributive politics of regime change in two key ways. First, they allow for a two-sector economy, with a governing elite comprising the landed aristocracy and an urban industrial elite excluded from political power under authoritarian institutions. Total inequality in the economy is a function of inequality in the landed sector, inequality in the industrial sector, and the relative size of each. Second, authoritarian (landed) elites can tax the industrial bourgeoisie, thus giving the industrial elite an incentive to seek constraints on autocratic rule. Third, in Ansell and Samuels' model, rising industrial inequality means a rising industrial elite, generating a larger gap between them and industrial workers, though the industrial masses are richer than the peasantry. A number of results follow, of which we highlight just a couple. Rising land inequality reduces the likelihood of bourgeois rebellion by giving the landed elite greater repressive capacities and increasing their expected losses under democracy. As industrial inequality rises, however, the industrial elite have more to lose to confiscatory taxation and thus greater incentive to push for partial democracy (in which they have the ability to constrain the government, though the poor remain politically excluded) as well as greater resources with which to mobilize and achieve it. Full democracy, brought on by joint mass and bourgeois rebellion, is most likely as the industrial sector grows in relative size, giving the urban masses more to lose to autocratic expropriation and more resources with which to mobilize and rebel. 

These three theoretical frameworks thus posit rather differing relationships between inequality and democracy. Taking these theoretical logics as forms of background knowledge, we would consider it possible that inequality reduces the likelihood of democracy or that it increases the likelihood of democracy. Yet one feature that all three theories have in common is a claim that distributional grievances drive demands for regime change. Moreover, in both Boix and Acemoglu and Robinson, less economically advantaged groups are, all else equal, more likely to demand democracy the worse their relative economic position.  Ansell and Samuels' model, on the other hand, suggests that relative deprivation may cut both ways: while poorer groups may have more to gain from redistribution under democracy, better-off groups have more to fear from confiscatory taxation under autocracy. In all three frameworks, *mobilization* by groups with material grievances is critical to transitions to democracy: elites do not voluntarily cede power.

In their qualitative analysis of "Third Wave" democratizations, Haggard and Kaufman point to additional factors, aside from inequality, that may generate transitions. Drawing on previous work on 20th century democratic transitions (e.g., @huntington1993third, @linz1996problems), they pay particular attention to international pressures to democratize and to elite defections. 

## A Structural Causal Model

We now need to express this background knowledge in the form of a structural causal model. Suppose that we are interested in the case-level causal effect of inequality on democratization of a previously autocratic political system. Suppose further, to simplify the illustration, that we conceptualize both variables in binary terms: inequality is either high or low, and democratization either occurs or does not occur. This means that we want to know, for a given case of interest, whether high inequality (as opposed to low inequality) causes democracy to emerge, prevents democracy from emerging, or has no effect (i.e., with democratization either occurring or not occurring independent of inequality). We can represent this query in the simple, high-level causal model shown in Figure \@ref(fig:dagdemochigh). Here, the question, "What is the causal effect of high inequality on democratization in this case?" is equivalent to asking what the value of $\theta^D$ is in the case, where the possible values are $\theta_{00}^D, \theta_{01}^D, \theta_{10}^D$, and $\theta_{11}^D$. We assume here that the case's nodal type, $\theta^D$, is not itself observable, and thus we are in the position of having to make inferences about it.

Drawing on the grammar of causal graphs discussed in Chapter \@ref(models), we can already identify possibilities for learning about $\theta^D$ from the other nodes represented in this high-level graph. Merely observing the level of inequality in a case will tell us nothing since $I$ is not $d-$connected to $\theta^D$ if we have observed nothing else. On the other hand, only observing the outcome---regime type---in a case *can* give us information about $\theta^D$ since $D$ *is* $d-$connected to $\theta^D$. For instance, if we observe $D=1$ (that a case democratized), then we can immediately rule out $\theta_{00}^D$ as a value of $\theta^D$ since this type does not permit democratization to occur. Further, conditional on observing $D$, $I$ is now $d-$connected to $\theta^D$: in other words, having observed the outcome, we can additionally learn about the case's type from observing the status of the causal variable. For example, if $D=1$, then observing $I=1$ allows us additionally to rule out the value $\theta_{10}^D$ (a negative causal effect).

Now, observing just $I$ and $D$ alone will always leave two nodal types in contention. For instance, seeing $I=D=1$ (the case had high inequality and democratized) would leave us unsure whether high inequality caused the democratization in this case ($\theta^D=\theta_{01}^D$) or the democratization would have happened anyway ($\theta^D=\theta_{11}^D$). This is a limitation of $X, Y$ data that we refer to in @humphreys2015mixing as the "fundamental problem of type ambiguity." Note that this does not mean that we will be left indifferent between the two remaining types. Learning from $X, Y$ data alone---narrowing the types down to two---can be quite significant, depending on our priors over the distribution of types. For example, if we previously believed that a $\theta_{00}^D$ type (cases in which democracy will never occur, regardless of inequality) was much more likely than a $\theta_{11}^D$ type (democracy will always occur, regardless of inequality) and that positive and negative effects of inequality were about equally likely, then ruling out the $\theta_{00}^D$ and $\theta_{10}^D$ values for a case will shift us toward the belief that inequality caused democratization in the case. This is because we are ruling out both a negative effect and the type of null effect that we had considered the most likely, leaving a null effect that we consider relatively unlikely.

```{r dagdemochigh, echo = FALSE, fig.width = 6, fig.height = 4, fig.align="center", fig.cap = "Simple democracy, inequality model"}

hj_dag(x = c(1,2,2),
       y = c(1,1,2),
       names = c(
         "Inequality (I)",
         "Democ'n (D)",  
         expression(paste(theta^D))
         ),
       arcs = cbind( c(1,3),
                     c(2,2)),
       title = "A high-level model of democratization",
       add_functions = 0, 
       contraction = .28, 
       padding = .2
)

```


Nonetheless, we can increase the prospects for learning by *theorizing* the relationship between inequality and democratization. Given causal logics and empirical findings in the existing literature, we can say more than is contained in Figure \@ref(fig:dagdemochigh) about the possible structure of the causal linkages between inequality and democratization. And we can embed this prior knowledge of the possible causal relations in this domain in a lower-level model that is consistent with the high-level model that most simply represents our query. 

If we were to seek to fully capture them, the models developed by Boix, Acemoglu and Robinson, and Ansell and Samuels would, each individually, suggest causal graphs with a large number of nodes and edges connecting them. Representing all variables and relationships jointly contained in these three models would take an extremely complex graph. Yet there is no need to go down to the lowest possible level---to generate the *most* detailed graph---in order to increase our empirical leverage on the problem.

We represent in Figure \ref{fig:lowdem} one possible lower-level model consistent with our high-level model. Drawing on causal logics in the existing literature, we unpack the nodes in the high-level model in two ways:

1. We interpose a mediator between inequality and democratization: mobilization ($M$) by economically disadvantaged groups expressing material grievances. $M$ is a function of both $I$ and of its own response-type variable, $\theta^M$, which defines its response to $I$. In inserting this mediator, we have extracted $\theta^M$ from $\theta^D$, pulling out that part of $D$'s response to $I$ that depends on $M$'s response to $I$.

2.  We specify a second influence on democratization, international pressure ($P$). Like $\theta^M$, $P$ has also been extracted from $\theta^D$; it represents that part of $D$'s response to $I$ that is conditioned by international pressures.


```{r lowdem, echo = FALSE, fig.width = 8, fig.height = 5, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:lowdem} A lower-level model of democratization in which inequality may affect regime type both directly and through mobilization of the lower classes, and international pressure may also affect regime type."}

hj_dag(x = c(1,1.5,3,3, 1.5, 2.2),
       y = c(1,2,2,3, 3, 3),
       names = c(
         "Inequality (I)",
        "Mobilization (M)",  
         "Democratization (D)",  
         expression(paste(theta^{D[lower]})),
        expression(paste(theta^M)),
         "Int'l pressure (P)"
         ),
       arcs = cbind( c(1,2, 4, 6, 5, 1),
                     c(2,3, 3, 3, 2, 3)),
       title = "A lower-level model of democratization",
       add_functions = 0, 
       contraction = .2, 
       padding = .2
)


```



In representing the causal dependencies in this graph, we allow for inequality to have (in the language of mediation analysis) both an "indirect" effect on democratization via mobilization and a "direct" effect. The arrow running directly from $I$ to $D$ allows for effects of inequality on democratization beyond any effects running via mobilization of the poor, including effects that might run in the opposite direction. (For instance, it is possible that inequality has a positive effect on democratization via mobilization but a negative effect via any number of processes that are not explicitly specified in the model.)  The graph also implies that there is no confounding: since there is no arrow running from another variable in the graph to $I$, $I$ is modeled as exogenous.

The lower-level graph thus has two exogenous, response-type nodes that will be relevant to assessing causal effects: $\theta^M$ and $\theta^{D_{lower}}$. $\theta^M$, capturing $I$'s effect on $M$, ranges across the usual four values for a single-cause, binary setup: $\theta_{00}^M, \theta_{01}^M, \theta_{10}^M$, and $\theta_{11}^M$. 

$\theta^{D_{lower}}$ is considerably more complicated, however, because this node represents $D$'s response to three causal variables: $I$, $M$, and $P$. One way to put this is that the values of $\theta^{D_{lower}}$ indicate how inequality's direct effect will depend on mobilization (and vice-versa), conditional on whether or not there is international pressure. We need more complex notation than that introduced in Chapter 5 in order to represent the possible response types here. 


The result is $2^8=256$ possible response types for $D$. With 4 response types for $M$, we thus have 1024 possible combinations of causal effects between named variables in the lower-level graph. How do these lower-level response types map onto the higher-level response types that are of interest? In other words, which combinations of lower-level types represent a positive, negative, or zero causal effect of inequality on democratization? 

To define a  causal effect of $I$ in this setup, we need to define the "joint effect" of two variables as being the effect of changing both variables simultaneously (in the same direction, unless otherwise specified). Thus, the joint effect of $I$ and $M$ on $D$ is positive if changing both $I$ and $M$ from $0$ to $1$ changes $D$ from $0$ to $1$. We can likewise refer to the joint effect of an increase in one variable and a decrease in another. Given this definition, a positive causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns:


1. **Linked positive mediated effects.** $I$ has a positive effect on $M$; and $I$ and $M$ have a *joint* positive effect on $D$ when $P$ takes on whatever value it takes on in the case. 

2. **Linked negative mediated effects** $I$ has a negative effect on $M$; and $I$ and $M$ have a *joint* negative effect on $D$ when $P$ takes on whatever value it takes on in the case. 

3. **Positive direct effect** $I$ has no effect on $M$ and $I$ has a positive effect on $D$ at whatever value $M$ is fixed at and whatever value $P$ takes on in the case. 


If we start out with a case in which inequality is high and democratization has not occurred (or inequality is low and democratization *has* occurred), we will be interested in the possibility of a negative causal effect. A negative causal effect of inequality on democratization emerges for any of the following three sets of lower-level response patterns:


4. **Positive, then negative mediated effects** $I$ has a positive effect on $M$; and $I$ and $M$ have a *joint* negative effect on $D$ when $P$ takes on whatever value it takes on in the case. 

5. **Negative, then joint negative mediated effects** $I$ has a negative effect on $M$; and jointly increasing $I$ while decreasing $M$ generates a decrease in $D$ when $P$ takes on whatever value it takes on in the case. 

6. **Negative direct effects** $I$ has no effect on $M$ and $I$ has a negative effect on $D$ at whatever value $M$ is fixed at and whatever value $P$ takes on in the case. 

Finally, all other response patterns yield *no* effect of inequality on democratization.

Thus, for a case in which $I=D=1$, our query amounts to assessing the probability that $\theta^M$ and $\theta^D_{lower}$ jointly take on values falling into conditions 1, 2, or 3. And for a case in which $I \neq D$, where we entertain the possibility of a negative effect, our query is an assessment of the probability of conditions 4, 5, and 6.

### Forming Priors

We now need to express prior beliefs about the probability distribution from which values of $\theta^M$ and $\theta^D_{lower}$ are drawn. We place structure on this problem by drawing a set of beliefs about the likelihood or monotonicity of effects and interactions among variables from the theories in Boix, Acemoglu and Robinson, and Ansell and Samuels. As a heuristic device, we weight more heavily those propositions that are more widely shared across the three works than those that are consistent with only one of the frameworks. We intend this part of the exercise to be merely illustrative of how one might go about forming priors from an existing base of knowledge; there are undoubtedly other ways in which one could do so from the inequality and democracy literature.

Specifically, the belief that we embed in our priors about $\theta^M$ is:

*  **Monotonicity of $I$'s effect on $M$**: In Acemoglu and Robinson, inequality should generally increase the chances of---and, in Boix, should never prevent---mobilization by the poor. Only in Ansell and Samuels' model does inequality have a partial downward effect on the poor's demand for democracy insofar as improved material welfare for the poor increases the chances of autocratic expropriation; and this effect is countervailed by the greater redistributive gains that the poor will enjoy under democracy as inequality rises.^[In addition, as the industrial bourgeoisie become richer, which increases the Gini, this group faces a greater risk of autocratic expropriation. If we consider the rising bourgeosie's mobilization to be mobilization by a materially disadvantaged group, then this constitutes an additional positive effect of inequality on mobilization.] Consistent with the weight of prior theory on this effect, in our initial run of the analysis, we rule out negative effects of $I$ on $M$. We are indifferent in our priors between positive and null effects and between the two types of null effects (mobilization always occurring or never occurring, regardless of the level of inequality). We thus set our prior on $\theta^M$ as: $p(\theta^M=\theta^M_{10})=0.0$, $p(\theta^M=\theta^M_{00})=0.25$, $p(\theta^M=\theta^M_{11})=0.25$, and $p(\theta^M=\theta^M_{01})=0.5$. We relax this monotonicity assumption, to account for the Ansell and Samuels logic, in a second run of the analysis.

For our prior on democracy's responses to inequality, mobilization, and international pressure ($\theta^D_{lower}$), we extract the following beliefs from the literature:

*  **Monotonicity of direct $I$ effect: no positive effect**: In none of the three theories does inequality promote democratization via a pathway *other than* via the poor's rising demand for it. In all three theories, inequality has a distinct negative effect on democratization via an increase in the elite's expected losses under democracy and thus its willingness to repress. In Ansell and Samuels, the distribution of resources also affects the probability of success of rebellion; thus higher inequality also reduces the prospects for democratization by strengthening the elite's hold on power. We thus set a zero prior probability on all types in which $I$'s direct effect on $D$ is positive for any value of $P$. 

* **Monotonicity of $M$'s effect: no negative effect**: In none of the three theories does mobilization reduce the prospects of democratization. We thus set a zero probability on all types in which $M$'s effect on $D$ is negative at any value of $I$ or $P$. 

* **Monotonicity of $P$'s effect: no negative effect**: While international pressures are only discussed in Haggard and Kaufman's study, none of the studies considers the possibility that international pressures to democratize might prevent democratization that would otherwise have occurred. We thus set a zero probability on all types in which $P$'s effect is negative at any value of $I$ or $M$. 

In all, this reduces the number of nodal types for $D$ from 256 to just 20.

For all remaining, allowable types, we set flat priors.


In remaining 20  allowable types can involve a rich range of interactions between international pressure, inequality, and mobilization, including::

1. Types for which $P$ has no moderating effect

2. Types for which $P=1$ creates an "opportunity" for $X$ to have an effect that it does not have at $P=0$; at $P=1$ and $X=0$, $D$ takes on the value it does when $X=0$ and $X$ has an effect, but does not take on this value when $P=0$ and $X=0$

3. Types for which $P=1$ is a causal "complement" to $X$, allowing $X$ to have an effect it did not have at $P=0$; at $P=1$ and $X=1$, $D$ takes on the value it does when $X=1$ and $X$ has an effect, but does not take on this value when $P=0$ and $X=1$

4. Types for which $P=1$ "substitutes" for $X$, generating the outcome that $X=1$ was necessary to generate at $P=0$; at $P=1$ and $X=0$, $D$ takes on the value it does when $X=1$ and $X$ has an effect, but does not take on this value when $P=0$ and $X=0$

5. Types for which $P$ "eliminates" $X$'s effect, preventing $X=1$ from generating the outcome it generates when $P=0$; at $P=1$ and $X=1$, $D$ does not take on the value it does when $X=1$ and $X$ has an effect, but does take on this value when $P=0$ and $X=1$



Since $P$ conditions the effect of $I$, we must also establish a prior on the distribution of $P$. In this analysis, we set the prior probability of $P=1$ to 0.5, implying that before seeing the data we think that international pressures to democratize are present half the time.

## Results

We can now choose nodes in addition to $I$ and $D$ to observe from the lower-level model. Recall that our query is about the joint values of $\theta^M$ and $\theta^{D_{lower}}$. By the logic $d-$separation, we can immediately see that both $M$ and $P$ may be informative about these nodes when $D$ has already been observed. Conditional on $D$, both $M$ and $P$ are $d-$connected to both $\theta^M$ and $\theta^{D_{lower}}$. Let us see what we learn, then, if we search for either mobilization of the lower classes or international pressure or both, and find either clue either present or absent.

We consider four distinct situations, corresponding to four possible combinations of inequality and democratization values that we might be starting with. In each situation, the nature of the query changes. Where we start with a case with low inequality and no democratization, asking if inequality caused the outcome is to ask if the lack of inequality caused the lack of democratization. Where we have high inequality and no democratization, we want to know if democratization was prevented by high inequality (as high inequality does in Boix's account). For cases in which democratization occurred, we want to know whether the lack or presence of inequality (whichever was the case) generated the democratization. 

Inference is done by applying Bayes rule to the observed data given the priors. Different "causal types" are consistent or inconsistent with possible data observations. Conversely the observation of data lets us shift weight towards causal types that are consistent with the data and  away from those that are not.  As a simple illustration if we observe $D=1$ then we would shift weight from types for which $D$ is always 0, given  the observed data, to types for which $D$ can be 1 given the observed data.   



```{r basicPIMDmodel, eval = TRUE, message = FALSE, include = FALSE}

model <- make_model("I -> M -> D <- P; I -> D") %>%
  
         set_restrictions(c( 
           "(M[I=1] < M[I=0])",
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])")) %>%
  
         set_priors()   
  
```




```{r cinfer253, include = FALSE}
I1D1 <- conditional_inferences(model, 
                            query = "D[I=1] > D[I=0]", 
                            parameters = NULL, 
                            given = "D==1 & I==1")

I0D1 <- conditional_inferences(model, 
                            parameters = NULL, 
                            query = "D[I=1] < D[I=0]", 
                            given = "D==0 & I==0")

```




### Inferences for cases with observed democratization

We first turn to  cases in which democratization has occurred---the category of cases that Haggard and Kaufman examine.

For these cases we  use data from @haggard2012inequality to show the inferences we would draw using this procedure and the actual observations made for a set of 8 cases. 

Haggard and Kaufman consider only cases that democratized, so all cases in this table have the value $D=1$. We show here how confident we would be that the level inequality caused democratization if (a) we observed only the cause and effect ($I$ and $D$); (b) we additionally observed either the level of mobilization by disadvantaged classes or the level of international pressure; and (c) if we observed both, in addition to $I$ and $D$. Note that countries labels are marked in the  "full data" cells in the lower right  quadrant, but their corresponding partial data cells can be read by moving to the left column or the top row (or to the top left cell for the case with no clue data).

In coding countries' level of inequality, we rely on Haggard and Kaufman's codings using the Gini coefficient from the Texas Inequality dataset.  In selecting cases of democratization, we use the codings in @cheibub2010democracy, one of two measures used by Haggard and Kaufman. Our codings of the $M$ and $P$ clues come from close readings of the country-specific transition accounts in @haggard2012distributive, the publicly shared qualitative dataset associated with @haggard2012inequality. We code $M$ as $1$ where the transition account refers to anti-government or anti-regime political mobilization by economically disadvantaged groups, and as $0$ otherwise. For $P$, we code $P=0$ is international pressures to democratize are not mentioned in the transition account. The main estimates refer to analyses with only qualitative, monotonicity restrictions on our priors. We also show in square brackets the estimates if we allow for a negative effect of inequality on mobilization but believe it to be relatively unlikely.


#### $I=0, D=1$: Low inequality democracies


In a case that had low inequality and democratized, did low inequality cause democratization, as Boix's thesis would suggest? Looking at the first set of cases in Table \@ref(tab:HK8cases1), did Mexico, Albania, Taiwan, and Nicaragua democratize because they had relatively low inequality? Based only on observing the level of inequality and the outcome of democratization, we would place a `r round(I0D1[1,1],3)` probability on inequality having been a cause. What can we learn, then, from our two clues?

We are looking here for a negative effect of $I$ on $D$, which in our model can only run via a direct effect, not through mobilization. Thus, the learning from $M$ is limited for the same reason as in an $I=1, D=0$ case. And $M$ is modestly informative as a moderator for the same reasons and in the same direction, with observing mobilization generally reducing our confidence in inequality's negative effect relative to observing no mobilization. In our four cases, if we observe the level of mobilization, our confidence that inequality mattered goes up slightly (to `r round(I0D1[2,1], 3)`) in Mexico and Taiwan, where mobilization did not occur, and goes down slightly in Albania and Nicaragua (to `r round(I0D1[3,1], 3)`) where mobilization did occur.



```{r HK8cases1, echo = FALSE}
I0D1_Cases <- cases_table(
  conditional_inferences(
    model,   parameters = NULL,  query = "D[I=1] < D[I=0]",   given = "D==1 & I==0"), 
  case_names  = c("Mexico (2000)", "Taiwan (1996)", 
                  "Albania (1991)", "Nicaragua (1984)"))

kable(I0D1_Cases, digits = 3, caption = "Four cases with low inequality and  democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.")

```


Looking for the international pressure clue is, however, highly informative, though the effect runs in the opposite direction as in an $I=1, D=0$ case.  It is observing the absence of international pressure that makes us more confident in low inequality's effect. Since democratization *did* occur, the presence of international pressure makes it less likely for low inequality to have generated the outcome since international pressure could have generated democratization by itself. Once we bring this second clue into the analysis, Mexico and Taiwan sharply part ways: seeing no international pressure in Mexico, we are now much more confident that inequality mattered for the Mexican transition (`r round(I0D1[2,2],3)`); seeing international pressure in Taiwan, we are now substantially less confident that inequality mattered to the Taiwanese transition (`r round(I0D1[2,3], 3)`). Similarly, observing $P$ sharply differentiates the Albanian and Nicaraguan cases: seeing no international pressure in the Albanian transition considerably boosts our confidence in inequality's causal role there (`r round(I0D1[3,2], 3)`), while observing international pressure in the Nicaraguan transition strongly undermines our belief in an inequality effect there (`r round(I0D1[3,3],2)`).


#### $I=1, D=1$: High inequality democracies

Where we see both high inequality and democratization, the question is whether high inequality caused democratization via a positive effect. Considering the second set of cases in Table \ref{tab:HK8cases2}, did high inequality cause Mongolia, Sierra Leone, Paraguay, and Malawi to democratize?

Observing only the level of inequality and the democratization outcome, we would have fairly low confidence that inequality mattered, with a posterior on that effect of `r round(I1D1[1,1],1)`. Let us see what we can learn if we also observe the level of mobilization and international pressure.

As in an $I=0, D=0$ case, $M$ can now be highly informative since this positive effect has to run through mobilization. Here it is the observation of a lack of mobilization that is most telling: high inequality cannot have caused democratization, given our model, if inequality did not cause mobilization to occur. There is no point in looking for international pressure since doing so will have no effect on our beliefs. Thus, when we observe no mobilization by the lower classes in Mongolia and Paraguay, we can be certain (given our model) that high inequality did *not* cause democratization in these cases. Moreover, this result does not change if we also go and look for international pressure: neither seeing pressure nor seeing its absence shifts our posterior away from `r round(I1D1[1,1],2)`. 


```{r HK8cases2, echo = FALSE}

I1D1_Cases <- cases_table(
  conditional_inferences(
    model, parameters = NULL,  query = "D[I=1] > D[I=0]",   given = "D==1 & I==1"), 
  case_names  = c("Mongolia (1990)", "Paraguay (1989)", "Sierra Leone (1996)", "Malawi (1994)"))

kable(I1D1_Cases, caption = "Four cases with high inequality and  democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use priors assuming only monotonic effects.")

```

If we do see mobilization, on the other hand---as in Sierra Leone and Malawi---we are slightly more confident that high inequality was the cause of democratization (`r round(I1D1[3,1], 3)`). Moreover, if we first see $M=1$, then observing international pressure can add much more information; and it substantially differentiates our conclusions about the causes of Sierra Leone's and Malawi's transitions. Just as in an *$I=0, D=1* case, it is the absence of international pressure that leaves the most "space" for inequality to have generated the democratization outcome. When we see the absence of pressure in Sierra Leone, our confidence that high inequality was a cause of the transition increases to `r round(I1D1[3,2],3)`; seeing pressure present in Malawi reduces our confidence in inequality's effect to `r round(I1D1[3,3],3)`.


### Cases with incomplete data

We next first  causal relations for cases that did not democratize. These cases are not included in @haggard2012inequality but our model nevertheless characterizes our beliefs for these cases also.

The results for cases that did not democratize (at the time in question) are presented in Table \@ref(tab:Tapp1) and Table \@ref(tab:Tapp2). Each table shows, for one kind of case, our posterior beliefs on the causal responsibility of $I$ for the outcome for different search strategies.


#### $I=0, D=0$: Non democracy with low inequality

To begin with $I=0, D=0$ cases, did the lack of inequality cause the lack of democratization (as, for instance, at the lefthand end of the Acemoglu and Robinson inverted $U$-curve)? 


```{r Tapp1, echo = FALSE,  cache = FALSE, message = FALSE, warning = FALSE} 


# Did I=0 cause D = 0
I0D0 <- conditional_inferences(model, 
                            parameters = NULL, 
                            query = "D[I=1] > D[I=0]", 
                            given = "D==0 & I==0")

kable(dplyr::select(I0D0, P,M, posterior),digits = 3, caption = "No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.")
```


We start out, based on the $I$ and $D$ values and our model, believing that there is a `r round(I0D0[1,5],3)` chance that low inequality prevented democratization. We then see that our beliefs shift most dramatically if we go looking for mobilization and find that it was present. The reason is that any positive effect of $I$ on $D$ has to run through the pathway mediated by $M$ because we have excluded a positive direct effect of $I$ on $D$ in our priors. Moreover, since we do not allow $I$ to have a negative effect on $M$, observing $M=1$ when $I=0$ must mean that $I$ has no effect on $M$ on this case, and thus $I$ cannot have a positive effect on $D$ (regardless also of what we find if we look for $P$). If we do *not* observe mobilization when we look for it, we now think it is somewhat more likely that $I=0$ caused $D=0$ since it is still possible that high inequality *could* cause mobilization. 

We also see that observing whether there is international pressure has a substantial effect on our beliefs. When we observe $M=1$ (or don't look for $M$ at all), the presence of international pressure increases the likelihood that low inequality prevented democratization. Intuitively, this is because international pressure, on average across types, has a positive effect on democratization; so pressure's presence creates a greater opportunity for low inequality to counteract international pressure's effect and prevent democratization from occurring that otherwise would have (if there had been high inequality and the resulting mobilization).


#### $I=1, D=0$: Non democracy with high inequality

In cases with high inequality and no democratization, the question is whether high inequality prevented democratization via a negative effect, as theorized by Boix. That negative effect has to have operated via inequality's direct effect on democratization since our monotonicity restrictions allow only positive effects via mobilization. Here, the consequence of observing $P$ is similar to what we see in the $I=0, D=0$ case: seeing international pressure greatly increases our confidence that high inequality prevented democratization, while seeing no international pressure moderately reduces that confidence. There is, returning to the same intuition, more opportunity for high inequality to exert a negative effect on democratization when international pressures are present, pushing toward democratization. 



```{r Tapp2, echo = FALSE}

I1D0 <- conditional_inferences(model, 
                            parameters = NULL, 
                            query = "D[I=1] < D[I=0]", 
                            given = "D==0 & I==1")

kable(dplyr::select(I1D0, P,M, posterior), digits = 3, caption = "Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.")
```

Here, however, looking for $M$ has more modest effect than it does in an $I=0, D=0$ case. This is because we learn less about the indirect pathway from $I$ to $D$ by observing $M$: as we have said, we already know from seeing high inequality and no democratization (and under our monotonicity assumptions) that any effect could not have run through the presence or absence of mobilization. 

However, $M$ provides some information because it, like $P$, acts as *moderator* for $I$'s direct effect on $D$ (since $M$ is also pointing into $D$). As we know, learning about moderators tells us something about (a) the rules governing a case's response to its context (i.e., its response type) and (b) the context it is in. Thus, in the first instance, observing $M$ together with $I$ and $D$ helps us eliminate types inconsistent with these three data points. For instance, if we see $M=0$, then we eliminate any type in which $D$ is 0, regardless of $P$'s value, when $M=0$ and $I=1$. Second, we learn from observing $M$ about the value of $M$ under which $D$ will be responding to $I$. Now, because $M$ is itself potentially affected by $I$, the learning here is somewhat complicated. What we learn most directly from observing $M$ is *the effect of $I$ on $M$* in this case. If we observe $M=1$, then we know that $I$ has no effect on $M$ in this case; whereas if we observe $M=0$, $I$ might or might not have a positive effect on $M$. Learning about this $I \rightarrow M$ effect then allows us to form a belief about how likely $M$ would be to be 0 or 1 if $I$ changed from $0$ to $1$; that is, it allows us to learn about the context under which $D$ would be responding to this change in $I$ (would mobilization be occurring or not)? This belief, in turn, allows us to form a belief about how $D$ will respond to $I$ given our posterior beliefs across the possible types that the case is. 

The net effect, assuming that we have not observed $P$, is a small upward effect in our confidence that inequality mattered if we see no mobilization, and a small downward effect if we see mobilization. Interestingly, if we *do* observe $P$, the effect of observing $M$ reverses: observing mobilization increases our confidence in inequality's effect, while observing no mobilization reduces it. 


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::

Process tracing model in practice

Using the `CausalQueries` package we can set this model up, along with restrictions, as follows.

```{r restrictedmodelch7, eval = FALSE}
model <- 
  make_model("I -> M -> D <- P; I -> D") %>%
  set_restrictions("(M[I=1] < M[I=0])") %>%
  set_restrictions("(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])")
```

We can query a single quantity as follows:

```{r, eval = FALSE}
query_model(
  model, 
  query = "D[I=0]==0",
  given = "D==1 & I==1",
  using = "parameters")
```

::::
<br>


## Theory dependence

Haggard and Kaufman set out to use causal process observations to test inequality-based theories of democratization against the experiences of "Third Wave" democratizations. Their principal test is to examine whether they see evidence of distributive conflict in the process of democratization, defined largely as the presence or absence of mobilization prior to the transition. They secondarily look for other possible causes, specifically international pressure and splits in the elite. 

In interpreting the evidence, Haggard and Kaufman generally treat the absence of mobilization as evidence against inequality-based theories of democratization as a whole (p. 7). They also see the *presence* of distributive mobilization in cases with high inequality and democratization as evidence against the causal role of inequality (p. 7). These inferences, however, seem only loosely connected to the logic of the causal theories under examination. Haggard and Kaufman express concern that inequality-oriented arguments point to "cross-cutting effects" (p. 1) of inequality, but do not systematically work through the implications of these multiple pathways for empirical strategy. Our analysis suggests that a systematic engagement with the underlying models can shift that interpretation considerably. Under the model we have formulated, where inequality is *high*, the absence of mobilization in a country that democratized is indeed damning to the notion that inequality mattered. However, where inequality is *low*---precisely the situation in which Boix's theory predicts that we will see democratization---things are more complicated. If we assume that inequality cannot prevent mobilization, then observing no mobilization does not work against the claim that inequality mattered for the transition; indeed, it slightly supports it, at least given what we think is a plausible model-representation of arguments in the literature. Observing the absence of inequality in such a case, however, can undercut an inequality-based explanation if (and only if) we believe it is possible that inequality might prevent mobilization that would otherwise have occurred. Further, in cases with high inequality and democratization, it is the *absence* of mobilization by the lower classes that would least consistent with the claim that inequality mattered. Observing mobilization, in contrast, pushes in favor of an inequality-based explanation.

Moreover, it is striking that Haggard and Kaufman lean principally on a mediator clue, turning to evidence of international pressure and elite splits (moderators, or alternative causes) largely as secondary clues to identify "ambiguous" cases. As we have shown, under a plausible model given prior theory, it is the moderator clue that is likely to be much more informative.

Of course, the model that we have written down is only one possible interpretation of existing theoretical knowledge. It is very possible that Haggard and Kaufman and other scholars in this domain hold beliefs that diverge from those encoded in our working model. The larger point, however, is that our process tracing inferences will inevitably *depend*---and could depend greatly---on our background knowledge of the domain under examination. Moreover, formalizing that knowledge as causal model can help ensure that we are taking that prior knowledge systematically into account---that the inferences we draw from new data are consistent with the knowledge that we bring to the table.

The analysis also has insights regarding case selection. Haggard and Kaufman justify their choice of only $D=1$ cases as a strategy "designed to test a particular theory and thus rests on identification of the causal mechanism leading to regime change" (p. 4). Ultimately, however, the authors seem centrally concerned with assessing whether inequality, as opposed to something else, played a key causal role in generating the outcome. As the results above demonstrate, however, there is nothing special about the $D=1$ cases in generating leverage on this question. The tables for $D=0$ show that, given the model, the same clues can shift beliefs about as much for $D=0$ as for $D=1$ cases. We leave a more detailed discussion of this kind of issue in model-based case-selection for Chapter \@ref(caseselection).

Finally we emphasize that all of the inference in this chapter depends on a model that is constrained by theoretical insights but not one that is trained by data. Although we are able to make many inferences using this model, given the characteristics of a case of interest, we have no empirical grounds to justify these inferences. In Chapter \@ref(mixingapp) we show how this model can be trained with broader data from multiple cases and in Chapter \@ref(evaluation)  we illustrate how the model itself can be put into question. 

<!-- THE FOLLOWING TWO CONDITIONS ARE REMOVED BECAUSE, I AM ALMOST POSITIVE, THEY ARE ALREADY COVERED BY THE MONOTONICITY CONDITIONS. \item **$I=1$ dampens $M$'s effect**: Acemoglu and Robinson model inequality's effect on elite preferences (our direct effect) and and its effect on the poor's preferences (essentially, our effect via mobilization) as countervailing. While the poor want democracy more as inequality rises, the elite become more determined to avoid it. Thus, as inequality rises (holding the level of mobilization constant), mobilization should be less likely to succeed. The studies under consideration, moreover, provide no reason to think the opposite: that mobilization should become more likely to succeed as inequality goes up, holding constant the level of mobilization. To reflect these beliefs, we place zero probability on those types in which democracy will not occur at low levels of inequality but in which mobilization has a positive effect at a high level of inequality at any value of $P$. This yields average prior beliefs weighted toward inequality dampening mobilization's effect. -->
<!-- \item **$M=1$ dampens $I$'s effect**: Likewise, where disadvantaged groups have mobilized, inequality should be less likely to have a negative direct effect. At high levels of mobilization, it will be more difficult for elite preferences (which higher inequality drives against democratizing) to prevail. And, based on the logics in the literature, it should never be the case that mobilization enhances inequality's negative effect on democratization. We thus place zero probability on those types in which, for any value of $P$, democratization would occur at high levels of inequality but in which $I$ has a negative effect when $M=1$, generating a belief set weighted toward the belief that mobilization dampens inequality's negative effect. -->


<!--chapter:end:08-PT-application.Rmd-->

# Integrated inferences  {#mixing}



```{r packagesused08, include = FALSE}
source("_packages_used.R")
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We extend the analysis of Chapter \@ref(pt) to settings in which we can  simultaneously learn from cross case data on treatment and outcomes and causal process data for a subset of cases. In doing so we update our theory from cases and *use* our updated theory to draw case-level inferences. While our process tracing was entirely theory-informed, mixed-data inference is also *data*-informed.
::::
<br>


<!-- to do: likelihood principle -->
<!-- cluster illustration -->
<!-- bring impications of  partial data out of wrinkles / section: in the conclusion-->
<!-- conclusions: mixing and missing data-->

<br>

In this chapter we generalize the model developed in Chapter \@ref(pt) to research situations in which we have data on multiple cases.  

We start with a conceptual point: the structure introduced in Chapter 6 for single-case analysis can be used *as is* for multi-case analysis. Thus, the conceptual work for mixed methods inference from models has been done already. Our goal for the rest of the chapter is thus more technical than conceptual---to show how to shift focus beyond sample level queries and to exploit assumptions regarding independence across cases to generate simpler models of causal processes that affect many units. As we do so, we provide microfoundations for the models in Chapter \@ref(ptapp) (as with those in @humphreys2015mixing)  with the probative value of clues derivable from a causal structure and data rather than provided directly by researchers.


## Sample inference

Conceptualized correctly, there is no deep difference between the logic of inference used in single-case and in multi-case studies. This is not because any single "case" can be disaggregated into many "cases," thereby allowing for large $n$ analysis on small problems [@king1994designing]. Rather, the opposite: fundamentally, model-based inference always involves comparing *a* pattern of data with the logic of the model. Studies with multiple cases can, in fact, be conceptualized  as single-case studies: we always draw our inferences from a single *collection* of clues, whether those clues have come from one or from many units.

In practice, when we move from a causal model with one observation to a causal model with multiple observations, we can use the structure we introduced in Chapter \@ref(pt) but simply replace nodes that have a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors) drawn from multiple cases. We then make inferences about causal relations between nodes from seeing the values of those nodes' (or other nodes') vectors.

To illustrate, consider the following situation. Suppose that our model includes a binary treatment $X$ that is assigned to 1 with probability 0.5; an outcome, $Y$; and a third "clue" variable, $K$, all observable. We posit an unobserved variable $\theta^Y$, representing $Y$'s nodal type, with $\theta^Y$ taking on values in $\{a,b,c,d\}$ with equal probability. (We interpret the types in $\{a,b,c,d\}$ as defined in Section \@ref(counterfactualmodel).) In addition to pointing into $Y$, moreover, $\theta^Y$ affects $K$. In particular, $K=1$ whenever $X$ has an effect on $Y$, while $K=1$ with a 50% probability otherwise. In other words, our clue $K$ is informative about $\theta^Y$, a unit's nodal type for $Y$. As familiar from Chapters \@ref(pt) and \@ref(ptapp), when we observe $K$ in a case we can update on causal effects within the case since that $K$ value will have different likelihoods under different values of $\theta^Y$.

So far, we have described the problem at the unit level. Let's now consider a two-case setup. We do this by exchanging scalar nodes for vectors:

* We have a treatment node, $X$, that can take on one of four values, $(0,0), (0,1), (1,0), (1,1)$ with equal probability. 
* $\theta^Y$ is now a vector with two elements that can take on one of 16 values $(a,a), (a,b),\dots (d,d)$ as determined by $\lambda_\theta$. We might imagine a uniform distribution over these 16 elements.
* $Y$ is a vector that is generated by  $\theta^Y$ and $\X$ in the obvious way (e.g., $X=(0,0), \theta^Y=(a,b)$ generates outcomes $Y=(1,0)$)
* The vector $K$ has the same domain as $X$ and $Y$, and element $K[j]=1$ if $\theta^Y[j]=b$.

Now, consider a causal estimand. In a single-case setup, we might ask whether $X$ has an effect on $Y$ in the case. For a multi-case setup, we might ask what the Sample Average Treatment Effect, $\tau$, is. Note a subtle difference in the nature of the answers we seek in these two situations. In the first (single-case) instance, our estimand is binary---of the form: "is the case a $b$ type?"---and our answer is a probability. In the multi-case estimation of the sample average treatment effect ("SATE"), our estimand is categorical and our answer is a probability distribution: we are asking "what is the probability that $\tau$ is 0?," "what is the probability that $\tau$ is .5?", and so on.

<!-- We will consider three counterfactual possibilities. In one, both units have $X$ forced to 0. In the other two, one unit has $X$ set to 0 and the other has $X$ set to 1. Thus, we are interested in the average effect of changing one unit to treatment while the other is held in control. -->

While the estimand shifts, we can use the tools introduced for single-case process tracing in Chapters \@ref(pt) and \@ref(ptapp) to analyze this (superficially) multi-case study. To begin, our prior on the probability that $\tau=1$ is the prior that $X$ has a positive effect on $Y$ in both cases, that is, that $\theta^Y = (b,b)$: just 1 in 16. 

Now, suppose that we observe that, for both units, $X=1$ and $Y=1$. This data pattern is consistent only with four possible $\theta$ vectors: $(b,b), (d,d), (b, d), (d,b)$. Moreoever each of these four is equally likely to produce the data patter we see). So our belief that  $\tau=1$ now shifts from 1 in 16 to to 1 in 4. Next, suppose that we further observe the data pattern $\mathbf K = (1,1)$. The probability of this pattern for $\Theta$ vector $(b,b)$ ($\tau = 1$) is 1. And for the type vectors $(d,d), (b, d), (d,b)$, the probability of this $\mathbf K$ pattern is $.25, .5,$ and $.5$, respectively. Applying Bayes' rule, our updated belief that $\tau = 1$ is then $1/(1 + .25 + .5 + .5) = 4/9$. 

We can similarly figure out the posterior probability on any possible value of $\tau$ and so build up a full posterior distribution. And we can do so given any $\mathbf K$ pattern (i.e., $\mathbf K$ realization) across the cases. Thus, if we observe the data pattern $\mathbf K = (0,1)$, the probability of this pattern for type vector $(b,b)$ ($\tau = 1$)  is 0. For the type vectors $(d,d), (b, d), (d,b)$ it is $.25, 0, .5$, respectively.  The table below represents the posterior distribution over a set of discrete treatment effect values given different $K$ patterns observed.

| $X$ pattern | $Y$ pattern | $K$ pattern | $\tau = -1$ | $\tau = -.5$ | $\tau = 0$ | $\tau = .5$ | $\tau = 1$ |
|-------------|-------------|-------------|-------------|--------------|------------|-------------|------------|
| (1,1)       | (1,1)       | (1,1)       | 0           | 0            | 1/9        | 4/9         | 4/9        |
| (1,1)       | (1,1)       | (1,0)       | 0           | 0            | 1/3        | 2/3         | 0          |
| (1,1)       | (1,1)       | (0,0)       | 0           | 0            | 1          | 0           | 0          |


The conceptual point is that the general logic of inference with multiple units is the same as that with one unit. In both situations, we work out the likelihood of any given data *pattern* for each possible set of values of model parameters and update our beliefs about those parameters. And, from our posterior distribution over model parameters (e.g., $\Theta^Y$), we then derive a posterior distribution over the possible answers to our query (e.g., values of $\tau$).^[Representing node values in vector forms like this allows for  vector-level mappings that imply more complex dependencies between units. For instance we might imagine instead that we observe $K=1$ if and only if $\theta^Y = (b,b)$, in which case observation of $K$ lets us distinguish between $\tau = 1$ and $\tau = .5$ but not between $\tau = .5$ and $\tau = 0$.]

<!-- Two points are worth highlighting however. The first is that rather than updating over the query directly (for instance $\tau = .5$) directly, we update over the underlying parameter vector and map from underlying parameters to the query of interest. The second is that representing node values in vector forms like this allows for  vector-level mappings that imply more complex dependencies between units. For instance we might imagine instead that we observe $K=1$ if and only if $\theta = (b,b)$, in which case observation of $K$ lets us distinguish between $\tau = 1$ and $\tau = .5$ but not between $\tau = .5$ and $\tau = 0$. -->


## From sample queries to general processes 

Although the core conceptual logic is the same for multi-case and single-case inference, going forward, we operationalize these problems somewhat differently. 

For the remainder of this chapter, and for the rest of the book, when we focus on muti-case studies, we will set our sights primarily on models that describe general processes. Rather than seeking to understand the average effect in a set of cases, we seek to understand the causal relations that gave rise to the set of cases. From these we sometimes draw inferences to cases but in general our models will involve queries  pitched in general terms.

There are two  reasons for this. The first is that we are interestied in learning across cases: To figure out how what we see in one case provides insight for what is happening in another. We do this by using data on some cases to update our beliefs about a general model that we think is of relevance for other cases. Thus we seek to learn about a general model. The second reason is  more practical. The first is that if we can think of units as draws from a large population, and then invoke independence assumptions across types, then we can greatly reduce complexity by analyzing problems at the unit level rather than at the population level. In the 2-case example above, the vector $\theta^Y$ could take on any of 16 values ($(a,a), (a,b),\dots (d,d)$). At the case level, however, the node $\theta^Y$ can take on only 4 values ($\{a,b,c,d\}$), yet we can learn about each case's $\theta^Y$ value from data drawn from all the cases. Thinking about it this way simplifies the problem by greatly reducing the parameter space, but it is not free. It requires invoking the assumption that (potential) outcomes across units do not depend on each other. If we cannot stand by that assumption, then we will need to build independence failures into our models.

Taking this step, the procedure we now use in the mixed methods works as follows.


### Set up


1. **A DAG**. As for process tracing, we begin with a graphical causal model specifying possible causal linkages between nodes. Our "chain" model for instance has DAG: $X \rightarrow M \rightarrow Y$.

2. **Nodal types**. Just as in process tracing, the DAG and variable ranges define the set of possible nodal types in the model---the possible ways in which each variable is assigned (if exogenous) or determined by its parents (if endogenous). For the $X \rightarrow M \rightarrow Y$ model there are 2 types for $\theta^X$, 4 for $\theta^M$, and 4 for $\theta^Y$.  

3. **Causal types**. A full set of nodal types gives rise to a full set of causal types, encompassing all possible combinations of nodal types across all nodes in the model. We let $\theta$ denote an arbitrary causal type. For a $X \rightarrow M \rightarrow Y$ model, one possible causal type would be $\theta = (\theta^X_1, \theta^M_{01}, \theta^M_{01})$.

4. **Parameters.**  As before, we use $\lambda^V$ to denote the probabilities of $\theta^V$ for a given node, $V$. Recall that in process tracing, we sought to learn about $\theta$ and our priors were given by $\lambda$. When we shift to multi-case inference, $\lambda$ becomes the parameter that we want to learn about: we seek to learn about the probability of different types arising in  a population (or the *shares* of types in a large population). 
 
5. **Priors**. In the process tracing setup, we treat $\lambda$ as given: we do not seek to learn about $\lambda$, and uncertainty over $\lambda$ plays no role. When we get to observe data on multiple cases, however, we have the opportunity to learn *both* about the cases at hand *and* about the population. Moreover, our level of uncertainty about population-level parameters will shape our inferences. We thus want our parameters (the $\lambda$'s) to be drawn from a prior *distribution* --- a distribution that expresses our uncertainty and over which we can update once we see the data. While different distributions may be appropriate to the task in general, uncertainty over proportions (of cases, events, etc.) falling into a set of discrete categories is usefully described by a Dirichlet distribution, as discussed in Chapter \@ref(bayeschapter). Recall that the parameters of a Dirichlet distribution (the $\alpha$'s) can be thought of as conveying both the relative expected proportions in each category and our degree of uncertainty. 


```{r,  fig.width = 4, fig.width = 8, out.width = "95%", echo = FALSE, fig.cap = "Types, parameters, and priors"}
source("data/parameters.R")
dag_3d(add_M = TRUE, func = TRUE)
```


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
**Box: Setting priors**

For a model with no unobserved confounding, setting priors requires specifying a prior distribution for each node. Specifically, we are expressing, for each node, a prior belief about the share of the population that is of each nodal type. We use a Dirichlet distribution, which allows us to indicate both our "best guess" about nodal type shares and our degree of uncertainty about those shares. The parameters of a Dirichlet distribution (the $\alpha$'s) are provided as vectors of positive numbers with one number for each nodal type. The relative size of each number indicates our prior belief about the relative share of each nodal type. The absolute sizes indiciates our degree of prior certainty in those beliefs. 
For a simple $X \rightarrow Y$ model, we have two $\alpha$ parameter sets: one for $X$'s types and one for $Y$'s types. 

For $X$'s types, we specify $\alpha^X_0$ and $\alpha^X_1$, corresponding to the nodal types $\theta^X_0$ and $\theta^X_1$, respectively. A distribution of the form ($\alpha^X_0=100, \alpha^X_1=100)$, for instance, implies a great deal of confidence that the population is composed about equally of $\theta^X_0$ and $\theta^X_1$ cases (or, equivalently, that $\lambda^X_1$ is around 0.5). In contrast, a distribution of the form ($\alpha^X_0=.1, \alpha^X_1=.1)$ implies a very high level of uncertainty: we believe that either most cases are $\theta^X_0$'s or that most cases are $\theta^X_1$'s, but we are not sure which. 

<!-- AJ: NEED SOME MORE REASONING ABOUT THIS 0.1, 0.1 SCENARIO. WHAT DO ALPHA VALUES < 1 MEAN?  -->

For $Y$'s types, we specify $\alpha^Y_{00}$, $\alpha^Y_{10}$, $\alpha^Y_{01}$, and $\alpha^Y_{11}$, corresponding to the nodal types $\theta^Y_{00}$, $\theta^Y_{01}$, and so on. So, for instance:

* $\alpha^Y_{00}=1$, $\alpha^Y_{10}=1$, $\alpha^Y_{01}=1$, and $\alpha^Y_{11}=1$ yields a uniform prior distribution in which all share allocations of types in the population are equally likely. 
* $\alpha^Y_{00}=3$, $\alpha^Y_{10}=3$, $\alpha^Y_{01}=3$, and $\alpha^Y_{11}=3$ puts somewhat more weight on share allocations in which the shares are relatively equal. 
* $\alpha^Y_{00}=5$, $\alpha^Y_{10}=5$, $\alpha^Y_{01}=10$, and $\alpha^Y_{11}=5$ puts greater weight positive causal effects ($\theta^Y_{01}$) than on the other three types.

In a model without unobserved confounding, we set our beliefs about nodal type shares for each node independently. Thus, we can express more confidence in our beliefs about one node than about another by setting their $\alpha$ values at different absolute levels. However, we would need to introduce unobserved confounding into a model in order to express beliefs about *pairings* of nodal types *across* nodes --- for instance, the belief that $\theta^Y_{01}$ is more likely when $\theta^X_1 = 1$. 

::::



### Inference

Inference then works by figuring out the probability of the data given different possible parameter vectors, $\lambdas$, and then applying Bayes' rule. In practice we proceed as follows. 


**Distributions over causal types.** We first need characterize our beliefs about causal types given any possible parameter vector $\lambda$. Imagine a draw of one possible value of $\lambda$ from the prior. This $\lambda$ vector implies a set of nodal type shares for all nodes. That set of nodal type shares implies, in turn, a distribution over *causal* types ($\theta$). For instance, the probability of causal type $\theta = (\theta^X_1, \theta^Y_{01}, \theta^M_{01})$ is simply $p(\theta|\lambda)=\lambda^X_1\lambda^M_{01}\lambda^Y_{01}$.  More generally:

$$p(\theta|\lambda) = \prod_{k,v:\theta^v_k\in\theta}\lambda^v_k$$



**Event probabilities**. Each causal type in turn implies a single data realization, or data type. For instance $\theta = (\theta^X_1, \theta^M_{01}, \theta^Y_{01})$ implies data $X=1, M=1, Y=1$. Let $D(\theta)$ denote the data type implied by causal type $\theta$. A single data type, however, may be implied by multiple causal types. We use $\Theta(d)$ to denote the set of causal types that imply a given data type:

$$\Theta(d) : \{\theta| D(\theta) = d \}$$

The probability of a given data type $d$, is then: 

$$w_d = \sum_{\theta \in \Theta(d)}p(\theta|\lambda)$$

And we use $\mathbf w$ to denote the vector of event probabilities over all data types. 

To illustrate, a data type $d = (X=1, M =1, Y=1)$  is consistent with four different causal types in the $X\rightarrow M\rightarrow Y$ model: $\Theta(d) = \{(\theta^X_0, \theta^M_{01}, \theta^Y_{01}), (\theta^X_0, \theta^M_{11}, \theta^Y_{01}), (\theta^X_0, \theta^M_{01}, \theta^Y_{11}), (\theta^X_0, \theta^M_{11}, \theta^Y_{11})\}$. The probability of the data type is then calculated by summing up the probabilities of each causal type that implies the event: $w_{111}:=\lambda^X_1(\lambda^M_{01} + \lambda^M_{11}))(\lambda^Y_{01} + \lambda^Y_{11})$. 

In practice, calculating the full $\mathbf w$ vector is made easier by the construction of a "parameter matrix" and an "ambiguity matrix", just as for process tracing, that tells us which causal types are consistent with a particular data type.  

We use Tables  \@ref(tab:ambigmatrixmix) and \@ref(tab:parammmatrixmix)  to illustrate how to calculate the event probability for each data type for a given parameter vector $\lambda$. Starting with data type $X=0, Y=0$ (first column of the ambiguity matrix), we see that the consistent causal types are ($\theta^X_0, \theta^Y_{00}$) and ($\theta^X_0, \theta^Y_{01}$), in rows 1 and 4. We then turn to columns 1 and 4 of the parameter matrix to read off the probability of each of these causal types---in each case given by the probability of the nodal types that it is formed out of. This gives $.4 \times .3$ and $.4\times .2$ giving a total probability of $0.2$ for the $X=0, Y=0$ event.  All four event probabilities, for the four data types, are then calculated in the same way.

In practice we do this all using matrx operations. 

```{r ambigmatrixmix, echo = FALSE, include = TRUE}
XY <- make_model("X -> Y") %>% set_parameters(c(.4, .6, .3, .2, .2, .3))
ambiguityXY  <- get_ambiguities_matrix(XY)
kable(ambiguityXY, caption = "An ambiguity matrix for a simple $X \\rightarrow Y$ model (with no unobserved confounding). Rows are causal types, columns are data types.")
```

```{r parammmatrixmix, echo = FALSE, include = TRUE}
paramXY <- get_parameter_matrix(XY) 
paramXY_with_params <-cbind(paramXY, '$\\lambda$' = XY$parameters_df$param_value)
kable(paramXY_with_params, caption = "A parameter matrix for a simple $X \\rightarrow Y$ model (with no unobserved confounding), indicating a single draw of $\\lambda$ values from the prior distribution.")
```


**Likelihood**. Now that we know the probability of observing each data pattern in a *single* case given $\lambda$, we can use these event probabilities to aggregate up to the likelihood of observing a data pattern across multiple cases (given $\lambda$). For this aggregation, we make use of an independence assumption: that each unit is independently drawn from a common distribution. Doing so lets us move from a categorical distribution that gives the probability that a single case has a particular data type to a *multinomial* distribution that gives the probability of seeing an arbitrary data pattern across any number of cases. 

Specifically, with discrete variables, we can think of a given multiple-case data pattern simply as a set of counts across categories. For, say, $X, Y$ data, we will observe a certain number of $X=0, Y=0$ cases (which we notate as $n_{00}$), a certain number of $X=1, Y=0$ cases ($n_{10}$), a certain number of $X=0, Y=1$ cases ($n_{01}$), and a certain number of $X=1, Y=1$ cases ($n_{11}$). A data pattern, given a particular set of variables observed (a search strategy), thus has a multinomial distribution. The likelihood of a data pattern under a given search strategy, in turn, takes the form of a multinomial distribution conditional on the number of cases observed, $n$, and the probability of each data type, given a $\lambda$ draw. More formally, we write:

$$d \sim \text{Multinomial}(n, w(\lambda))$$ 

Let us assume now that we have a 3-node model, with $X, Y$, and $M$ all binary. Let $\mathbf n_{XYM}$ denote an 8-element vector recording the number of cases in a sample displaying each possible combination of $X,Y,M$ data, thus: $\mathbf D= \mathbf n_{XYM}:=(n_{000},n_{001},n_{100},\dots ,n_{111})$. The elements of $\mathbf n_{XYM}$ sum to $n$, the total number of cases studied. Likewise, let the event probabilities for data types given $\lambda$ be registered in a vector, $\mathbf w_{XYM}=(w_{000},w_{001},w_{100},\dots ,w_{111})$. The likelihood of a data pattern, $\mathbf D$ is then:

<!-- AJ: is my added use of mathbf above to indicate vectors right? If so, it occurs to me that we should be mathbf'ing lambda just about everywhere. And should it be mathbf D in the equation below?-->

$$
p(d|\lambda) = 
  \text{Multinom}\left(n_{XYM}|\sum n_{XYM}, w_{XYM}(\lambda)\right)  \\
$$
In other words, the likelihood of observing a particular data pattern given $\lambda$ is given by the corresponding value of the multinomial distribution given the data probabilities. 


4. **Estimation**. We now have all the components for updating on $\lambda$. Applying Bayes rule (see Chapter @\ref(bayeschapter)), we have:

$$p(\lambda | d) = \frac{p(d | \lambda)p(\lambda)}{\int_{\lambda'}{p(d | \lambda')p(\lambda')}}$$
In the `CausalQueries` package this updating is implemented in `stan`, and the result of the updating is a dataframe that contains a collection of draws from the posterior distribution for $\lambda$. Table \@ref(tab:posteriortable) illustrates what such a dataframe might look like for an $X\rightarrow M \rightarrow Y$ model. Each row represents a single draw from $p(\lambda|d)$. The 10 columns represent shares for each of the 10 nodal types in the model, under each $lambda$ draw. 

```{r posteriortable, echo = FALSE}
if(do_diagnosis)
make_model("X -> M -> Y") %>% update_model(data.frame(X = 0:1, Y = 0:1)) %>% write_rds("saved/posterior_chain.rds")

read_rds("saved/posterior_chain.rds")$posterior_distribution %>%
head() %>% kable(digits = 2, caption = "An illustration of a posterior distribution for a $X \\leftarrow M \\leftarrow Y$ model. Each row is a draw from $p(\\lambda|d))$")

```


5. **Querying**. 

Once we have generated a posterior distribution for $\lambda$, we can then query that distribution. The simplest queries relate to values of $\lambda$. For instance, if we are interested in the probability that $M$ has a positive effect on $Y$, given an updated $X \rightarrow M \rightarrow Y$ model, we want to know about the distribution of $\lambda^M_{01}$. This distribution can be read directly from column 9 ($Y01$) of Table  \@(tab:posteriortable). More complex queries can all be described as summaries of combinations of these columns. For instance, the query, "What is the average effect of $M$ on $Y$" is a question about the distribution of $\lambda^M_{01} -  \lambda^M_{10}$, which is given by the difference between columns 9 and 8 of the table. Still more complex queries may require keeping some nodes constant while varying others, yet all of these can be calculated as summaries of the combinations of columns of the posterior distribution, following the rules described in Chapter \@ref(questions).

Table \@ref(tab:chainillustration) shows examples of a full mapping from data to posteriors. We begin with a simple chain model of the form $X\rightarrow M \rightarrow Y$ with flat priors over nodal types and report inferences on a set of queries (columns) for difference data types (rows).

```{r chainillustration, echo = FALSE }
model <- make_model("X->M->Y")

datas <- list(
  none = data.frame(X = NA, M = NA, Y = NA),
  X1Y1 = data.frame(X = 0:1, M = NA, Y = 0:1),
  X1M1Y1 = data.frame(X = 0:1, M = 0:1, Y = 0:1),
  X1Y1x10 = data.frame(X = rep(0:1, 5), M = NA, Y = rep(0:1, 5)),
  X1M1Y1x10 =data.frame(X = rep(0:1, 5), M = rep(0:1, 5), Y = rep(0:1, 5))
  )

if(do_diagnosis)
chain_illustration <- 
  
  lapply(datas, function(d) 
    update_model(model, d, keep_transformed = TRUE, iter = 4000) %>%
         query_model(list(a = "Y[X=1] < Y[X=0]",
                          b = "Y[X=1] > Y[X=0]",
                          c = "Y[X=1]==0 & Y[X=0]==0",
                          d = "Y[X=1]==1 & Y[X=0]==1",
                          ate_XM = "M[X=1] - M[X=0]",
                          ate_MY = "Y[M=1] - Y[M=0]",
                          ate_XY = "Y[X=1] - Y[X=0]",
                          PC = "Y[X=1] > Y[X=0]",
                          PC_M0 = "Y[X=1] > Y[X=0]",
                          PC_M1 = "Y[X=1] > Y[X=0]"),
                     given = c(rep(TRUE, 7), "X==1 & Y==1", "X==1 & Y==1 & M==0",  "X==1 & Y==1 & M==1"),
                     using = "posteriors")) %>%
  write_rds("saved/08_chain_illustration.rds")

chainillustration <- read_rds("saved/08_chain_illustration.rds") %>%
 bind_rows(.id = "Data") %>% select(-Using, - Given, - Case.estimand, - sd) %>%
  pivot_wider(names_from = Query, values_from = c(mean))%>% select(-"PC_M0", -"PC_M1")

chainillustration %>% mutate(Data = c("No data", "2 cases X, Y data only", "2 cases, X, M, Y data", "10 cases: X, Y data only", "10 cases: X, M, Y data")) %>%
  kable(digits = 2, caption = "Inferences on a chain model given different amounts of data (all on the diagonal, with X=0, Y=0 or X=1, Y=1). Columns 1-4 are shared of reduced form relations between X and Y, columns 5 - 8 show $\\tau_{ij}$---average effects of $i$ on $j$; the last column shows the probability of causation.", col.names = c("Data", "a", "b", "c", "d", "$\\tau_{XM}$", "$\\tau_{MY}$", "$\\tau_{XY}$", "PC"))
  
```


<!-- |     **Causal types** $\rightarrow$     | $\theta^X_0,\theta^Y_{00}$ | $\theta^X_1,\theta^Y_{00}$ | $\theta^X_0,\theta^Y_{10}$ | $\theta^X_1,\theta^Y_{10}$ | $\theta^X_0,\theta^Y_{01}$ | $\theta^X_1,\theta^Y_{01}$ | $\theta^X_0,\theta^Y_{11}$ | $\theta^X_1,\theta^Y_{11}$ | Parameter values (a draw from the prior) | -->
<!-- |:--------------------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:-----------------------------------------:| -->
<!-- | **Population parameters** $\downarrow$ |                            |                            |                            |                            |                            |                            |                            |                            |                                           | -->
<!-- |              $\lambda^X_0$             |              0             |              1             |              0             |              1             |              0             |              1             |              0             |              1             |                    0.4                    | -->
<!-- |              $\lambda^X_1$             |              1             |              0             |              1             |              0             |              1             |              0             |              1             |              0             |                    0.6                    | -->
<!-- |            $\lambda^Y_{00}$            |              1             |              1             |              0             |              0             |              0             |              0             |              0             |              0             |                    0.3                    | -->
<!-- |            $\lambda^Y_{10}$            |              0             |              0             |              1             |              1             |              0             |              0             |              0             |              0             |                    0.2                    | -->
<!-- |            $\lambda^Y_{01}$            |              0             |              0             |              0             |              0             |              1             |              1             |              0             |              0             |                    0.2                    | -->
<!-- |            $\lambda^Y_{11}$            |              0             |              0             |              0             |              0             |              0             |              0             |              1             |              1             |                    0.3                    | -->
<!-- Table: (\#tab:parammmatrixmix). A parameter matrix for a simple $X \rightarrow Y$ model (with no unobserved confounding), indicating a single draw of $\lambda$ values from the prior distribution. -->

### Wrinkles

#### Unobserved confounding.

When there is unobserved confounding, we need parameter sets that allow for a joint distribution over nodal types. Unobserved confounding, put simply, means that there is confounding across nodes that is not captured by nodes and edges represented on the DAG. More formally, in the absence of unobserved confounding, we can treat the distribution of nodal types for a given node as independent of the distribution of nodal types for every other node. Unobserved confounding means that we believe that nodal types may be correlated across nodes. Thus, for instance, we might believe that those units assigned to $M=1$ have different potential outcomes for $Y$ than those assigned to $M=0$ -- i.e., that the probability of $M=1$ is correlated with whether or not $M$ has an effect on $Y$. To allow for such a correlation, we have to allow $\theta^M$ and $\theta^Y$ to have a joint distribution. There are different ways to do this in practice, but a simple approach is to split the parameter set corresponding to the $Y$ node into two: we specify one distribution for $\theta^Y$ when $M=0$ and a separate distribution for $\theta^Y$ when $M=1$.  For each of these parameter sets, we specify two $\alpha$ parameters representing our priors. We can draw $\lambda$ values for these conditional nodal types from the resulting Dirichlet distributions, as above, and can then calculate causal type probabilities in the usual way. Note that if we do this in an $X \rightarrow M \rightarrow Y$ model, we have one 2-dimensional Dirichlet distribution corresponding to $X$, one 4-dimensional Dirichlet distribution corresponding to $M$, and two 4 dimensional distributions corresponding to $Y$. In all, with 1+3+3+3 degrees of freedom: exactly the number needed to represent a joint distribution over all $\theta^X, \theta^M, \theta^Y$ combinations.

<!-- AJ: Why do we specify only "two" alpha parameters for each parameter set above? Not 4? -->

<!-- AJ: Check my math on degrees of freedom -->

In the figure below we represent this confounding by indicating parameters values $\lambda_{MY}$ that determine the joint distribution over $\theta_M$ and $\theta_Y$.


```{r,  fig.width = 4, fig.width = 8, out.width = "95%", echo = FALSE, fig.cap = "Types, parameters, and priors, with confounding"}
dag_3d(add_M = TRUE, func = FALSE, confounding = TRUE)
```

<!-- AJ: dag_3d file seems to have disappeared. -->


#### Sampling and the likelihood principle


<!-- AJ: I haven't done much in this subsection, but I find this subsection quite hard to draw out the main point of. Does it need to be this complex? Could we not focus on just one of the approaches and then briefly signal that we can also construct it in different way, without walking through the other way? Running through both just seems like we're heading off on a bit of a tangent from the perspective of this chapter's objectives. 

I also don't know what it means for something to be "the same up to a constant." -->


In constructing a likelihood function, we need to take the sampling strategy into account. Consider, for instance, the following *conditional* data strategy: we collect data on $X$ and $Y$ in 2 cases, and we then measure $M$ in any case in which we observe $X=1, Y=1$.

The probability of each data type is as given in table below:


|type:     |prob:                        |
|----------|-----------------------------|
|$X1M0Y1$  |$\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{11}+\lambda^Y_{10})$|
|$X1M1Y1$  |$\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01})$|
|$X0Y0$    |$\lambda^X_0(\lambda^M_{00}+\lambda^M_{01})(\lambda^Y_{00}+\lambda^Y_{01}) + \lambda^X_0(\lambda^M_{10}+\lambda^M_{11})(\lambda^Y_{00}+\lambda^Y_{10})$|
|$X0Y1$    |$\lambda^X_0(\lambda^M_{00}+\lambda^M_{01})(\lambda^Y_{10}+\lambda^Y_{11}) + \lambda^X_0(\lambda^M_{10}+\lambda^M_{11})(\lambda^Y_{01}+\lambda^Y_{11})$|
|$X1Y0$    |$\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{00}+\lambda^Y_{01}) + \lambda^X_1(\lambda^M_{01}+\lambda^M_{11})(\lambda^Y_{00}+\lambda^Y_{10})$|

The two observations can be thought of as a multinomial draw from these five event types.

Alternatively they can also be thought of as the product of a draw from a strategy in which a set of units is drawn with observations on $X,Y$ only and another set is drawn with observations on $X, M, Y$.

In the single multinomial view we have the probability of seeing data with $X=Y=0$ in one case and $X=1, M=0, Y=1$ in another is:

* $2P(X=0, Y=0)P(X=1, M=0, Y=1)$

In the conditional strategy view we have

* $2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)$

In the two strategy view we have

* $P(X=0, Y=0)P(X=1, M=0, Y=1)$

which is the same up to a constant.

Say rather than conditioning $X=Y=1$ to examine $M$ one of the two cases were chosen at random to observe $M$ and it just so happened to be be a case with $X=Y=1$:

| type:    | prob:                                                                         |
|----------|-------------------------------------------------------------------------------|
|$X0Y0$    |$0.5\lambda^X_0(\lambda^M_{00}+\lambda^M_{01})(\lambda^Y_{00}+\lambda^Y_{01}) + 0.5\lambda^X_0(\lambda^M_{10}+\lambda^M_{11})(\lambda^Y_{00}+\lambda^Y_{10})$|
|$X0Y1$    |$0.5\lambda^X_0(\lambda^M_{00}+\lambda^M_{01})(\lambda^Y_{10}+\lambda^Y_{11}) + 0.5\lambda^X_0(\lambda^M_{10}+\lambda^M_{11})(\lambda^Y_{01}+\lambda^Y_{11})$|
|$X1Y0$    |$0.5\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{00}+\lambda^Y_{01}) + 0.5\lambda^X_1(\lambda^M_{01}+\lambda^M_{11})(\lambda^Y_{00}+\lambda^Y_{10})$|
|$X1Y1$    |$0.5\lambda^X_1(\lambda^M_{00}+\lambda^M_{10})(\lambda^Y_{10}+\lambda^Y_{11}) + 0.5\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01})$ + |
|$X0M0Y0$  |$0.5\lambda^X_0(\lambda^M_{00}+\lambda^M_{01}))(\lambda^Y_{00}+\lambda^Y_{01})$|
|$X0M1Y0$  |$0.5\lambda^X_0(\lambda^M_{11}+\lambda^M_{10}))(\lambda^Y_{00}+\lambda^Y_{10})$|
|...       |                                                                               |
|$X1M1Y1$  |$0.5\lambda^X_1(\lambda^M_{11}+\lambda^M_{01})(\lambda^Y_{11}+\lambda^Y_{01})$|


In the single multinomial view we have the probability of seeing data with $X=Y=0$ in one case and $X=1, M=0, Y=1$ in another is now:

* $2P(X=0, Y=0)P(X=1, M=0, Y=1)$

In the conditional strategy view we have

* $2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)$

In the two strategy view we have

* $P(X=0, Y=0)P(X=1, M=0, Y=1)$

which is the same up to a constant.



<!-- ## Illustration  -->

<!-- Consider a generalization of the models introduced in Chapter 6 in which a treatment $X$ is a cause of both $K$ and $Y$, and outcome $Y$ is a product of both $X$ and $K$. Though $K$ is both a mediator and a moderator for the effect of $X$. There are now 16 nodal types for $Y$, 4 for $K$ and 2 for $X$, yielding 32 causal types. -->

<!-- To allow for the possibility of non-random selection of $X$ we will assume that the assignment probability for $X$ depends on $U^Y$. This is a feature shared also in the baseline model when we specify $\pi$ as a function of types $a$,$b$,$c$,$d$. -->

<!-- Our piors requires specifying: -->

<!-- 1. A distribution over the 15-dimensional simplex representing possible values of $\lambda^Y$--which in turn determine types $u^Y$. -->
<!-- 2. A distribution over the 3-dimensional vector representing possible values of $\lambda^K$,  which in turn determine types $u^K$. -->


<!-- The model is restricted in various ways. We assume now confounding in the assignemnt of $X$. Less obviously we implicitly assume that $K$ is independent of $\theta^Y$ conditional on $X$. -->

<!-- With these elements in hand, however, all we need now is to provide a mapping from these fundamental parameters to the parameters used in the baseline model to form the likelihood.  -->


<!-- The key transformation is the identification of causal types resulting from the 64 combinations of $\lambda^Y$ and $\lambda^K$. These are shown below. -->

<!-- TABLE TO SHOW CAUSAL TYPES -->

<!-- Consider the following matrices of values for $u_Y$ and $u_K$, where $\lambda_{pq}^{rs}$ is the probability that $u^Y = t_{pq}^{rs}$, meaning that $Y$ would take the value $p$ when $X=0, K=0$,  $q$ when $X=0, K=1$,  $r$ when $X=1, K=0$,  and $s$ when $X=1, K=1$. Similarly $\lambda_{w}^{z}$ is the probability that $u^K$ takes value  $t_{w}^{z}$  meaning that $K$ takes the value $w$ when $X=0$ and $z$ when $X=1$. -->


<!-- TABLE TO SHOW CONDITIONAL PROBABILITIES OF K GIVEN X=1 AND TYPE -->

<!-- These types are the *transformed parameters*; the probability of a type is just the sum of the probabilities of the fundamental types that compose it, formed by taking the product of the $\lambda^Y$ and $\lambda^K$ values marked in the rows and columns of  table \ref{tab:types}.  -->

<!-- Similarly $\phi_{tx}$ can be constructed as the probability of observing $K$ conditional on this type (again, sums of products of probabilities associated with cells in table  \ref{tab:types}). For instance, using the row and column indices in exponents (GIVE FULL LABELS) from table \ref{tab:types}: -->

<!-- $$\phi_{b1}=\frac{\lambda_K^2(\lambda_Y^2+\lambda_Y^4+\lambda_Y^6+\lambda_Y^8)+\lambda_K^4(\lambda_Y^2+\lambda_Y^4+\lambda_Y^{10}+\lambda_Y^{12})}{ -->
<!-- \lambda_K^1(\lambda_Y^3+\lambda_Y^4+\lambda_Y^7+\lambda_Y^8)+\lambda_K^2(\lambda_Y^2+\lambda_Y^4+\lambda_Y^6+\lambda_Y^8)+\lambda_K^3(\lambda_Y^3+\lambda_Y^4+\lambda_Y^11+\lambda_Y^{12})+\lambda_K^4(\lambda_Y^2+\lambda_Y^4+\lambda_Y^{10}+\lambda_Y^{12})}$$ -->



<!-- With these transformed parameters in hand, the likelihood is exactly the same as that specified in the baseline model. -->

#### Case inference following population updating

We are often in situations in which we observe patterns in $n$ units and then seek to make an inference about one or more of the  $n$ cases  conditional on *both* the case level data and the broader patterns in the full data. 

Divide cases into set $S^0, S^1$ where $S^0$ is the set for which we wish to make case level inferences and $S^1$ is the collection of other cases for which we have data. 

In such cases should one use the data from $S^0$  when updating on population estimands or rather update using $S^1$ only and use information on $S^1$ for the case level inferences only? 

The surprising answer is that it is possible to do both, though exactly how queries are calculated depends on the method used. 

Let $\Lambda$ denote a collection of possible population parameters with typical element $\lambda^i$. Let $p$ denote a distribution over $\Lambda$ (after updating on data from set $S^1$), with typical element $\lambda^i$. Let $X$ denote possible data for cases in $S^0$ with realization $x$. 

Let $d^i$ denote the probability of observing data $X = x$ for a case (or set of cases) given $\lambda^i$.

Let $\tau^{|x}$ denote a query of interest---where the query is conditional in the sense that it relates to cases with data $x$. An example might be: what is the effect of $X$ on $Y$ in a case in which $M=1$ and $Y=1$. Let $q^i_j$ denote the probability that $\tau^{|x} = \tau_j^{|x}$ when $\lambda = \lambda^i$ for a case with data $X=x$.  Note $q^i_j$  can be written $z^i/d^i$ where $z^i_j = \Pr(\tau^{|x} = \tau^{|x}_j, X=x | \lambda^i)$. 

To illustrate say in an $X\rightarrow Y$ model we were interested the effect of $X$ on $Y$ in a case with $X=1, Y=1$. Then $d^i = (\lambda^i)^X_1((\lambda^i)^Y_{01} + (\lambda^i)^Y_{11})$ is the probability of observeing ($(X=1, Y=1)$. Then for query $\tau^{|x}_j = 1$ (did $X$ cause $Y$) we have  $z^i_j =  (\lambda^i)^X_1((\lambda^i)^Y_{01})$, and so the probability of this query for this case given $\lambda^i$ is: $q^i_j = \frac{(\lambda^i)^Y_{01}}{(\lambda^i)^Y_{01} + (\lambda^i)^Y_{11}}$

The posterior on $\tau^{S^0}$ for the cases in $S^0$ that provide data $x$, is then:


$$\Pr(\tau^{|x} = \tau_j^{|x}) = \frac{p^iz^i_j}{\sum_k p^kd^k}$$

This can be calculated from the prior $p$ (that is the distribution on $\Theta$ after updating on cases in $S^1$ only).

Notice however that (a) the *posterior* distribution on $\lambda^i$ given observation of $x$ in the $S^0$ set is $\frac{p^id^i}{\sum_k p^kd^k}$ and (b) $p^iz^i_j = p^id^iq^i$. It follows that this quantity can also be interpreted as the posterior mean of $q^i$, after observing both $S^0$ and $S^1$. 

We therefore have two approaches to calculatin these sample quantities: either take the posterior mean (posterior to $S^0$ and $S^1$), over the distrubution of $\lambda$  of the conditional probability of the estimand given the case data in $S^0$, or take the expected probability of $\tau$ given the prior (after observing $S^1$ only) and condition  on the probability of the case level data in $S^0$).


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
**Box: Case inference and population data**

When calculating queries in `CausalQueries` you can specify whether you are interested in "case level" inquiries or population inquiries. A case level inquiry  of the form $\tau^{|X}$ is calculated (`case_level = TRUE`) via $\frac{\int_{\lambda}p(\tau, x|\lambda)p(\lambda)d\lambda}{\int_{\lambda}p(x|\lambda)p(\lambda)d\lambda}$. In contrast, the mean of population query (`case_level = FALSE`) is $\int_{\lambda} \frac{p(\tau, x|\lambda)}{p(x|\lambda)}p(\lambda)d\lambda$.

<!-- * DISCUSS SHARE INTERPRETATION -->
<!-- * HIGHLIGHT FOURTH ROW HAS DOUBLE COUNTING   -->
The example below shows a case where these differ and illustrates two ways in which inferences on sample queries can be made.

```{r, eval = FALSE}
 make_model("X -> Y <- W") %>%
    set_restrictions(labels  = list(Y = c("0101", "0111")), keep = TRUE) %>%
    set_priors(alpha = 100000) %>%
    set_priors(node = "W", alpha = c(.5, .5)) %>%
  
  update_model(data.frame(X=1,Y=1)) %>%
  
  query_model("Y[X=1] > Y[X=0]", 
             given = "X==1 & Y==1", 
             case_level = c(FALSE, TRUE),
             using = c("priors", "posteriors"),
             expand_grid = TRUE)
```

```{r, echo = FALSE}
if(do_diagnosis)
 make_model("X -> Y <- W") %>%
    set_restrictions(labels  = list(Y = c("0101", "0111")), keep = TRUE) %>%
    set_priors(alpha = 100000) %>%
    set_priors(node = "W", alpha = c(.5, .5)) %>%
  update_model(data.frame(X=1,Y=1), refresh = 0, chains = 8, iter = 20000) %>%
  query_model("Y[X=1] > Y[X=0]", 
             given = "X==1 & Y==1", 
             case_level = c(FALSE, TRUE),
             using = c("priors", "posteriors"),
             expand_grid = TRUE) %>% arrange(Query) %>% 
  write_rds("saved/casepop.rds")

  read_rds("saved/casepop.rds") %>%
  kable(digits = 3, caption = "The same answers are found using the prior distribution and setting `case_estimand` to `TRUE`, and using the posterior distribution but setting `case.estimand` to `FALSE`. Other answers are incorrect")
```

::::
`

## Mixed methods

We do not need data on all nodes in order to implement the procedure. If we have data  on only some of the nodes in a model, we follow the same basic logic as with partial process-tracing data. In calculating the probability of a pattern of partial data, we use all columns (data types) in the ambiguity matrix that are consistent with the partial data. 

So, for instance, if we have an $X \rightarrow Y$ model but observe only $Y=1$, then we would retain both the $X=0, Y=1$ column and the $X=1, Y=1$ column. We then calculate the probability of this data type by summing causal-type probabilities for all causal types that can produce *either* $X=0, Y=1$ *or* $X=1, Y=1$.

What if our data have been collected via a mixture of search strategies? Suppose, for instance, that we have collected $X,Y$ data for a set of cases, and have additionally collected data on $M$ for a random subset of these. We can think of this mixed strategy as akin to conducting quantitative analysis on a large sample while conducting in-depth process tracing on part of the large-$N$ sample. We can then summarize our data in two vectors, an 8-element $n_{XYM}$ vector ($(n_{000},n_{001},\dots n_{111}$) for the cases with process-tracing ($M$) observations, and a 4-element vector $n_{XY*} = (n_{00*},n_{10*},n_{01*},n_{11*}$ for the partial data on those cases on which we did not conduct process tracing. Likewise, we now have two sets of data probabilities: an 8-element vector for the set of cases with complete data, $w_{XYM}$, and a 4-element vector for those with partial data, $w_{XY*}$. 

Let $n$ denote the total number of cases examined, and $k$ the number for which we have data on $M$. Assuming that each observed case represents an independent, random draw from the population, we can form the likelihood function as a *product* of multinomial distributions, one representing the complete-data (process-traced) cases and one representing those with only $X,Y$ data: 


$$\Pr(\mathcal{D}|\theta) = 
  \text{Multinom}\left(n_{XY*}|n-k, w_{XY*}\right) \times \text{Multinom}\left(n_{XYM}|k, w_{XYM}\right)$$

We can construct likelihood functions in a similar fashion for any arbitrary mixture of search strategies.

<!-- Illustration -->

## Considerations 

### Probative value can be derived from a causal structure plus data

In Chapter \@ref(pt), we discussed the fact that a DAG by itself is insufficient to generate learning about causal effects from data on a single case; we also need informative prior beliefs about population-level shares of nodal types. 

When working with multiple cases, however, we *can* learn about causal relations  when starting with nothing more than the DAG and data. In particular, we can simultaneously learn about case-level queries and justify our inferences from population-level data patterns. 

<!-- AJ: Reading this now, the wording "and justify our inferences from population-level data patterns" is confusing. Are the *inferences* coming from the pop-level data? Or is the *justifying* coming from the pop-level data? Also, I think this is kind of a backwards way of saying it? I'd think we'd want to say, we can simultaneously learn about population-level queries and justify our inferences about case-level queries. Agreed? -->

For instance, in an $X \rightarrow M \rightarrow Y$ model, even if we start with flat priors over $M$'s nodal types, observing a correlation (or no correlation) between $X$ and $M$ across multiple cases provides information about $X$'s effect on $M$. Simply, a stronger, positive (negative) $X, M$ correlation implies a stronger positive (negative) effect of $X$ on $M$. In turn, a stronger $X,M$ correlation implies a stronger effect of $X$ on $Y$ since, under this model, that effect has to run through an effect of $X$ on $M$.  

What's more, data from multiple cases can *provide* probative value for within-case inference. Suppose, for the $X \rightarrow M \rightarrow Y$ model, that we start with flat priors over all nodal types. As discussed in Chapter \@ref(pt), observing $M$ in a single case cannot be informative about $X$'s effect on $Y$ in that case. If we have no idea of the direction of the intermediate causal effects, then we have no idea which value of $M$ is more consistent with an $X \rightarrow M$ effect or with an $M \rightarrow Y$ effect. But suppose that we first observe data on $X$ and $M$ for a group of cases and find a strong positive correlation between the two variables. We now update to a belief that any effect of $X$ on $M$ is more likely to be positive than negative. Now, let's say we look at one of our other cases in which $X=1$ and $Y=1$ and want to know if $X=1$ caused $Y=1$. Knowing now that any such effect would most likely have operated via a positive $X \rightarrow M$ effect means that observing $M$ will be informative: seeing $M=1$ in this case will be more consistent with an $X \rightarrow Y$ effect than will $M=0$. The same logic, of course, also holds for observing cross-case correlations between $M$ and $Y$.

Our ability to draw probative value from cross-case data will depend on the causal model we start with. For instance, if our model allows $X$ also to have a direct effect on $Y$, our ability to learn from $M$ will be more limited. We explore this issue in much greater detail in Chapter \@ref(#caseselection).


### Learning without identification

Some causal queries are *identified* while others are not. When a query is identified, each true value for the query is associated with a unique data distribution given infinite data. Thus, as we gather more and more data, our posterior on the query should converge on the true value. When a query is not identified, multiple true values of the query will be associated with the same data distribution given infinite data. With a non-identified query, our posterior will never converge on a unique value regardless of how much data we collect since multiple answers will be equally consistent with the data. A key advantage of causal model framework, however, is that we can *learn* about queries that are not identified.

We can illustrate the difference between identified and non-identified causal questions by comparing an $ATE$ query to a probability of causation ($PC$) query for a simple $X \rightarrow Y$ model. When asking about the $ATE$, we are asking about the average effect of $X$ on $Y$, or the difference between $\lambda^Y_{01}$ (the share of units with positive effects) and $\lambda^Y_{10}$ (share with negative effects). When asking about the $PC$, we are asking, for a case with given values of $X$ and $Y$, about the probability that $X$ caused $Y$ in that case. And a $PC$ query is defined by a different set of parameters. For, say, an $X=1, Y=1$ case and a $X \rightarrow Y$ model, the probability of causation is given by just $\lambda^Y_{01}$. 

Let us assume a "true" set of parameters, unknown to the researcher, such that $\lambda^Y_{01} = 0.6$, $\lambda^Y_{10} = 0.1$ while we set $\lambda^Y_{00} = 0.2$ and $\lambda^Y_{11} = 0.1$. Thus, the true average causal effect is $0.5$. We now use the parameters and the model to simulate a large amount of data ($N=10,000$). We then return to the model, set flat priors over nodal types, and update the model using the simulated data. We graph the posterior on our two queries, the $ATE$ and the probability of positive causation in an $X=1, Y=1$ case, in Figure \@ref(fig:PChist).


```{r, echo = FALSE} 
if(do_diagnosis){
ATE_dist <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "Y[X=1] - Y[X=0]"
                   )
PC_dist <- query_distribution(
                   model = updated, 
                   using = "posteriors",
                   query = "Y[X=1] > Y[X=0]",
                   given = "X==1 & Y==1"
                   )
write_rds(ATE_dist, "saved/appendix_ATE_dist.rds")
write_rds(PC_dist, "saved/appendix_PC_dist.rds")
}
ATE_dist <- read_rds("saved/appendix_ATE_dist.rds")
PC_dist  <- read_rds("saved/appendix_PC_dist.rds")
```

```{r PChist, echo = FALSE, fig.cap = "ATE is identified, PC is not identified but has informative bounds"}
par(mfrow = c(1,2))
hist(ATE_dist, xlim = c(-1,1), main = "Posterior on ATE", xlab = "ATE")  
hist(PC_dist, xlim = c(0,1), main = "Posterior on PC", xlab = "Probability X=1 caused Y=1")  
```

The figure illustrates nicely the difference between an identified and non-identified query. While the $ATE$ converges on the right answer, the probability of causation fails to converge even with a massive amount of data. We see instead a range of values for this query on which our updated model places roughly equal posterior probability.

Importantly, however, we see that we *do* learn about the probability of causation. Despite the lack of convergence, our posterior rules out a wide range of values. While our prior on the query was 0.5, we have correctly updated toward a range of values that includes (and happens to be fairly well centered over) the true value ($\approx 0.86$). 

A distinctive feature of updating a causal model is that it allows us to learn about non-identified quantities in this manner. We will end up with "ridges" in our posterior distributions: ranges or combinations of parameter values that are equally likely given the data. But our posterior weight can nonetheless shift toward the right answer.

At the same time, for non-identified queries, we have to be cautious about the impact of our priors. As $N$ becomes large, the remaining curvature we see in our posteriors may simply be function of those priors. One way to inspect for this is to simulate a very large dataset and see whether the curvature remainsXXXXXXXXXX A second approach would be to do sensitivity analyses by updating the model on the same data with different sets of priors to see how this affects the shape of the posterior.

<!-- Identified quantity is that each true theta, with infinite data, has a unique data distribution associated with it. If not identified, multiple different thetas will have different infinite-data distributions associated with them. E.g., COE. If it's the same, you won't know which it is. This is why we'll get ridges in the posteriors. So as you scale up data, you might rule out other things, but those theta's will always scale up together. So you'll never discriminate between those, but you'll learn about those things relative other things. -->

<!-- Use Fig. 6.1 from Causal Models guide. -->

<!-- So that's still learning. We can learn about unidentified quantities.   -->

<!-- But the bad news is that, for a not-identified quantity, your priors will matter: the curvature that gives us a maximum in our posterior will be driven by the posteriors. To distinguish between what part of the posterior is from the data and what's from the data would be to simulate infinite data and see what happens to the  -->

<!-- Can also do sensitivity analyses by changing priors. -->



```{r modelm1m2}
model <- make_model("X1 -> M1 -> Y <- M2 <- X2")

# restrict such that *only* M1 OR M2 could cause Y -- can we create a DD test? / achieve identification

```


### Beyond binary data

While the setup used in this book involves only binary nodes, the approach readily generalizes to non-binary data. Moving beyond binary nodes allows for considerably greater flexibility in response functions. For instance, moving from binary to merely 3-level ordinal $X$ and $Y$ variables allows us to represent non-linear and even non-monotonic relationships. It also allows us pose more complex queries, such as, "What is the probability that  $Y$ is linear in $X$?", "What is the probability that  $Y$ is concave in $X$?", or "What is the probability that $Y$ is monotonic in $X$?"

To move to non-binary measurement, we need to be able to expand the nodal-type space to accommodate the richer range of possible relations between nodes that can take on more than two possible values. Suppose, for instance, that we want to operate with variables with 4 ordinal categories. In an $X \rightarrow Y$ model, $Y$'s nodal types have to accommodate 4 possible values that $X$ can take on, and 4 possible values that $Y$ can take on for any value of $X$. This yields $4^4 = 256$  nodal types for $Y$ and 1024 causal types (compared to just 8 in a binary setup). 

The `CausalQueries` package, set up to work most naturally with binary nodes, can be used to represent non-binary data as well. The trick, as it were, is to express integers in base-2 and then represent the integer as a series of 0's and 1's on multiple nodes. In base-2 counting we would represent four integer values for $X$ (say, 0, 1, 2,3) using $00, 01, 10, 11$. If we use one binary node, $X_1$ to represent the first digit, and a second node $X_2$ to represent the second, we have enough information to capture the four values of $X$. The mapping then is: $X_1 = 0, X_2 = 0$ represents $X=0$; $X_1 = 0, X_2 = 1$ represents $X=1$;   $X_1 = 1, X_2 = 0$ represents $X=2$; and $X_1 = 1, X_2 = 1$ represents $X=3$. We construct $Y$ in the same way. We can then represent a simple $X \rightarrow Y$ relation as a model with two $X$ nodes each pointing into two $Y$ nodes: $Y_1 \leftarrow X_1 \rightarrow Y_2, Y_1 \leftarrow X_2 \rightarrow Y_2$. To allow for the full range of nodal types we need to allow a joint distribution over $\theta^{X_1}$ and $\theta^{X_2}$ and over $\theta^{Y_1}$ and $\theta^{Y_2}$, which results in 3 degrees of freedom for $X$ and 255 for $Y$, as required.

In the illustration below with two 4-level variables, we generate data ($N=100$) from a non-monotonic process with the following potential outcomes: $Y(0)=0, Y(1)=1, Y(2)=3, Y(3) = 2$. We then update and report on posteriors on potential outcomes.

We make use of a couple of helper functions to simplify working with conversions from statements on integers to statements on the binary representation of the integers.

```{r}
# A function to generate an integer from values on 2 binary nodes
to_int <- function(X1, X2) strtoi(paste0(X1, X2), base = 2)

# A function to express a query on integer nodes into a query on
# binary nodes
Y_x <- function(x){
  X <- R.utils::intToBin(x) 
  X <- case_when(X ==  "0" ~ c(0,0), X == "1"  ~ c(0,1),
                 X == "10" ~ c(1,0), X == "11" ~ c(1,1))
  paste0("to_int(Y1[X1=", X[1], 
         ", X2=", X[2], "], Y2[X1=", X[1], ", X2=", X[2], "])")}
```


Data from this model looks like this:

```{r}
data <- 
  
  make_model("X1 -> Y1 <- X2; X1 -> Y2 <- X2") %>%
  set_parameters(node = "Y1", label = "0101", parameters = 1) %>%
  set_parameters(node = "Y2", label = "0110", parameters = 1) %>%
  
  make_data(100, using = "parameters") %>%
  mutate(X = to_int(X1, X2), Y = to_int(Y1, Y2))
```

```{r, echo = FALSE}
data %>% slice(1,  50, 70, 100) %>% kable(caption = "Data from non binary model (selection of rows)")
```

Updating and querying is done in the usual way:


```{r, eval = FALSE}
make_model("X1 -> Y1 <- X2;  X1 -> Y2 <- X2; X1 <-> X2; Y1 <-> Y2") %>%
  update_model(data) %>%
  query_model(list(Y_x(0), Y_x(1), Y_x(2), Y_x(3)), using = "posteriors")
```


```{r, echo = FALSE}
if(do_diagnosis) 
model_flexible <- make_model("X1 -> Y1 <- X2;  X1 -> Y2 <- X2; X1 <-> X2; Y1 <-> Y2") %>%
  write_rds(model_flexible, "saved/nonbinary_flexible.rds")

full_model <- read_rds("saved/nonbinary_flexible.rds")

full_model <- full_model %>%
  set_parameters(node = "Y1", label = "0101", parameters = 1) %>%
  set_parameters(node = "Y2", label = "0110", parameters = 1)

data <- make_data(full_model, 100, using = "parameters") %>%
  mutate(X = to_int(X1, X2), Y = to_int(Y1, Y2))

if(do_diagnosis) 
  update_model(model, data, keep_transformed = TRUE, chains = 4, iter = 4000) %>%
  write_rds("saved/ordinal.rds")

if(do_diagnosis) 
query <- 
  
  read_rds("saved/ordinal.rds") %>%
  
  query_model(list(Y_x(0), Y_x(1), Y_x(2), Y_x(3)),
             using = "posteriors") %>% 
  mutate(Q = c("Y(0)", "Y(1)", "Y(2)", "Y(3)"),
         'True value' = c(0,1,3,2)) %>%
  write_rds("saved/ordinal_query.rds")

read_rds("saved/ordinal_query.rds")  %>%
  select(Q, Using, 'True value', mean, sd) %>% 
  kable(digits = 2, caption = "Posteriors on potential outcomes for non binary model")

```

We see that the model performs well. As in the binary setup, the posterior reflects both the data and the priors. And, as usual, we have access to a full posterior distribution over all nodal types and can thus ask arbitrary queries of the updated model. 

The greatest challenge posed by the move to non-binary data is computational. If  $Y$ takes on $m$ possible values and has $k$ parents, each taking on $r$ possible values, we then have $m^{r^k}$ nodal types for $Y$. Thus, the cost of more granular measurement is complexity -- an explosion of the parameter space -- as the nodal type space expands rapidly with the granularity of measurement and the number of explanatory variables With three 3-level ordinal variables pointing into the same outcome, for instance, we have $3^{27} = 7.6$ *trillion* nodal types!

We expect that, as measurement becomes more granular, researchers will want to manage the complexity by placing structure onto the possible patterns of causal effects. Structure, imposed through model restrictions, can quite rapidly tame the complexity. For some substantive problems, one form of structure we might be willing to impose is monotonicity. In a $X \rightarrow Y$ model with 3-level variables, excluding non-monotonic effects brings down the number of nodal types from 27 to 17.  Alternatively, we may have a strong reason to rule out effects in one direction: disallowing negative effects, for instance, brings us down to 10 nodal types. If we are willing to assume linearity, the number of nodal types falls further to 5.

<!-- REMOVE REFERENCE TO OLS; SAY WE DEAL WITH THIS ELSEWHERE IN THE CONCLUSION. -->

<!-- Of course, standard approaches to empirical modeling typically impose a great deal of structure -- consider, for instance, the ubiquity of the linearity assumption in regression modeling -- for precisely the same reason: to simplify the parameter space. As with other forms of empirical modeling, researchers working with causal models will need to decide how they want to trade off model fit and complexity of the parameter space in the choices they make about nodal measurement and model restrictions.  -->



<!-- We can still think in terms of types -->

<!-- With 3 levels of X and 3 levels of Y, you have 27 possible types. But if you impose linearity, only 5 types. If it's non-negative, there will be a few more. -->

<!-- Warning: gets very complicated -->

<!-- Possible strategy is to impose structure, which other approaches also do.  -->

<!-- Asymptotically approaches linearity -->




### Measurement error

One potential application of the approach we have described in this chapter to integrating differing forms of data is to addressing the problem of measurement error. The conceptual move to address measurement error in a causal model setup is quite simple: we incorporate the error-generating process into our model.

Consider, for instance, a model in which we build in a process generating measurement error on the dependent variable.

$$X \rightarrow Y  \rightarrow Y_\text{measured} \leftarrow \text{source of measurement error}$$ 

Here $X$ has an effect on the true value of our outcome of interest, $Y$. The true value of $Y$, in turn, has an effect on the value of $Y$ that we measure, but so too does a potential problem with our coding process. Thus, the measured value of $Y$ is a function of both the true value and error.

To motivate the setup, imagine that we are interested in the effect of a rule restricting long-term care staff to working at a single site ($X$) on outbreaks of the novel coronavirus in long-term care facilities ($Y$), defined as infections among two or more staff or residents. We do not directly observe infections, however; rather, we observe positive results of PCR tests. We also know that testing is neither comprehensive nor uniform. For some units, regular random testing is carried out on staff and residents while in others only symptomatic individuals are tested. It is the latter arrangement that potentially introduces measurement error.  

If we approach the problem naively, ignoring measurement error and treating $Y_\text{measured}$ as though it were identical to $Y$, a differences in means approach might produce attenuation bias---insofar as we are averaging between the true relationship and 0.  

We can do better with a causal model, however. Without any additional data, we can update on both $\lambda_Y$ and $\lambda^{Y_\text{measured}}$, and our posterior uncertainty would reflect uncertainty in measurement. We could go further if, for instance, we could reasonably exclude negative effects of $Y$ on  $Y_\text{measured}$. Then, if we observe (say) a negative correlation between $X$ and $Y_\text{measured}$, we can update on the substantive effect of interest -- $\lambda^Y$ -- in the direction of a larger share of negative effects: it is only *via* negative effects of $X$ on $Y$ that a negative correlation between $X$ and $Y_\text{measured}$ could emerge. At the same time, we learn about the measure itself as we update on $\lambda^{Y_\text{measured}}$: the negative observed correlation $X$ and $Y_\text{measured}$ is an indicator of the degree to which $Y_\text{measured}$ is picking up true $Y$.

We can do better still if we can collect more detailed information on at least some units. One data strategy would be to invest in observing $Y$, the true outbreak status of each unit, for a subset of units for which we already have data on $X$ and $Y_\text{measured}$ --- perhaps by implementing a random-testing protocol at a subset of facilities. Getting better measures of  $Y$ for some cases will allow us to update more directly on $\lambda^Y$, the true effect of $X$ on $Y$, for those cases. But just as importantly, observing true $Y$ will allow us to update on measurement *quality*, $\lambda^{Y_\text{measured}}$, and thus help us make better use of the data we have for those cases where we only observe $Y_\text{measured}$. This strategy, of course, parallels a commonly prescribed use of mixed methods, in which qualitative research takes place in a small set of units to generate more credible measures for large-$n$ analysis (see, e.g., @seawrightbook). 

<!-- A second data strategy (which could be combined with the first) would be to collect data, for some subset of units, on the source of the measurement error. If we collected data on the use of random vs. symptomatic-only testing, we could then update on two further parameters: $\lambda^\text{source of measurement error}$ and on the part of $\lambda^{Y_\text{measured}}$ representing response of $Y_\text{measured}$. In other words, we would learn both about how prevalent the conditions generating measurement error are and about much they throw off our measure.  -->

In the illustration below, we posit a true average effect of $X$ on $Y$ of 0.6. We also posit an average "effect" of $Y$ on measured $Y$ of just 0.7, allowing for measurement error. 

In this setup, with a large amount of data, we would arrive at a differences-in-means estimate of the effect of $X$ on *measured* $Y$ of about 0.42. Importantly, this would be the effect of $X$ on $Y_{\text{measured}}$ --- not the effect of $X$ on $Y$ --- but if we were not thinking about the possibility of measurement error, we would likely conflate the two, arriving at an estimate far from the true value. 

We can improve on this "naive" estimate in a number of ways using a causal model, as shown in Table \@ref(tab:measurmenterror). First, we can do much better simply by undertaking the estimation within a causal model framework, even if we simply make use of the exact same data. We write down the following simple model $X \rightarrow Y \rightarrow Y_\text{measured}$, and we build in a monotonicity restriction that disallows negative effects of $Y$ on $Y_{\text{measured}}$. As we can see from the first row in Table \@ref(tab:measurmenterror), our mean estimate of the $ATE$ moves much closer to the true value of 0.6. 

Second, we can add data by gathering measures of "true" $Y$ for 20\% of our sample. As we can see from the second row in the table, this investment in additional data does not change our posterior mean much but yields a dramatic increase in precision. In fact, as we can see by comparison to the third row, partial data on "true" $Y$ yields an estimate that is almost the same and almost as precise as the one we would arrive it with data on "true" $Y$ for *all* cases.

```{r, echo = FALSE}
model <- make_model("X -> Y -> Yobs") %>%
  set_restrictions(decreasing("Y", "Yobs")) %>%
  set_parameters(parameters = c(.5, .5, .1, .1, .7, .1, .1, .8, .1))

```

```{r measurmenterror, echo = FALSE}

model2 <- make_model("X -> Y -> Yobs1; Y -> Yobs2") %>%
  set_restrictions(decreasing("Y", "Yobs1")) %>%
  set_restrictions(decreasing("Y", "Yobs2")) %>%
  set_parameters(parameters = c(.5, .5, .1, .1, .7, .1, .1, .8, .1, .1, .8, .1))

set.seed(1)
data <- make_data(model2, n = 2000) %>%
  mutate(Yobs = Yobs1)

# estimatr::difference_in_means(Yobs ~ X, data = data)

if(do_diagnosis)
list(
  no_Y =  update_model(model, data = data %>% select(X, Yobs)) %>%
    query_model(te("X", "Y"), using = "posteriors"),
  
  some_Y =  update_model(model, data %>%
                           mutate(Y = ifelse(randomizr::complete_ra(2000, prob = .2)==1, Y, NA))) %>%
    query_model(te("X", "Y"), using = "posteriors"),
  
  all_Y =  update_model(model, data) %>%
    query_model(te("X", "Y"), using = "posteriors")) %>%
  bind_rows(.id = "Data") %>%
  write_rds("saved/measurement_error.rds")

Qs <-   read_rds("saved/measurement_error.rds")

Qs %>% mutate(Data = c("Data on Y measured only", "Data on true Y for 20% of units", "Data on true Y")) %>% select(Data, Using, mean, sd) %>% 
  kable(caption = "Inferences on effects on true Y given measurement error (true ATE = .6)", digits = 2)

```

<!-- * What if we are naive: attenuation bias with binary -->
<!-- * What if we are aware of risks but don't know how bad bad coding is -->
<!-- * What if we observed bad coding in a subsample -->

<!-- We have assumed no measurement error; in applications there could be considerable interest in measurement error. On one hand clue information may contain information about possible mismeasurement on $X$ and $Y$; on the other hand there might interest in whether measured clues adequately capture those features of a causal process that is thought to be measureable.   -->

<!-- The probability of different types of measurement error can be included among the set of parameters of interest, with likelihood functions adjusted accordingly. Suppose, for instance, that with probability $\epsilon$ a $Y=0$ case is recorded as a $Y=1$ case (and vice versa). Then the event probability of observing an $X=1$,$Y=1$ case, for example, is $\epsilon \lambda_a \pi_a + (1-\epsilon) \lambda_b \pi_b + \epsilon \lambda_c \pi_c + (1-\epsilon) \lambda_d \pi_d$. %If instead there were measurement error on $X$ but not on $Y$, then the event probability would be: $\epsilon \lambda_a (1-\pi_a) + (1-\epsilon) \lambda_b \pi_b + \epsilon \lambda_d (1-\pi_d) + (1-\epsilon) \lambda_d \pi_d$.  -->
<!-- Similar expressions can be derived for measurement error on $X$ or $K$. Specifying the problem in this way allows us both to take account of measurement error and learn about it. -->

An alternative strategy might involve gathering multiple measures of $Y$, each with their own independent source of error. Consider the model, $X \rightarrow Y \rightarrow Y_\text{measured[1]}; Y \rightarrow Y_\text{measured[2]}$. Assume again a true $ATE$ of $X$ on $Y$ of 0.6, that $Y$ has an average effect of 0.7 on both $Y_\text{measured[1]$ and $Y_\text{measured[2]$, and no negative effects of true $Y$ on the measures.^[Importantly, this model assumes nodal types for $Y_\text{measured[1]$ and $Y_\text{measured[2]$ are independent of one another (no unobserved confounding), implying independent sources of measurement error in this setup.] In this setup, updating on the true $Y$ can be thought of as a Bayesian version of "triangulation", or factor analysis. The results in Table \@ref(tab:measurmenterror2) are based the same data as in the previous example but now augmented with the second noisy measure for $Y$. 

```{r, cache = TRUE, echo = FALSE}
model <- make_model("X -> Y -> Yobs1; Y -> Yobs2") %>%
  set_restrictions(decreasing("Y", "Yobs1")) %>%
  set_restrictions(decreasing("Y", "Yobs2")) %>%
  set_parameters(parameters = c(.5, .5, .1, .1, .7, .1, .1, .8, .1, .1, .8, .1))
```

```{r measurementerror2, cache = TRUE,echo = FALSE}
set.seed(1)
data <- make_data(model, n = 2000) 

if(do_diagnosis)
update_model(model, 
             data = 
               data %>% select(X, Yobs1, Yobs2)) %>%
    query_model(te("X", "Y"), using = "posteriors") %>%
  write_rds("saved/measurement_error_2.rds")

read_rds("saved/measurement_error_2.rds") %>% mutate(Data = "Two noisy measures") %>% 
  select(Data, Using, mean, sd)  %>% 
  kable(caption = "Inferences on effects on true Y given two noisy measures  (true ATE = .6)", digits = 2)

```

As we can see, two noisy measures perform about as well as access to full data on the true $Y$ (as in Table \@ref(tab:measurmenterror)). 

The main point here is that measurement error matters for inference and can be taken directly into account within a causal model framework. Confusing measured variables for variables of interest will obviously lead to false conclusions. But if measurement concerns loom large, we can respond by making them part of our model and learning about them. We have illustrated this point for simple setups, but more complex structures could be just as well envisioned, such as those where error is related to $X$ or, more perniciously, to the effects of $X$ on $Y$.

### Spillovers

A common threat to causal inference is the possibility of spillovers: a given unit's outcome being affected by the treatment status of another (e.g., possibly neighboring) unit. We can readily set up a causal model to allow for estimation of various quantities related to spillovers. 

Consider, for instance, the causal model represented in Figure \@ref(fig:spillover). We consider here a cluster of 3 units across which spillovers might occur. We might imagine, for instance, a cluster of geographically proximate villages separated from other clusters such that spillovers might occur between villages within a cluster, but can be ruled out across clusters. Here $X_i$ and $Y_i$ represent village $i$'s treatment status and outcome, respectively. The pattern of directed edges indicates that each village's outcome might be affected both by its own and by its neighbors' treatment status.

We now simulate data that allow for spillovers. Specifically, while independently assigning $X_1$ and $X_2$ to treatment $50 \%$ of the time, we (a) set $Y_1$ equal to $X_1$, meaning that Unit 1 is affected only by its own treatment status and (b) set $Y_2$ equal to $X_1 \times X_2$, meaning that Unit 2 is equally affected by its own treatment status and that of its neighbor, such that $Y_2 = 1$ only if both Unit 2 and its neighbor are assigned to treatment. 

We simulate 100 observations from this data-generating process and then update a model (with flat priors over all nodal types).

Now we can extract a number of spillover-relevant causal quantities from the updated model. First we ask: what is the average effect of exposing a unit *directly* to treatment ("only_self_treated") when the neighboring unit is untreated? Under the data-generating process that we have posited, we know that this effect will be $1$ for Unit 1 (which always has a positive treatment effect) and $0$ for Unit 2 (which sees a positive effect of $X_2$ only when $X_1 = 1$), yielding an average across the two units of $0.5$. We see from Table XXXX that we update, given our 100 observations, from a prior of 0 to a posterior mean of 0.371, approaching the right answer.

A second question we can ask is about the spillover by itself: what is the average treatment effect for a unit of its neighbor being assigned to treatment when the unit itself is not assigned to treatment ("only_other_treated")? We know that the correct answer is $0$ since Unit 1 responds only to its own treatment status, and Unit 2 requires that both units be assigned to treatment to see an effect. Our posterior estimate of this effect is right on target, at 0.

We can then ask about the average effect of *any* one unit being treated, as compared to no units being treated ("one_treated"). This is a more complex quantity. To estimate it, we have to consider what happens to the outcome in Unit 1 when only $X_1$ shifts from control to treatment, with $X_2$ at control (true effect is $1$); what happens to Unit 1 when only $X_2$ shifts from control to treatment, with $X_1$ at control (true effect is $0$); and the same two effects for Unit 2 (both true effects are $0$). We then average across both the treatment conditions and units. We arrive at a posterior mean of $0.186$, not far from the true value of $0.25$.

Finally, we can ask about the average effect of both treatments going from control to treatment ("both_treated"). The true value of this effect is $1$ for both units, and the posterior has shifted quite far in the direction of this value.

Obviously, more complex setups are possible. We can also model the process in a way that allows for more learning (pooling) across units. In the present model, learning about effects for Unit 1 in a cluster tells us nothing about effects for Unit 2 in a cluster because they are set up to have completely independent nodal types. We could instead treat all units as drawn from the same population: we could represent this, for instance, in a graph with just one $Y$ and two treatment nodes pointing into it, one for the unit's own treatment status and one for its neighbor's treatment status.

<!-- AJ: Can we substitute some other word for "cluster" above to avoid confusion with the concept of clusters as we discuss them in the next subsection? -->

<!-- The dataset will now be structured such that the "unit" of analysis is the *cluster*. WAIT, I AM NOT SURE THIS IS RIGHT. CAN'T WE HAVE THE UNITS BE VILLAGES, WITH THE COLUMNS IN THE DATASET BEING X1, X2, X3, Y_i FOR ALL VILLAGES? -->

```{r spillover, cache = TRUE, echo = FALSE, fig.width = 8, fig.height = 4,  fig.align="center", out.width='.5\\textwidth', fig.cap = "\\label{fig:spillover} A causal model allowing for spillovers across 3 units, in which each unit's treatment status can affect other units' outcomes.", include = FALSE,  eval = FALSE}
par(mar=c(1,1,3,1))
hj_dag(x = c(0, 0, 0, 1, 1, 1),
       y = c(3, 2, 1, 3, 2, 1),
       names = c(expression(paste(X[1])), expression(paste(X[2])), expression(paste(X[3])), expression(paste(Y[1])), expression(paste(Y[2])), expression(paste(Y[3]))),
       arcs = cbind( c(1, 1, 1, 2, 2, 2, 3, 3, 3),
                     c(4, 5, 6, 4, 5, 6, 4, 5, 6)),
       title = "A causal model allowing for spillovers",
       padding = .1, contraction = .15) 

```


<!-- AJ: Fig above isn't displaying. -->


```{r, cache = TRUE,  echo = FALSE}
model <- make_model("X1 -> Y1 <- X2 -> Y2 <- X1")

spillover_queries <- list(
    only_self_treated = 
      "(Y1[X1=1, X2=0] - Y1[X1=0, X2=0] +
        Y2[X1=0, X2=1] - Y2[X1=0, X2=0])/2",
    
    only_other_treated = 
      "(Y1[X1=0, X2=1] - Y1[X1=0, X2=0] +
        Y2[X1=1, X2=0] - Y2[X1=0, X2=0])/2",

    one_treated = 
      "((Y1[X1=1, X2=0] + Y1[X1=0, X2=1])/2 - Y1[X1=0, X2=0] +
        (Y2[X1=1, X2=0] + Y2[X1=0, X2=1])/2 - Y2[X1=0, X2=0])/2",
    
    both_treated = 
      "((Y1[X1=1, X2=1] - Y1[X1=0, X2=0]) +
        (Y2[X1=1, X2=1] - Y2[X1=0, X2=0]))/2")
```

```{r, cache = TRUE, echo = FALSE}

plot(model)

data <- fabricatr::fabricate(N = 100, 
                  X1 = rbinom(N,1,.5), 
                  X2 = rbinom(N,1,.5), 
                  Y1 = X1,
                  Y2 = X1*X2)
if(do_diagnosis)
update_model(model, data, keep_transformed = TRUE) %>% 
  write_rds("saved/spillovers.rds")


if(do_diagnosis)
  read_rds("saved/spillovers.rds")  %>% 
  query_model(spillover_queries, using = "posteriors") %>% 
  write_rds("saved/spillovers_query.rds")

read_rds("saved/spillovers_query.rds") %>% select(-Given, -Case.estimand) %>% kable(caption = "Spillovers queries", digits = 2)

```

<!-- Spillovers may also be addressed through an appropriate definition of causal types. For example a unit $i$ that is affected either by receiving treatment or via the treatment of a neighbor, $j$, might have potential outcomes $Y_i(X_i,X_j)=\max(X_i,X_j)$ while another type that is not influenced by neighbor treatment status has  $Y_i(X_i,X_j)=\max(X_i)$. With such a set-up, relevant clue information might discriminate between units affected by spillovers and those unaffected.    -->

<!-- - With current structure, you can estimate various spillover relevant quantities -->
<!-- - Can think about a dataset in which columns are individuals: a column for each individual's treatment status and a column for each individual's outcome status. A unit is a cluster of individuals in a "closed" system without spillovers beyond them,. -->
<!-- - Units are now groups. Can ask what's the difference in outcome for a unit if $N$ of its neighbors are treated vs. no neighbors treated.  -->
<!-- - Could model an assignment process for spillover experiments:  -->

### Clustering

We can also represent some forms of clustering, understood as the presence of an exogenous but unobserved factor that influences outcomes for some subgroup of units. For instance, we might be interested in the effects of training on individuals' employment prospects, while recognizing that individuals living in the same neighborhood will be affected by common neighborhood-level features that we cannot directly observe. 

We can capture some forms of clustering by placing the cluster-level factor on the graph.^[In this illustration the two units in each pair are treated as separate nodes rather than as repeated instances of realizations of the same node. Implicitly then the effect for one unit type (men, say) can be quite independent of the effect of another type (women, say). Indeed, here they are linked only through the unobserved variable $W$.] Let us imagine that we are studying the effect of providing public health insurance coverage on health outcomes for individuals in two-adult households that have a single earner. Our units of analysis are individuals, but these units are clustered into pairs within households.

We can represent this situation via the following structural model:

```{r, cache = TRUE, echo = FALSE}
model <- make_model("X1 -> Y1 <- W -> Y2 <- X2") %>%
  set_restrictions("(Y1[X1=1, W=0] != Y1[X1=0, W=0])")  %>%
  set_restrictions("(Y2[X2=1, W=0] != Y2[X2=0, W=0])")  

plot(model)
```

Let Unit 1 be the adult in paid employment and Unit 2 the adult not in paid employment, with $X_i$ indicating the provision of public health insurance to Unit $i$ and $W$ representing an unobserved household-level factor that moderates causal effects for both units. We restrict the model such that, more specifically, $W$ shuts off effects of health insurance for both individuals. By representing earners and non-earners separately on the graph, we allow for different effects of health insurance for these two different kinds of individuals. By representing the cluster-level factor, $W$, on the graph, we also allow for *learning* across types of units: seeing effects (or non-effects) for one kind of unit allows us to update on $W$'s value, which in turn provides information about effects for the other type of unit.

One question we can ask with this setup is: would we learn more from concentrating our observations within a smaller number of clusters or spreading them out across clusters? In Table \@ref(tab:cluster), we show results from two different data-collection strategies. In one instance, we observe both Unit 1 and Unit 2 in two clusters. In the other instance, we observe only Unit 1 in two clusters and only Unit 2 in two clusters. Both sets of data are drawn from a process in which the true treatment effect is XXXXXX. In both situations, we calculate the same estimand, the average treatment effect, defined as the effect of providing health insurance, averaged across the two unit types.

<!-- AJ: Is the true ATE here just from flat priors? So 0? -->

<!-- AJ: Needs a final paragraph speaking to the results. WHY DO WE THINK THE SECOND STRATEGY IS BETTER? SEEM VERY SIMILAR. -->


```{r cluster, cache = TRUE, echo = FALSE}
data_1 <- data.frame(X1 = 0:1, Y1 = 0:1, X2 = 0:1, Y2 = c(1,1))
data_2 <- data.frame(
  X1 = c(0, 1, NA, NA), Y1 = c(0,1, NA, NA),
  X2 = c(NA, NA, 0, 1), Y2 = c(NA, NA, 1, 1))

if(do_diagnosis)
update_model(model, data_1 , keep_transformed = TRUE) %>% 
  write_rds("saved/model_clustered.rds")

if(do_diagnosis)
update_model(model, data_2, keep_transformed = TRUE) %>%
  write_rds("saved/model_unclustered.rds")

Q <- "(Y1[X1=1] - Y1[X1=0])/2 + (Y2[X2=1] - Y2[X2=0])/2"

rbind(
  query_model(read_rds("saved/model_clustered.rds"), Q, using = "posteriors"),
  query_model(read_rds("saved/model_unclustered.rds"), Q, , using = "posteriors")) %>%
  mutate(Data = c("2 obs from each of 2 clusters", "1 obs from each of 4 clusters"), Query = "Average effect") %>% 
  select(Data, mean, sd) %>% 
  kable(digits = 3, caption = "Data from many pairs is more informative than the same data from fewer pairs.")

```

### Parameteric models

<!-- AJ: Needs writing -- or cut?? -->

### Prior data/beliefs "channel" the learning from new data 

When we learn from new data, we always update *conditional* on any prior information. Consider the following example. Suppose that we are working with our familiar $X \rightarrow M \rightarrow Y$ model. We first observe a large amount of $X,Y$ data in which the two variables are strongly and positively correlated, thus indicative of a positive $ATE$ of $X$ on $Y$. Next, we turn to process tracing a small number of cases: suppose we collect data on $M$ in one $X=1, Y=1$ case and one $X=0, Y=0$ case, and we observe $M=1$ in both cases. Well, $M$ is uncorrelated with $X$ across these two cases, constituting evidence against an effect of $X$ on $Y$. Since these are both cases in which a positive effect *could* have been operating, this finding will reduce our posterior on the share of positive effects in the population and, in turn, on the $ATE$. 

However, the strong prior information on the $ATE$ that we began with still anchors our updating. Our downward updating on the $ATE$ will be modest since our posterior is always a compromise between our (here, strong) priors and new information. More precisely, we will update less on the $ATE$, about which we had strong prior information, than we will update about the share of positive effects, about which our prior data provided weaker information. 

In addition, there is a knock-on effect for our beliefs about the share of negative effects in the population. If we have a strong prior about the value of the $ATE$, and our beliefs about the share of positive effects goes down substantially, then our beliefs about the share of negative effects must *also* fall. (Recall that the $ATE$ is simply the share of positive effects minus the share of negative effects.) Intuitively, we can think of our beliefs about negative effects as updating to "preserve" our beliefs about the $ATE$. And note that, if we had had *no* prior information about average effects, then learning about positive effects would have have had no implications for our beliefs about negative effects since there would be no overall constraint on the relationship between positive- and negative effect shares. 

A more general way to describe this dynamic is that learning about a kind of case that we directly observe can generate "second-hand" learning about a kind of case that we do not directly observe *through* the constraint on our beliefs imposed by the our priors. This is, really, just a special instance of our priors generating probative value: our prior on the $ATE$ can make evidence about positive effects informative about negative effects. If we had flat priors on the $ATE$, learning about positive effects would have no impact on our beliefs about negative effects.

A parallel example arises when we want to learn about a model with multiple causal pathways. Consider the model $X \rightarrow M \rightarrow Y \leftarrow N \leftarrow X$, where $X$ can have an effect on $Y$ through either $M$ or $N$. And let us set priors such that we believe the the two paths to be equally likely. Suppose that, as before, we have started with a substantial amount of $X,Y$ data indicative of a large positive $ATE$. Now, we look for data on $M$ in a handful of cases and find an $M$ pattern inconsistent with any kind of effect through $M$. What happens to our beliefs about the $ATE$? In general, finding evidence against one way an effect can happen should reduce our confidence in the effect happening at all. However, if we have started out with a strong prior on the $ATE$ but equal prior weight on the $M$ and $N$ pathways, then what we will see is countervailing updating across the two pathways: while our confidence in the operation of the $M$ pathway will fall substantially, our posterior on effects operating via the $N$ pathway will *rise* --- because of the constraint on the total effect imposed by our strong priors on the $ATE$. And our $ATE$ beliefs will fall only modestly. Evidence against the $M$-pathway effect will function as evidence for the $N$-pathway effects and, to a limited degree, as evidence against a total effect. 

A further implication for process tracing is that there will generally be sharp limits to what we can learn about total effects if we study mediators along only *some* of the theorized pathways if we already have some prior information about total effects. The difficulty is that whatever we learn from the mediators we *do* observe will be offset by countervailing shifts in our beliefs about other pathways, generated by the constraint in our prior knowledge about the total effect. Suppose, for instance, that we start with some belief that economic development makes democracy more likely, and we believe that there may be two mechanisms: one operating through a rising middle class and one operating through a more robust and organized working class. Suppose then that we examine data on the organization of the working class and find that it does not vary with per capita GDP. We will then, of course, reduce our confidence in the working-class pathway. However, we must also *increase* our confidence in the operation of the middle-class pathway --- because (a) we have prior reason to believe that the overall development $\rightarrow$ democracy effect exists and (b) we have not observed a mediator along the middle-class pathway. On balance, then, learning about just the one pathway will not have a large impact on beliefs about the overall effect of GDP on democratization. The larger lesson here is that, if our process tracing strategy involves the examination of mediators to learn about total $X \rightarrow Y$ effects, then how much we stand to learn depends on how comprehensively our examination of mediators covers the plausible pathways connecting $X$ to $Y$.

To be clear, we do not need to collect mediator clues on all *possible* pathways. If we have strong priors that one or more possible pathways are very unlikely, then we might safely be able to avoid collecting observations along those pathways without substantially reducing the prospects for learning. 

Also, the specific point that we are making here applies to using mediator data to answer queries about the effect of $X$ on $Y$. If instead we want to know which *particular pathway* is operating, then the lesson here is quite different, and more encouraging. For one thing, collecting evidence on just one pathway can be highly informative about the operation of that pathway. For another, if we do have strong priors on the $ATE$, then learning about one pathway can *also* be informative about other pathways, just as we saw in our $M$- and $N$-pathway example.


<!-- ## Principles of reasoning about learning -->

<!-- As we did in Chapter \@ref(pt), we provide here some guidance in how to reason about the learning that arises from mixed data. We focus especially on ways in which learning from multiple cases differs from learning from a single case. -->

<!-- AJ: We'd discussed this going here, but I think it would be better placed in a substantive chapter. This is where people will be more likely to look for it.  -->

<!-- 1. Learning requires uncertainty. And expected learning goes up as you become more uncertain about what you’ll find.  If your causal model puts a very high probability on X having a positive effect on M, and you already know X’s value, you should expect to learn very little from observing M since you’re very likely to see exactly the M value you expect given X. (Currently in Chap. 12)((And we want to make research design choices based on expected learning, not based on the mere possibility of learning: yes, our beliefs will shift if we look for M and find the unexpected value. But because that data-realization is highly unlikely, we expect the learning from observing M to be minimal.  -->

<!-- 2. Pure within-case (or n=1) learning requires informative priors about the nodes to be observed. For instance, in a chain model, where we want to go and observe M, it’s not enough to have an informative prior about the X->Y relationship. We need an informative prior about the X->M or M->Y link in order to learn from M. For instance, are positive X->M effects more common than negative ones? -->

 

<!-- 4. If there are different ways a query can be satisfied, evidence against one of those ways is evidence against the query as a whole. Say we have a two-path model — with one direct and one indirect path — and we want to know if X affects Y. We observe a mediator, M, along the indirect path in a set of cases. If the M data pattern is inconsistent with an indirect effect, then this is also evidence against an overall effect. In general, finding evidence against one way the effect can happen reduces our confidence in the effect happening at all.  -->



<!-- Moreover, the degree to which prior data constrains learning depends, of course, on how much prior data we have. In the $X \rightarrow M \rightarrow Y$ model, if we have observed only a small amount of prior $X,Y$ data, then observing $M$ in a handful of cases will lead us to update more strongly on the $ATE$ in the above examples, then observing the M data in the on-the-regression-line cases will have a weaker impact on our beliefs about negative effects, and a bigger impact on our beliefs about ATE. Ditto for the 2-path model example. However, where prior data/beliefs on the ATE are strong, we’ll learn less about the ATE, and more about negative effects (or the direct path).  -->

<!-- 7. If there are different ways in which a query can be satisfied, evidence against the likelier way is stronger evidence against the query than is evidence against an unlikelier way. In the 2-path model, if we started out thinking that the indirect effect was more likely than the direct effect, then evidence against the direct effect will have a bigger impact on our beliefs about the overall model.  -->

<!-- 8. It is difficult to get empirical leverage on very unlikely queries. And queries may be unlikelier than they appear. Suppose we start with the 2-path model, and want to know if X has a positive effect on Y that rests on a chain of positive effects via M. And suppose, importantly, that we begin with flat priors over all nodal types. Our intuitions likely tell us that this is exactly the kind of question for which an observation of M is the perfect empirical strategy. And that intuition is, in a sense, correct: we can indeed learn about the query by observing M. Seeing M=1 in an X=Y=1 case, for instance. would be evidence consistent with the query while M=0 would be inconsistent. Fine. ((But we will only learn a little from this observation. The reason is that the query itself has a very low prior probability. It may actually not be obvious at first glance just how unlikely our query is to be true. (After all, the model has two causal paths, and we’re asking if positive effects run through one of them, right? Not quite.) Seeing this requires us to think about the joint probabilities implied by the query. First, the query requires X to have a positive effect on M, which we think there’s only a 25% chance of. In addition, the query puts a very narrow constraint on Y’s possible nodal types:  Y has to have a nodal type in which M has a positive effect on Y when X does not change, and in which X does not have a positive effect on Y unless M changes from 0 to 1. This pair of conditions is met by only 2 of Y’s 16 nodal types, implying a 12.5% chance. The prior on the query is thus 0.25 x 0.125 = 0.03125. Thus, while observing M=0 takes the probability of the query down to 0%, we started out very close to 0%! And observing M=1 results in only a small uptick, to about 6% because there remain many type combinations consistent with M=1 but that do not fit through the needle-eye of this query. -->

<!-- ADD REFERENCE TO TABLE 1 OF FOR MIXED DATA "Ability and Achievement" Otis Duncan -->


<!--chapter:end:09-mixing-methods.Rmd-->

# Integrated Inferences Application: Inequality and Democracy Revisited {#mixingapp}


```{r packagesused09, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
# options(mc.cores = parallel::detectCores())
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We extend the analysis of Chapter \@ref(ptapp) using a model that lets us update about causal processes from data on inequality and democratization from many cases and data on causal processes---mobilization and pressures---from a subset of cases.  We then use the updated model to draw both population-level and case-level inferences.
::::
<br>



<!-- FLAG: Change causal types to unit types everywhere. -->

## A trained model

Whereas in Chapter \@ref(ptapp) we took the model as given and sought to draw inferences individual cases given data on those cases, the model now becomes an object that we both learn from and learn about. In essence we use the data on many cases to update our beliefs about the general model and then use this "trained" model to make inferences about cases. 

Instead of positing a belief over the nodal types for a given case, $\theta$, we now need to posit a belief over the *distribution* of nodal types---that is, over $\lambda$. For instance, whereas in the simple process-tracing model we *specified* the shares of nodal types for $M$, we now specify a *distribution* over the nodal type shares, using a Dirichlet distribution to ensure that the shares across types are always constrained to add up to 1. We do the same, of course, for all nodes. Because we set a prior distribution over nodal types (rather than fixed proportions), we can now update on these population-level distributions as the model confronts data. 

The same applies to beliefs about confounding. Recall that we allow for unobserved confounding by allowing $\lambda$ to include beliefs about the *joint* distributions of nodal types, and we set priors on these joint distributions as well. In the application below, we focus on potential confounding in the relationship between inequality and mobilization: the possibility that inequality may be more or less likely in places where inequality would induce mobilization. Here we do not express informed prior beliefs about the direction or magnitude of such confounding; we set up the parameter matrix to allow for the possibility of confounding and set a flat prior over its direction and magnitude. We can, in turn, learn about confounding from the data.

We begin with the same basic model as we used in Chapter \@ref(ptapp), with inequality ($I$) potentially affecting democratization ($D$) both through a direct pathway and through an indirect pathway mediated by mobilization ($M$). International pressure ($P$) is also a "parent" of democratization. 

Further, we impose the same set of qualitative restrictions, ruling out a negative effect of inequality on mobilization, a direct positive effect of inequality on democratization, a negative effect of mobilization on democracy, and a negative effect of pressure on democratization. Note that this setup allows for inequality to have a positive (through mobilization) effect on democratization, a negative (direct) effect on democratization, or no effect at all. 

Finally, we allow for confounding. The theoretical intuition we want to instantiate in the model is that the level of inequality could be endogenous to inequality's effect on mobilization. In particular, in places where mobilization would pose a mobilizational threat, governments may work harder to reduce inequality. To allow for this possibility, we need to create distinct elements of $\lambda$ representing the conditional distribution of $I$'s nodal types given $M$'s: one parameter for $\theta^I$'s distribution when $M$'s nodal type is $\theta^M_{01}$, and another parameter for $\theta^I$'s distribution when $M$'s nodal type is something else. 




```{r model47, message = FALSE, echo = FALSE}
model <- make_model("I -> M -> D <- P; I -> D; I <-> M") %>% #Specify the DAG

         # Monotonicity restrictions
         set_restrictions(c( 
           "(M[I=1] < M[I=0])", 
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))
  
```


This model, with confounding, is represented graphically as in Figure \@ref(fig:pimdgraph). The possibility of confounding is represented with the bidirected edge, connecting $I$ and $M$. 


```{r pimdgraph, echo = FALSE, fig.cap="Democracy and Inequality Model"}
plot(model)
```



## Data

As in Chapter \@ref(ptapp), we will confront the model with data drawn from our coding of the case narratives in the Supplementary Material for @haggard2012inequality. However, rather than implementing the analysis case-by-case, we now derive leverage from the joint distribution of the data available across all cases. 

Table \@ref(tab:data82) gives a snapshot of the data.



```{r data82, echo = FALSE, warning = FALSE}
data <- CausalQueries::democracy_data 

#data <- read.csv("data/pimd2.csv") %>% 
#  mutate(I2 = I2*1) %>% select(country, P2, I2, M2, D2) %>%
#  rename(P = P2, I = I2, M= M2, D = D2)

kable(head(data), caption = "Data (snapshot) derived from Haggard and Kaufman (2012)")

```

Note that this is not a rectangular dataset in that Haggard and Kaufman's collection of clues was conditional on the outcome, $D=1$: they gathered qualitative data on the presence of international pressure and the presence of mass-mobilization *only* for those cases that democratized. This is not an uncommon case-selection principle. The analyst often reasons that more can be learned about how an outcome arises by focusing in on cases where the outcome of interest has in fact occurred. (We assess this case-selection intuition, in the context of model-based inferences, in Chapter \@ref(caseselection).)

The raw correlations between variables is shown in Table \@ref(tab:pimdcorr). Some correlations are missing because, as mentioned, data on some variables were only gathered conditional on the values of others. For those quantities where we do see correlations, they are not especially strong. There is, in particular, a weak overall relationship between inequality and democratization --- though, of course, this is consistent with inequality having heterogeneous effects across the sample. The strongest correlation in the data is between $P$ and $M$, which are assumed to be uncorrelated in the model, though this correlation is also quite weak.   

```{r pimdcorr, echo = FALSE, warning  = FALSE, message = FALSE}
kable(cor(data[,-1], use = "pairwise.complete.obs"), digits = 3)
kable(cor(data[,-1], use = "pairwise.complete.obs"), digits = 3)
```



## Inference

With data and model in hand, we can now update our model to get posteriors on the distribution $\lambda$ from which we can generate beliefs over all  causal relations. 
<!-- FLAG: Need to talk a bit through what's happening here.  -->

```{r ch9updatePIMD, include = FALSE}
if(do_diagnosis){
  model <- update_model(model, data)
  write_rds(model, "saved/PIMD_updated_2.rds")
  }

model <- read_rds("saved/PIMD_updated_2.rds")

```


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::

The code below shows how to define the Democratization model in `CausalQueries`.

The first line provides the DAG and indicated confounding between I and M. The second line imposes monotonicity restrictions. 

```{r modelblock, message = FALSE, echo = TRUE, eval = FALSE}
model <- 
  
  make_model("I -> M -> D <- P; I -> D; I <-> M") %>% 
  
  set_restrictions(c( 
           "(M[I=1] < M[I=0])", 
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))
  
```

We can then figure out the set of parameters needed to characterize all possible causal relations between these nodes that are consistent with the model. These parameters are used to specify a likelihood function, which, combined with priors on parameters and the data lets us update on the distribution of causal relations. In `CausalQueries` all this is done using `update_model()`.

```{r, eval = FALSE}

model <- update_model(model, data)
```

```{r dimparammatrix, include = FALSE}
param_mat <- get_parameter_matrix(model)
dim(param_mat)
```

::::
<br>

What do we find?

### Did inequality *cause* democratization?

We have used the data to update on $\lambda$: our beliefs about the distributions of nodal types, including about their joint distributions (i.e., confounding). We first use this to make claims about types, similar to what we did in Chapter 8 but now with a model that has been trained on data.

In Table \@ref(tab:HK10cases1) we first ask what we would now infer about cases given democratization and low inequality and different observations on mobilization and international pressure. This table is the analogue of Table REF in Chapter 8.

```{r HK10cases1, echo = FALSE}
I0D1_Cases <- cases_table(
  conditional_inferences(
    model, parameters = NULL, 
  query = "D[I=1] < D[I=0]",   given = "D==1 & I==0"), 
  case_names  = c("Mexico (2000)", "Taiwan (1996)", 
                  "Albania (1991)", "Nicaragua (1984)"))

kable(I0D1_Cases, digits = 3, caption = "Four cases with low inequality and  democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for four cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012). Analyses here use a model with assumptions on monotonic effects but magnitudes of effects updated from data.")

```

In Table \@ref(tab:HK10cases1) we ask what we would now infer about cases given democratization and *high* inequality and different observations on mobilization and international pressure. This table is the analogue of Table REF in Chapter 8.



```{r HK10cases2, echo = FALSE}

I1D1_Cases <- cases_table(
  conditional_inferences(
    model,   parameters = NULL,  
    query = "D[I=1] > D[I=0]",   
    given = "D==1 & I==1"), 
  case_names  = c("Mongolia (1990)", "Paraguay (1989)", "Sierra Leone (1996)", "Malawi (1994)"))

kable(I1D1_Cases, caption = "Four cases with high inequality and  democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012).  Analyses here use a model with assumptions on monotonic effects but magnitudes of effects updated from data.r")

```


### Did inequality *prevent* democracy?

As before we can also ask questions about causes that have not democratized -- even though we have no additional data about these cases in particular.


```{r Tapp101, echo = FALSE,  cache = FALSE, message = FALSE, warning = FALSE} 


# Did I=0 cause D = 0
I0D0 <- conditional_inferences(model, 
                            using = "posteriors", 
                            query = "D[I=1] > D[I=0]", 
                            given = "D==0 & I==0")

kable(dplyr::select(I0D0, P,M, posterior),digits = 3, caption = "No inequality and No democratization: Was no inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.")
```

```{r Tapp102, echo = FALSE}

I1D0 <- conditional_inferences(model, 
                            using = "posteriors", 
                            query = "D[I=1] < D[I=0]", 
                            given = "D==0 & I==1")

kable(dplyr::select(I1D0, P,M, posterior), digits = 3, caption = "Inequality and No democratization: Was inequality a cause of no democratization? Analyses here use priors assuming only monotonic effects.")
```



## From cases to population

One set of questions we can ask of the updated model is about the probability that high inequality causes democratization. We can pose this question at different levels of conditioning. For instance, we can ask:



1. **For all cases**. For what proportion of cases in the population does inequality have a positive effect on democratization? 

2. **For all cases displaying a given causal state and outcome**. Looking specifically at those cases that in fact had high inequality and democratized, for what proportion was the high inequality a cause of democratization?

3. **For cases displaying a given causal state and outcome, and with additional clues present or absent.** What if we have also collected clues on mediating or moderating nodes? For instance, for what proportion of high-inequality, democratizing cases *with* mobilization did inequality cause the outcome? For what proportion *without* mobilization? Likewise for the presence or absence of international pressure? Importantly, comparing a given estimate with and without a given clue amounts to an assessment of the clue's probative value. 

We ask `CausalQueries` now to query $\lambda$'s posterior distribution to generate posterior distributions for each of these quantities. We can define our queries quite simply in terms of the causal types that correspond to the effect of interest and then take the conditional probability of these. We present the code and results of these operations below and in Figure \@ref(fig:mixedhist).



<!-- FLAG: Not sure how much it is worth saying here substantively about the results given how strongly they are driven by the restrictions. -->

```{r, echo = FALSE}


if(do_diagnosis){
pimd_queries_2 <- 
  
  model %>% 
  
  query_model(
    using = "posteriors",
    query = "D[I=1] > D[I=0]",
    case_level = TRUE,
    given = c(TRUE, 
              "D==1 & I==1",  
              "D==1 & I==1 & M==1",
              "D==1 & I==1 & M==0",
              "D==1 & I==1 & M==1 & P==1",
              "D==1 & I==1 & M==1 & P==0")
)

write_rds(pimd_queries_2, "saved/09_pimdqueries_2c.rds")
}

pimd_queries_2 <- read_rds("saved/09_pimdqueries_2c.rds")

pimd_queries_2 %>% select(Given, mean) %>% kable(digits = 2, caption = "Probability that inequality increases chances of democratization given:")

```

In figure @\ref(fig:pimdqueries), we graph the priors for each of these queries next to the posteriors.

```{r pimdqueries, include = FALSE, echo = FALSE}

if(do_diagnosis){
pimd_queries <- list(
  #For what proportion of cases does inequality have a positive effect on democratization?
  prob_b = query_distribution(
    model = model,
    using = "posteriors",
    query = "D[I=1] > D[I=0]"
  ),
  
  #For what proportion of I=D=1 cases did inequality cause democratization?
  PC =  query_distribution(
    model = model,
    using = "posteriors",
    query = "D[I=1] > D[I=0]",
    given = "D==1 & I==1"
  ),
  
  #For what proportion  of I=M=D=1 cases did inequality cause democratization?
  PC2 = query_distribution(
    model = model,
    using = "posteriors",
    query = "D[I=1] > D[I=0]",
    given = "D==1 & I==1 & M==1"
  ),
  
  #And what about for the I=D=1 cases where there was no mobilization?
  PC3 = query_distribution(
    model = model,
    using = "posteriors",
    query = "D[I=1] > D[I=0]",
    given = "D==1 & I==1 & M==0"
  ),
  
  PC4 = query_distribution(
    model = model,
    using = "posteriors",
    query = "D[I=1] > D[I=0]",
    given = "D==1 & I==1 & M==1 & P==1"
  ),
  
  # And what about for the I=D=1 cases where there was no mobilization?
  PC5 = query_distribution(
    model = model,
    using = "posteriors",
    query = "D[I=1] > D[I=0]",
    given = "D==1 & I==1 & M==1 & P==0"
  )
)

write_rds(pimd_queries, "saved/09_qimdqueries_2.rds")
}


pimd_queries <- read_rds("saved/09_qimdqueries_2.rds")

```


```{r mixedhist, echo =FALSE, fig.cap = "Posteriors on Causes of Democratization"}

x <- lapply(pimd_queries, function(a) data.frame(p = as.vector(a))) %>%
  bind_rows(.id = "Q") %>%
  mutate(Q = factor(Q, c("prob_b", "PC", "PC2", "PC3", "PC4", "PC5"),
                    c("I=1 causes D=1", "D=1 caused by I=1", "D=1 caused by I=1 | M = 1", "D=1 caused by I=1 | M = 0", "D=1 caused by I=1 | M = 1, P=1",  "D=1 caused by I=1 | M = 1, P=0")))

x %>% ggplot(aes(p)) + geom_histogram() + facet_wrap(~Q, scales = "free_y") + theme_bw()  + xlim(-.1,1) + xlab("probability")
         
```

We can see that the share of cases overall in which inequality causes democratization is estimated to be very low, with a good deal of confidence. The proportion is considerably higher for those cases that in fact experienced high inequality and democratization. The proportion of positive causal effects is believed to be even higher for those in which mobilization occurred. Moreover, the proportion of $I=1, D=1$ cases with a positive effect of inequality on democratization is even higher when an alternative cause---international pressure---is absent, though our uncertainty about this share is also very high. 

We also see that the absence of mobilization tells us for certain that democratization was not caused by inequality. Interestingly, however, this result derives purely from the model restrictions, rather than from the data: under the restrictions we imposed, a positive effect of inequality can operate *only* through mobilization.

Turning now to the cases in which democratization did not occur, Figure \@ref(fig:mixedhist2) asks for what proportion of cases overall inequality has a negative effect on democratization; for what proportion of $I=1, D=0$ cases inequality prevented democratization; and this latter query conditional on different clue realizations. 

```{r pimdnegqueries, include = FALSE, warning = FALSE}

if(do_diagnosis){
  
pimdneg_queries <- list(

  prob_a = query_distribution(
                   model = model, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]"),

PCneg =  query_distribution(
                   model = model, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   given = "D==0 & I==1"),

PC2neg = query_distribution(
                   model = model, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   given = "D==0 & I==1 & M==1"),

PC3neg = query_distribution(
                   model = model, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   given = "D==0 & I==1 & M==0"),

PC4neg = query_distribution(
                   model = model, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   given = "D==0 & I==1 & M==1 & P==1"),

PC5neg = query_distribution(
                   model = model, 
                   using = "posteriors",
                   query = "D[I=0] > D[I=1]",
                   given = "D==0 & I==1 & M==0 & P==0"))

write_rds(pimdneg_queries, "saved/09_pimdnegqueries.rds")
}

```

```{r mixedhist2, echo =FALSE, fig.cap = "Posteriors on Causes of Democratization"}

read_rds("saved/09_pimdnegqueries.rds") %>%
  
  lapply(function(a) data.frame(p = as.vector(a))) %>%
  bind_rows(.id = "Q") %>%
  mutate(Q = factor(Q, 
                    c("prob_a", "PCneg", "PC2neg", "PC3neg", "PC4neg", "PC5neg"),
                    c("I=1 causes D=0", "D=0 caused by I=1", "D=0 caused by I=1 | M = 1", "D=0 caused by I=1 | M = 0", "D=0 caused by I=1 | M = 1, P=1",  "D=0 caused by I=1 | M = 1, P=0"))) %>% 
  ggplot(aes(p)) + 
  geom_histogram() + facet_wrap(~Q, scales = "free_y") + theme_bw()  + xlim(-.1,1) + xlab("probability")
         
```


We see that inequality appears, overall, more commonly to prevent democratization than to cause it. We are, moreover, most confident that inequality played a preventive role in those cases in which there was mobilization and international pressure---both of which *could* have generated democratization---but still no democratization occurred. 


### Contribution to case-level inference

There are two ways of thinking about the learning we derive from the above estimates. On the one hand, we can think of our estimands in population-level terms. The results in Figures \@ref(fig:mixedhist) and \@ref(fig:mixedhist2) can be understood as our estimates of the *share* of cases in the population, with a given set of characteristics, for which a particular causal effect holds. 

Yet these distributions, by the very same token, represent our beliefs about the probability that $I$ had a positive or negative (depending on the query we are talking about) causal effect in an individual case for which we have seen a given data pattern. Thus, for instance, from Figure \@ref(fig:mixedhist) we can see  our posterior belief about the proportion of $I=D=M=1$ cases in the population for which $I=1$ caused $D=1$. But this also tells us, for an individual case randomly selected from the population of $I=D=M=1$ cases, the probability that high inequality caused democratization in the case. To put the point differently, suppose we want to know whether $I=1$ was a cause of $D=1$ in a randomly selected case, so we do process tracing and observe $M=1$: this same estimate answers the case-level query, telling us the probability that $I=1$ caused $D=1$ in the case at hand. 

The key difference between case-level inference from mixed data and the pure process-tracing setup from Chapter \@ref(ptapp) is that now our case-level inferences are informed by *data* from the population, rather than merely by a set of prior beliefs about the population. Consider, for instance, the difference in posteriors on the probability of a negative effect for the query for $I=1, D=0$ given $M=0$ (mean of about 0.31) as compared to the query for $I=1, D=0$ given $M=1$ (about 0.42). That difference in what we believe depending on $M$'s value represents the probative value of the $M$ clue given the model as *updated* from the data. Of course, we have also made assumptions in building the model, including motonicity restrictions, and we speak to their role below. But the general point is that, in mixed-data inference, we can learn empirically about the probative value of any given node, rather than drawing probative value purely from theory.


### How much do we get from the model vs. the data?

We might wonder, at the same time, how much we are in fact learning from the data, as compared to what we built into the model at the outset, including through the monotonicity restrictions that we imposed. To examine this, in Figures \@ref(fig:dcaused) and \@ref(fig:dprevented), we plot the mean (and one-standard-deviation error bars) of our prior and our posterior on the same set of queries. We see that there is almost no shift in beliefs for the positive-effect queries, with a somewhat greater shift in means for the negative-effect queries. However, our uncertainty about negative effects shrinks some, though remains quite high. 

By comparing the prior and posterior estimates given $M=1$ and given $M=0$, we can also assess whether we have learned about $M$'s informativeness. We see that, in this example, we do not happen to learn anything from the data about $M$'s probative value: the difference in beliefs between each estimate given $M=1$ and that estimate given $M=0$ remains about the same in our posteriors as it was in our priors. 

```{r ppcomparison, include = FALSE}
# Estimands can be calculated for both the prior and posterior distributions.

model <- set_prior_distribution(model)

givens11 <- list(TRUE, "D==1 & I==1", "D==1 & I==1 & M ==1", "D==1 & I==1 & M==0")
givens01 <- list(TRUE, "D==0 & I==1", "D==0 & I==1 & M ==1", "D==0 & I==1 & M==0")

if(do_diagnosis){

model_prior <- update_model(model)  

ch09_queryresults <- list(
  
result_priors = query_model(
                   model = model_prior, 
                   using = "posteriors",
                   queries =  list("D[I=1] > D[I=0]"),
                   given = givens11),

result_posteriors = query_model(
                   model = model, 
                   using = "posteriors",
                   queries =  list("D[I=1] > D[I=0]"),
                   given = givens11),

prevent_priors = query_model(
                   model = model_prior, 
                   using = "posteriors",
                   queries =  list("D[I=0] > D[I=1]"),
                   given = givens01),

prevent_posteriors = query_model(
                   model = model, 
                   using = "posteriors",
                   queries =  list("D[I=0] > D[I=1]"),
                   given = givens01))

write_rds(ch09_queryresults, "saved/ch09_queryresultsqueries.rds")
}

ch09_queryresults_queries <- read_rds("saved/ch09_queryresultsqueries.rds")

Given_labels <- c(
`Nothing` = "-",
`D==1 & I==1`="D==1 & I==1",
`D==1 & I==1 & M ==1`="D==1 & I==1 & M ==1",
`D==1 & I==1 & M==0`= "D==1 & I==1 & M==0",
`D==0 & I==1`="D==0 & I==1",
`D==0 & I==1 & M ==1`="D==0 & I==1 & M ==1",
`D==0 & I==1 & M==0`= "D==0 & I==1 & M==0") 

pp_comparison <- ch09_queryresults_queries  %>% bind_rows(.id = "case") %>%
  mutate(using = ifelse(grepl("priors", case), "Priors", "Posteriors"),
         query = ifelse(grepl("result", case), "Caused", "Prevented"),
         Given = factor(Given, rev(Given_labels), rev(names(Given_labels))))
  
```

```{r dcaused, echo = FALSE, fig.cap = "Priors and posteriors on the probability that inequality caused democratization (probability that I has a positive effect on D) given different kinds of observed case-level data. The error bars show plus or minus one standard deviation of the posterior variance. "}

pp_comparison %>% filter(query == "Caused") %>% 
  ggplot(aes(mean, Given, color = using)) + 
  geom_point(position=position_dodge(.5)) + 
  theme_bw() +
  geom_errorbarh(aes(xmin=mean - sd, xmax=mean + sd), 
                size=.2, height = .1,
                position=position_dodge(.5))

```

```{r dprevented, echo = FALSE, fig.cap = "Priors and posteriors on the probability that inequality prevented democratization (probability that I has a negative effect on D) given different kinds of observed case-level data. The error bars show plus or minus one standard deviation of the respective distribution's variance."}
pp_comparison %>% filter(query == "Prevented") %>% 
  ggplot(aes(mean, Given, color = using)) + 
  geom_point(position=position_dodge(.5)) + 
  theme_bw() +
  geom_errorbarh(aes(xmin=mean - sd, xmax=mean + sd), 
                size=.2, height = .1,
                position=position_dodge(.5))





```




<!--chapter:end:10-mixed-application.Rmd-->

# Mixing models {#mm}


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We provide four examples of situations in which, by combining models, researchers learn more than they could from any single model.
::::
<br>




```{r packagesused15, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
library(DeclareDesign)
```

In the previous two chapters, we described one form of integration that structural causal models can enable: the systematic combination of (what we typically think of as) qualitative and quantitative evidence for the purposes of drawing population- and case-level causal inferences. One feature of the analyses we have been considering so far is that the integration is essentially "nested." We are, for instance, integrating quantitative evidence for a large set of cases with qualitative evidence for a *subset* of those cases. We are, moreover, drawing inferences from the set of cases we observe to a population *within which* that sample of cases is situated. 

In this chapter, we examine how we can use structural causal models to integrate across studies or settings that are, in a sense, more disjointed from one another: across studies that examine different causal relationships altogether; study designs that require different assumptions about exogeneity; and contexts across which the causal quantities of interest may vary.

1. **Integrating across a model** Often, individual studies in a substantive domain examine distinct segments of a broader web of causal relationships. For instance, while one study might examine the effect of $X$ on $Y$, another might examine the effect of $Z$ on $Y$, and yet another might examine the effect of $Z$ on $K$. We show in this chapter how we can integrate across such studies in ways that yield learning that we could not achieve by taking each study on its own terms. 

2. **Integrating between experimental and observational studies** One form of multi-method research that has become increasingly common is the use of both observational and experimental methods to study the same basic causal relationships. While an experiment can offer causal identification in a usually local or highly controlled setting, an observational analysis can often shed light on how the same relationships operate "in the wild," if with greater risk of confounding. Usually, observational and experimental results are presented in parallel, as separate sources of support for a causal claim. We show how, in a causal model setup, we can use experimental and observational data *jointly* to address questions that cannot be answered when the designs are considered separately.

3. **Transporting knowledge across contexts** Researchers are sometimes in a situation in which they can identify causal quantities in a particular setting --- say, from a randomized controlled trial implemented in a specific local context --- but want to know how those inferences travel to other settings. Would the intervention work differently in other countries or regions? As we will explain, with an appropriately specified causal model and the right data from the original context, we can draw inferences about causal relationships in other contexts.

4. **Models in hierarchies.** Sometimes researchers learn about the same types of processes in different settings. By thinking of the processes in each setting as deriving from a family of processes, researchers can learn from observations in one setting about causal processes in another and also learn about the nature of heterogeneity between settings. 


Before delving into the details of these strategies, we make one key qualification explicit: each of these approaches requires us to believe that setting-, or study-, specific  causal model can be nested within a lower level, "encompassing," model that operates across the multiple settings that we are learning from and want to draw inferences about. Encompassing models, of course, can specifically take heterogeneity across settings into account, for instance by including in the model moderators that condition the effects of interest. But we have to believe that we have indeed captured in the model any ways in which relationships vary across the set of contexts across which we are integrating evidence or transporting inferences. 

Put differently, and perhaps more positively, we see social scientists commonly seeking to transport knowledge or combine information informally across studies and settings. Often such efforts are motivated, sometimes implicitly, by an interest in or reliance on general theoretical propositions. The approaches that we describe below force the researcher to be *explicit* about the underlying causal beliefs that warrant that integration while also ensuring that the integration proceeds in a way that is logically consistent with stated beliefs.


## A jigsaw puzzle: Integrating across a model

Generating knowledge about a causal domain often involves cumulating learning across studies that each focus in on some specific part of the domain. For instance, scholars interested in the political economy and democratization might undertake studies focused on the relationship between inequality and mass protests; studies on the role of mass mobilization in generating regime change; pathways other than mass mobilization through which inequality might affect democratization; studies of the role of international sanctions on the likelihood that autocracies will democratize; and studies of the effects of democratization on other things, such as growth or the distribution of resources. 

We can think of these studies as each analyzing data on a particular part of a broader, more encompassing causal model. In an informal way, *if* findings "hold together" in a reasonably intuitive way, we might be able to piece together an impression of the overall relations among variables in this domain.  Yet an informal approach becomes more difficult for complex models or data patterns and, more importantly, will leave opportunities for learning unexaploited.

Consider this simple DAG, in which both $X$ and $Z$ are causes of $Y$, and $Z$ also causes $K$. Now imagine three studies, all conducted in contexts in which we believe this model to hold: 

```{r jigsaw, eval = TRUE, echo = FALSE}
model <- make_model("X -> Y <- Z -> K") %>%

          set_parameters(
            statement = list("(Y[X=1, Z = 1] > Y[X=0, Z = 1])",  
                             "(K[Z = 1] > K[Z = 0])"),
            node = c("Y","K"), 
            parameters = c(.24,.85))

plot(model)

```


1. Study 1 is an RCT in which $X$ is randomized, with data collected on both $Y$ and $K$. $K$ is collected. $Z$ is not observed.
1. Study 2 is a factorial experiment, in which $X$ and $Z$ are independently randomized, allowing an examination of the joint effects of $X$ and $Z$ on $Y$. $K$ is not observed.
2. Study 3 is an experiment randomizing $Z$, with only $K$ observed as an outcome. $X$ and $Y$ are not observed.

Now, let's say that our primary interest is in the relationship between $X$ and $Y$. Obviously, Study 1 will, with a sufficiently large sample, perform just fine in estimaing the average treatment effect of $X$ on $Y$. However, what if we are interested in a case-oriented query, such as the probability of causation: the probability, say, $X=1$ caused $Y=1$ in a given $X=1, Y=1$ case? 

We know that within-case, process-tracing clues can sometimes provide probative value on case-level estimands like the probability of causation, and we have observed $K$ in the Study 3 cases. So what if we combine the $X$, $Y$, and $K$ data? 

```{r, echo = FALSE}

df <- make_data(model, 300, using = "parameters") %>%
  
      mutate(study = rep(1:3, each = 100),
             Z = ifelse(study == 1, NA, Z),
             K = ifelse(study == 2, NA, K),
             X = ifelse(study == 3, NA, X),
             Y = ifelse(study == 3, NA, Y)
             )

```

```{r, echo = FALSE}
if(do_diagnosis){

updated1 <- update_model(model, filter(df, study == 1))
updated2 <- update_model(model, filter(df, study == 2))
updated3 <- update_model(model, filter(df, study == 3))
updated_all <- update_model(model, df)

subs <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0")
subs2 <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0",
              "X == 1 & Y == 1 & K == 1 & Z == 1",
              "X == 1 & Y == 1 & K == 0 & Z == 1",
              "X == 1 & Y == 1 & K == 1 & Z == 0",
              "X == 1 & Y == 1 & K == 0 & Z == 0")

# If updating done using case data only
result1 <- query_model(updated1, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result2 <- query_model(updated2, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result3 <- query_model(updated3, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result4 <- query_model(updated_all, queries = "Y[X=0] == 0", given = subs2, using = "posteriors")

write_rds(list(result1, result2, result3, result4), "saved/10a_frankenstein.rds")
}

```


<!-- AJ: Suggest we flip the study ordering to the X-Y experiment is STudy 1. Makes a more natural progression with the prose. -->

A simple analysis of the graph tells us that $K$ cannot help us learn about $Y$'s potential outcomes since $K$ and $Y$ are $d$-separated by $Z$, and we have not observed $Z$ in Study 3. We see this confirmed in Table \@ref(tab:frank1). 

In the first pair of rows, we  show the results of analyses in which we have simulated data from the whole model, then updated using the Study 1 observations. We give here the posterior mean on the probability of causation for an $X=Y=1$ case, conditional on each possible value that $K$ might take on. As we can see, our beliefs about the estimand remain unaffected by $K$'s value, meaning that it contains no information about $X$'s effect in the case.

<!-- FLAG: AJ: When we say data on K is not available in S1, do we mean we haven't used K to *update* the model? But we are then simulating what would we would infer if we *did* collect $K$ for a case in S1? This is my read of the code. Or have I misread? Are we imagining a situation in which we simply don't have K at all, so by definition can't learn from K? I've written the text below on the first assumption, so will need changing if that's wrong. -->

We see that the same thing is true for each of the other studies. In study 2, we have not used $K$ to update the model, and so have not learned anything form the data about $K$'s relationship to the other variables. Thus, we have no foundation on which to ground probative value fo $K$. In study 3, we understand the $Z,K$ relationship well, but know nothing quantitatively about how $Z$ and $X$ relate to $Y$. Thus, we have learned nothing from Study 3 about what observing $K$ might tell us about the effect of $X$ on $Y$.
 

```{r frank1, echo = FALSE}

frank <- read_rds("saved/10a_frankenstein.rds")

kable(
  cbind(Study = c(1, NA, 2, NA, 3, NA),
        rbind(
    frank[[1]][,-c(1,3)],
    frank[[2]][,-c(1,3)], 
    frank[[3]][,-c(1,3)])), 
  caption = "The clue $K$ uninformative in all three studies")
```


We can do much better, however, if we combine the data and update *jointly* across all model paramaters. The results are shown in Table \@ref(tab:frank4). Updating simultaneously across the studies allows us, in a sense, to bridge across inferences. In particular, inferences from Study 2 make $Z$ informative about $Y$'s potential outcomes under different values of $X$. Meanwhile, inferences from the data in Study 3 allow us to use information on $K$ to update on values for $Z$. As we now see in rows 1 and 2, having updated the model in an integrated fashion, $K$ now *is* informative about the probability of causation, with our posterior mean on this query changing substantially depending on the value of $K$ that we observe in a case.  

Rows 3-4 highlight that the updating works through inferences on $Z$: we see that if $Z$ is already known (we show this for $Z=1$, but it holds for $Z=0$ as well), then there are no additional gains from knowledge of $K$. 


```{r frank4, echo = FALSE}
kable(frank[[4]][1:4,-c(1,3)], caption = "Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$", digits = 2)
```


<!-- In sum, the collection of studies collectively provides a justification for learning from $K$ when assessing a case level effect of $X$ on $Z$ in study 1.  -->

<!-- FLAG: AJ: Is something wrong with the study numberings in this next paragraph? -->

<!-- FLAG: I find the reference to justifying a model confusing here. It's not going to be self-evident to readers how that's what we've just done. It seems like we've *started* with a model and used it. But I'm not 100% clear on what you want to say here. Can you clarify? -->

We devote Chapter 15 to a discussion of how we justify a model. However, we note already that in this example we have an instance in which a researcher (examining a case in study 3) might wish to draw inferences using $K$, but she does not have anything in study 1 that justifies using $K$ for inference. However with access to studies 2 and 3, and conditional on the overall model, she has a justification for process tracing strategy. The general principle is that weaker commitments to lower level theories ---here the causal structure---can justify more fully inferences from more fully specified higher-level theories.  

<!-- AJ: Also finding the last sentence above confusing. -->




## Combining observational and experimental data

Experimental studies are often understood as the "gold standard" for causal inference. This is, in particular, because of the ability of a randomized trial (given certain assumptions, such as "no spillovers") to eliminate sources of confounding. By design, an experiment removes from the situation processes that, in nature, would generate a correlation between selection into treatment and potential outcomes. An experiment thereby allows for an unbiased estimate of the average causal effect of the treatment on the outcome. 

At the same time, an interesting weakness of experimental studies is that, by dealing so effectively with selection into treatment, they limit our ability to learn about selection and its implications in the real world. Often, however, we want to know what causal effects would be specifically for units that *would* in fact take up a treatment in a real-world, non-experimental settings. This kind of problem is studied for example by @knox2019design.

Consider, for instance, a policy that would make schooling subsidies available to parents, with the aim of improving educational outcomes for children. How would we know if the policy was effective? A source of confounding in an observational setting might be that those parents who apply for and take up the subsidy might also be those who are investing more in their children's education in other ways as compared to those parents who do not apply for the subsidy. To eliminate this problem, we might design an experiment in which parents are randomly assigned to receive (or not receive) the subsidy and compare outcomes between children in the treatment and control groups. With a no-spillovers assumption, we can extract the $ATE$ of the receipt of subsidies. 

What this experiment cannot tell us, however, is how much the policy will boost educational outcomes outside the experiment. That is because the causal quantity of interest, for answering that question, is *not* the $ATE$: it is the average treatment effect for the *treated* ($ATT$), given real-world selection effects. That is, the policymaker wants to know what the effect of the subsidy will be for the children of parents who *select into* treatment. One could imagine the real-world $ATT$ being higher than the $ATE$ if, for instance, those parents who are informed and interested enough to take up the subsidy also put the subsidy to more effective use. One could also imagine the $ATT$ being lower than the $ATE$ is, for instance, there are diminishing marginal returns to educational investments and the self-selecting parents are already investing quite a lot. 

Even outside a policy context, we may be interested in the effect of a causal condition *where* that causal condition emerges. To return to our inequality and democracy example, we may want to know what would have happened to autocracies with low inequality *if* they had had high inequality -- the standard average-treatment effect question. But we might also be interested in knowing how much of a difference high inequality makes *in the kinds of cases* where high inequality tends to be occur -- where the effect could be very different. 

With such questions, we are in a sort of bind. The experiment cannot tell us *who* would naturally select into treatment and what the effects would be for them. Yet an observational study faces the challenge of ruling out confounding. Ideally, we would like to be able to combine the best features of both: use an experiment to deal with confounding and use observational data to learn about those whom nature assigns to treatment.

We can achieve this form of integration with a causal model. We do so by creating a model in which random assignment is nested within a broader set of assignment processes. We plot the model in Figure \@ref(fig:appcombexpob)

At the substantive core of this model is the $X \rightarrow Y$ relationship. However, we give $X$ a parent that is fully exogenous, $Z$, to capture a random-assignment process. We give $X$ a second parent, $O$, that is confounded with $Y$: $O$ here represents the observational scenario. Finally, we include a "switch" variable, $R$, that determines whether $X$ is randomly assigned or not. So when $R=1$, $X$ is determined solely by $Z$, with $X=Z$. When $R=0$, we are in an observational setting, and $X$ is determined solely by the confounded $O$, with $X=O$.

A few notes on the parameter space. Parameters allow for complete confounding between $O$ and $Y$, but $Z$ and $Y$ are unconfounded. $X$ has only one causal type since its job is to operate as a conveyor belt, simply inheriting the value of $Z$ or $O$, depending on $R$. 

Note also that this model assumes the exclusion restriction that entering the experimental sample ($R$) is not related to $Y$ other than through assignment of $X$. 


```{r copobsetup, message = FALSE, warning = FALSE, echo = FALSE}
model <- make_model("R -> X -> Y; O -> X <- Z; O <-> Y") %>%
  
	set_restrictions("(X[R=1, Z=0]!=0) | (X[R=1, Z=1]!=1) | (X[R=0, O=0]!=0) | (X[R=0, O=1]!=1)")

```

```{r appcombexpob, message = FALSE, warning = FALSE, echo = FALSE, fig.cap="A model that nests an observational and an experimental study. The treatment $X$ either takes on the observational value $O$, or the assigned values $Z$, depending on whether or not the case has been randomized, $R$."}

plot(model)
```

```{r, echo = FALSE, include = FALSE}
P <- get_parameter_matrix(model)
kable(P[,1:4])
```


Now, let us imagine true parameter values such that $X$ has a $0.2$ average effect on $Y$. However, the effect is different for those who are selected into treatment in an observational setting: it is positive ($0.6$) for cases in which $X=1$ under observational assignment, but negative ($-0.2$) for cases in which $X=0$ under observational assignment. (See appendix for complete specification.) 

When we use the model to analyze the data, we will start with flat priors on the causal types. 

```{r, echo = FALSE}

model <- model %>%
	set_parameters(node = "Y", confound = "O==0", parameters = c(.8, .2,  0,  0)) %>%
	set_parameters(node = "Y", confound = "O==1", parameters = c( 0,  0, .6, .4))

```

The implied true values for the estimands of interest, and our priors on those estimands, are displayed in Table \@ref(tab:fusionestimands).

<!-- AJ: I think the talk of parameters and priors is less intuitive than it could be in a table and passage like this, though I realize it maps onto CQ argument names. By "parameters" we mean *true* values in the simulation, right? Can we convert to "truth" in the "Using" column? Truth vs. priors seems nicely intuitive. -->

```{r fusionestimands, echo = FALSE}

if(do_diagnosis){
result <- query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = c("parameters", "priors"), 
    expand_grid = TRUE)
write_rds(result, "saved/10a_fusionestimands.rds")
}

read_rds("saved/10a_fusionestimands.rds") %>% kable(caption = "Estimands in different sites", digits = 2)

```


```{r, echo = FALSE}
data <- make_data(model, n = 800)
```


Now, we generate data from the model, using the posited "true" parameter values, and then update the model using these data. 

We begin by analyzing just the observational data (cases where $R=0$) and display the results in Table \@ref(tab:fusiondim). Recall that the true average effect of $X$ on $Y$ is $0.2$. Naive analysis of the observational data, taking a simple difference in means between the $X=0$ and $X=1$ cases, yields a strongly upwardly biased estimate of that effect, of 0.0806. 

```{r fusiondim, echo  = FALSE, warning = FALSE, message = FALSE}
x <- estimatr::difference_in_means(Y~X, data = filter(data, R==0))
kable(summary(x)[[1]], digits = 3, caption = "Inferences on the ATE from differences in means")
```

In contrast, when we use `CausalQueries` to update on the full causal model and use both the experimental and observational data, we get the much more accurate results shown in Table \@ref(tab:fusionCQ). Moving down the rows, we show here the estimate of the unconditional $ATE$, the estimate for the observational context ($R=0$), and the estimate for the experimental context ($R=1$). Unsurprisingly, the estimates are identical across all three settings since, in the model, $R$ is $d$-separated from $Y$ by $X$, which is observed. And, as we see, the posterior means are very close to the right answer of $0.2$.

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, data), "saved/10a_exp_obs.rds")
  }
updated <- read_rds("saved/10a_exp_obs.rds")
```

```{r fusionCQ, echo = FALSE}
result <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result, caption = "Estimates on the ATE for observational ($R=0$) and experimental ($R=1$) set.")
```




```{r, eval = FALSE, echo = FALSE}
updated_no_O <- update_model(model, dplyr::filter(data, R==1))
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(do_diagnosis){
  write_rds(update_model(model, dplyr::filter(data, R==1)), "saved/10a_exp_obs_2.rds")
  }
updated_no_O <- read_rds("saved/10a_exp_obs_2.rds")
```

```{r appcombexpopp8, echo = FALSE, include = FALSE}
result <- query_model(
    updated_no_O, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors")
kable(result)
```

Since the model used both the experimental and the observational data, we might wonder from where the leverage derived: did the observational data improve our estimates of the average treatment effect, or do our inferences emerge strictly from the experimental data? In the appendix, we show results when we updating using experimental data only.
 Comparing the two sets of results, we find there that we do indeed get a tightening of posterior variance and a more accurate result when we use both the observational and experimental data, but the experimental data alone are quite powerful, as we should expect for an estimate of the $ATE$. The observational data do not add a great deal to an $ATE$ estimate, and the gains from observational data would be smaller still (and the experimental results even more accurate) if the experimental sample were larger. 

<!-- FLAG: Need to create above table in appendix -->

However, what we can learn about uniquely from this model and the combined observational and experimental data is *heterogeneity* in effects between those that are in treatment and those that are in control *in the observational* setting. In Table \@ref(combexpobsattatc), we display the results of $ATT$ and $ATC$ queries of the updated model. In the first two rows, we see that, in the experimental setting, the average effect of $X$ on $Y$ is the same on both the treated and control groups, exactly as we would expect under random assignment. In the third row, we see the estimate of $X$'s average effect for those assigned "by nature" to the control group in the observational setting, extracting a result close to the "true" value of $-0.2$. The final row shows our estimate of the treatment effect for those who are selected into treatment in the observational setting, again getting close to the answer implied by the underlying data-generating process ($0.6$). 

```{r combexpobsattatc, echo = FALSE}
result2 <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list("R==1 & X==0", "R==1 & X==1", "R==0 & X==0", "R==0 & X==1"),
    using = "posteriors")

kable(result2, caption = "Effects of $X$ conditional on $X$ for units that were randomly assigned or not.  Effects of $X$ do not depend on $X$ in the experimental group, but they do in the observational group becuase of seld selection. ")
```

We can learn nothing about the observational $ATT$ or $ATC$ from the experimental data alone, where there $ATT$ and $ATC$ are the same quantity. And in the observational data alone, we are hobbled by confounding of unknown direction and size. What the mixed model and data, in effect, are able to do is (a) learn about the $ATE$ experimental data, (b) use inferences on the $ATE$ to separate true effects from confounding in the observational data and thus learn about the direction and size of the confounding in those data, and (c) estimate the treatment effect for the $X=0$ group and for the $X=1$ group, respectively, in the observational data *using* knowledge about confounding in these data. By mixing the experimental and observational data, we can learn about how the treatment has affected those units that, in the "real" world of the observational setting, selected into treatment *and* about how the treatment *would* affect those that selected into control. 

The numbers in our toy example, while purely notional, can help us see why the observational $ATT$ and $ATC$ might be of great interest to decision makers where strong causal heterogeneity is a possibility. Based on the experimental data alone, we might conclude that the policy that makes $X=1$ available is a good bet, given its positive $ATE$ (assuming, of course, that $Y=1$ is a valued outcome). And, of course, the observational data alone would not allow us to confidently conclude otherwise. What the integrated analysis reveals, however, is that $X$ in fact has a *negative* mean effect on those who would be most likely to take up the treatment. The strong positive effect for the control strongly shapes the experimental results but will go unrealized in the real world. 
In a similar vein, these estimates can aid causal explanation. Seeing the positive $ATE$ might lead us to infer that most of the $X=1, Y=1$ cases we observe in the world are ones in which $X=1$ caused $Y=1$. The observational $ATT$ estimates point in a very different direction, however, indicating that these are cases in which $X$ is least likely to have a positive effect and, thus, where $Y=1$ was most likely generated by some other cause.

We note that the results here relate to the LATE theorem [@angrist1995identification]. Imagine using data only on (a) the experimental group in control and (b) the observational group, some of whom are in treatment. We can conceptualize our design as one in which the observational group are "encouraged" to take up treatment, allowing us to estimate the effect for the "compliers" in the observational setting: those that self-select into treatment. Conversely, we could use data only on (a) the experimental group in treatment and (b) the observational group, some of whom are in control. This is a design in which the observational group are "encouraged" to take the control condition, allowing us to estimate the effect for the "compliers" in this group (those that self select into control). 

<!-- AJ: Can't we instead frame the second estimate as the estimate for the never takers? -->

## Transportation of findings across contexts

In some circumstances, we study the effect of $X$ on $Y$ in one context (a country, region, or time period, for instance) and then want to make inferences about these effects in another context (say, another country, region, or time period). We may face the challenge that effects are heterogeneous, and that conditions that vary across contexts may be related to treatment assignment, to outcomes, and to selection into the sample. For example, we might study the relationship between inequality and democratization in low-income countries and then want to know how those effects travel to middle-income settings. However, the level of income may have implications jointly for the level of inequality and for how likely inequality is to generate regime change, meaning that causal effects uncovered in the first context cannot be assumed to operate in the second context.

This is the problem studied by @pearl2014external. In particular,  @pearl2014external show for which nodes data are needed in order to "licence" external claims, given a model. 

We illustrate with a simple model in which an observable confounder has a different distribution across contexts. In the model drawn in Figure \@ref(extval), $Context$ determines the distribution of the confounder, $W$. We set a restriction such that the value of $W$ in Context 1 is never less than the value of $W$ in Context 0; our priors are otherwise flat over the remaining nodal types in the model.  


<!-- AJ: Macartan, can you interpret the parameter values for the sentence above? Not sure I know how to read the complements and decreasing statements correctly. -->

<!-- AJ: Note I've changed Case to Context everwhere, which I think is better ,more general. -->

```{r extval, echo = FALSE, fig.cap= "Extrapolation when confounders have different distributions across cases."}

model <- make_model("Context -> W  -> X -> Y <- W") %>%
  set_restrictions("W[Context = 1] < W[Context = 0]") %>%
  set_parameters(node = "X", statement = "X[W=1]>X[W=0]", parameters = 1/2)%>%
  set_parameters(node = "Y", statement = complements("W", "X", "Y"), parameters = .17) %>%
  set_parameters(node = "Y", statement = decreasing("X", "Y"), parameters = 0) 

plot(model)


```


```{r appev2, echo = FALSE}
if(do_diagnosis){
appev2 <-
  query_model(model,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("priors", "parameters"), expand_grid = TRUE) 
write_rds(appev2, "saved/10a_appev2.rds")
}

read_rds("saved/10a_appev2.rds")  %>% 
  kable(caption = "Priors and true values (parameters) for three estimand: the frequency of $W$, the effect of $X$ on $Y$, and the effect conditional on $W=1$")

```

<!-- AJ: Incidence is a confusing label. Can we make Incidence_W or something like that? -->

We show priors and true values for the estimands (drawn from the parameters) in Table \@ref(tab:appev2). We see that the incidence of $W=1$ is higher in Context 1 than in Context 0, both in our priors and in the "truth" posited by the assigned parameter values. The "true" $ATE$ of $X$ on $Y$ is also higher in Context 1, though this is not reflected in our priors. The average treatment effect conditional on $W$ is the same in both contexts, whether we work from priors or assigned parameter values, as it must be given the model. That is, in this model the $ATE$ varies conditional on $W$ --- and it varies conditional *only* on $W$.  

<!-- incidence in as well as the ATE of $X$ on $Y$ is larger in case 1 than in case 0 (in parameters, though not in priors). However the effect of $X$ on $Y$ conditional on $W$ is the same in both places.  -->

We now update the model using data from one context and then see if we can transport those findings to the other context. Specifically, we update using data on $X, Y,$ and $W$ from Context 0. We then use the updated beliefs to draw inferences about Context 1, using data *only* on $W$ from Context 1. In Table \@ref(appev3), we show our posteriors on the queries of interest as compared to the truth, given the parameter values. 




```{r appev3, echo = FALSE}
if(do_diagnosis){

  data <- make_data(model, n = 10000, 
                  vars = list(c("Context", "W"), c("X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Context == 0"))

  transport <- update_model(model, data)

  write_rds(query_model(transport,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport.rds")
}

q <- read_rds("saved/10a_transport.rds")

kable(q, caption = "Extrapolation when two sites differ on $W$ and $W$ is observable in both contexts")

```

<!-- AJ: Not sure we need the Incidence and CATE rows for this table. -->

By comparing the $ATE$ estimates using our posteriors and the estimates using the assigned parameter values, we see that we have done well in recovering the effects, *both* for the context we studied (i.e., in which we observed $X$ and $Y$) and for the context we did not study.  We can think of the learning here as akin to post-stratification. We have learned from observing $X, Y$, and $W$ in Context 0 how $X$'s effect depends on $W$. Then we use those updated beliefs when confronted with a new value of $W$ in Context 1 to form a belief about $X$'s effect in this second context. Of course, getting the right answer from this procedure depends, as always, on starting with the correct model.

We can also see, in Table \@ref(appev4), what would have happened if we had attempted to make the extrapolation to Context 1 without data on $W$ in that context.  We would get the wrong answer for Context 1, though we would also report greater posterior variance. The higher posterior variance here captures the fact that we know things could be different in Context 1, but we don't know in what way they are different. 

<!-- AJ: This last set of results seems wrong. The estimates using posteriors seem again to be very close to those using parameters, and variances are not higher than in previous table. Seems we're still using info on W in code below?  -->

<!-- Note that we get the CATE right since in the model this is assumed to be the same across cases. -->


```{r appev4, echo = FALSE}
if(do_diagnosis){

  data2 <- make_data(model, n = 10000, 
                  vars = list(c("Context"), c("W", "X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Context == 0"))

  transport2 <- update_model(model, data2)

  write_rds(query_model(transport2,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/10a_transport2.rds")
}

q2 <- read_rds("saved/10a_transport2.rds")

kable(q2, caption = "Extrapolation when two contexts differ on $W$ and $W$ is not observable in target context")

```


<!-- FLAG: ADD  A SITUATION WITH AN ARROW FROM Case to Y and WHO THAT WE DO NOT HAVE IDENTIFICATION -->

## Multilevel models, meta-analysis

<!-- AJ: Need a better section heading. If we have a good name for this approach, we can also use this when we make comparisons across approaches in next section. -->

<!-- AJ: Transition here needs some work, I think.  -->

A key idea in Bayesian meta-analysis is that when you analyze multiple studies together you learn not only about common processes that give rise to the different results seen in different sites, but you also learn more about each study from seeing the other studies.

A classic setup is provided in @gelman2013bayesian, in which we have access to estimates of effects and uncertainty in eight sites (schools), $(b_j, se_j)_{j \in \{1,2,\dots,8\}}$. To integrate learning across these studies we employ a  "hierarchical model"
that treats each $b_j$ as a draw from distribution $N(\beta_j, se_j)$ (and, in turn treats each  $\beta_j$ is a draw from distribution $N(\beta, \sigma)$). In that setup we want to learn about the superpopulation parameters  $\beta, \sigma$, but we also get to learn more about the study level effects $(\beta_j)_{j \in \{1,2,\dots,8\}}$ by studying them jointly.

<!-- AJ: Not clear to me that Gelman here is the right way to set up the analysis in this subsection. Doesn't Gelman's setup work more like our setup in the next section, with settings treated as being drawn stochastically from a common distribution at the superpop level? -->

<!-- We define a model in which $X$ points into $Y$, and in which $Setting$ is a third node that also points into $Y$. Thus $Setting$ can potentially moderate the effect of $X$ on $Y$. We observe an experiment that takes place in $Setting$ 0 and also in $Setting$ 1. Note that, in this setup, any differences in the distribution of effects of $X$ on $Y$ between the two settings can *only* arise because of $Setting$. That is, we are here assuming the same causal effects across settings conditional on $Setting$ itself. -->

<!-- We then consider the different conclusions we draw for the effect of $X$ on $Y$ in each setting, depending on whether we pool data across settings and on our priors over how much $Setting$ matters for $X$'s effects. -->

<!-- We design the simulation so that in $X$'s true causal effect is stronger in $Setting$ 0 than in $Setting$ 1. We then generate data from these true parameters for each setting, 100 cases for each, and run the analysis in different ways, with results displayed in Table \@ref(settingmatters). In the first two rows, we show $ATE$ posteriors from separate analyses of the data for each setting. We see that we recover an $ATE$ that is higher in $Setting 0$ than in $Setting 1$, as expected. -->

<!-- In the third row (Integrated flat priors), we use data from both settings to estimate an overall $ATE$. Moreover, we put equal weight on all possible $Y$ types, i.e., on all possible joint effects of $X$ and $Setting$, meaning that we provide substantial scope for $Setting$ to moderate $X$'s effects. As expected, the $ATE$ looking across both settings lies between the $ATE$ for each in the unpooled analyses.  -->

<!-- <!-- AJ: Why is the ATE in "Integrated, flat priors" not more centered between the ATE in the first two rows? It's much lower than I'd have expected. Does allowing interactions pull the ATE down in some way? --> 

<!-- In rows 4 and 5, we then estimate the $ATE$ in each setting separately, again using flat priors that allow for substantial interactions with setting. Differently from the first two rows, however, we are using data from both settings in estimating effects for each setting. Consider what happens we estimate the $ATE$ for $Setting$ 0: we are using our posterior over $Y$'s nodal type shares to answer this query. Of course, data from $Setting$ 0 itself provides what we might think of as the most context-*specific* information about the causal effect in that particular setitng. But data from $Setting$ 1, where we also observe $X$ and $Y$ values, *also* provides information about those shares, and this additional information drawn from $Setting$ 1 is reflected in the integrated estimate for $Setting$ 0 -- and vice versa for the integrated $Setting$ 1 estimate. With the pooling of data, we see that the $ATE$ for each setting.... Also note that the standard deviation of the posterior has shrunk, reflecting the fact that we have brought more data to bear on the question. -->

<!-- <!-- AJ: I don't know how to finish the above sentence because I don't get the lack of convergence -- the lower ATE for Setting 1. Especially since we *do* get the expected convergence with weak interactions priors. --> 

<!-- Finally, in the last three rows, we set priors such that the moderating effect of $Setting$ is believed to be weak. Here we see that the $ATE$'s for the two settings converge more strongly, reflecting the influence of our low-heterogeneity priors. Moreover, our uncertainty shrinks further here, reflecting the fact that -- if we believe that heterogeneity across settings is low -- then we also believe that data from one setting is *more* informative about the other setting: i.e., we get a bigger boost in statistical power from the pooling under these priors.  -->

<!-- <!-- AJ: Why is the overall $ATE$ higher here than with flat priors? Must be that the interactions themselves depress the ATE. Less interaction then means higher ATE?? --> 



<!-- ```{r settingmatters, echo=FALSE} -->


<!-- if(do_diagnosis){ -->

<!--   model_1 <- make_model("X -> Y") %>%  -->
<!--     set_parameters(node  = "Y", parameters = c(.1, .1, .7, .1)) -->
<!--   model_2 <- make_model("X -> Y") %>%  -->
<!--     set_parameters(node  = "Y", parameters = c(.1, .2, .6, .1)) -->
<!--   model_3 <- make_model("X -> Y <- Setting")  -->
<!--   model_4 <- model_3 %>% set_priors(2) %>%  -->
<!--     set_priors(statement = interacts("Setting", "X", "Y"), alphas  = .1) -->

<!--   df_1 <- simulate_data(model_1, 100) %>% mutate(Setting = 0) -->
<!--   df_2 <- simulate_data(model_2, 100) %>% mutate(Setting = 1) -->

<!--   model_1 <- update_model(model_1, df_1) -->
<!--   model_2 <- update_model(model_2, df_2) -->
<!--   model_3 <- update_model(model_3, rbind(df_1, df_2), iter = 8000) -->
<!--   model_4 <- update_model(model_4, rbind(df_1, df_2), iter = 8000) -->

<!--   q1 <- query_model(model_1, list(`Setting 0` = te("X","Y")), using = "posteriors") -->
<!--   q2 <- query_model(model_2, list(`Setting 1` = te("X","Y")), using = "posteriors") -->

<!--   q3_1 <- query_model(model_3, list(`Integrated (flat priors)` = te("X","Y")),  -->
<!--                    given = c(TRUE, "Setting==0", "Setting==1"),  -->
<!--                    expand_grid = TRUE, using = "posteriors") -->

<!--   q3_2 <- query_model(model_3, interacts("Setting", "X", "Y"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->

<!--   q4_1 <- query_model(model_4, list(`Integrated (low heterogeneity)` = te("X","Y")),  -->
<!--                    given = c(TRUE, "Setting==0", "Setting==1"),  -->
<!--                    expand_grid = TRUE, using = "posteriors") -->

<!--   q4_2 <- query_model(model_4, interacts("Setting", "X", "Y"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->

<!--   write_rds(list(q1, q2, q3_1, q3_2, q4_1, q4_2), "saved/10a_multilevel.rds") -->

<!-- } -->

<!--   multilevel <- read_rds("saved/10a_multilevel.rds") -->

<!--     rbind( -->
<!--     multilevel[[1]],  multilevel[[2]], -->
<!--   multilevel[[3]],  -->
<!--   multilevel[[5]]) %>% kable(caption = "Inferences from separate analyses and from integrated analysis (meta analysis) given (a) flat priors and (b) expectation of similar effects across studies", digits =2) -->
<!-- ``` -->

<!-- <!-- We see in both cases a drop in our estimates for effects in Setting 1 in both cases, relative to the single study case. Where weak heterogeneity is assumed we also see a rise in estimates for Setting 2.  -->

<!-- Further, we can use these same updated models to update specifically on the amount of heterogeneity across settings. We operationalize heterogeneity here as the share of units that *would* respond differently to $X$ if they were in a different setting. In Table XXXX, we compare our prior on this quantity to our posterior. Where we started out with flat priors --- allowing for a great deal of heterogeneity --- we see that the data bring these beliefs downward. This makes sense given that, under the true data-generating process, the effects in the two settings are only moderately different. Conversely, where we start with a very low prior on heterogeneity, the data lead us to believe there is *more* heterogeneity than we had initially believed (though, given the strength of the prior that we have used in this example, we can still see its formidable efect on the posterior). -->

<!-- ```{r, echo = FALSE} -->
<!--     multilevel[[4]] %>% kable(caption = "Interaction | Flat priors") -->
<!--   multilevel[[6]] %>% kable(caption = "Interaction | Expected homogeneity") -->

<!-- ``` -->

<!-- ```{r, include = FALSE} -->
<!-- # Approach 2 -->

<!-- model_5 <- make_model("X -> Y <- Setting -> W -> Y") %>%  -->
<!--   set_restrictions("Y[X=1, W=1] != Y[X=0, W=1]") -->
<!-- length(model_5$nodal_types$Y) -->


<!-- if(do_diagnosis){ -->

<!--   model_5 <- update_model(model_5, rbind(df_1, df_2), iter = 8000) -->

<!--   q5_1 <- query_model(model_5, list(`Integrated (Latent W)` = te("X","Y")),  -->
<!--                    given = c(TRUE, "Setting==0", "Setting==1"),  -->
<!--                    expand_grid = TRUE, using = "posteriors") -->

<!--   q5_2 <- query_model(model_5, interacts("Setting", "X", "Y"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->
<!--   q5_3 <- query_model(model_5, "W==1",  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->
<!--   q5_4 <- query_model(model_5, c("W[Setting=1]-W[Setting=0]"),  -->
<!--                    using = c("priors", "posteriors"), -->
<!--                    expand_grid = TRUE)  -->

<!--   write_rds(list(q5_1, q5_2, q5_3, q5_4), "saved/10a_multilevel_latent.rds") -->

<!-- } -->


<!-- ``` -->

<!-- ## Real multilevel -->

<!-- AJ: Need a better section heading -->

<!-- In the situations we have considered so far, we are learning from and about the particular contexts that we are studying. We are combining experimental data from one setting, for instance, with observational data from another. Or we are using updating from one context to draw an inference about another setting. Our inferences in these setups are limited strictly to the settings at hand, however.  -->

A hierarchical model like this allows us to think about the populations in our study sites as themselves drawn from a larger population ("superpopulation") of settings. And, crucially, it allows us in turn to use data in the study sites to learn about that broader superpopulation of settings.

<!-- AJ: Is "superpopulation" right here and below? -->

<!-- For instance, we might be studying the effect of an individual's relative location in the income scale on their preferences for democracy. We might collect data in a set of 10 countries and estimate the average causal effect of relative income in each of them. We can readily average inferences across these countries or study country-level moderators of the effect to explain differences in effects across the 10 countries. Yet these data from the 10 countries also contain information of a more general sort: they tell us something about the "superpopulation" of settings from which these 10 countries have been "drawn." -->

Although often used in the context of linear models with parameters for average causal effects, this logic works just as well with the kinds of causal models we have been using in this book. 

Let's review how our analytic setup has worked so far. At each node in a causal model, we conceptualize a given case as having a particular nodal type. The case's nodal type is drawn from a distribution of nodal types in the population of cases from which this case has been drawn. When we do process tracing, we consider that population-level distribution to be a set of fixed shares of nodal types in the population: say, for node $Y$, we might believe that half the cases in the population are $\lambda^Y_{01}$, a quarter are $\lambda^Y_{00}$, and a quarter are $\lambda^Y_{11}$. We then use data from the case to update on the case's nodal types (or on the combination of nodal types that correspond to some case-level query), given the population-level shares. 

When we engage in population-level inference, we begin with *uncertainty* about the population-level shares of types, and we express our prior beliefs about those shares as a Dirichlet *distribution*. So, for instance, our beliefs might be centered around a $\lambda^Y_{01}=0.5, \lambda^Y_{00}=0.25, \lambda^Y_{11}=0.25$ breakdown of shares in the population; and we also express some degree of uncertainty about what the breakdown is. Now, when we analyze data on some number of cases, we can update both on those cases' types and on our beliefs about the distribution of types in the population -- perhaps shifting toward a higher share of $\lambda^Y_{01}$'s (and with a change in the distribution's variance).

We can also, as in the last section, build a model in which there are multiple settings, possibly differing on some population-level characteristic. Fundamentally, however, the setup in the last section still involved population-level inference in that we were assuming that the *type shares* ($\lambda$ values) are the same across settings. The settings might differ in the value of a moderating variable, but they do not differ in the shares of cases that *would* respond in any given way to the moderator (and other causal conditions). The data allow us to update on what those common, cross-setting type proportions are.

When we build a hierarchical model, each case is still understood as being embedded within a population: our cases might be citizens, say, each embedded within a country. The key difference from population-level inference is that we now conceive of there being *multiple* populations -- say, multiple countries -- each drawn from a population of populations, or superpopulation. Now, we think of each population (country) as having its own set of type shares for each node. And we think of each country's type shares as being drawn from a Dirichlet distribution of type shares (for each node) that lives at the superpopulation level. Moreover, we are *uncertain* about what that distribution at the superpopulation level *is*. We uncertain around what type proportions the superpopulation-level distribution is centered, and we are uncertain about how dispersed this distribution is. While the distribution's central tendency will be related to the mean type shares for countries, its variance will determine the degree of *heterogeneity* across countries in their type shares. 

To summarize, in population-level inference, we express uncertainty about the population's type shares with a Dirichlet prior, at the population level, on which we update. In the hierarchical setting, we are uncertain both about the population-level type shares and the superpopulation Dirichlet from which each node's type shares are drawn. We express our uncertainty about each superpopulation Dirichlet by positing a prior distribution over the Dirichlet's alpha parameters.

Now, when we observe data on citizens within countries, we can update our beliefs about types fora the particular citizens we observe, about type shares in the population of citizens within each country that we study, *and* on the parameters of the Dirichlet distribution from which population shares have been drawn. In updating on the last of these, we are learning not just about the countries we observe but also about those we do not directly observe. 

<!-- AJ: Check that all of the above is right! -->

<!-- AJ: Need something here about how the approach to hierarchical models in Gelman et al is different from or related to how we're operationalizing them. -->

We illustrate with a simulation using a simple $X,Y$ model. We imagine that we are studying the $X \rightarrow Y$ relationship in `n` countries. Each country has a parameter distribution drawn from  common Dirichelets. We start off with flat priors over the alpha arguments of the superpopulation Dirichlets. 

<!-- AJ: I am confused by the statement that "Each country has a parameter distribution drawn from  common Dirichelets." I am generally struggling a bit with this bit still, and so may have this point wrong above. I am thinking: we have a set of alphas (Dirichlet parameters) for each *country* -- so we have a prior distribution of shares for each country. And the country-level alphas have been drawn from a common distribution of *alphas* at the superpop level. Is that right? Or is all the randomness at the superpop level, and we're now thinking of each country as having a specific set of shares, and our prior and posterior distributions are ONLY Dirichlet distributions at the superpop level? -->


<!-- MH: No we do not have alphas for each country; each country does and always did have a specific set of shares. This was true before but we represented our prior uncertainty over the specific shares using the Dirichlet. We *still* are uncertain over the shares but now we beliefe there *is* a a Dirichlet distribution from which these are drwan (before the Dirichlet just represented our uncertainty; now it represents the actual data generation process); we just don;t know what the Dirichlet is so we are are uncertain over it (before we did know what the Diriclet is since that just meant we knew what our priors are; now we don;t know what it is because it represents the true dgp). Easiest to think through this focussed only on the distribution of a binary variable X. (a) For each person X is either 0 or 1. (b) in a study the distribution of X is given by p (prob X = 1); in a single country study we want to figure out what p is and we have a prior distribution over possible values of p, using a Beta distribution with parameters a,b.  (c) in a multicountry study we think that there is a different p is every country and we now imagine these are drawn from a beta distribution with parameters a,b; but we don;t know what a and b are. So we put priors over a and b (inverse gamma priors). -->

<!-- AJ: OK, revised text accordingly. -->

We assign a particular true set of superpopulation parameter values that, for the analytic exercise, is treated as unknown and that we would like to recover. In this true world, the probability of assignment to $X=1$ is .4, and the average treatment effect is .1. Using these true parameter values, we simulate $X, Y$ data for $n=8$ countries.

```{r, comment = "", include = FALSE}
model   <- make_model("X->Y")
alpha_X <- c(6, 4)*1
alpha_Y <- c(.5, .4, .6, .5)*5

n <- 8

if(do_diagnosis){
  
  # Draw parameter values for each study
  lambda_X = rdirichlet(n, alpha_X)
  lambda_Y = rdirichlet(n, alpha_Y)
  
  # Draw data for each study
  data_events <- 
    lapply(1:n,
           function(j)
             make_data(model, n = 50,
                       parameters = c(lambda_X[j,], lambda_Y[j,])) %>%
             collapse_data(model))
  
  write_rds(list(lambda_X=lambda_X, lambda_Y=lambda_Y, data = data_events), "saved/10a_truth.rds")
  
  }

# Load
truth <- read_rds("saved/10a_truth.rds")
data_events <- truth$data
data_events

```

<!-- AJ: I think you'll need to do some elaboration for this step, Macartan. -->


```{r, include = FALSE}

# Updating

#We prepare `stan` data and update:


# Stan data is the same as for CausalQueries except we provide a data matrix instead of a vector
data_multilevel   <- CausalQueries:::prep_stan_data(model = model, data = data_events[[1]])
data_multilevel$Y <- sapply(data_events, function(j) j$count)
data_multilevel$n_studies <- ncol(data_multilevel$Y)


if(do_diagnosis)
  rstan::stan(file = "multilevel_dirichlet.stan",  data = data_multilevel, iter = 12000) %>%
  write_rds("saved/10a_meta_pooled.rds")

if(do_diagnosis)
  lapply(1:length(data_events), function(s)
    update_model(model, data = data_events[[s]], 
                 data_type = "compact", iter = 4000)$posterior_distribution) %>%
  write_rds("saved/10a_meta_unpooled.rds")

pooled   <-  read_rds("saved/10a_meta_pooled.rds")
unpooled <-  read_rds("saved/10a_meta_unpooled.rds")
```



```{r plotalphas, echo = FALSE, fig.cap="Joint distributions over alpha parameters."}

# Results

# We use a custom summary function to summarize some results from these models and provide results in a graph and table.

alphas <- rstan::extract(pooled, pars = "alpha")$alpha

par(mfrow = c(1,3))
    plot(alphas[,1], alphas[,2] )
    plot(alphas[,3], alphas[,6] )
    plot(alphas[,4], alphas[,5] )

```

```{r, include = FALSE}
my_summary <- function(pooled, unpooled, truth){
  
  meta    <-  rstan::extract(pooled, pars = "alpha")$alpha
  studies <-  rstan::extract(pooled, pars = "lambdas")$lambdas
  
  out1 <- data.frame(
    analysis = c("Pooled", "Pooled", "Pooled"),
    study = c("Meta", "Meta", "Meta"),
    estimand = c("ATE", "Concentration", "Prob(X=1)"),
    mean = c(
      mean((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      mean((meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      mean((meta[,2])/(meta[,1] + meta[,2]))),
   sd = c(
      sd((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      sd((meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      sd((meta[,2])/(meta[,1] + meta[,2]))))
  
  my_sum <- function(df, s=1, label = NA)  
    data.frame(
               analysis = c(label, label),
               study = c(paste(s),paste(s)),
               estimand = c("ATE", "Prob(X=1)"),
               mean =     c(mean(df[,5] - df[,4]), mean(df[,2])),
               sd =       c(sd(df[,5] - df[,4]), sd(df[,2])))
  
  out2 <- lapply(1:dim(studies)[3], function(s) my_sum(studies[,,s], s, label = "Pooled"))
  
  out3 <- lapply(1:length(data_events), function(s) unpooled[[s]] %>% my_sum(s, "Unpooled"))

  out4 <- data.frame(
    analysis = rep("truth", nrow(truth$lambda_X)),
    study = as.character(rep(1:nrow(truth$lambda_X), 2)),
    estimand = rep(c("ATE", "Prob(X=1)"), each = nrow(truth$lambda_X)),
    mean = c(truth$lambda_Y[,3] - truth$lambda_Y[,2], truth$lambda_X[,2]),
    sd = NA)
  
  
  bind_rows(list(out3, out2, out1, out4)) %>%
    arrange(estimand)
  }
```

In Figure \@ref(fig:plotalphas), we graph our posterior beliefs about the superpopulation parameters. We do this by plotting two alpha parameters against each other at a time. In the first panel, we plot the alphas for $X=0$ and $X=1$. In the next panel, we plot the alpha's corresponding to $c$ types against those corresponding to $d$ types. And in the third panel we plot the alpha's corresponding to $a$ types against those corresponding to $b$ types.

As we can see, each distribution falls roughly along a diagonal. Probability mass located further up the diagonal represents worlds in which the superpopulation Dirichlet distribution of type shares is relatively low in variance. Thus, the more that our posterior beliefs are concentrated toward a graph's northeast corner, the lower the heterogeneity we have inferred there to be in the relevant type shares across countries. Meanwhile, the dispersion of probability mass *away* from the diagonal represents greater posterior *uncertainty* about the heterogeneity across countries, arising from greater variance about the posterior distribution of the alphas.

We can think of a concentration parameter here that is operationalized as the sum of the $\alpha^j$ terms for a node, $j$, with a higher value representing lower overall heterogeneity. 

<!-- This would be 4 for a flat distribution ($(1,1,1,1)$ and should be a lot higher with this example.  -->


```{r, warning = FALSE, message = FALSE, echo = FALSE, fig.cap = "Estimates from individual analyses and from meta analysis", include = FALSE}
results <- my_summary(pooled, unpooled, truth)  
```

```{r metaplot, echo = FALSE, fig.cap="Updating on study level parameters from integrated analyses. "}
results %>% filter(estimand == "ATE") %>%
ggplot(aes(mean, study, color = analysis)) + geom_point()

```

In Figure \@ref(fig:metaplot) we turn to the causal query of interest and show a comparison of three $ATE$ estimates for each country: in blue, we show the unpooled estimate, or the estimate we get for each country using only data from that country; in red, we see the pooled estimates, or the estimate we get for each country using data from *all* countries to inform that country's parameter estimates; and in black, we plot the truth as posited for this simulation. As we can see, the pooled estimates are all closer to the center than the unpooled estimates: this is because we are effectively using data from all countries to discount extreme features of the data observed in a given country. Put differently, the pooled data serve the function of a prior when it comes to drawing inferences about a single country: our inference is a compromise between the data from that country and the beliefs we have formed from the pooled data. We can also see that, for most countries, the pooling helps: the regularization provided by the pooling gives us an estimate closer to the truth for most of the settings.

<!--chapter:end:11-fusion.Rmd-->

# (PART) Design Choices {-}


# Clue Selection as a Decision Problem {#clue}

```{r packagesused11, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
We draw out the implications of the causal model approach for clue-selection strategies. We introduce a tool for generating an optimal decision tree for clue selection given.
::::
<br>


We now mobilize the causal models framework for the purposes of making research-design choices. We start with the problem of clue-selection: determining which qualitative data to gather on a case. Evidently it makes sense to gather clues that have large probative value, but whether or not a given clue will have probative value depends on the model we are working with and the question we are asking. As we will also see, a clue's informativeness can depend on what other clues have already been collected. Finding out that the butler had no motive may be informative for the claim that he is innocent, but it may not be useful if we already know that he had no opportunity. 

We have already provided some insight into the problem in Chapter \@ref(pt), where we showed how relations of $d-$connection can tell us when a clue is *possibly* informative about a query. In this chapter, we go further to show how we can use our causal model to figure out which clues and clue-selection strategies are likely to be *most* informative about thequery we seek to address.

<!-- ## Core logic -->

<!-- To motivate our thinking about clue-selection, consider again our running example with the free press and government removal. We can use this toy example to see, intuitively, how researchers may have a choice among observations that could be informative, and how the informativeness of an observation can depend on what is already known. In Figure \@ref(fig:running), we showed  how one can use the structural equations to provide a set of conditional causal graphs that let one see easily what caused what at different values of the exogenous nodes $S$ and $X$. Each of these plots graphs a particular context. We can thus readily see what the answer to a particular query is in a particular context. Turning things around, we can also see, *given a query*, which nodes are informative about the probability that the query is true.^[With larger graphs, continuous variables, and more stochastic components, it may not be feasible to graph every possible context; but the strategy for inference remains the same.]   -->

<!-- <!-- FLAG: I'm having trouble getting the logical progression here. Seems somewhat broken in the sense that it's incomplete. Conceptually, we might think of informativeness situations as taking four possible forms of interest: a clue is always informative; never informative; informative only conditional on something else (obviously, what the something else is can vary); conditional only in the absence of something else. We seem to be covering always informative and conditionally uninformative. Seems odd not to also show always uninformative and informative only conditional on something else. But not sure if we can do this with this example. --> -->

<!-- For example, suppose that one can see that $Y=0$ but does not know the causal effect of $X$ on $Y$.  This is equivalent to saying that we know that we are in panel $A$, $B$, or $C$ but do not know which of these we are in. Would it be helpful to collect the clue $S$ if one has no other information? Defining the query in terms of $\theta$ values, the question becomes  whether $\Pr(\theta^S=\theta^S_{0}|X=0,Y=0)$: the difference between the contexts in the two panels is that $S=0$ when, and only when, $X=0$ causes $Y=0$. Given the independence of $X$ and $S$, $\Pr(\theta^S=\theta^S_{0}|X=0,Y=0) = \lambda^S_1$ (the simple assignment propensity).  Observing the value of $S$, then, *fully* answers the query.^[Graphically what is important is that $S$ is informative not because it is $d-$connected with $Y$, but because it is $d-$connected to the query variable---here, simply, to itself.] -->

<!-- We can also see instances in this example of  how existing data can make clues *uninformative*. Suppose that one wanted to know if $X$ causes $C$ in a case. As we can see from inspection of the panels, this query is equivalent to asking whether $S=1$ (as $X$ causes $C$ only in those two panels ($B$ and $D$) where $S=1$). Data on $R$ is unconditionally informative about this query as $R$ is $d-$connected to $S$. For example, $R=1$ implies $S=0$. However, if $C$ and $X$ are already known, then $R$ is no longer informative because $C$ and $X$ together $d-$separate $R$ from $S$.^[We can come to the same conclusion by reasoning with the graphs: if $X=0$ and $C=1$, we know we are in subfigure $A$ or $B$, and $X$ causes $C$ only in panel $B$. However, $R$ is of no help to us in distinguishing between the two contexts as it takes the same value in both graphs.] -->

<!-- The running example also lets us demonstrate how informative clues can be found in many different places in a graph.  -->

<!-- 1. **Informative spouses** Spouses---parents of the same child---can inform on one another. As we have seen in other examples, when an outcome has multiple causes, knowing the value of one of those causes helps assess the effect(s) of the other(s). For example, here, $S$ and $X$ are both parents of $C$; $S$ is thus informative for assessing whether $X$ causes $C$. Indeed this query, written in terms of roots, is simply $P(S)$:  $X$ causes $C$ if and only if $S=1$. Likewise, $S$ causes $C$ (negatively) if and only if $X=1$.  -->

<!-- 2. **Pre-treatment clues.** Did the absence of media reports on corruption ($R=0$) cause government survival ($Y=0$)? Look to the pre-treatment clue, $X$: $X=0$ is a smoking gun establishing that the absence of a report produced government survival. Or, substantively, if there were a free press, then a missing report would never be a cause of survival since it would occur only in the absence of corruption, which would itself be sufficient for survival. More broadly, this example illustrates how knowledge of selection into treatment can be informative about treatment effects.  -->

<!-- 3. **Post-outcome clues.** Suppose we observe the presence of a free press ($X=1$) and want to know if it caused a lack of corruption ($C=0$), but cannot observe the level of corruption directly. Observing $Y$---which occurs after the outcome---is informative here: if $X=1$, then $X$ causes $C$ (negatively) if and only if $Y=0$. When an outcome is not observed, a consequence of that outcome can be informative about its value and, thus, about the effect of an observed suspected cause.  -->

<!-- 4. **Mediators as clues**: We see a politically sensitive government ($S=1$) and its survival ($S=0$). Did the government survive because of its sensitivity to public opinion? Here, the mediation clue $C$ is helpful: a lack of corruption, $C=0$, is evidence of $S$'s negative effect on $Y$.  -->

<!-- And, of course, different clues can be  informative in different ways for different types of estimand. -->

<!-- Needed then is a systematic way for identifying what clues to look for for answering a given type of causal question, given what we already know---and perhaps, in what order to look for them. -->

## A model-informed approach to clue-selection

The representation of inference problems as one of querying a Bayesian model points to a relatively simple method for selecting the most informative clues for collection. Consider, first, a situation in which one can invest in collecting various forms of evidence on a case and wants to know the expected probative value of all possible collections of evidence that one could gather. 

We can assess alternative strategies through the following procedure:

1. Define the model. 
2. Define a query on the model.
3. Define a data strategy: a set of clues for which one might search (e.g., observe the value of $Y$).
4. Given prior data, figure out the probability of different realizations of the new data.
5. For each possible realization, calculate the posterior variance. 
5. Calculate the *expected* posterior variance for the data strategy by taking a weighted average of the variances arising from the different data realizations, with weights given by the probability of observing the data-realization in question.
6. Repeat steps 3-4 for different data strategies that we wish to compare.

This procedure then allows us to choose the clue strategy with the lowest expected posterior variance. 

A still more sophisticated strategy would, for multiple clues, take sequence into account: it would tell us which clues to search for later in the process given the realization of clues sought earlier. The path-dependence of clue selection arises from the possibility that the informativeness of a clue may depend on the value of other nodes in the model. A given clue $K_2$, for instance, may be informative if another clue $K_1$ has the value of 1 but not if it has the value 0. 

We provide tools for both of these approaches and illustrate them below for both the running example and the democracy application.

### Clue selection with a simple example

We first spell out how this works using a hypothetical example and then explore further using our democratization model.

Consider a model of government survival in office in which retaining office depends on not being perceived as corrupt by the public, graphed in Figure \@ref(fig:running). We take two conditions as being exogenous in this model. First, a country may or may not have a free press ($X$). Second, the country's government may or may not be sensitive to public opinion ($S$).^[Government sensitivity here can be thought of as government sophistication (does it take the actions of others into account when making decisions?) or as a matter of preferences (does it have a dominant strategy to engage in corruption?).] In terms of causal relations, we then allow that the government's decision about whether to engage in corruption ($C=1$) may depend on whether the government is sensitive to public opinion and whether there is a free press (that might reveal that corruption). Moreover, we allow that whether the press will report on the corruption ($R=1$) may depend on whether there is government corruption and whether the press is free. Finally, whether the government will be removed from office ($Y=1$) may depend on whether it has acted corruptly and whether this gets reported in the press. 

```{r running, echo = FALSE, fig.width = 11, fig.height = 9, fig.align="center", out.width = "70%", fig.cap = "The figure shows a simple causal model. $S$ and $X$ are stochastic, other nodes fully determined by their parents, as shown in bottom right panel.", fig.align="center", warning = FALSE, eval = FALSE}


x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)

names = c("S:\nSensitive\ngovernment\n\n", "\nX:\nFree Press", "C:\n Corruption", "R:\n Media report", "Y:\nGovernment\nreplaced")

hj_dag(x =  x,
       y = y,
       names = c(names),
       arcs = cbind( c(1,2,2, 3, 4, 3),
                     c(3,3,4, 5, 5, 4)),
       title = "Free Press and Government Survival",
       add_functions = 0,
       contraction = .15,
       #add_functions_text = "Structural Equations: Y = CR, R = CX, C = 1-XS",
       padding = .2)
```

To simplify the illustration we work with a highly restricted version of this model. At node $C$, we allow only for the possibility that corruption is always present ($\theta^C_{1111}$) and for the possibility that corruption is always present except when there is both a free press ($X=1$) and sensitivity to public opinion ($S=1$) ($\theta^C_{1110}$). At $R$, we allow only for $\theta^R_{0001}$ and $\theta^R_{0000}$: the possibility that there is reporting on corruption if and only if there is corruption and a free press, and the possibility that there is never reporting on corruption. Finally, at $Y$, we restrict to just one nodal type, $\theta^Y_{0001}$: the government will remain in office unless there is both corruption ($C=1$) and reporting on corruption ($R=1$).

To summarize the intuition, then, governments will fall only if there is both corruption and reporting on corruption. We are uncertain whether or not corruption is always present; but if corruption is ever absent, it can only be because there exists both a free press and a government that cares about public opinion. We are also uncertain whether or not media reporting on corruption is always absent; but if it is ever present, it can only be because there is both corruption and a free press. One implication is that governments that are sensitive to public opinion will never fall because they will always eschew corruption when a free press --- the only mechanism that can generate reporting on corruption --- is present. In turn, the presence of a free press can only matter for government survival if governments are *not* sensitive and thus do not strategically adjust their behavior in response to the risk of reporting.

This model is formally defined in `CausalQueries` as follows:

```{r ch11definerunningmodel1, warning = FALSE}
model <- 
  
  make_model("S -> C -> Y <- R <- X; X -> C -> R") %>%
  
  set_restrictions(labels = list(C = c("1110", "1111"), 
                                 R = c("0001", "0000"), 
                                 Y = c("0001")), 
                   keep = TRUE) 

```

```{r plot, echo = FALSE, include = FALSE}
plot(model)
```

Suppose now that our query is whether $X$ has a positive effect on $Y$. Using this model we can ask how likely different data realizations are and what we would infer about our query from each possible data realization, given existing data. We illustrate for a situation in which we already know that $Y=0$. 

```{r cinfers, echo = TRUE, message = FALSE, warning = FALSE, include = FALSE}
possible_inferences <-  
  
  conditional_inferences(model, 
                         query = list(COE = "(Y[X=1] > Y[X=0])"), 
                         given = "Y==0")

```

Application of the function `conditional_inferences`  produces a matrix with the results. We reproduce these as Table \@ref(tab:showstrats5xx). The first five columns of Table \@ref(tab:showstrats5xx) define the data realizations. The matrix includes all combinations of possible realized values for all available clue strategies given that we have already observed $Y=0$. A "0" or "1" represents the observed value for a node that we have chosen to observe while $NA$ under a node indicates that that node is not observed under the given strategy. Thus, for instance, in the first 5 rows we are collecting data on all nodes. In the next three rows, we have sought data on all nodes except $S$.

In the sixth column, we see the inference we would make from each data-realization: the posterior probability that $X$ has a positive effect on $Y$, given that $Y=0$. The final column indicates the probability of each data-realization, given the chosen strategy. (Note that, in calculating these probabilities, we are *not* conditioning on the prior observation, $Y=0$). 

<!-- FLAG: AJ: Let's add posterior variance to Table \@ref(tab:showstrats5xx) -->


```{r showstrats5xx, echo = FALSE, warning = FALSE}
# colnames(possible_inferences)[1:2] <- c("Prior var", "Var given W")

kable(possible_inferences[possible_inferences$prob >0, ], caption="Inferences given different data patterns. ", row.names = FALSE)
```

Each inference, under each data-realization, also has an associated posterior variance, or level of uncertainty. Thus, given the probability of each data-realization conditional on a given clue strategy, it is easy to assess the *expected* reduction in variance from a given clue strategy. We present these expected posterior variances for all possible clue strategies, given the prior observation of $Y$, in Table \@ref(tab:scxrylearning). 


```{r ch11ExpectedLearning, warning = FALSE, include = FALSE}

strats <- list("X", "S", "C", "R", c("X", "R"), c("X", "S"), c("X", "C"), c("C", "R"), c("C", "S"), c("S", "R"), c("X", "C", "S"), c("X", "C", "R"), c("X", "S", "R"), c("C", "S", "R"), c("X", "C", "S", "R"))

learning <- sapply(strats,  function(strategy) {

  expected_learning(model, 
                    query = list(COE = "(Y[X=1] > Y[X=0])"), 
                    strategy = strategy, 
                    given = "Y==0", 
                    parameters = NULL)
  })
```


```{r scxrylearning, echo = FALSE}
kable(t(learning), col.names = c("Strategy", "Given", "Prior belief", "Prior Uncertainty", "Expected Posterior Uncertainty"))
```
  

We operationalize higher levels of expected learning from a strategy as a greater expected reduction in variance upon observing the data. We can see a couple of patterns here. 

* By far the biggest gains in learning come from observing $X$. We can see this most readily by comparing the 1-clue strategies to one another. But in general, any strategy that includes observing $X$ always does substantially better than the comparable strategy that excludes $X$. The intuition here is fairly straightforward: if we want to know whether $Y=0$ was caused by $X=0$, and start out very uncertain about $X$'s value, we should expect to learn a good deal from figuring out whether $X$ is in fact equal to $0$.

* There are also considerable gains from observing $S$ or $C$ by themselves. Consider, first, why observing $S$ is informative. $S$ is potentially informative because it tells us something about whether $X$ can affect $Y$ by affecting $R$. Remember that a government is removed only if there is both corruption ($C=1$) and reporting on corruption ($R=1$). Moreover, there is only reporting on corruption (if ever) if $C=1$. Thus, for both of these reasons, $X$ can only have a positive effect on government removal (by causing reporting on corruption) if $C=1$: i.e., if there is corruption. And $S$ tells us something about what $C$'s value is likely to be if $X$ were set to 1. 

> Specifically, if we observe $S=0$, then we know for sure that $C=1$, regardless of $X$, since $C$ is always 1 when $S=0$ under both permitted nodal types for $C$. If $S=1$, on the other hand, there's a lower chance that $C$ would be equal to 1 if $X$ were set to 1: in one of $C$'s permitted nodal types, there is always corruption; but in the other type, sensitive governments avoid corruption when there is a free press, so $X$ moving to 1 would give us $C=0$. We have put equal prior probabilities on these two nodal types. Thus, if we observe $S=1$, we conclude that there is a lower probability that $C$ will take on the value necessary for $X$ to exert a positive effect on $Y$ than if we observe $S=0$.

> Why, then, is $C$ informative? If we observe $C=0$, then we know that $X$ must be equal to 1 since, under permitted nodal types for $C$, there is an absence of corruption *only* in the presence of a free press and sensitive governments. And if $X=1$ with $Y=0$, a positive effect is ruled out with certainty. If we observe $C=1$, then there remains some possibility that $X=0$ as well as some possibility $C$ would remain at 1 if $X$ were set to 1 (depending on $C$'s unknown nodal type), allowing $X$ to yield a positive effect on $Y$ through $R$.
 
* There are no gains from observing $R$ if $Y=0$. We can see why by looking at our table of data possibilities consistent with $Y=0$ (Table \@ref(showstrats5xx)). As we can see, there is no possibility of observing anything other than $R=0$ if we have already seen $Y=0$. We can see why by thinking, jointly, about how $Y$ is determined and how $R$ is determined. $Y$ can be 0 either because $C=0$ or $R=0$. So if $R$ were equal to $1$, this must mean that $C$ was $0$. However, a necessary condition for $R$ to be 1, under $R$'s permitted nodal types, is $C=1$ and $X=1$. In other words, the condition under which $R$ could be 1 is a condition under which $Y$ would not be 0. Thus, if we already know $Y=0$, we know $R=0$, and there is no gain from actually looking for $R$. 

* Once we decide to observe $X$, then the next-most informative clue to add to our research design is $S$: $X, S$ has the lowest expected posterior variance of any of the 2-clue strategies. And, in fact, there are no gains to adding $C$ to $X$, relative to observing $X$ by itself. 

> Let us develop the intuition underlying this result. 

> Imagine that we have already observed $X$'s value. If $X=1$, then (given $Y=0$), a positive effect is immediately ruled out with certainty, rendering any further observations of no value. If we observe $X=0$, however, then (under this causal model) we know for certain that $C=1$, simply because $C=1$ for both of $C$'s permitted nodal types when $X=0$ (there is always corruption when there is no free press). Thus, there is nothing to be gained by observing $C$. (We have already seen why there is nothing to be gained from observing $R$.)

> Why, we might still ask, are there possible gains to observing $S$ even if we're going to observe $X$? $S$ is informative because it tells us something about whether $X$ can affect $Y$ by affecting $R$. The potential gains from observing $S$ with $X$ arise from the possibility that we may see $X=0$ (since $X=1$ woudl decide the matter by itself). If $X=0$, then we still need to know whether $Y$ *would* be 1 if we changed $X$ to 1. As discussed above, *that* depends on whether $C$ would be $1$ if $X$ were set to 1, and (as, again, explained above) $S$ is informative on that matter.
 
* We see, further, in the table --- and it follows from the above logic --- that we cannot improve on an $X, S$ strategy by gathering more data. Thus, if the search for information is costly, looking only for $X$ and $S$ dominates all 3- and 4-clue strategies.

* Clues can be more informative jointly than separately, and the expected gains to observing one clue can depend on which other clues we plan to observe. To see this, notice that among the 1-clue strategies, observing $C$ by itself is slightly *more* informative than observing $S$ by itself. However, if we are planning to observe $X$, then the gains flip, and it is only $S$ that offers additional useful information. As we have discussed, observing $X$ makes observing $C$ uninformative while $S$ remains informative as a moderator of $X$'s effect.

We would add that the pattern here forms part of a broader point that we wish to emphasize in this chapter: while process tracing often focuses on examining steps along causal pathways, it will often be the case that we learn more from *moderators*, like $S$ in this model, than from mediators, like $C$ and $R$. We return to this point below. 


### Dependence on prior beliefs

As the foregoing discussion already suggests, optimal clue strategies can depend on our prior beliefs about causal relationships among the variables in the model.  We illustrate this point here, examining how evaluation of clue strategies shift as we relax restrictions on nodal types and set informative priors over nodal types.

**Relaxing restrictions.** In the analysis above, we allowed for just two (of 16 possible) nodal types at both $C$ and $R$, effectively expressing strong beliefs about how $C$'s and $R$'s values are determined. But what if we are less certain than this? 

Suppose that we are not sure that corruption can be prevented only through a combination of a free press and government sensitivity. We think it possible that government sensitivity itself might be sufficient: that $S$ might have a negative effect on $C$ regardless of $X$'s value. (Perhaps, for instance, there are means other than via a free press through which the public might learn of government corruption.) We allow for this causal possibility by expanding the set of kept nodal types for $C$ to include $\theta^C_{1010}$ in defining the model. 

```{r model, warning = FALSE}
model <- 
  
  make_model("S -> C -> Y <- R <- X; X -> C -> R") %>%
  
  set_restrictions(labels = list(C = c("1110", "1111", "1010"), 
                                 R = c("0001", "0000"), 
                                 Y = c("0001")), 
                   keep = TRUE) 
 
```


```{r cinfer2, echo = TRUE, message = FALSE, warning = FALSE, include = FALSE}
possible_inferences <-  
  
  conditional_inferences(model, 
                         query = list(COE = "(Y[X=1] > Y[X=0])"), 
                         given = "Y==0")

```


```{r ch11ExpectedLearning2, warning = FALSE, include = FALSE}

strats <- list("X", "S", "C", "R", c("X", "R"), c("X", "S"), c("X", "C"), c("C", "R"), c("C", "S"), c("S", "R"), c("X", "C", "S"), c("X", "C", "R"), c("X", "S", "R"), c("C", "S", "R"), c("X", "C", "S", "R"))

learning <- sapply(strats,  function(strategy) {

  expected_learning(model, 
                    query = list(COE = "(Y[X=1] > Y[X=0])"), 
                    strategy = strategy, 
                    given = "Y==0", 
                    parameters = NULL)
  })
```


```{r scxrylearning2, echo = FALSE}
kable(t(learning), col.names = c("Strategy", "Given", "Prior belief", "Prior Uncertainty", "Expected Posterior Uncertainty"))
```

The diagnosis of strategies under this adjusted set of beliefs, for the same query (whether $X$ has a positive effect on $Y$) and prior data ($Y=0$) as before, are displayed in Table \@ref(scxrylearning2).

We see that, among 1-clue strategies, observing $X$ is still the best choice. The best 2-clue strategy is also still $X, S$. Where things change most significantly, however, is among 3-clue strategies: now, we can do even better by additionally observing $C$. The reason is that, with greater uncertainty about its nodal types, $C$'s value is no longer known when $X=0$: it is now possible that $C=0$ when $X=0$ since we think it possible that $C$'s nodal type is $\theta^C_{1010}$. Since $C$'s value bears on whether $X$ can have an effect via $R$, we can thus in this situation potentially learn something by observing $C$.

We can also see $C$'s enhanced informational value throughout the table. Among 1-clue strategies, observing $C$ alone generates greater learning here than it does under the original setup. More strikingly, among 2-clue strategies we see that observing $C$ can now generate learning even if we have *already* observed $X$ (whereas there was no gain from strategy $X, C$ relative to $X$ under the original model). While $X, S$ is still a better strategy than $X, C$, the change in diagnosis could matter if, for instance, we cannot observe $S$ for some reason or if observing $S$ is much more costly than observing $C$.

Moreover, the expected variance reduction from observing $S$ is also greater under the new model, for 1- and 2-clue strategies. To see the informal intuition here, note that $S$ is potentially informative about $C$'s value as a parent of $C$. And we now believe (with the added nodal type for $C$) that there may be an additional way in which $S$ could matter for $C$, and thus provide information about its value. Moreover, since the added nodal type has $S$ exerting a negative effect on $C$ regardless of $X$'s value, $S$ can now be informative even if we have already observed $X=0$. 

Finally, we can see that nothing has changed in regard to $R$, about whose nodal types we have retained the same beliefs. It is still uniformly unprofitable to observe $R$ because we still know $R$'s value whenever $X=0$.

This exercise also suggests a further interesting principle of clue-selection: that potential informativeness rests on uncertainty about what we will find.


**Changing priors.** We can also see what happens when, rather than permitting new nodal types, we have informative beliefs about the prevalence of permitted types. We can provide a simple demonstration by expressing stronger prior beliefs about $S$'s nodal type. Suppose we believe most governments to be sensitive to public opinion. This would imply that we should put greater prior weight on $\theta^S_1$ than on $\theta^S_0$. We can do this by setting a higher $\alpha$ value corresponding to $S=1$, and telling `gbiqq` to set all paramater values (the $\lambda$'s for each nodal type) to the means of the prior distributions:

```{r ch11strategies_chunk_slowxx, warning = FALSE}

#Set priors for S with more weight on S=1. Should make S less informative -- the more informative value for S is expected to be less likely??

model_priors <-  model %>%
  
  set_priors(node = "S",
             statement = "S[]==1",
             alphas = 10) %>%
  
  set_parameters(type = "prior_mean")

```

These priors put roughly a 0.91 probability on $S=1$. 

```{r cinfer3, echo = TRUE, message = FALSE, warning = FALSE, include = FALSE}
possible_inferences <-  
  
  conditional_inferences(model_priors, 
                         query = list(COE = "(Y[X=1] > Y[X=0])"), 
                         given = "Y==0")

```


```{r ch11ExpectedLearning3, warning = FALSE, include = FALSE}

strats <- list("X", "S", "C", "R", c("X", "R"), c("X", "S"), c("X", "C"), c("C", "R"), c("C", "S"), c("S", "R"), c("X", "C", "S"), c("X", "C", "R"), c("X", "S", "R"), c("C", "S", "R"), c("X", "C", "S", "R"))

learning <- sapply(strats,  function(strategy) {

  expected_learning(model_priors, 
                    query = list(COE = "(Y[X=1] > Y[X=0])"), 
                    strategy = strategy, 
                    given = "Y==0", 
                    parameters = NULL)
  })
```


```{r scxrylearning3, echo = FALSE}
kable(t(learning), col.names = c("Strategy", "Given", "Prior belief", "Prior Uncertainty", "Expected Posterior Uncertainty"))
```

We see the results of this new set of diagnoses, with informative priors on $S$'s nodal types, in Table \@ref(scxrylearning3). Comparing with Table \@ref(scxrylearning), a number of features stand out. First is the much lower *prior* variance under the new model: having a strong prior belief about $S$'s value gives us stronger prior beliefs about whether $X$ could have caused $Y$ since such an effect *depends* on $S$'s value. A second striking difference is that searching for $S$ is expected to be much less informative in this model. The reason is simple: we now have a strong prior belief about what we are likely to find when we search for $S$. We *could* be surprised, but we should not *expect* to be. In the original model, in contrast, we were maximally uncertain about $S$'s value --- believing it had a 0.5 chance of being 1 --- and so there was much more to be gained by looking. 

While not shown here, we get essentially the same result if we flip our priors and put much greater weight on $S=0$, rather than on $S=1$. 


### Clue selection for the democratization model 

We now apply this approach to the model of democratization that we worked with in Chapters \ref(ptapp) and \ref(mixingapp).

We start by specifying the democratization model, with negative effects ruled out for $I \rightarrow M$, $M \rightarrow D$, and $P \rightarrow D$ and a positive direct effect ruled out for $I \rightarrow D$.

```{r model2, echo = FALSE, message = FALSE}

pimd <- make_model("I -> M -> D <- P; I -> D") %>% #Specify the DAG
  
         set_restrictions(c( 
           "(M[I=1] < M[I=0])", 
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))   #Exclude a set of negative-effect and positive-effect nodal types.

```

Now, let us assume that we have already observed high inequality and the outcome of democratization in a case, and we want to know whether high inequality caused democratization. The decision we confront is what combination of the other variables --- mobilization or international pressure --- we should collect data on: we could observe nothing further; observe $P$ only; observe $M$ only; or observe both $P$ and $M$. In Table \@ref(possible_outcomespimd_i1), we show all possible data realizations from all possible clue-selection strategies, the inferences we would draw from each realization, and the probability of that realization (not conditionining on $I=D=1$.)

```{r cinfer4, echo = FALSE}

inferences_pimd <-
  conditional_inferences(pimd, query = "D[I=1] > D[I=0]", given = "I==1 & D==1")

kable(inferences_pimd, caption = "\\label{possible_outcomespimd_i1} Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a positive effect on $D$) given that data realization.", digits = 3)

```

We show in Table \@ref(pimdlearn_i1d1) how we expect uncertainty to be reduced by different research designs. In this table, we show these reductions for the two kinds of cases in which democratization does occur. The first row displays the variance on our posterior belief about the effect of $I$ on $D$ before we observe anything at all. The next three rows show our expectations for looking for $P$ only; looking for $M$ only; and looking for both. The clearest message here is that, if we had to choose between clues, we should observe $P$: given our model (including our priors on the types), we reduce our uncertainty more by learning about an alternative cause than by learning about a mediator. 

We also see that the mediator is much more informative when the causal effect we are looking for is one that *could* have operated via the mediator, as compared to when the mediator is informative only as a moderator of the cause's direct effects.





```{r explearning, echo = FALSE, message = FALSE, warning = FALSE}

if(do_diagnosis){
ELpimd1 <-
  rbind(
expected_learning(pimd, query = "D[I=0] == 0", strategy = NULL,       given = "I==1 & D==1"),
expected_learning(pimd, query = "D[I=0] == 0", strategy = "P",        given = "I==1 & D==1"),
expected_learning(pimd, query = "D[I=0] == 0", strategy = "M",        given = "I==1 & D==1"),
expected_learning(pimd, query = "D[I=0] == 0", strategy = c("P","M"), given = "I==1 & D==1"))
write_rds(ELpimd1, "saved/11_ELpimd1.rds")
}

ELpimd1 <- read_rds("saved/11_ELpimd1.rds")

x_strategy <- c("None", "P", "M", "P and M")

kable(cbind(strategy = x_strategy, ELpimd1[,-1]), caption = "\\label{pimdlearn_i1d1} Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a positive effect on $D$?) given different clue seeking  stratgies for cases in which we have observed high inequality and democratization.", digits = 3)

```

We turn next to considering those cases with low inequality that democratized, asking whether democratization occurred because of a *negative* effect of inequality. The possible data realizations, resulting inferences, and data probabilities are shown in Table \@ref(possible_outcomespimd_i0), while the expected learning estimates for each clue strategy are given in Table \@ref(pimdlearn_i0d1). The pattern here is similar, though somewhat starker: substantially greater gains to observing the moderator, $P$, than the mediator $M$. The gains to observing $M$ here are very small indeed. We can already see from comparing the relevant rows in the data-possibilities table how little our posterior beliefs shift depending on $M$'s realized value. $M$ is far less informative for assessing $I$'s causal effect for an $I=0, D=1$ case than for a $I=1, D=1$ case. The reason is that, in the former situation, we are looking for a positive effect while in the latter situation, we are looking for a negative effects; but only positive effects can operate through the mobilization pathway under the model restrictions. Thus, $M$ is uninformative as a mediator of $I$'s effect in an $I=0, D=1$ (though it is informative as a moderator for such a case, but less so).

```{r cinfer5, echo = FALSE}

inferences2_pimd <-
  conditional_inferences(pimd, query = "D[I=1] < D[I=0]", given = "I==0 & D==1")

kable(inferences2_pimd, caption = "\\label{possible_outcomespimd_i0} Table shows possible data patterns for P and M given I = 0 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a negative effect on $D$) given that data realization.", digits = 3)

```



```{r pimdlearn, echo = FALSE, message = FALSE, warning = FALSE}

if(do_diagnosis){
ELpimd2 <-
  rbind(
expected_learning(pimd, query = "D[I=1] == 0", strategy = NULL,       given = "I==0 & D==1"),
expected_learning(pimd, query = "D[I=1] == 0", strategy = "P",        given = "I==0 & D==1"),
expected_learning(pimd, query = "D[I=1] == 0", strategy = "M",        given = "I==0 & D==1"),
expected_learning(pimd, query = "D[I=1] == 0", strategy = c("P","M"), given = "I==0 & D==1"))
write_rds(ELpimd2, "saved/11_ELpimd2.rds")
}
ELpimd2 <- read_rds("saved/11_ELpimd2.rds")

x_strategy <- c("None", "P", "M", "P and M")

kable(cbind(strategy = x_strategy, ELpimd2[,-1]), caption = "\\label{pimdlearn_i0d1}Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a negative effect on $D$?) given different  clue seeking  stratgies for cases in which we have observed low inequality and democratization.", digits = 3)

```



<!-- ```{r, echo = FALSE, message = FALSE} -->

<!-- pimd_confound <- make_model("I -> M -> D <- P; I -> D", add_priors = FALSE) %>% #Specify the DAG -->

<!--          set_restrictions(c(  -->
<!--            "(M[I=1] < M[I=0])",  -->
<!--            "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))  %>% #Exclude a set of negative-effect and positive-effect nodal types. -->

<!--             set_confound(list(I = "(M[I=1] == 1) & (M[I=0] == 0)"))  #Allow I to have a distinct conditional distribution when M's nodal type is \theta^M_{01}.  -->

<!-- new_parameters <-pimd_confound$parameters  -->

<!-- new_parameters[1:2] <- c(.7, .3)  -->

<!-- pimd_confound <- set_parameters(pimd_confound, new_parameters)  -->

<!--     ``` -->

<!-- In Table \@ref(possible_outcomespimd_i1con), we can see the inferences we would derive (about $I$'s positive effect on $D$) from the possible data realizations for $I=1, D=1$ cases. Comparing with the parallel set of inferences for the model without confounding (Table \@ref(possible_outcomespimd_i1)), the key difference is that all non-zero priors and posteriors on the query are lower in the presence of confounding. The reason is straightforward. The confounding built into this model makes $I$'s value additionally informative about the causal effect of $I$ on $M$ since we have expressed a belief that $I$ is more likely to be $0$ where $I$ has a positive effect on $M$. Moreover, $I$ must have a positive effect on $M$ in order for $I$ to have a positive effect on $D$. Thus, in the model with confounding, $I=1$ is already a clue that speaks against a positive $I \rightarrow D$ effect, whereas $I=1$ is not informative in this same way in the model without confounding.   -->


<!-- ```{r, echo = FALSE} -->

<!-- inferences_pimdcon <- -->
<!--   conditional_inferences(pimd_confound, query = "D[I=1] > D[I=0]", given = "I==1 & D==1") -->

<!-- kable(inferences_pimdcon, caption = "\\label{possible_outcomespimd_i1con} Table shows possible data patterns for P and M given I = 1 and D = 1, and assuming confounding, together with the probability of observing each data realization given data is sought on a variable and the posterioron the query (does $I$ have a positive effect on $D$) given that data realization.", digits = 3) -->

<!-- ``` -->
<!-- In Table \@ref(pimdlearn_i1d1con), we see the expected learning results for the $I=1, D=1$ cases, given a model with confounding, which we can compare to the comparable results in Table \@ref(pimdlearn_i1d1), for the model without confounding. Differences are modest. We are in general somewhat less uncertain in the model with confounding, regardless of strategy, which likely is an artifact of the fact that our posterior means are closer to $0$. Likewise, the scope for uncertainty-reduction is lower in the new model. In addition, however, it looks like the difference in expected learning between observing $P$ and observing $M$ is slightly smaller in the model with confounding. -->


<!-- ```{r, echo = FALSE, message = FALSE, warning = FALSE} -->

<!-- ELpimdconfound1 <- -->
<!--   rbind( -->
<!-- expected_learning(pimd_confound, query = "D[I=0] == 0", strategy = NULL,       given = "I==1 & D==1"), -->
<!-- expected_learning(pimd_confound, query = "D[I=0] == 0", strategy = "P",        given = "I==1 & D==1"), -->
<!-- expected_learning(pimd_confound, query = "D[I=0] == 0", strategy = "M",        given = "I==1 & D==1"), -->
<!-- expected_learning(pimd_confound, query = "D[I=0] == 0", strategy = c("P","M"), given = "I==1 & D==1")) -->

<!-- x_strategy <- c("None", "P", "M", "P and M") -->

<!-- kable(cbind(strategy = x_strategy, ELpimdconfound1[,-1]), caption = "\\label{pimdlearn_i1d1con}Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a positive effect on $D$?) given different  clue seeking  stratgies for cases in which we have observed high inequality and democratization, with confounding.", digits = 3) -->

<!-- ``` -->


<!-- ```{r, echo = FALSE} -->

<!-- inferences2_pimdcon <- -->
<!--   conditional_inferences(pimd_confound, query = "D[I=1] < D[I=0]", given = "I==0 & D==1") -->

<!-- kable(inferences2_pimdcon, caption = "\\label{possible_outcomespimd_i0con} Table shows possible data patterns for P and M given I = 0 and D = 1, assuming confounding, together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a negative effect on $D$) given that data realization.", digits = 3) -->

<!-- ``` -->



<!-- ```{r, echo = FALSE, message = FALSE, warning = FALSE} -->

<!-- ELpimdconfound2 <- -->
<!--   rbind( -->
<!-- expected_learning(pimd_confound, query = "D[I=1] == 0", strategy = NULL,       given = "I==0 & D==1"), -->
<!-- expected_learning(pimd_confound, query = "D[I=1] == 0", strategy = "P",        given = "I==0 & D==1"), -->
<!-- expected_learning(pimd_confound, query = "D[I=1] == 0", strategy = "M",        given = "I==0 & D==1"), -->
<!-- expected_learning(pimd_confound, query = "D[I=1] == 0", strategy = c("P","M"), given = "I==0 & D==1")) -->

<!-- x_strategy <- c("None", "P", "M", "P and M") -->

<!-- kable(cbind(strategy = x_strategy, ELpimdconfound2[,-1]), caption = "\\label{pimdlearn_i0d1con}Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a negative effect on $D$?) given different  clue seeking  stratgies for cases in which we have observed low inequality and democratization, with confounding.", digits = 3) -->

<!-- ``` -->

Now, let us see what happens as we revise the model, making it less restrictive. We do this, first, by allowing for confounding between two nodes in the model, international pressure and democratization. In particular, we allow for the possibility that, in order to generate a perception of foreign-policy influence and success, other states may target democratization pressure on autocratic regimes that are likely to democratize in the presence of pressure. This includes regimes that will democratize *only* if pressured as well as those that will democratize in the presence of pressure but where pressure itself was not a cause. We use the `set_confound` function to define distinct parameters for $P$'s nodal type when this is the case. The confound condition here is extremely easy to define: it is simply all unit types in which $D=1$ when $P=1$ ((D[P=1] == 1)). This includes all unit types (combinations of nodal types) that generate democratization in the presence of international pressure. 

Having set the confound condition, we can then express beliefs (parameter values) that $P=1$ is more common relative to $P=0$ when the condition is met than otherwise.^[It will be recalled that, in single-case inference we must express beliefs about population-level shares of nodal types. This includes expressing beliefs about the parameters defining the confounding.] We keep all other parameter values flat across the nodal types that are not excluded.

```{r pimdconfound, echo = FALSE, message = FALSE}

pimd_confound2 <- make_model("I -> M -> D <- P; I -> D") %>% #Specify the DAG
  
  set_restrictions(c( 
    "(M[I=1] < M[I=0])", 
    "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))  %>% #Exclude a set of negative-effect and positive-effect nodal types.
  
set_confound(list(P = "(D[P=1] == 1)")) %>%

set_priors(alphas = c(90), statement = "P[]==1", confound = list(P = "(D[P=1] == 1)")) %>%

set_parameters(type = "prior_mean")  


#new_parameters <-pimd_confound2$parameters 

#new_parameters[1:2] <- c(.05, .95) 

#pimd_confound2 <- set_parameters(pimd_confound2, new_parameters)  

```



```{r possoutpimdconf, echo = FALSE}

inferences_pimdconf <-
  conditional_inferences(pimd_confound2, query = "D[I=1] > D[I=0]", given = "I==1 & D==1")

kable(inferences_pimdconf, caption = "\\label{possible_outcomespimd_conf} Table shows possible data patterns for P and M given I = 1 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior on the query (does $I$ have a positive effect on $D$) given that data realization, with confounding involving $P$.", digits = 4)

```

```{r pimdlearni1d1con, echo = FALSE, message = FALSE, warning = FALSE}
ELpimdconfound4 <-
  rbind(
    expected_learning(pimd_confound2, query = "D[I=0] == 0", strategy = NULL,       given = "I==1 & D==1"),
    expected_learning(pimd_confound2, query = "D[I=0] == 0", strategy = "P",        given = "I==1 & D==1"),
    expected_learning(pimd_confound2, query = "D[I=0] == 0", strategy = "M",        given = "I==1 & D==1"),
    expected_learning(pimd_confound2, query = "D[I=0] == 0", strategy = c("P","M"), given = "I==1 & D==1"))

x_strategy <- c("None", "P", "M", "P and M")

kable(cbind(strategy = x_strategy, ELpimdconfound4[,-1]), caption = "\\label{pimdlearn_i1d1con}Prior estimand, prior variances and expected posterior variances for the query (does $I$ have a positive effect on $D$?) given different  clue seeking  stratgies for cases in which we have observed high inequality and democratization, with confounding.", digits = 4)
```

We display the inferences we *would* draw from different clue strategies and data realizations in Table \@ref(possible_outcomespimd_conf); and we show the resulting diagnoses of clue strategies, for the situation in which we have already observed $I=1$ and $D=1$, in Table \ref@(tab:pimdlearn_i1d1con). As we can see from the latter table, the presence of a confound involving $P$ reduces this clue's relative expected contribution to learning, making it now *less* informative in expectation than $M$. 

Working backwards, we can readily identify the reason for this in Table \@ref(tab:possible_outcomespimd_conf). We see here that observing $P=1$ moves our beliefs very little off of our prior of $0.091$ because, given the confounding, we already strongly expect to see $P=1$ in a case that democratized; actually observing $P=1$ contains only a small amount of new information. Our beliefs over the query change a great deal if we observe $P=0$, since the absence of pressure makes it much more likely that democratization occurred because of high inequality. In fact, $P$'s value is far more consequential than $M$'s. However, we can also see from the last column of the table that the most impactful realization of $P$'s value is also extremely unlikely under the model (given $I=D=1$). Thus, $P$ *can* be highly informative under this model, but it is very unlikely to be.

We turn next to examining clue strategies for a different kind of query. So far we have concerned ourselves with queries about causal effects, but we now examine a query in which we care about the *pathway* through which an effect occurs. We need to adjust the model to allow for multiple pathways since, under the restrictions we have been using so far, positive effects of inequality can run only indirectly and negative effects can only run directly. We now remove the restriction that excluded a negative effect of $I$ on $M$ and instead specify a prior belief that such a negative effect is less likely than $M$'s other nodal types. 


```{r model3, echo = FALSE, message = FALSE}

pimd_priors <- make_model("I -> M -> D <- P; I -> D") %>% #Specify the DAG
  
         set_restrictions(c(
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))  %>% #Exclude a set of negative-effect and positive-effect nodal types. Here do NOT exclude negative effects of I on M

#         set_priors(alphas = list(M = c(`(M[I=0] > M[I=1])` = 0.5)))  #Set less weight on negative effect of I on M

           set_parameters(node = "M", statement = "(M[I=0] > M[I=1])", parameters = 0.125)  #Set less weight on negative effect of I on M



```

To isolate how the query matters from how the restrictions matter, we first diagnose clue strategies for a causal-effect query under this new model: given $I=0, D=1$, did $I$ have a negative effect on $D$? Inferences conditional on data-realizations and clue strategies are displayed in Table \@ref(tab:possible_outcomespimd_priors_effect) and expected posterior variances in Table \@ref(tab:pimdlearn_priors_effect). 

Starting with Table \@(tab:possible_outcomespimd_priors_effect), a comparison with the parallel results for the model that excludes negative $I \rightarrow M$ effects -- Table \@ref(tab:possible_outcomespimd_i0) -- is informative. Similar to what we saw in Chapter \@ref(pt_app), the inferences we draw from observing $M$'s values *flip* in direction when we allow for negative effects of $I$ on $M$. When such effects were excluded, $M$ was informative only as a moderator of $I$'s direct negative effect on $D$. An observation of $M=1$ counted as evidence against $I=0$ being the cause of $D=1$ since $M=1$ could be the cause (given that $M$ could have a positive effect on $D$); $M=0$ counted, by a similar logic, as evidence in favor of $I$'s negative effect. Once we relax the restriction and allow negative effects of $I$ on $M$, $M$ is additionally informative as a *mediator* along a second pathway through which $I$ can have a negative effect on $M$. Now, observing $M=0$ cuts in two directions: on the one hand, it rules out a negative effect of $I$ running through $M$ (informativeness as mediator); on the other hand, it makes it more likely that $I=0$ caused $D=1$ indirectly (informativeness as moderator). We can see that, in this situation, the information we get from $M$ as a mediator overwhelms that which we get from $M$ as a moderator since our posterior on $I$'s causal effect now moves *downwards* if we observe $M=0$. $M=1$, of course, cuts both ways as well by a parallel logic, but with the net effect being an upward shift in our posterior on the causal effect. 

So how informative overall is $M$ as compared to $P$? In the model allowing negative $I \rightarrow M$ effects, we are now learning in two ways from $M$ rather than one; so our intuition might be that $M$ has become more informative than it was in our original model. In Table \@ref(tab:pimdlearn_priors_effect), we see that this is not at all the case! Much as in Table \@ref(tab:pimdlearn_i0d1), for the same query and given data but under our original model, we still see very little---indeed, slightly less---reduction in expected posterior variance from the search for $M$. $M$ may be informative as both moderator and mediator in the new model, but what we learn from the mediation in effect *undoes* some of the learning from the moderation by pushing our inferences in the opposing direction.


```{r tableinferencespriors_effect, echo = FALSE}

inferences_priors_effect <-
  conditional_inferences(pimd_priors, query = "D[I=1] < D[I=0]", given = "I==0 & D==1")

kable(inferences_priors_effect, caption = "\\label{possible_outcomespimd_priors_effect} Table shows possible data patterns for P and M given I = 0 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization for the query: does I have a negative effect on D?", digits = 3)

```

```{r pimdlearnpriorseffect, echo = FALSE, message = FALSE, warning = FALSE}
ELpimd_priors_effect <-
  rbind(
    expected_learning(pimd_priors, query = "D[I=1] < D[I=0]", strategy = NULL,       given = "I==0 & D==1"),
    expected_learning(pimd_priors, query = "D[I=1] < D[I=0]", strategy = "P",        given = "I==0 & D==1"),
    expected_learning(pimd_priors, query = "D[I=1] < D[I=0]", strategy = "M",        given = "I==0 & D==1"),
    expected_learning(pimd_priors, query = "D[I=1] < D[I=0]", strategy = c("P","M"), given = "I==0 & D==1"))

x_strategy <- c("None", "P", "M", "P and M")

kable(cbind(strategy = x_strategy, ELpimd_priors_effect[,-1]), caption = "Prior estimand, prior variances and expected posterior variances for the query (does I have a negative effect on D that is mediated by M?) given different  clue-seeking  stratgies for cases in which we have observed low inequality and democratization.", digits = 4)
```

Now, let's see how things look when we are interested not in $I$'s causal effect on $D$, but in whether it had an effect via particular pathway. Specifically, given $I=0$ and $D=1$, did $I$ have a negative effect on $D$ that was mediated by $M$? Following our discussion of mediation in Chapter \@ref(questions), we define the query as asking whether the following are true:

1 Does $I$ have a negative effect on $M$? (in code, $M[I=1] < M[I=0]$)
2 Does the change in $M$ resulting from a change from $I=1$ to $I=0$ cause a change in $D$ from $0$ to $1$? ($(D[M=M[I=0]] > D[M=M[I=1]])$)
3 Does $I$'s effect on $D$ *depend* on $I$'s effect on $M$? In other words, would we still get the $I \rightarrow D$ effect if $M$ were fixed at the value that it takes on when $I=1$? ($(D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])$)

Inferences conditional on data strategies and realizations are displayed in Table \@ref(possible_outcomespimd_path). We saw in Table \@ref(tab:possible_outcomespimd_priors_effect) that seeing $M=0$ slightly reduces our confidence that there was a negative effect of $I$ on $D$. However, we see now that observing $M=0$ entirely *eliminates* the possibility that this effect was mediated by $M$. We also see that the data realization under which we update most strongly in favor of a negative causal effect--$P=0, M=1$--is also the realization most supportive of a belief in a mediated negative effect. 

Turning to expected learning from alternative strategies, shown in Table \@ref(pimdlearn_path), we can see clearly -- by comparison to Table \ref@(tab:pimdlearn_priors_effect) -- how optimal clue strategies depend on the query of interest. Whereas $M$ is only slight informative about $I$'s causal effect--with $P$ the more informative clue--we expect to learn much more from $M$ about the pathway-specific query, and $M$ is far more informative than $P$ for this query.


```{r possibleoutcomespimdpath, echo = FALSE}

inferences_pimdpath <-
  conditional_inferences(pimd_priors, query = "(M[I=1] < M[I=0]) & 
                     (D[M=M[I=0]] > D[M=M[I=1]]) & 
                     (D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])", given = "I==0 & D==1")

kable(inferences_pimdpath, caption = "\\label{possible_outcomespimd_path} Table shows possible data patterns for P and M given I = 0 and D = 1 together with the probability of observing each data realization given data is sought on a variable and the posterior given that data realization for the pathway query: does I have a negative effect on D that is mediated by M?", digits = 3)

```

```{r pimdlearnpath, echo = FALSE, message = FALSE, warning = FALSE}
ELpimd_path <-
  rbind(
    expected_learning(pimd_priors, query = "(M[I=1] < M[I=0]) & 
                     (D[M=M[I=0]] > D[M=M[I=1]]) & 
                     (D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])", strategy = NULL,       given = "I==0 & D==1"),
    expected_learning(pimd_priors, query = "(M[I=1] < M[I=0]) & 
                     (D[M=M[I=0]] > D[M=M[I=1]]) & 
                     (D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])", strategy = "P",        given = "I==0 & D==1"),
    expected_learning(pimd_priors, query = "(M[I=1] < M[I=0]) & 
                     (D[M=M[I=0]] > D[M=M[I=1]]) & 
                     (D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])", strategy = "M",        given = "I==0 & D==1"),
    expected_learning(pimd_priors, query = "(M[I=1] < M[I=0]) & 
                     (D[M=M[I=0]] > D[M=M[I=1]]) & 
                     (D[I=1, M=M[I=1]] == D[I=0, M=M[I=1]])", strategy = c("P","M"), given = "I==0 & D==1"))

x_strategy <- c("None", "P", "M", "P and M")

kable(cbind(strategy = x_strategy, ELpimd_path[,-1]), caption = "\\label{pimdlearn_path}Prior estimand, prior variances and expected posterior variances for the query (does I have a negative effect on D that is mediated by M?) given different  clue-seeking  stratgies for cases in which we have observed low inequality and democratization.", digits = 4)
```

<!-- Now, what if we already know that $I$ has a negative effect on $D$ and we want to know via which pathway that effect operates---the direct or the indirect?  -->


<!-- ```{r model6, echo = FALSE, message = FALSE, eval = FALSE} -->

<!-- pimd_priors <- make_model("I -> M -> D <- P; I -> D") %>% #Specify the DAG -->

<!--          set_restrictions(c( -->
<!--            "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))  %>% #Exclude a set of negative-effect and positive-effect nodal types. Here do NOT exclude negative effects of I on M -->

<!--          set_priors(alphas = list(M = c(`(M[I=0] > M[I=1])` = 0.5))) %>% #Set less weight on negative effect of I on M -->

<!-- ``` -->



<!-- FLAG: actual cause and notable cause ?? -->

## Dynamic Strategies

The clue-collection strategies described above assume that researchers select the full set of clues to be gathered in advance and do not alter their strategies as they go along. However, the expected informativeness of a given clue may depend on the values of *other* clues that we could observe first. Thus, if we have the flexibility to adjust clue-selection as we observe data, then we select an optimal strategy in a dynamic sense, taking into account earlier observations in selecting later ones. 

Given $n$ nodes, a dynamic data collection strategy will be of the form:
$$\sigma = \{K_1, (K_2|K_1 = 1), (K_2|K_1 = 0), (K_3|K_1=1, K_2 =0)\dots\}$$

where each $K_j$ is en element of the nodes on the graph, or is the empty set. Thus, we start with observing $K_1$; then, whether we choose to observe $K_2$ depends on the value of $K_1$; whether we choose to observe $K_3$ depends on the value of $K_1$ and (if we observed it) $K_2$; and so on. A strategy *vector* specifies a series of conditional clue-search actions: it identifies the first clue sought and then which clues are sought conditional on the realization of all prior clues sought.  

Each possible strategy has an associated expected reduction in variance. We can also build in an expected cost associated with each clue, allowing us to treat clue-selection as a problem of optimizing over informativeness and cost. 

Let's illustrate with the corruption example, using the more restricted version of the model (in which we allow for only two nodal types at $C$ and at $R$ and only one nodal type at $Y$). Imagine a situation in which we know that $Y=0$ and are interested in whether $Y=0$ because of $S$ (the value of which we have not observed). We consider strategies in which we first seek information on one node and then, conditional on what we find, look or do not look for data on one other node. With five nodes, one already known, there are $4 \times 4^2$ strategies of this form.

To consider the simplest subset of these, consider the strategies that involve looking first at $S$. If we learn here that the government was not sophisticated, then this answers the query in the negative: the government could not have remained in power because it was sophisticated. If we learn that the government *was* sophisticated, then it might have been the cause, but we do not yet know. Our next move might be to examine whether there was a free press: learning that there was or was not a free press will settle the matter since sophistication will have caused the government's survival if and only if there is a free press.


```{r ch11definerunningmodel3, warning = FALSE}

# Make model
model <-
    make_model("S -> C -> Y <- R <- X; X -> C -> R") %>%
       set_restrictions(
       labels = list(C = c("1110", "1111"),
                     R = c("0001", "0000"),
                     Y = c("0001")),
       keep = TRUE)

```


```{r runexam, echo = FALSE}
# Implement for running examples


# Calculate expected n and variance
if(do_diagnosis){


# Generate strategies list
strategies <- two_step_strategies(model)

# Evaluate list of strategies
results <-
  strategy_evaluation(
    model,
    strategies  = strategies,
    query = "(Y[S=1] != Y[S=0])",
    given = "Y==0",
    prices = c(1, 1.4, 2.5, .5, .7))

    write_rds(results, "saved/ch11_tradeoffs.rds")
  }

results <- read_rds("saved/ch11_tradeoffs.rds")


# Plot results
# Which labels to print?
extremes_for_var <- function(D)
sapply(1:nrow(D), function(i) {
j <- D[i, ]
S <- filter(D, (expected_variance - j["expected_variance"][[1]])^2 < .000001) %>%
mutate(dominated = expected_costs < j["expected_costs"][[1]],
 undominated = expected_costs > j["expected_costs"][[1]])
(!any(S$dominated)) 
# | (!any(S$undominated)) 
})    

show <- extremes_for_var(results)

with(results, {
  plot(expected_variance, expected_costs, type = "n", xlab = "Expected Variance", ylab = "Expected Costs", xlim = c(.035, .075), main = "Variance / cost tradeoffs", ylim = c(-.3, 4))
  points(expected_variance, expected_costs)
  text(expected_variance[show], expected_costs[show] - .3, labs[show], cex = .8)

})
```


We represent each of these 3 2-step strategies (3 of many possible ones) in Table XXXX, along with the expected variance reduction associated with each. In addition, we indicate each strategy's expected cost. Here we assume, for simplicity, that each clue has a cost of 1. We can see that we expect to learn the same amount from strategies 2 and 3, but strategy 3 comes at a lower expected cost because we have a 50\% chance of only having to collect 1 observation, depending on what we observe.  


| Strategy | Step 1 | Step 2 if 0 | Step 2 if 1 | Expected variance | Expected Cost |
|----------|--------|-------------|-------------|-------------------|---------------|
| 1        | S      | None        | None        |       0.167       | 1             |
| 2        | S      | X           | X           |       0           | 2             |
| 3        | S      | None        | X           |       0           | 1.5           |
|          |        |             |             |                   |               |

Table XXXX: Illustration of three (of many) possible two step strategies.

We can, of course, also calculate the expected costs of different strategies while allowing different clues to come at different costs. Figure \@ref(fig:runexam) plots all 2-clue strategies, assuming $Y=0$ has already been observed, based on two criteria: expected variance-reduction for the query, "Did $S$ cause $Y$?" and expected cost. For this exercise, we set differential clue prices such that $X$ is the most costly clue to collect, followed by $C$, then $S$, then $Y$, then $R$. We have labeled the strategies that lie along the frontier of optimal strategies, with our choice dependent on how we want to trade off cost against learning. Among the notable points along this frontier, we see that the cheapest strategy among those that minimize variance involves gathering $S$ and then gathering $C$ if and only if we observe $S=1$. We can also see that the lowest-variance strategy that minimizes costs involves gathering $S$ only and then stopping.

Here we implement the same exercise for the basic democracy model. We illustrate for a situation in which we know there is high inequality and democratization has occurred, and we want to know if high inequality caused the democratization. We will assume here that mobilization is easy to observe (low-cost) but pressure is difficult (high-cost).


```{r model7, echo = FALSE, message = FALSE}

pimd <- make_model("I -> M -> D <- P; I -> D") %>% #Specify the DAG
  
         set_restrictions(c( 
           "(M[I=1] < M[I=0])", 
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))   #Exclude a set of negative-effect and positive-effect nodal types.

```


```{r strategies, echo = FALSE}

strategies <- two_step_strategies(pimd) %>% 
  filter((one %in% c("P", "M"))) %>% 
  filter(is.na(two_0) | (two_0 %in% c("P", "M")))%>% 
  filter(is.na(two_1) | (two_1 %in% c("P", "M")))

# Calculate expected n and variance
if(do_diagnosis){


# Evaluate list of strategies
results_pimd_tradeoff <-
  strategy_evaluation(
    pimd,
    strategies  = strategies,
    given = "D==1 & I==1", 
    query = "(D[I=1] != D[I=0])", 
    prices = c(1, 1.5, 1, 1))

    write_rds(results_pimd_tradeoff, "saved/ch11_tradeoffs_pimd.rds")
  }

results_pimd_tradeoff <- read_rds("saved/ch11_tradeoffs_pimd.rds")


results_pimd_tradeoff <- 
  results_pimd_tradeoff %>% 
  mutate(labs = ifelse(expected_n ==2, "All", as.character(labs)))

with(results_pimd_tradeoff, {
  plot(expected_variance, expected_costs, type = "n", xlab = "Expected Variance", ylab = "Expected Costs", 
       ylim = c(-.0, 2.8), 
       main = "Variance / cost tradeoffs" , 
       xlim = c(.1035, .1125)
       )
#  points(expected_variance, expected_costs)
  text(expected_variance, expected_costs, labs, cex = .8)

})

```


We can see here that four strategies are non-dominated by any alternative. These are, in order of increasing cost:

1. Observe $M$ first, then stop. This strategy has relatively high expected uncertainty but minimizes costs relative to any other strategy: we observe just one clue, and it's the cheaper one.
2. Observe $P$ first, then stop. We'll learn more from this strategy than from Strategy 1, though at higher cost. Still there is no other strategy that allows us to reduce costs without increasing variance.
3. Observe $P$ first; if $P=0$, observe $M$, otherwise stop. We, again, get uncertainty reduction here, relative to Strategy 2, but again at higher cost.
4. Observe $M$ first; if $M=0$, stop; if $M=1$, observe $P$. This strategy gets us the lowest expected posterior variance of any strategy. Moreover, it is not the highest-cost strategy, which would be to observe both clues no matter what. Once we've observed $M=0$, we get nothing from the additional investment in $P$ since $M=0$ already tells us that $I$ could not have had a positive effect on $D$.

Note also that both Strategy 3 and Strategy 4 are *conditional* two-clue strategies: they involve first seeking one clue and seeking a second clue only under one of the possible realizations of the first clue. But they have different outcomes. Perhaps most interestingly, we don't expect to learn the most by starting with the most probative clue. If we start with the more informative clue, $P$, observing $M$ only if $P=0$, we expect to end up with *more* uncertainty than if we start with the less informative clue, $M$, and observe $P$ only if $M=1$. 


```{r tradeoffspimdconfound, echo = FALSE, message = FALSE, eval = FALSE}

confound = list(P = "(D[P=1] == 1)")

pimd_confound2 <- make_model("I -> M -> D <- P; I -> D") %>% #Specify the DAG
  
  #Exclude a set of negative-effect and positive-effect nodal types.
  set_restrictions(c(decreasing("I", "M"), increasing("I", "D"), 
                     decreasing("M", "D"), decreasing("P", "D"))) %>% 
  
  set_confound(confound) %>%
  
  set_parameters(confound = confound, alphas = c(.05, .95), node = "P")

strategies <- two_step_strategies(pimd_confound2) %>% 
  filter((one %in% c("P", "M"))) %>% 
  filter(is.na(two_0) | (two_0 %in% c("P", "M")))%>% 
  filter(is.na(two_1) | (two_1 %in% c("P", "M")))

# Calculate expected n and variance
if(do_diagnosis){

# Evaluate list of strategies
results_pimd_tradeoff_confound <-
  strategy_evaluation(
    pimd_confound2,
    strategies  = strategies,
    given = "D==1 & I==1", 
    query = "(D[I=1] != D[I=0])", 
    prices = c(1, 1.5, 1, 1))

    write_rds(results_pimd_tradeoff_confound, "saved/ch11_tradeoffs_pimd_confound.rds")
  }

results_pimd_tradeoff_confound <- read_rds("saved/ch11_tradeoffs_pimd_confound.rds")


results_pimd_tradeoff_confound <- 
  results_pimd_tradeoff_confound %>% 
  mutate(labs = ifelse(expected_n ==2, "All", as.character(labs)))

with(results_pimd_tradeoff_confound, {
  plot(expected_variance, expected_costs, type = "n", xlab = "Expected Variance", ylab = "Expected Costs", 
       ylim = c(-.0, 2.6), 
       main = "Variance / cost tradeoffs" , 
       xlim = c(.080, .083)
       )
#  points(expected_variance, expected_costs)
  text(expected_variance, expected_costs, labs, cex = .8)

})

```

  
<!-- ## More complex problems -->

<!-- Illustration of clue inference for a continuous problem. -->



## Conclusion

Explicit statement of a causal model---including prior beliefs over roots---allows one to assess what will be inferred from all possible observations. This opens the way for simple strategies for assessing what data is most valuable, and in what order it should be gathered. 

We are conscious that here we are pushing the basic logic to the limits. In practice, researchers will often find it difficult to describe a model in advance and to place beliefs on nodes. Moreover, the collection of new data could easily give rise to possibilities and logics that were not previously contemplated. Nothing here seeks to deny these facts. The claim here is a simpler one: insofar as one can specify a model before engaging in data gathering, the model provides a powerful tool to assess what data it will be most useful to gather. 




<!--chapter:end:12-clue-selection.Rmd-->

# Case selection for mixed methods inference {#caseselection}

```{r packagesused13, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
With a causal model in hand, together with priors over parameters, we can assess in advance what conclusions we will draw from different observations and assess what kinds of observations are most worth seeking. In this chapter, we put this research-design approach to work on the problem of case-selection: given a set of cases on which we already have $X,Y$ data, which cases will it be most advantageous to choose for more in-depth investigation? As we show, the optimal case-selection strategy will depend jointly on the model we start with and the causal question we seek to answer.
::::



A critical decision for scholars employing mixed methods is to determine when and where to look deep and when to look wide. We address this question if two steps, first addressing the problem of case selection and second the question of qualitative/quantitative mixing.  

## Case selection strategies

A host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of $X$, $Y$ data. Perhaps the most common strategy is to select cases in which $X=1$ and $Y=1$ and look to see whether in fact $X$ caused $Y$ in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases "on the regression line" or, for some purposes, cases "off the regression line" (e.g., @Lieberman2005nested). Some scholars suggest ensuring variation in $X$ (most prominently, @king1994designing), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on $X$ or $Y$ (@FL2008, @HerronQuinn). 

Which cases we should choose will likely depend on the purposes to which we want to put them. 

A matching strategy for instance---selecting cases that are comparable on many features but that differ on $X$---replicates at a small scale the kind of inference done by matching estimators with large-$n$ data. The strategy draws leverage from $X,Y$ variation rather than from within-case information beyond what is available in the measurement of $X$ and $Y$. 
<!-- (FLAG: Citations needed.) -->

Other treatments seek to use qualitative information to check assumptions made in $X, Y$ analysis: for example, is the measurement of $X$ and $Y$ reliable in critical cases? 
<!-- (FLAG: Citations needed)  -->
For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the $X,Y$ inferences: for example influential cases that have unusually extreme values on $X$ and $Y$.^[Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.] Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims.

A third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman's "model-building" mode of "nested analysis" (@Lieberman2005nested). Here it may be cases off the regression line that are of interest.

@weller2014finding focus on (a) X/Y relations and (b) whether the cases are useful for hypothesis generation. 

In what follows, we focus on a simpler goal: given existing $X, Y$ data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of $X$ on $Y$?

The basic insight of this chapter is simple enough: *the optimal strategy for case selection for a model-based analysis are a function of the model we start with and the query we seek to address*, just as we saw for the optimal clue-selection strategy in Chapter \@ref(clue). Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as:

* go where the probative value is, and
* sample from $X$ and $Y$ values in proportion to their occurrence in the population,
* invest in collections of cases that provide complementary learning. 

Beyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with $X, Y, K$ data.

Most closely related to our analysis in this chapter is the contribution of @HerronQuinn, who build on @SeawrightGerring2008. While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches.  As we do, @HerronQuinn focus on a situation with binary $X,Y$ data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, $Z_i$  is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to "treatment", or an $X=1$ value). @HerronQuinn assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly.  

Our setup differs from that in @HerronQuinn in a few ways.  @HerronQuinn paramaterize differently, though this difference is not important.^[@HerronQuinn have a parameter $\theta$ that governs the distribution of data over $X$ and $Y$ and then, conditional on $X,Y$ values, a set of parameters $\psi_{xy}$ that describe the probability of a case's being of a given causal type. We take both $\theta$ and $\psi_{xy}$ to derive from the fundamental distribution of causal types and assignment probabilities. Thus, for example, $\psi_{00}$ from @HerronQuinn corresponds to $\frac{(1-\pi_b)\lambda_b}{(1-\pi_b)\lambda_b + (1-\pi_c)\lambda_c}$ in our notation. The difference in  paramaterization does have implications for interpretations of the priors. For example flat priors over $\theta$ and $\psi$ implies a tighter distribution that a uniform prior over the causal types. In fact @HerronQuinn use priors with greater variance than uniform in any event.] Perhaps the most important difference between our analysis and that in @HerronQuinn  is that we connect the inference strategy to process-tracing approaches. Whereas @HerronQuinn assume that causal types can be read directly, we assume that these are inferred *imperfectly* from evidence and we endogenize the informativeness of the evidence to features of the inquiries. Moreover, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases.

<!-- As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of $X$.  -->

<!-- Are Herron and Quinn's priors Jeffrey priors? -->

Here we assume that the case selection decision is made after observing the $XY$ distribution and we explore a range of different possible contingency tables. In  @HerronQuinn the distribution from which the contingency tables are drawn is fixed, though set to  exhibit an expected  observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large $XY$ data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases.

Another important difference, is that in many of their analyses, @HerronQuinn take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has *beliefs* about the true treatment effect that correspond to their priors, and for whom there is therefore no *expected* bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher's beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)?

Finally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by @SeawrightGerring2008) and evaluating them, we define a strategy as the particular distribution over $XY$ cells to be examined and proceed to examine *every possible strategy* given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies---those strategies that perform similarly---emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies.

Despite these various differences, our results will agree in key ways with those in @HerronQuinn.


## No general rules

Case selection is about choosing in which cases to make observations. We obviously want to look for evidence in cases where that evidence is likely to be most informative. And the informativeness of a case depends, in turn, on our model and our query.

Although it might be tempting to seek general case-selection rules of the form "examine cases in which $X=1$ and $Y=1$" or "ignore cases in which $X=0$ and $Y=1$," it is easily demonstrated that which cases will be (in expectation) more informative depends on models and queries. 

Suppose we have observed $X$ and $Y$ values for a set of random draw of cases from a population. Suppose further that we know, for this population, that:

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = 1$
  * $\Pr(Y=1|X=1, K = 0) = .5$
  * $\Pr(Y=1|X=0, K = 1) = 0$
  * $\Pr(Y=1|X=1, K = 1) = .9$

One way to read this set of statements is that $X$'s causal effect on $Y$ varies with $K$. We do not know, however how common $K$ is. Thus we do not know either the average effect of $X$ or the probability that $X$ caused $Y$ in a case with particular $X, Y$ values.

What do the above statements tell us about $K$'s *informativeness*? The beliefs above imply that if, $X=Y=1$, then $K$ is a "doubly decisive" clue for assessing whether, in a given case, $X$ causes $Y$. In particular, we see that for an $X=Y=1$ case, observing $K=1$ implies that $X$ caused $Y$: this is because, otherwise, $Y$ would have been 0. We also see that $K=0$, in an $X=1, Y=1$ case implies that $X$ did not cause $Y$ since $Y$ would have still been 1 even if $X$ were 0.  So an $X=Y=1$ case would be a highly informative place to go looking for $K$.

However, if we had a case in which $X=Y=0$, then learning $K$ would be entirely uninformative for the case. In particular, we already *know* that $K=1$ in this case as the statements above exclude the possibility of a case in which $X=Y=0$ and $K=0$. So there would be nothing gained by "looking" to see what $K$'s value is in the case. Moreover, this knowledge is not enough to tell us whether $X=0$ caused $Y=0$.

For the same reason, we can learn nothing from $K$ in an $X=0, Y=1$ case since we know that $K=0$ in such a case. On the other hand, if we chose an $X=1, Y=0$, then $K$ would again be doubly decisive, with $K=0$ implying that $X=1$ caused $Y=0$, and $K=1$ implying that $X=1$ did not cause $Y=0$.

We have chosen extreme values for this illustration --- our beliefs could, of course, allow for gradations of informativeness, rather than all-or-nothing identification --- but the larger point is that beliefs about the way world work can have a powerful effect on the kind of case in which learning is possible. And note that in this example, there is nothing special about where a case lies relative to a (notional) regression line: informativeness in this setup happens to depend on $X$'s value entirely. But, again, this is a particular feature of this particular set of beliefs about the world.

```{r, eval= FALSE, echo = FALSE}
py_k0 = .5
py_k1 = .9

py_k1 / (py_k0 + py_k1)

(1-py_k1) / (1-py_k0 + 1-py_k1)

```

Suppose, now, that we were interested in a population estimand: the average effect of $X$ on $Y$. We can see that this is equal to $\Pr(K=1)\times.9 + (1-\Pr(K-0))\times(-.5)) = 1.4\times Pr(K=1)-.5$. To estimate this estimand, we need only determine the prevalence of $K=1$ in the population. It might seem that this means that it is irrelevant what type of case we choose: why not use pure random sampling to determine $K$'s prevalence. As noted above, however, we have more information about the likely value of $K$ in some kinds of cases than in others. Thus, for this population-level estimand as well, selecting an $X=1$ case will be informative while selecting an $X=0$ case will not be informative.

At the same time, not all $X=1$ cases are equally informative. Should we choose an $X=1$ case in which $Y=1$ or one in which $Y=0$? In both types of case, $K$ is doubly decisive for the probability of causation in the case. However, the two kinds of cases are differentially informative about $K$'s prevalence in the population. For an $X=1, Y=1$ case, we think it moderately likely that $K=1$ (specifically, assuming a prior of $\Pr(K=1)$ we think $\Pr(K=1 | X=1, Y=1) = \frac{.9}{.9+.5}=.64$). For an  $X=1, Y=0$ case, we think $K=1$ is quite unlikely ($\Pr(K=1 | X=1, Y=0) = \frac{.1}{.5+.1}=.17$). In other words, we are much more uncertain about the value of $K$ in the $X=Y=1$ case than in the $X=1, Y=0$ case. In this setup, we would thus expect to learn more about the average treatment effect by choosing to observe $K$ in an on-the-diagonal case than in an off-the-diagonal case.

<!-- FLAG: What does "assuming a prior of $\Pr(K=1)$" add to above paragraph? -->

Specifically, let $\kappa$ denote $\Pr(K=1)$. Suppose that we begin thinking it equally likely that $\kappa=\kappa^H = .5$ and $\kappa=\kappa^L=0$. Suppose, further, that the distribution of $X$ is such that, for any randomly drawn case, $\Pr(X=1) = .5$. We then observe one case with $X=1, Y=1$ and another with $X=1, Y=0$. From that information alone, we can update over $\kappa$. Specifically, conditioning on $X=1$, when we observe the data pattern, $D$, in which $Y=0$ in one case and $Y=1$ in the other, the posterior on $\kappa$ is:

$$p(\kappa = \kappa^L|D) =  \frac{p(D|\kappa^H)}{p(D|\kappa^H)+p(D|\kappa^L)}=\frac{.5}{.5 + .25\times(2\times.5\times.5 + 2\times.9\times.1 + 2\times(.9\times.5 +.1\times.5))}$$

<!-- FLAG: Is the above expression not missing the prior? -->

<!-- FLAG: I am not sure what we're trying to get out of this expression, so unsure how to tie this up. -->


In summary, under the stipulated beliefs about the world, we can learn most about the population $ATE$ by selecting an $X=Y=1$ for study. We can also learn about the case-level effect in such a case as well as in an $X=1, Y=0$ case. If we are interested in the case level estimand for any $X=0$ case, then there are no gains from any case-selection strategy since we know $K$'s value based on $X$ and $Y$'s value.

<!-- FLAG: AJ: Check my rewrite of last statement.  -->

There is nothing preferable in general about an $X=1$ case. Under a different set of beliefs about the world, we would expect to learn more in an $X=Y=0$ than in an $X=Y=1$ case. Suppose, for instance, that we have a model in which: 

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = .5$
  * $\Pr(Y=1|X=1, K = 0) = 0$
  * $\Pr(Y=1|X=0, K = 1) = .5$
  * $\Pr(Y=1|X=1, K = 1) = 1$

In this world, we learn nothing from observing a case in which $X=Y=1$---since we already know that $K=1$. In contrast, if $X=Y=0$, then if we learn that $K=1$, we know that, were $X=1$, $Y$ would have been 1; and if instead we observe $K=0$, we know that $Y$ would have (still) been 0 if $X$ were 1. Now, $K$ is doubly decisive for an $X=Y=0$ case but unhelpful for an $X=Y=1$ case. 

The two off-diagonal cases may also be different in the opportunities for learning that they present. Suppose that you knew that: 

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = 0$
  * $\Pr(Y=1|X=1, K = 0) = .5$
  * $\Pr(Y=1|X=0, K = 1) = 1$
  * $\Pr(Y=1|X=1, K = 1) = .5$

For an $X=1, Y=0$ case, if you observe $K=1$ you know that if $X$ were 0, $Y$ would have been 1; but if $K=0$, you know that if $X$ were 0, $Y$ would still have been 0. In that case $K$, would be doubly decisive for $X=1$ causing $Y=0$.  But in an $X=0, Y=1$, we already know $K=0$ before we go looking.   
  
Say instead that you knew that: 

  * $X \rightarrow Y \leftarrow K$
  * $\Pr(Y=1|X=0, K = 0) = .5$
  * $\Pr(Y=1|X=1, K = 0) = 0$
  * $\Pr(Y=1|X=0, K = 1) = .5$
  * $\Pr(Y=1|X=1, K = 1) = 1$

It may be better to select a case that is not "like" the cases you want to make inferences about.

<!-- FLAG: Not getting the point here. How is this different from the setup two up from here? -->


## Specific case walk through

We introduce a flexible model-based approach to comparing the prospective learning from alternative case-selection strategies. To help explore the intuition behind this strategy, we start by walking through a simplified setup.

```{r selectioncode, echo = FALSE}

# We define a model
model <- make_model("X->M->Y")  %>%
 set_restrictions(c("(Y[M=1]<Y[M=0])", "(M[X=1]<M[X=0])")) %>%
 set_parameter_matrix() %>%
 set_parameters(type = "flat")

# We imagine some preexisting data we have observed
data  <-  data.frame(X = c(0,0,0,1,1,1), M = NA, Y = c(0,0,1,0,1,1))
observed <-  collapse_data(data, model, drop_family = TRUE)

# We can then imagine what data we might observe if we examine M inside some subset of cases
A_on_regression_line <- 
  make_possible_data(
    model, 
    observed = observed, vars = "M", 
    N = c(1,1), 
    condition = c("X==1 & Y==1", "X==0 & Y==0"), 
    prefix = "A")
names(A_on_regression_line)[-c(1:2)] <- c("A1", "A2", "A3", "A4")

B_off_regression_line <- 
  make_possible_data(model, observed = observed, vars = "M", N = c(1,1), 
                     condition = c("X==1 & Y==0", "X==0 & Y==1"), prefix = "B")
names(B_off_regression_line)[-c(1:2)] <- c("B1", "B2", "B3", "B4")

C_X1Y1_only <- 
  make_possible_data(model, observed = observed, vars = "M", N = 2, 
                     condition = c("X==1 & Y==1"), prefix = "C")
names(C_X1Y1_only)[-c(1:2)] <- c("C1", "C2", "C3")

```

Consider a situation in which our model is $X \rightarrow M \rightarrow Y$. Suppose, further, that we restrict the nodal types such that $X$ cannot have a negative effect on $M$, and $M$ cannot have a negative effect on $Y$, with flat priors over all remaining nodal types. Imagine then that we begin by collecting only $X,Y$ data on six cases and obtain the following data pattern:

```{r observed, echo = FALSE}
kable(observed)
```

These $X,Y$ data already give us some information about the causal effect of $X$ on $Y$. Yet, we want to learn more by examining some subset of these cases more deeply -- and, specifically, by colelcting data on $M$ for two of these cases. Which cases should we select? We are considering two strategies, each conditional on $X$ and $Y$ values:

* Strategy $A$ chooses two cases on the regression line, one randomly drawn from the $X=Y=0$ cell and one randomly drawn from the $X=Y=1$ cell
* Strategy $B$ chooses off the regression line, one randomly drawn from the $X=1, Y=0$ cell and one randomly drawn from the $X=0, Y=1$ cell

How can we evaluate these strategies prospectively? 

We start by recognizing that different strategies yield different *possible* data patterns. For instance, Strategy $A$ (on the line) could possibly give us a data pattern that includes the observation $X=0, M=0, Y=0$. Yet Strategy $A$ cannot possibly yield a data pattern that includes the observation $X=1, M=0, Y=0$ --- because it does not involve the inspection of $M$ in an $X=1, Y=0$ case --- whereas Strategy $B$ (off the line) *can* yield a pattern that includes this observation. And neither strategy can possibly yield a pattern that includes both $X=1, M=0, Y=0$ and $X=0, M=1, Y=0$. 

In Table \@ref(tab:chselillustration), we represent the full set of possible data patterns that can arise from each strategy, with the possible data patterns for Strategy $A$ or $B$ labeled $A1, A2$, etc. or $B1, B2$, etc., respectively. As we can see, there are four possible data patterns from each, representing the 4 different combinations of $M$ values we might find across the two cases selected for deeper investigation. In the comparison presented here, none of the possible data patterns overlap across strategies.

The next step is to grapple with the fact that not all of the possible data realizations for a given strategy are equally *likely* to emerge. We represent the data probabilities (calculated by CQTools) near the bottom of the table. How likely a data pattern is to emerge will depend on the model, any restrictions or priors we have built into the model, and any updating of beliefs that arises from the pure $X,Y$ data. Note, for instance, that data pattern $A3$ is much more likely to emerge than the other data patterns possible under Strategy $A$. This is for two reasons. One is that $A3$ involves $M$ covarying with $X$ and $Y$, a pattern consistent with $X$ having an effect on $Y$ -- since, in this model, $X$ can only affect $Y$ if it affects $M$ and if $M$ effects $Y$. Data patterns $A1$ and $A4$ have $M$ constant between the two cases, even as $X$ and $Y$ vary; this is a pattern inconsistent with $X$ having an effect on $Y$. $A3$, then, is more likely than $A1$ or $A4$ because the restrictions on the model plus the evidence from the $X,Y$ data make us believe that $X$ *does* have an average effect on $Y$. Second, we believe $A3$ is more probable than $A2$ because of the model's restrictions: the model allows positive effects of $X$ on $M$ and of $M$ on $Y$ (a way of generating $A3$), but rules out negative intermediate effects (a way of generating $A2$).

Finally, each possible data realization will (if realized) generate (possible) updating of our beliefs about the query of interest. In the second-to-last row of the Table \@ref(tab:chselillustration), we can see the mean of the posterior distribution (for the $ATE$ of $X$ on $Y$) under each data pattern. What is of particular interest in assessing case-selection strategies is the *variance* of the posterior distribution, which represents our *uncertainty*. This posterior variance on the $ATE$, for each data pattern, is represented in the table's final row. We can see that our level of posterior uncertainty varies across possible data realizations. We operationalize the expected learning under each case-selection strategy as the *expected* reduction in posterior variance. 

<!-- FLAG: Need to redo Table to conform to text: 2 strategies only. -->


```{r dtypes, echo = FALSE}
# Combine data types
df <- A_on_regression_line[,-2] %>% 
  merge(B_off_regression_line[,-2], by = "event", all = TRUE) %>%
  merge(C_X1Y1_only[,-2], by = "event", all = TRUE) %>%
  dplyr:::mutate_if(is.numeric, ~replace(., is.na(.), 0))

# table(duplicated(t(df)))
```


```{r illustration, echo = FALSE}

if(do_diagnosis){

# We define a model
model <- make_model("X->M->Y")  %>%
 set_restrictions(c("(Y[M=1]<Y[M=0])", "(M[X=1]<M[X=0])")) %>%
 set_parameter_matrix() %>%
 set_parameters(type = "jeffreys")
  
 strategies <- 
   list(A_on_regression_line, B_off_regression_line, C_X1Y1_only)

 	write_rds(strategies, "saved/14_illustration_strategies.rds")
  
 
 estimates_dbs <- lapply(strategies, function(s) 
    make_estimates_database(model, observed, possible_data = s, queries = "Y[X=1]-Y[X=0]")
    )
    
	write_rds(estimates_dbs, "saved/14_illustration_estimates.rds")

# The reference model is the original model updated with the XY data we've already seen.
 reference_model <- update_model(model, data)
 
	write_rds(reference_model, "saved/14_reference_XMY_updated_model.rds")

 probabilities <- lapply(strategies, function(s) 
#    average_data_probabilities(reference_model, s, using = "posteriors", sims = 3000)

   apply(reference_model$posterior_distribution, 1, function(pars) {
  					make_data_probabilities(model = reference_model, pars = pars, possible_data = s, normalize = TRUE)
  			})
   )
   
	write_rds(probabilities, "saved/14_illustration_probabilities.rds")

}

  reference_model <- read_rds("saved/reference_XMY_updated_model.rds")
	probabilities   <- read_rds("saved/illustration_probabilities.rds")
  strategies      <- read_rds("saved/illustration_strategies.rds")
  estimates_db    <- read_rds("saved/illustration_estimates.rds")
  
  digits <- 3

  df <- rbind(df, 
          c("Probability", round(unlist(probabilities),digits)),
          c("Posterior mean",   (unlist(lapply(estimates_db, function(j)   unlist(unlist(lapply(j, function(k) k$mean))))))),
          c("Posterior variance",   round(unlist(lapply(estimates_db, function(j)   unlist(unlist(lapply(j, function(k) k$mean^2))))), digits)))
```

```{r chselillustration, echo = FALSE}  
kable(df, caption = "Each column shows a possible distribution of data that can be generated from a given strategy. We calculate the probability of each data possibility, given the data seen so far, and the posterior variance associated with each one.", digits = 2)
```

 
From the probability of each data type (given the model and the $X,Y$ data seen so far) and the posterior variance given each data realization, the implied *expected* variance is easily calculated as a weighted average. The expected posterior variances for our two strategies are summarized in Table \@ref(tab:exppostvar):

```{r exppostvar, echo = FALSE}
expected_posterior_var <- sapply(1:3, function(i) 
  sum(probabilities[[i]]*unlist(lapply(estimates_db[[i]], function(x) x$sd^2)))
 )
kable(data.frame(Strategy = c("Online", "Offline", "X=1, Y=1"), Variance = expected_posterior_var), digits = 3)
```


In this example, we see that we would expect to be better off---in the sense of having less posterior uncertainty---by focusing our process-tracing efforts on the regression line than off the regression line. We save an account of the intuition underlying this result for the discussion of our more extensive set of simulations below. 

The key takeaway here are the core elements of our model-based approach to assessing case-selection strategies:

1. Derive from the model the full set of possible data patterns under each case-selection strategy being assessed
2. Calculate the probability of each data pattern given the model (with any priors or restrictions), the prior $X,Y$ data, and the strategy
3. Generate a posterior distribution on the query of interest for each data pattern 
4. Use the probability of and posterior variance under each data pattern toalculate the expected posterior distribution on the query of interest for each strategy



```{r test, include = FALSE}
# source("_packages_used.R")

model <- make_model("X -> M -> Y")
  
gen_observed_data <- function(n00=1, n01=1, n10=1, n11=1,times = 1) {
  data.frame(X = c(0,0,1,1), Y =c(0,1,0,1), M = NA, ns = c(n00, n01, n10, n11)*times) %>%
    tidyr::uncount(ns) %>%
    collapse_data(model, drop_family = TRUE)}


# Data strategies

data_strats <- function(N = 4) {
  if(N%%2 != 0) stop("even Ns only please")
  list(
    all_on = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                  conditions = list("X==0 & Y==0", "X==1 & Y==1")),
    all_off = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==0 & Y==1", "X==1 & Y==0")),
    all_x1 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==1 & Y==0", "X==1 & Y==1")),
    all_y1 = list(N=c(N,N)/2, withins = TRUE, vars = "M",
                   conditions = list("X==0 & Y==1", "X==1 & Y==1"))
  )
}

# Queries

# if(!exists("fit")) fit <- fitted_model()

# Diagnosis function 

if(do_diagnosis){
test_corr <- 
  diagnose_strategies(
    analysis_model = model,
    data_strategies = data_strats(2),
    given = gen_observed_data(6,2,2,6),
    queries = list(ATE = "Y[X=1]-Y[X=0]", 
                   a = "Y[X=1]<Y[X=0]", 
                   b = "Y[X=1]>Y[X=0]", 
                   c = "(Y[X=1]==0) & (Y[X=0]==0)", 
                   d = "(Y[X=1]==1) & (Y[X=0]==1)",
                   a1 = "M[X=1]<M[X=0]", 
                   b1 = "M[X=1]>M[X=0]", 
                   c1 = "(M[X=1]==0) & (M[X=0]==0)", 
                   d1 = "(M[X=1]==1) & (M[X=0]==1)",
                   a2 = "Y[M=1]<Y[M=0]", 
                   b2 = "Y[M=1]>Y[M=0]", 
                   c2 = "(Y[M=1]==0) & (Y[M=0]==0)", 
                   d2 = "(Y[M=1]==1) & (Y[M=0]==1)"
                   ),
    subsets = list(TRUE, TRUE,  TRUE,  TRUE),
    sims = 8000,
    fit = fit, 
    refresh = 1000,
    iter = 8000)

write_rds(test_corr, "saved/test_corr.rds")

test_flat <- 
  diagnose_strategies(
    analysis_model = model,
    data_strategies = data_strats(2),
    given = gen_observed_data(4,4,4,4),
    queries = list(ATE = "Y[X=1]-Y[X=0]", 
                   a = "Y[X=1]<Y[X=0]", 
                   b = "Y[X=1]>Y[X=0]", 
                   c = "(Y[X=1]==0) & (Y[X=0]==0)", 
                   d = "(Y[X=1]==1) & (Y[X=0]==1)",
                   a1 = "M[X=1]<M[X=0]", 
                   b1 = "M[X=1]>M[X=0]", 
                   c1 = "(M[X=1]==0) & (M[X=0]==0)", 
                   d1 = "(M[X=1]==1) & (M[X=0]==1)",
                   a2 = "Y[M=1]<Y[M=0]", 
                   b2 = "Y[M=1]>Y[M=0]", 
                   c2 = "(Y[M=1]==0) & (Y[M=0]==0)", 
                   d2 = "(Y[M=1]==1) & (Y[M=0]==1)"
                   ),
    subsets = list(TRUE, TRUE,  TRUE,  TRUE),
    sims = 8000, fit = fit, refresh = 1000, iter = 8000)


write_rds(test_flat, "saved/test_flat.rds") 
}

test_corr <- read_rds("saved/test_corr.rds") 
test_flat <- read_rds("saved/test_flat.rds") 

# key thing to note is for $all_x1[[1]] a2 = b2
# and for $all_x1[[2]] a1 = b1 
# so in both cases shareas of a and b remains contstant


```

<!-- For intuition consider first the case with flat prior data: -->

<!-- * if we examine two cases in different quadrants *on the diagonal* (one $X=Y=0$ case and one $X=Y=1$ case) : -->
<!--   * if  we find that $M$ is the same in the two cases then we  increase our belief that $X$ has no effect at all on $Y$ and reduce our confidence that $X$ had a positive or a negative effect on $Y$. Since we are looking on the regression line however, our confidence that $X$ had a *positive* effect on $Y$ is more strongly reduced, producing a  posterior centered on a small negative effect.  -->
<!--   * Conversely if we see that $M$ is different in the two cases, then we have a correlation of the same sign between both $X$ and $M$ and between $M$ and $Y$  cases. We increase our confidence that $X$ mattered for $Y$ in general an din particular that it had a positive effect, resulting in a posterior centered on a small positive ATE. -->

<!-- * if we examine two cases in different quadrants *off the diagonal* (one $X=0, Y=1$ case and one $X=1, Y=0$ case) : -->
<!--   * if  we find that $M$ is the same in the two cases then we  increase our belief that $X$ has no effect at all on $Y$ and reduce our confidence that $X$ had a negative effect  $Y$ (producing a  posterior centered on a small positive effect).  -->
<!--   * Conversely if we see that $M$ is different in the two cases, then we have a correlation (of different signs) between both $X$ and $M$ and between $M$ and $Y$  cases. We increase our confidence that $X$ mattered for $Y$ in general and in particular that it had a negative effect, resulting in a posterior centered on a small negative ATE. -->

<!-- * If we examine data conditioning on the value of $X$ but with variation on $Y$ (for instance $X=1, Y=0$ and   $X=1, Y=1$)  -->
<!-- data patterns do not discriminate between X having a positive effect on Y an d X having a negative effect on Y. However variation in $M$ is more consistent with $M$ being responsive to $X$ and thus the chances that $X$ matters, positively or negatively, overall. In the  case with flat data this changes beliefs on positive and negative effects but not the difference between them. The ate then remains unchanged. FLAG work though intuition  more -->

<!-- * If we examine data conditioning on the value of $Y$ but with variation on $X$ (for instance $X=0, Y=1$ and   $X=1, Y=1$)  -->
<!-- data patterns do not discriminate between X having a positive effect on Y an d X having a negative effect on Y. However variation in $M$ is more consistent with $M$ being responsive to $X$ and thus the chances that $X$ matters, positively or negatively, overall. In the  case with flat data this changes beliefs on positive and negative effects but not the difference between them. The ate then remains unchanged. -->

<!-- For correlated data similar logics apply, but the effects are stronger for evidence on the regression line. The reason is that given correlated data we believe there are more units with positive effects than negative effects. When we find evidence against causal relations on the regression line that reduces our confidence for types with positive effects more than for types with negative effects and the  difference between the  shares with positive effects and negative effects smaller due to the fact that the beliefs in the shares with negative effects is not so strongly reduced. When we find evidence for causal relations this magnifies the  difference between beliefs in shares positive and shares negatives; these effects are magnified however since our confidence for the positive effects change more than for the negative effects, since we are examining cases on the regression line. Conversely when examining cases of the  regression line, the two forces offset each other.  -->



```{r cinfer, include = FALSE}
# Make sense given process tracing approach

long_data <-  expand_data(gen_observed_data(4,2,2,4), model)

if(do_diagnosis){
  updated <- update_model(model, long_data, iter = 14000, chains = 12) %>%
  set_parameters(param_type = "posterior_mean")
  write_rds(updated, "saved/ch13_longupdated.rds") 
}
updated <- read_rds("saved/ch13_longupdated.rds") 

get_parameters(updated)

conditional_inferences(updated, query = "Y[X=1]!=Y[X=0]", given = "!is.na(X) & !is.na(Y) & !is.na(M)") %>%
  arrange(X, Y, M)
```



## Case selection from causal models: a simulation-based approach

In this section, we generalize and more extensively illustrate the model-based approach by applying it to a wide range of models, queries, and case-selection strategies. We generate the results displayed here using `CausalQueries` together with `CQTools`.

In all scenarios examined here, we imagine a situation in which we have already observed some data (the values of some nodes from the causal model in some set of cases) and must now decide in which cases we should gather additional data. To simplify the setup, we will be assuming throughout that we are considering gathering additional observations in cases for which we already have *some* data. In other words, we are deciding which subset of the cases --- among those we have already gathered some data on --- we should investigate more *deeply*. (This is distinct from the question of "wide vs. deep", where we might decide to observe cases we have not yet seen at all.) 

The general intuition of the case-selection approach that we develop here is that we can use our causal model and any previously observed data to estimate what observations we are more or less likely to make under a given case-selection strategy, and then figure out how far off from the (under the model) true estimand we can expect to be under the strategy, given whatever causal question we seek to answer. 

We proceed as follows:

**DAG**. We start, as always, with a DAG representing our beliefs about which variables we believe to be direct causes of other variables. For the current illustrations, we consider two different DAGS: a simple mediation (or "chain") model, $X \rightarrow M \rightarrow Y$, and a two-path model, $X \rightarrow M \rightarrow Y \leftarrow X$.

**Restrictions or priors**. As when conducting mixed-method inference, we can set qualitative restrictions and/or differential quantitative weights on the (possibly conditional) nodal types in the model. We can also indicate our uncertainty over the latter, by setting the $\alpha$ parameters of the relevant Dirichlet distributions. For the current example, we start by setting flat priors over all nodal types and assume no unobserved confounding.

**Given data.** If we have already made observations of any of the model's nodes in some set of cases, we can use this information to condition our strategy for searching for further information. For instance, if we have observed $X$'s and $Y$'s value in a set of cases, we might select cases for process tracing based on their values of $X$ and/or $Y$. And, importantly, what we have already observed in the cases will affect the inferences we will draw when we observe additional data, including how *informative* a particular new observation is likely to be. For the present examples, we assume that we have already observed $X$ and $Y$ in a set of cases and found a positive correlation.

**Query**. We define our query. This could, for instance, be $X$'s average effect on $Y$ or it might be the probability that $X$ has a negative effect on $Y$ in an $X=1, Y=0$ case. We can use the general procedure to identify case-selection strategies for any causal query that can be defined on a DAG. And, importantly, the optimal case-selection strategy may depend on the query. The best case-selection strategy for answering one query may not be the best case-selection strategy for another query. 

**Define one or more strategies**. A strategy is defined, generically, as the search for data on a given set of *nodes*, in a given *number* of cases that are randomly selected *conditional* on some information we already have about potential cases. In the simulations below, our strategy will always involve uncovering $M$'s value in 1 or 2 cases. What we are wondering is how to choose these one or two cases for deeper analysis. 

**Possible data**. For each strategy, there are multiple possible sets of data that we could end up observing. In particular, the data we could end up with will be the $X,Y$ patterns we have already observed plus some pattern of $M$ observations. 

**Probability of the data**. We then calculate a probability of each possible data realization, given the model (with any restrictions or priors) and any data that we have already observed. In practice, we do this in `CQtools` via simulation. Starting with the model together with our priors, we update our beliefs about $\lambda$ based on the previously observed data. This posterior now represents our *prior* for the purposes of the process tracing. In the analyses below, we use the already-observed $X,Y$ correlation to update our beliefs about causal-type share allocations in the population. We then use this posterior to draw a series of $\lambda$ values. 

Given that the ambiguity matrix gives us the mapping from causal types to data realizations, we can calculate for each $lambda$ draw the probability of each data possibility given that particular $\lambda$ and the strategy. We then average across repeated $\lambda$ draws. Since $\lambda$'s are being drawn from our prior, we are automatically weighting more heavily those $\lambda$'s that we believe to be most likely.

**Posterior on estimate given the data**. For each data possibility, we can then use `CQtools` to ask what inference we would get from each data possibility, given whatever query we seek to answer, as well as the variance of that posterior. Examining the inferences from possible data-realizations, as we do below, can help us understand how the learning unfolds for different strategies. 

**Expected posterior variance under each strategy**. The quantity of ultimate interest is the posterior variance that we expect to end up with under each strategy. The expected posterior variance is simply an average of the posterior variances under each data possibility, weighted by the probability of each data possibility. We operationalize the expected learning under a strategy as the expected *reduction* in posterior variance arising from that strategy.


#### Models, queries, and strategies

Here we describe the dimensions along which we vary the features of the simulations.

*Models*

We illustrate the approach using a set of relatively simply models with core structural features that we believe will be fairly common in applied settings. The four structures that we examine are:

* Chain model: an $X \rightarrow M \rightarrow Y$ model, where $M$ is a mediator
* Confounded model: a model with $X \rightarrow Y$ and with $M$ as a confounder, pointing into both $X$ and $Y$
* Moderator model: an $X \rightarrow Y \leftarrow M$, where $M$ is a moderator
* Two-path model: a model in which $X \rightarrow M \rightarrow Y \leftarrow X$, meaning that $X$ can affect $Y$ both through a direct path and indirectly via $M$

For each of these causal structures, we consider both an unconstrained version (all nodal types permitted) and a monotonic version in which we use restrictions to exclude negative effects throughout the model.


*Queries*

We also examine a range of queries under each model:

* $ATE$: what is the average effect of $X$ on $Y$ for the population?
* Probability of positive causation: what is the probability that $Y=1$ because $X=1$ for a case randomly drawn from the population of $X=1, Y=1$ cases
* Probability of negative causation: what is the probability that $Y=1$ is due to $X=0$ for a case randomly drawn from the population of $X=0, Y=1$ cases
* Probability of an indirect effect: defined only for the two-path models, we estimate the probability that the effect of $X$ on $Y$ operates through the indirect path. More precisely, we ask, for an $X=1, Y=1$ case in which $X=1$ caused $Y=1$, what the probability is that that effect would have occurred if $M$ were held fixed at the value it takes on when $X=1$.^[In code, this somewhat complicated query is expressed as `"Y[X=0, M=M[X=1]]==Y[X=1, M=M[X=1]]"`, `given "(X == 1 & Y == 1) & (Y[X=1]>Y[X=0])"`.]

<!-- $X$'s effect on $M$  causal effect along the indirect pathway, through $M$. We calculate two indirect effects, one conditional on $X=0$ and one conditional on $X=1$. Each of these effects is defined as the effect on $Y$ of the change in $M$ that *would* result from a change in $X$ (from 0 to 1 or from 1 to 0, respectively), but with $X$ held constant (at 0 or 1, respectively). -->

*Strategies*

Finally, for each model-query combination, we assess the contributions of 7 strategies for selecting cases for process tracing, with inferences from the $X,Y$ data alone serving as our baseline. In the figures below, the strategies run along the $X-$axis of each graph and can be interpreted as follows:

* **Prior**: beliefs are based on $X,Y$ data only. 
* **1 off**: data on $M$ is sought in one case in the $X=1, Y=0$ cell
* **1 on**: data on $M$ is sought in one case in the $X=1, Y=1$ cell
* **2 off**: data on $M$ is sought in one $X=0, Y=1$ case and one $X=1, Y=0$ case
* **2 pos**: data on $M$ is sought for two cases in the $X=1, Y=1$ cell 
* **2 on**: data on $M$ is sought in one $X=1, Y=1$ case and one $X=0, Y=0$ case
* **fix $X$**: a strategy in which we seek $M$ in two cases in which a causal condition was present, with $X$ fixed at 1, one with $Y=0$ and one with $Y=1$
* **fix $Y$**: a strategy in which we seek $M$ in two cases in which a positive outcome was observed, with $Y$ fixed at 1, one with $X=0$ and one with $X=1$

These are all "pure" strategies in the sense that the number of units for which data on $M$ is sought in each cell is fixed. One could also imagine random strategies in which a researcher chooses at random in which cells to look. For example, if we choose one point at random, we are randomly choosing between a case on the regression line and a case off the line. The performance of a random strategy will be a weighted average of the pure strategies over which the random strategy is randomizing.

For all simulations, we assume prior $X,Y$ data of $N=6$, with a weak positive relationship (2 $X=1, Y=1$ cases, 2 $X=0, Y=0$ cases, and 1 case in each off-diagonal cell). And it is *from* these original 6 cases that we are selecting our cases for process tracing. In the experiments below, we do not examine how the prior data itself might affect the choice of case-selection strategies (as we do, for instance, with clue-selection in Chapter \@ref(clue)), but we invite the reader to explore these relationships by adjusting the code we provide. 


#### Results

The main results are shown in Figure \@ref(fig:caseselectionest) and Figure \@ref(fig:caseselectionvar). For each model-strategy-query combination, we figure out (a) all of the possible data realizations, (b) what inferences would be made on the query from each data realization, (c) how likely each data-realization is to arise given the strategy, the model, and the prior data, and (d) the resulting expected reduction in variance from each strategy. Result (d) is our measure of expected learning. 

The two figures take two different approaches to representing the value of alternative strategies.  In Figure \@ref(fig:caseselectionest), we examine the informativeness of strategies by showing how much our inferences depend on what we observe within the cases. Generally, a larger spread across points (for a given model-query-strategy combination) represents a greater opportunity for learning from the data. However, as expected learning is also a function of how likely each data realization is, we represent the probability of each potential inference via shading of the points. In Figure \@ref(fig:caseselectionvar) we directly plot expected learning, operationalized as the expected reduction in posterior variance. 

In the remainder of this section, we walk through the results and suggest, often tentatively, interpretations of some of the more striking patterns. We caution that reasoning one's way through expected learning for different model-query-strategy combinations, given a particular pattern in the prior data, can be tricky --- hence, our recommendation that researchers simulate their way to research-design guidance, rather than relying on intuition. 

<!-- These figuresare close connected of course, with a higher dispersion of estimates in Figure \@ref(fig:caseselectionest) implying a greater reduction in variance in Figure \@ref(fig:caseselectionvar). -->




```{r caseselectionest, fig.height = 7, fig.width = 10, fig.cap = "Inferences given observations", warning = FALSE, echo = FALSE} 

read_rds("saved/Case_selection_1.rds")
```


```{r caseselectionvar, fig.height = 7, fig.width = 10, fig.cap = "Reduction in variance on ATE and PC given strategies", echo = FALSE, warning = FALSE} 

read_rds("saved/Case_selection_2.rds")

```



<!-- We see also the reduction in variance for other estimands, notably the effects of $X$ on $M$, the effects of $M$ on $Y$ as well as the direct and indirect effects for our two-path models. We display these to help with interpretation of the main results but they also usefully demonstrate the applicability of the procedure to different types of queries we might be concerned with. -->

<!-- #### Priors: From the $X,Y$ data only -->


<!-- <!-- we see our pure prior on each query before we have seen the $X,Y$ correlation --- i.e., based strictly on the DAG and flat priors over nodal types. The story here is straightforward. We start out believing that, for the $X \rightarrow M$ relationship, the population is evenly divided across the four types, $\theta^M_{00}, \theta^M_{01}, \theta^M_{10}, \theta^M_{11}$ (so 0.25 share of each). Likewise for $\theta^Y$ in relation to the $M \rightarrow Y$ relationship. We can get a positive $X,Y$ effect from a causal type involving $\theta^M_{01}$ and $\theta^Y_{01}$ or from a causal type involving $\theta^M_{10}$ and $\theta^Y_{10}$. Each of those causal types has a 0.125 probability, implying a 0.25 share of positive effects. A mirror-image of that logic gets us to a 0.25 share of negative effects, with the remaining 0.5 of the population comprising causal types that yield null effects (involving either a null effect at one or both steps in the chain). Thus, the expected effect of $X$ on $Y$ for a randomly drawn case from the population, given nothing but the model, is 0.  --> -->

<!-- <!-- moving across the row, we see our prior beliefs for cases with more specific features. For cases with $X=1, Y=1$, our priors imply a positive effect 25\% of the time.^[This belief arises from a straightforward calculation involving the different combinations of nodal types that could generate $X=1, Y=1$ and their prior probabilities.] As we have flat priors over $M$'s type and its effects on $Y$, conditioning $M$'s value leaves this belief unaffected. By a parallel logic, the model and flat priors imply the belief that 0.25 of all $X=0, Y=1$ cases to involve a negative effect of $X$ on $Y$. --> -->

<!-- We turn first to the chain model. Before examining inferences from data, it is worth noting what a chain model with flat priors implies about the queries of interest. We start out believing that, for the $X \rightarrow M$ relationship, the population is evenly divided across the four types, $\theta^M_{00}, \theta^M_{01}, \theta^M_{10}, \theta^M_{11}$ (so 0.25 share of each). Likewise for $\theta^Y$ in relation to the $M \rightarrow Y$ relationship. We can get a positive $X,Y$ effect from a causal type involving $\theta^M_{01}$ and $\theta^Y_{01}$ or from a causal type involving $\theta^M_{10}$ and $\theta^Y_{10}$. Each of those causal types has a 0.125 probability, implying a 0.25 share of positive effects. A mirror-image of that logic gets us to a 0.25 share of negative effects, with the remaining 0.5 of the population comprising causal types that yield null effects (involving either a null effect at one or both steps in the chain). Thus, the average effect of $X$ on $Y$, given nothing but the model, is 0. Moreover, for an $X=1, Y=1$ case, the probability of a (positive) causal effect is also 0.25, as it is for a negative effect in an $X=0, Y=1$ case. -->

<!-- Now, we introduce data. In the "prior" column of each graph in Figure \@ref(fig:caseselectionest), we see our inferences from the initial set of 6 $X,Y$ observations -- that is, *prior* to process tracing. In this model, as there is no confounding, a positive $X,Y$ correlation is evidence of a higher relative share of cases in which $X$ has a positive effect on $Y$. This implies a positive average treatment effect as well as a greater than 0.25 chance of causation for an $X=1, Y=1$ case since the $X,Y$ pattern suggests that there are more cases where $X$ has a positive effect on $Y$ than there are cases in which $Y$ is fixed at 1.  -->

<!-- Notably, however, our beliefs for an $X=0, Y=1$ case are unaffected by observing the positive correlation. To return for a moment to our handy $a,b,c,d$ typology, the reason is that the positive $X,Y$ correlation affects our beliefs about the share of $b$ types relative to $a,c,$ and $d$ types (with reference to the overall $X \rightarrow Y$ effect), but it is uninformative about the relative shares of $a$ vs. $d$ types, the two types in contention for an $X=0, Y=1$ case. In the $X,Y$ data, the $X=0, Y=1$ case is equally likely to be an $a$ or a $d$, which is exactly what our prior belief was about a random $X=0, Y=1$ case.^[Put differently, the observed $X,Y$ correlation is equally consistent with all $X=0, Y=1$ cases being $a$'s, with them all being $d$'s, or with anything in between.] -->

<!-- (Again, conditioning on $M$'s value has no effect since we still have no information to shift us off our flat priors relating to $M$'s causes or effects.) -->

#### $N=1$ strategies, unconstrained models

Suppose that we can only conduct process tracing (observe $M$) for a single case drawn from our sample of 6 $X,Y$ cases. Should we choose a case from on or off the regression line implied by the $X,Y$ pattern?  In Figure \@ref(fig:caseselectionest), we can see that for all unconstrained models, our inferences are completely unaffected by the observation of $M$ in a single case, regardless of which case-selection strategy we choose and regardless of the query of interest. We see only 1 point plotted for the two $N=1$ strategies for all unconstrained models and all queries because the inference is the same regardless of the realized value of $M$. In Figure \@ref(fig:caseselectionvar), we see, in the same vein, that we expect 0 reduction in expected posterior variance from these $N=1$ strategies: they cannot make us any less uncertain about our estimates because the observations we glean cannot affect our beliefs.

To see why, let's first consider the on-the-line strategy. Not having observed $M$ previously, we still have flat priors over the nodal types governing $X$'s effect on $M$ and $M$'s effect on $Y$. That is to say, we still have no idea whether $X$'s positive effect on $Y$ (where present) more commonly operates through a chain of positive effects or a chain of negative effects. Thus, the observation of, say, $M=1$ in an $X=1, Y=1$ case is equally consistent with a positive $X \rightarrow Y$ (to the extent that effect operates via linked positive effects) and with no $X \rightarrow Y$ effect (to the extent positive effects operate through linked negative effects). Observing $M=1$ in an $X=1, Y=1$ case therefore tells us nothing about the causal effect in that case and, thus, nothing about the average effect either. 

Similarly, we have no idea whether $X$'s negative effect on $Y$ (where present) operates through a positive-negative chain or a negative-positive chain, making $M=1$ or $M=0$ in an $X=1, Y=0$ case both equally consistent with a negative or null $X \rightarrow Y$ effect, yielding no information about causation in the case. By a similar logic, observing $M=1$ in the $X=1, Y=1$ case is uninformative about negative effects in an $X=0, Y=1$ case, and observing $M=1$ in an $X=1, Y=0$ case tells us nothing about positive effects in an $X=1, Y=1$ case. 

The same logic applies to drawing inferences from $M$ as a moderator or to learning from $M$ about indirect effects. In the absence of prior information about effects, one case is not enough.


#### $N=1$ strategies, monotonic models

The situations changes, however, when we operate with models with montonicity restrictions. Now we can see that our inferences on the queries do generally depend on $M$'s realization in a single case and that we expect learning. For many model-query combinations, the two $N=1$ strategies perform comparably, but there are situations in which we see substantial differences. 

Most notably, in a chain model with negative effects ruled out by assumption, we learn almost nothing from choosing an off-the-line case: this is because we already know from the model itself that there can be no $X \right arrow Y$ effect in such a case since such an effect would require a negative effect at one stage. The only learning that can occur in such a case is about the prevalence of positive effects (relative to null effects) at individual stages ($X \rightarrow M$ and $M \rightarrow Y$), which in turn has implications for the prevalance of positive effects (relative to null effects) of $X$ on $Y$. Likely for similar reasons, in the monotonic two-path model, an on-the-line case is much more informative than an off-the-line case about the $ATE$ and about the probability that the effect runs via the indirect path. 

Interestingly, however, the on-the-line strategy is not uniformly superior for an $N=1$ process-tracing design. We appear to learn significantly more from an off-the-line case than an on-the-line case when estimating the share of positive effects in the population of $X=1, Y=1$ cases and operating with a monotonic confounded or two-path model. At first, these results seem surprising: why would we not want to choose an $X=1, Y=1$ case for learning about the population of $X=1, Y=1$ cases? One possible reason is that, in the on-the-line case, one data realization is much more likely then the other, while we are more uncertain about what we will find in the off-the-line case. For instance, in the confounding model with montonicity, in an $X=1, Y=1$ case we would learn about the prevalence of confounding from seeing $M=0$ (where confounding cannot be operating since negative effects are excluded) as opposed to $M=1$; but we do not expect to see $M=0$ when both of its children ($X$ and $Y$) take a value of 1 while negative effects are excluded. In an $X=1, Y=0$ case, however, $M=0$ and $M=1$ are about equally likely to be observed, and we can learn about confounding from each realization. We can see these differences in relative data probabilities from the shadings in the graphs, where we have more even shading for the possible inferences from the one-off strategy than for the one-one strategy. 

The general point here is that we expect to learn more from seeking a clue the more uncertain we are about what we will find, and some case-selection strategies will give us better opportunities to resolve uncertainty than do others. 


#### $N=2$ strategies, unconstrained models

Next, we consider the process tracing of *two* of our cases 6 cases. Now, because we are observing $M$ in two cases, we can learn from the variation in $M$ across these cases---or, more specifically, from its covariation with $X$ and with $Y$. This should matter especially for unconstrained models, where we start out with no information about intermediate causal effects (e.g., whether they are more often positive or more often negative). Thus, when we only process trace one case, we cannot learn about causal effects in the cases we process-trace since we don't know how to interpret the clue. In contrast, if we observe $M$ in two or more cases, we *do* learn about causal effects for those cases because of the leverage provided by observing covariation between the process-tracing clue and other variables.

<!-- Thus, while we have pointed out that observing $M$ within a case can lend $M$ probative value for *other* cases in which we might choose to observe $M$, $M$ has no probative value *for* that one case.  -->

We assess the expected gains from 5 $N=2$ stragies: examine two off-the line cases, one $X=1, Y=0$ case and one $X=0, Y=1$ case; examine two on-the-line cases, an $X=Y=0$ case and an $X=Y=1$ case; examine two treated, positive outcome ($X=Y=1$) cases; select on $X$ by examining two $X=1$ cases with different $Y$ values; and select on $Y$ by examining two $Y=1$ cases with different $X$ values. 

A key message of these results is that, with 2 cases, the performance of each strategy depends quite heavily on the model we start with and what we want to learn. For instance, when estimating the $ATE$, the on-the-line strategy in which we disperse the cases across cells (two-on) clearly outperforms both the dispersed off-the-line strategy (two-off) and an on-the-line strategy in which we concentrate on one cell (two-pos) *if* we are working with an unconstrained chain model, and the off-the-line strategy is clearly the worst-performing of the three. The differences in learning about the $ATE$ are more muted, however, for an unconstrained confounded model, and the two-pos strategy does *better* than the other two for a two-path model.

If we seek to learn about the probability of positive causation in an $X=1, Y=1$ case, then there is little difference between two-off and two-pos, with two-on performing best. We also see that two-pos has lost its edge in an unconstrained two-path model, with no strategy offering leverage. When estimating the probability of a *negative* effect for an $X=0, Y=1$ case, we see that the two-off strategy performs best for the chain model, *but* that the two-pos strategies offers the greatest leverage in a two-path model. Finally, when estimating the probability of an indirect positive effect in an unconstrained two-path model, we get the most from a two-on strategy, though the two-off strategy does moderately well.

In general, selecting conditional on a fixed value of $X$ or $Y$ (while dispersing on the other variable) does not do particularly well in unconstrained models, and it does not usually matter much which variable we fix on. There are exceptions, however. Perhaps most strikingly, in a two-path unconstrained model, we do relatively well in estimating the probability of an indirect positive effect when we fix $Y$ but stand to learn nothing if we fix $X$. Interestingly, fixing $Y$ fairly well dominates fixing $X$ across all model-query combinations shown, given the prior data pattern we are working with.

This pattern is particularly interesting in light of canonical advice in the qualitative methods literature. King, Keohane, and Verba [-@king1994designing] advise selecting for variation on the explanatory variable and, as a second-best approach, on the dependent variable. And they warn sternly against selection for variation on both at the same time. But note what happens if we follow their advice. Suppose we start with an unconstrained chain model, with to learn about the $ATE$ or probability of positive causation, and decide to select for variation on $X$, ignoring $Y$. We might get lucky and end up with a pair of highly informative on-the-line cases. But, depending on the joint $X,Y$ distributon in the population, we might just as easily end up with a fairly uninformative off-the-line case or $X=0, Y=1$, $X=1, Y=1$ pair. We do better if we intentionally select on *both* $X$ and $Y$ in this setup. This is equally true if we want to learn about the probability of negative effects in this model, in which case we want to choose an off-the-line case, or if we want to learn about positive indirect effects in a two-path model, where we want both $X$ and $Y$ to be 1. King, Keohane, and Verba's advice makes sense if all we are interested in is examining covariation between $X$ and the $Y$: then we can learn from forcing $X$ to vary and letting $Y$'s values fall where they may. However, seeking leverage from observation of a third variable is a different affair. As our simulations indicate, the strategy from which we stand to learn the most will depend on how we think the world works and what we want to learn.

<!-- We plot four points for each strategy (for each query), representing the inference we would draw from four possible realizations of $M$. In most analyses, the inferences are essentially the same for pairs of data-realizations, so only two dots are apparent.^[For instance, whether we see $M=1$ in the $X=0, Y=0$ case and $M=0$ in the $X=1, Y=1$ case or vice versa, the inference is the same for a model with no restrictions.] While unlabelled, the higher dot represents the inference we would draw if $M$ varies with $X$; the lower dot represents the inference we would draw if $M$ is constant across the two cases. -->

<!-- Let's consider the on-the-line strategy first. Consider what we infer if we observe $M$ covary (either positively or negatively) with $X$ in these cases. What we are seeing, then, is that the mediator covaries across cases with both the cause and the outcome, which constitutes evidence consistent with a causal effect of $X$ on $Y$. Since the observed pattern of covariation is consistent with a positive $X \rightarrow Y$ effect (via linked positive effects), our belief about the average effect of $X$ on $Y$ goes up. For the same reason, so too does our belief about the probability of causation in an $X=1, Y=1$ case.  -->

<!-- <!-- We see an even bigger boost in our estimate of the effect in an $X=1, Y=1$ case conditional on $M$ also being 1. Two things are generating this upward updating. First, just as for the unconditional estimate and the estimate conditional on $X=1, Y=1$, the process tracing generates a stronger belief that $X$ generally has a positive effect on $Y$. Second, as for the $N=1$ process tracing, seeing $M=1$ in two on-the-line cases (which we already believe are mostly positive-effect cases) makes $M$ informative as a clue in favor of a positive effect---indeed, more informative than if we'd only seen $M=1$ in one on-the-line case. --> 

<!-- Now, supppose we we observe $M=1$ or $M=0$ in both on-the-line cases. This data pattern represents clear evidence *against* positive effects. We have observed that $M$ is unresponsive to changes in $X$, and that $Y$ varies without $M$ varying---null effects at both stages of the chain. Now we see that our posterior on the $ATE$ drops to about $0$. We also see a sharp downward movement in the probability of causation for an $X=1, Y=1$ case, representing our updated belief that there are few positive effects.  -->

<!-- In Figure \@ref(fig:caseselectionvar), we see the consequences for the expected reduction in posterior variance: the large potential swings in inferences, depending on what we observe, generate the potential for quite a lot of learning. Likewise, in Figure \@ref(caseselectionvar2), we can see the learning about effects at each stage: the relatively large reduction in expected posterior variance in our estimates of the average effect of $X$ on $M$ and of $M$ on $Y$. -->

<!-- <!-- Interestingly, whereas conditioning additionally on $M=1$ generated in a *higher* estimated effect when we observed $M$ correlated with $X$ and $Y$, it generates a *lower* estimated effect when we observe $M=1$ in both cases. One reason is likely that we have updated our belief about $\theta^M$ such that we now believe that $X=1, M=1$ cases are more likely to be $\theta^M_{11}$'s (no effect) than $\theta^M_{01}$'s. We have not similarly updated with respect to $X=1, M=0$ cases and the $\theta^M_{00}$'s than $\theta^M_{10}$ shares. Thus, for an $X=1,Y=1$ case, additionally specifying that $M=1$ implies an even higher likelihood that the case contains a null effect.  --> 

<!-- Moving down the queries, we also see that process-tracing the on-the-line cases yields leverage on negative causal effects in an *off-the-line* case. In particular, though not directly indicated on the graphs, finding evidence consistent with positive effects in those cases leads us to update in favor of stronger *negative* effects as well, while finding $M$ constant in both cases leads to a downard shift in the probability of negative causation. Why is this? -->

<!-- Our answer is somewhat speculative, but we think there are two things going on. One is simply that the correlations between $M$ and $X$ and between $M$ and $Y$ across the two process-traced cases constitutes evidence of *effects*, period, at the intervening steps---a necessary condition for *any* $X \rightarrow Y$ effect, positive or negative. So, for an $X=0, Y=1$ case, a stronger belief that $X$ has some effect on $M$ and that $M$ has some effect on $Y$ implies a stronger belief in a negative $X \rightarrow Y$ effect.  -->

<!-- The second thing that is probably going on is constraint imposed by prior beliefs. Having seen the $X,Y$ data, we have previously formed a belief that the *average* effect of $X$ on $Y$ is about 0.05. When we find evidence in favor of positive effects, we update to a set of beliefs that are a compromise between the new data and prior beliefs. On the one hand, we upwardly update on the $ATE$. On the other hand, our beliefs are constrained from moving *too* far away from an $ATE$ of 0.05. Recall that, in a binary setting, the average effect in a population is the share of cases with positive effects minus the share with negative effects. Thus, the conservative influence of our priors means that, as we upwardly update on positive effects, we also upwardly update our beliefs about the prevalence of *negative* effects, though not by as much. The result is that our beliefs about the $ATE$ do not shift as much as they would if we had had flat priors going into the process tracing. By more deeply anchoring our beliefs about average effects, then, the prior $X,Y$ evidence means that any evidence in favor of positive effects also constitutes evidence in favor of negative effects.  -->

<!-- To put this point another way, while we learn *directly* about positive effects from process-tracing the on-the-line cases since these are cases in which positive effects can possible occur. But we then learn *indirectly* about negative effects from these cases *via* the prior-imposed constraint on the average effect. -->

<!-- We also consider an off-the-line $N=2$ strategy, selecting for process tracing one $X=1, Y=0$ case and one $X=0, Y=1$ case. Again, the two types of data-realization we consider are that $M$ varies with $X$ across the two cases (higher point)  or that $M$ is constant in both cases (lower point). One thing we can observe immediately is that we get less leverage from this strategy if we are interested in either the $ATE$ or positive effects. In Figure \@ref(fig:caseselectionest), looking at both the $ATE$ and probability of causation in an $X=1, Y=1$ case, the difference in our posterior beliefs between the two data-realizations is smaller for this strategy than it is for the two data-realizations of the on-the-line strategy. We likewise see less reduction in posterior expected variance for these queries with an off-the-line strategy. However, we learn more from the off-the-line strategy about the probability of *negative* causation.  -->

<!-- Unpacking this further, we know that we can learn directly about *negative* $X \rightarrow Y$ effects through process-tracing off-the-line cases. If we see $M$ covary with $X$ across these two cases---which also means covarying (though in the opposite direction) with $Y$---we have found evidence in favor of the operation of negative $X \rightarrow Y$ effects. Hence the upward movement in our posterior on the probability of negative causation. We also upwardly update our posterior on positive effects in an $X=1, Y=1$ case, though by a smaller amount. Again, some of this movement is likely attributable to the constraint imposed by the prior generated from the $X,Y$ data; and some of this updating is likely attributable to the fact that we have found evidence against null intermediate effects.  -->

<!-- <!-- Curiously, under this strategy and data-realization, our beliefs about effects conditional on $X=0, M=1, Y=1$ move in the *opposite* direction from our beliefs conditional just on $X=0,Y=1$. Why might this be? This is likely because of the way in which the process-tracing has lent probative value to $M$. The pattern of $M$ data is most consistent with any negative $X \rightarrow Y$ effect operating via a *positive* $X \rightarrow M$ effect followed by a *negative* $M \rightarrow Y$ effect. Yet the intermediate effects would have to have the opposite sign for $X=0, M=1, Y=1$ to be consistent with an $X \rightarrow Y$ effect. In other words, $M=1$ rules out the likeliest way in which an $X=0, Y=1$ case could contain an effect.  -->

<!-- If on the other hand we observe $M$ constant in both off-the-line cases, we have found evidence against the operation of negative effects. Hence the the strong downward movement of our posterior on the probability of negative causation. Likewise, we see (weaker) upward movement in our posterior on the $ATE$ and, for reasons discussed earlier, in our posterior on positive effects. In sum, an off-the-line strategy generates "direct" learning about negative effects and indirect, weaker learning about positive effects. We see the corresponding learning about average effects at each stage in the causal chain in Figure \@ref(caseselectionvar2). -->

<!-- The remaining puzzle is why the learning about the $ATE$ from the off-the-line strategy is weaker than under the on-the-line strategy. This is at first glance puzzling in that the two strategies generate symmetrical opportunities to observe an $M$ pattern that is consistent or inconsistent with causal effects. We believe there are two reasons, both relating to where the direct and indirect learning falls. -->

<!-- The first, more mechanical reason is that we start, given the $X,Y$ data, with a higher and higher-variance posterior on the prevalence of positive effects as compared to negative effects. So there is simply more scope for the data to influence our beliefs about positive effects, and the on-the-line cases are those in which we can learn "directly" about positive effects. In support of this view, note how much bigger the swing in beliefs, conditional on the data pattern, we see for the probability of positive causation than for the probability of negative causation. -->

<!-- The second reason is prevalence: given the $X,Y$ correlation, we believe that there are more cases out there that potentially contain positive effects than there are cases that potentially contain negative effects. Thus, *whatever* it is we learn about positive effects is going to have a bigger impact on our beliefs about the population as a whole than is whatever we learn about negative effects. Since on-the-line cases yield the most direct learning about positive effects, they are therefore the most informative about the population. -->

<!-- Note, importantly, that this finding is consistent with recommendations in the literature to investigate on-the-line cases [@Lieberman2005nested; @goertz2017multimethod],^[We set aside here Lieberman's recommendation to choose off-the-line cases for discovery-oriented "model-building" because we see that enterprise as distinct from the estimation endeavor to which our analysis is oriented.] but for a very different reason. Lieberman and Goertz emphasize the importance of examining cases where the theorized mechanism connecting $X$ to $Y$ might potentially play out, arguing that this would be in the set of cases that conform to theory in their $X,Y$ values. We would emphasize that, in the causal-model framework, one *can* learn about both overall effects and intermediate effects from off-the-line cases. The reason why on-the-line cases is simply that they are more *representative* about the universe about which we aim to learn (assuming we want to learn about the population from which the $X,Y$ pattern is drawn). -->

<!-- Finally, we consider $N=2$ case-selection strategies that fix the value of $X$ or the value of $Y$ in both cases, while seeking variation on the other variable. The "fix $X=1$" strategy selects one $X=1, Y=0$ case and one $X=1, Y=1$ case. The "fix $Y=1$" strategy selects one $X=0, Y=1$ case and one $X=1, Y=1$ case.  Across queries, we see that these two strategies generate roughly equivalent opportunities for learning. Interestingly, we also see that these strategies generates significantly *less* learning than selecting on *both* $X$ and $Y$, whether on-the-line or off-the-line.  -->

<!-- Why? When we select on-the-line or off-the-line, we are building in variation in both $X$ and $Y$ across the cases. In contrast, when we select on a value for $X$ or a value for $Y$, while letting the other variable vary, we are building in variation in only one variable. This means that the "signal" we get from the $M$ pattern is likely to be more ambiguous across the cases we end up with. For instance, if we examine an  $X=1, Y=1$ case and an $X=1, Y=0$ case, suppose we observe $M=1$ in both cases. That $M$ pattern is consistent with $X$ having an effect on $M$, but inconsistent with $M$ have an effect on $Y$. And what if, instead, we observe $M=1$ in the first case and $M=0$ in the second case? Then we have evidence of an $M \rightarrow Y$ effect but not of an $X \rightarrow M$ effect. In both scenarios, there is no strong implication for the higher-level $X \rightarrow Y$ causal queries we are interested in.   -->
<!-- We can see this logic reflected in the expected posterior variance reductions for the $ATE$ as compared to the $ATE$ at the individual causal stages. Looking at Figure \@ref(caseselectionvar2), we see that we actually expect to learn a good deal about the $X \rightarrow M$ effect and about the $M \rightarrow Y$ effect from a fix-$X$ or fix-$Y$ strategy. The problem is that we expect rarely to learn about both stages *at the same time* -- hence the limited learning about the $X \rightarrow Y$ effect.  -->

<!-- <!-- We note that the same difficulty would arise if we were to select for variation in $X$ or $Y$. If we select for variation in $X$, for instance, we could end up with an $X=0, Y=1$ case and an $X=1, Y=1$ case, for which $M$ will be uninformative. We could get lucky and end up with an informative pair ($X=0, Y=0$ and $X=1, Y=1$), but a strategy of selection conditional on one variable leaves that matter to chance.  -->


#### $N=2$ strategies, monotonic models

Generally speaking, we get more leverage across strategies, models, and queries if we are willing to rule out negative effects by assumption. The most dramatic illustration of this is in a comparison of the unconstrained to monotonic moderator and two-path models, where we face bleak prospects of learning about the $ATE$ and probability of positive effects in an unconstrained model, regardless of strategy. Imposing monotonicity assumptions on these two models makes for relatively similar $ATE-$learning opportunities across $N=2$ strategies while boosting the relative performance of two-on (best) and two-off (second-best) strategies for learning about the probability of positive causation.

Relative performance also flips in some places. For instance, whereas two-pos gives us the most leverage for estimating the $ATE$ in an unconstrained two-path model, the two-on strategy is optimal once we impose monotonicity. And two-pos leapfrogs two-off for estimating positive indirect effects when we go from an unconstrained to a monotonic two-path model. The opposite seems to be true for estimating the $ATE$ or the probability of positive causation in a confounded model, where two-off does relatively better when we introduce montonicity restrictions. 




<!-- #### Moderator models -->

<!-- What if $M$ appears in the model as a moderator, rather than a mediator, of $X$'s effect on $Y$: $X \rightarrow Y \leftarrow M$?  -->

<!-- Intuitively, we might think that we can learn from examining a moderator because of the fact that a moderator is another potential cause. So, for instance, if we see $M$ constant while $Y$ varies (or vice versa) this suggests evidence against $M$'s effect --- and, we might conjecture, *for* $X$'s effect on $Y$.  -->

<!-- Our simulations, broadly, speak against this logic. We see in Figure \@ref(fig:caseselectionest) that our inferences hardly budge conditional on different realizations of the moderator variable, regardless of case-selection strategy. We also see in Figure \@ref(fig:caseselectionvar) that we get very little reduction in expected posterior variance.^[There seems to be a bit of learning about the probability of causation, but attention to the $y$-axis scale reveals that the reductions in expected posterior variance here are very small.] The key reason, we think, is the non-rivalry of causes. Given no restrictions on $Y$'s nodal types and no prior evidence pertaining to $M$/$X$ interactions, evidence for or against $M$'s effect on $Y$ has little impact on our beliefs about $X$'s effect. -->

<!-- #### Two-path models -->

<!-- We turn now to a two-path model in which $X$ can have both an indirect effect on $Y$ via $M$ and a direct effect on $Y$. We first examine a version with flat priors over all nodal types and then versions in which we impose some monotonicity restrictions and priors over the two paths. We note that, for a model with multiple paths, we may be interested in an additional, mechanismic query, such as the effect operating along a *particular* path. -->

<!-- *Two-path model with flat priors.* In the two-path row of graphs, we see the results of the case-selection exercise for a two-path model with flat priors. In Figure \@ref(fig:caseselectionvar), we see that most case-selection strategies yield no prospect of learning about average effects of $X$ on $Y$ or about the probability that $X$ caused $Y$ in a given case.  -->

<!-- The reason, we think, is the combination of the double pathway and uninformtativeness of priors about intermediate effects. We go in with a flat distribution across nodal types $\theta^M$ and nodal types $\theta^Y$. We then acquire some information on the $X \rightarrow Y$ effect from the $X,Y$ data, but those effects are equally consistent with all possible pathways---direct, indirect, or combined. Suppose we then find evidence that cuts against the operation of the indirect pathway: $M=1$ is observed in two on-the-line cases with different $X$ values. So we downgrade the probability of $\theta^M_{01}$ relative to $\theta^M_{11}$. However, we are still able to preserve our prior on the effect of $X$ on $Y$ by updating our beliefs about $\theta^Y$ in a manner that places greater weight on direct effects. To put the point differently: high flexibility in our lower-level beliefs impedes learning about higher-level quantities from observations at the lower level. This is true regardless of which quadrants we select our cases from. -->

<!-- There is, however, one case-selection strategy that stands out: the selection of two $X=1, Y=1$ cases. The learning here relatively weak, but it is notable that our inferences about the $ATE$ and the probability of (especially negative) causation are affected by what we observe on $M$ across the two cases. It is not always easy to "intuit" one's way through updating on a causal model, and we are not sure why the 2-case $X=1, Y=1$ strategy performs better here. The learning about the $X \rightarrow Y$ causal effects is still minimal, however, even for this "optimal" strategy. -->

<!-- What if instead of the  $X \rightarrow Y$ causal effect, we are interested in the *pathway* along which that effect unfolds? Here $M$ can be informative, as we can see from the results in Figure \@ref(fig:caseselectionvar2), displaying expected reductions in posterior variance for a range of additional queries. Moving down the graph, we first see learning emerge for the intermediate causal effects, of $X$ on $M$ and of $M$ on $Y$. Indeed, even observing $M$ in a single case yields leverage on these two effects (since, for instance, $M=1$ in an $X=1$ case is inconsistent with a negative $X \rightarrow M$ effect in that case). As for our other queries, it is the on-the-line cases that are most informative about the intermediate effects. -->

<!-- We also see that we can learn from $M$ about the pathway through which $X$'s effect on $Y$ operates. Focusing on the indirect effect rows (one conditioning on $X=1$, one conditioning on $X=0$), we see that we need at least 2 cases to learn about these effects. We also see that, for $N=2$, we learn more about them from on-the-line than off-the-line cases. It is striking, however, that we still can learn about the mechanism through which a positive effect unfolds *by studying cases in which only a negative effect is possible*.  -->

<!-- *Two-path model with monotonicity restrictions.* The situation changes considerably if we come in with informative beliefs at the lower level. In the two-path-monotonic set of graphs, we see results for a two-path model in which we have imposed stronger beliefs about the direction of causation at the lower level, ruling out negative $X \rightarrow M$ and negative $M \rightarrow Y$ effects. Now we see in Figure \@ref(fig:caseselectionvar) that we can learn from all of the strategies. Perhaps most striking is that we can learn even just from $N=1$ strategies in this model. Observing $M=0$ in an $X=1, Y=1$ case decisively rules out the indirect pathway as a mechanism of causation, thus making a positive effect overall less likely in the case than if we observed $M=1$.  -->

<!-- We also see that our $N=2$ strategies are all informative. They are generally more weakly informative than for the chain model, reflecting the fact that we are still getting some countervailing updating of beliefs about the direct pathway. But the restrictions make $M$ sufficiently informative -- our beliefs about causation along the indirect pathway can shift by a sufficient magnitude -- that the updating about the direct pathway cannot fully offset it. So evidence for (against) the operation of the indirect pathway registers, on net, as evidence for (against) the operation of a total $X \rightarrow Y$ effect. -->

<!-- In Figure \@ref(fig:caseselectionest), we can also observe asymmetries in the learning that we do not see in the other models. That is, our beliefs move further in absolute-value terms from the baseline given by the pure $X,Y$ data depending on what we find. This asymmetry is driven by the asymmetry of the restrictions. In excluding intermediate negative effects along the indirect path, we have made the search for $M=1$ a "hoop" test for the operation of this path: it *might* be operating if $M=1$ (generating a small upward shift in the causal estimand) but is certainly not operating if $M=0$ (generating a larger downward shift). We derive further information here from the shading -- representing the probability of each data-realization -- which we see is also asymmetrical. In general, we see that the data pattern that would generate the larger shift in beliefs away from the prior is *less* likely to occur (a mathematically necessary feature -- otherwise, we could not logically hold the prior belief that we hold). -->

<!-- Another interesting result is that, under these restrictions, it is the *off*-the-line strategy that holds the potential for the greatest learning. We believe that the reason relates, again, to the asymmetry introduced by the restrictions. In an off-the-line pair of cases, we know that the indirect path cannot be fully operating: any $X \rightarrow Y$ effect here must be negative, but the restrictions only allow for a positive indirect effect. Another way to put this is that an off-the-line case is one in which we start out with the lowest prior on the mediating effects along the indirect path: it's the kind of case in which we least expect an effect of $X$ on $M$ or an effect of $M$ on $Y$. This creates an opportunity for an especially strong "surprise": in particular, if we observe $X=0, M=0, Y=1$ and $X=1, M=1, Y=0$ in the two off-the-line cases, we have found evidence in *favor* of one of the two intermediate effects, the $X \rightarrow M$ effect. Now, this pattern also constitutes evidence against the operation of an $M \rightarrow Y$ effect; but as we have said, we did not particularly expect to find such an effect in off-the-line cases, so this negative finding does not affect our beliefs much. So we see a large boost in our belief about the average effect of $X$ on $M$, a small drop on the estimated average effect of $M$ on $Y$, a considerable boost in our belief in the average indirect effect of $X$ on $Y$, and a concomitant upward shift in our beliefs about the $ATE$.  -->

<!-- Observing $M$ vary in the opposite direction across the two off-the-line cases --- so positively varying with $Y$ but not $X$ --- generates strong downward movement in our beliefs about the indirect effect and the $ATE$. It seems that the hit to the $X \rightarrow M$ effect matters more for the indirect (and total) effect than does the boost to the $M \rightarrow Y$ effect. -->

<!-- Looking on the line, in the monotonic two-path model, also yields opportunities to learn, but weaker ones. If we see $M$ vary in concert with $X$ and $Y$ across the one-the-line cases, we certainly shift our beliefs in indirect effects and the $ATE$ upward. But the shift is smaller than in the off-the-line comparison because these are cases in which we started out with a stronger expectation that the indirect path would be operating and, in turn, that $M$ would covary with $X$ and $Y$. There is simply less opportunity for surprise. -->

<!-- Notably, when we turn to our pathway query (indirect effects), the monotonicity restrictions do not make nearly as a big difference to the possibility of learning as they do for the $X \rightarrow Y$ effects. We also see that the on-the-line strategy performs best for the mediation query, in constrast to the $ATE$ query, as it does for the pathway query in the unrestricted version of the model. -->

<!-- *Two-path model with weak priors on direct path* Finally, we consider a model in which we have no restrictions but adopt priors under which the direct path is believed to be relatively weak. To think about what this means, recall that the limits to learning from $M$ in the two-path model with no informative priors or restrictions derive from the lack of constraint on countervailing updating about the direct path as we learn about the indirect path. When we start with a greater prior weight on the indirect than the direct path, however, we are dampening this countervailing updating. We should expect therefore to see more opportunities for learning from $M$. -->

<!-- As we see in Figure \@ref(fig:caseselectionvar), the priors do not make a large difference, but they do allow for strategies beyond the strategy of selecting two $X=1, Y=1$ cases to provide leverage on the $ATE$ and, even more so, on the probability of positive and negative causation. The second-most promising strategy for the $ATE$ is the on-the-line strategy, followed by the off-the-line strategy. As with the simple chain model, we see that the off-the-line strategy performs best for the negative probability of causation query, while the on-the-line strategy performs best for the probability of positive causation --- and, in fact, dominates the $X=1, Y=1$ strategy for that query. -->


<!-- FLAG: Cannot yet see why we learn more from the restricted 2-path model than from the flat 2-path. Yes, we can learn from n=1 about the indirect path. But in both cases, we can still have compensating updating on the direct path for anything we learn about the indirect path. In fact, the prior probability of positive effects on each path is very similar across the two models. Ignoring interactions, for indirect path, b share is like 1/8 in flat model, and 1/9 in restricted, vs. 1/4 for direct path. Is it just that the learning about the indirect path is stronger with restricted priors because M starts out as informative about the indirect path? So the learning about the indirect path more easily swamps the prior? -->

<!-- FLAG: Incorporate as needed above and below any implications of Jeffreys priors. -->

<!-- FLAG: Let's move the 2-pos strategy to the right of the 2 on strategy in the graphs.  -->









<!--chapter:end:13-case-selection.Rmd-->

# Going wide, going deep {#wideordeep}

```{r packagesusedwd, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
As we have argued, we can use a causal model, together with priors over parameters, to estimate the expected learning from different empirical strategies. We have so far demonstrated this approach to research design for clue-selection and case-selection. In this chapter, we turn to the problem of choosing between going "wide" and going "deep." We discuss the tradeoffs and communicate an intuition that clue data, even on a small number of cases, can be informative even when there is $X, Y$ data on a very large number of cases, but only if it provides information that cannot be gathered from $X,Y$ data, such as selection into treatment. Simulations suggest that going deep is especially valuable for observational research, situations with homogeneous treatment effects, and, of course, when clues have strong probative value.
::::


We continue exploring how we can leverage causal models in making research-design choices by thinking about the tradeoff between intensive (deep) and extensive (wide) analysis. Suppose that we have identified those clues that will be most informative and those cases in which it would be most valuable to conduct process tracing, given our beliefs about the world. A further question that we face is the quintessential dilemma of *mixing* methods: what mixture of quantitative and qualitative evidence is optimal? We have argued in in Chapter \ref(mixing) that the distinction between quantitative and qualitative inference is, in a causal-model framework, without much of a difference. But here we frame a more precise question: given finite resources, how should we trade off between studying a larger number of cases at a given level of intensiveness, on the one hand, and drilling down to learn more intensively about some subset of the cases in our sample? How should we decide between going "wide" and going "deep"?

Just as with the selection of clues and cases, examined Chapters \@ref(clue) and \@ref(caseselection), how much we should expect to learn from going wide versus going deep will depend on how we think the world works, as expressed in the causal model with which we start and as shaped by the data that we have seen at the point of making the wide-versus-deep decision. 

We examine here queries commonly associated with large-$N$, quantitative strategies of analysis (such as average treatment effects) as well as queries commonly associated with more case-oriented, qualitative approaches (queries about causal pathways and about causal effects at the case level). The analysis in this chapter makes clear the opportunities for integration across these lines of inquiry. We show that investing in-depth process tracing will sometimes make sense even when one aims to learn about average effects in a population. Likewise, collecting $X, Y$ data can sometimes help us draw inferences that will aid in case-level explanation. Particular kinds of case-level information can teach us about populations, and understanding population-level patterns can help us get individual cases right. 


##  Walk-through of a simple comparison

To build up our intuitions about how the optimal mix of strategies might depend on how the world works, let us explore a simple comparison of wide and deep strategies. We focus here on the question of how much we can learn from drilling deeper, given an initial set of $X,Y$ data and beliefs about the world.

Imagine a world in which we have a large amount of data on $X$ and $Y$ (2000 observations), and we see that $X$ and $Y$ are perfectly correlated. We might be tempted to infer that $X$ causes $Y$. And, if $X$ were randomly assigned, then we might be able to justify that inference. Suppose, however, that our data is observational and, in particular, we were aware of an observed confound, $M$, that might determine both $X$ and $Y$. In that situation, the effect of $X$ on $Y$ is not identified. As shown by @manski1995identification,  different priors could support beliefs about that effect lying anywhere between 0 and 1. From @pearl2009causality's backdoor criterion, we also know that if the right causal model is $X \rightarrow Y \leftarrow M \rightarrow X$, then data on $M$ would allow the effect of $X$ on $Y$ to be identified. We could estimate the effect of $X$ on $Y$ for $M=0$ and for $M=1$ and take the average. 

Suppose now that we aim to collect additional data, but that data on $M$ for a single unit is far more costly than data on $X$ and $Y$ for a single unit. We thus face a choice between gathering *a lot* more data on $X$ and $Y$ (say, for 2000 more cases) or gathering a *little* data on $M$ for a subset of cases---just 20 in this illustration. Which should we do? Is 20 cases enough to probe the causal model to see whether the correlation between $X$ and $Y$ is spurious or not?

We get an intuition for the answer by imagining the inferences we might draw in 3 extreme cases and compare these to the base case. Figure \@ref(fig:widedeepXYMX) illustrates. The figures are generated by forming a model with $X\rightarrow Y \leftarrow M \rightarrow X$, strong priors that $\Pr(M=1)=0.5$, and flat priors on all other nodal types. In other words, in our priors we think that $M$ is equally likely to be a 0 or 1 but do not make assumptions about how it is related to $X$ and $Y$. We first update the model with the $X,Y$ data --- and then choose between going wider and going deeper. 




```{r widedeepXYMX, echo = FALSE, fig.width = 10, fig.height = 8, fig.cap = "Posteriors on the ATE given different wide or deep data patterns."}

if(do_diagnosis){
  
  M <- make_model("X -> Y <- M -> X") %>% set_priors(node = "M", alpha = 100)

  dfM2 <- dfM1 <- df <-
    data.frame(X = rep(0:1, 1000), Y = rep(0:1, 1000), M = NA)  
  
  dfM1$M[1:20] <- rep(0:1, each = 10)
  dfM2$M[1:20] <- rep(0:1)
  
  out <- list(
    base = update_model(M, df, keep_transformed = TRUE, iter = 5000) %>% 
      query_distribution("Y[X=1]  - Y[X=0]", using = "posteriors"),
    wide = update_model(M, rbind(df, df), keep_transformed = TRUE, iter = 5000) %>% 
      query_distribution("Y[X=1]  - Y[X=0]", using = "posteriors"),
    deep1 = update_model(M, dfM1, keep_transformed = TRUE, iter = 5000) %>% 
      query_distribution("Y[X=1]  - Y[X=0]", using = "posteriors"),
    deep2 = update_model(M, dfM2, keep_transformed = TRUE, iter = 5000) %>% 
      query_distribution("Y[X=1]  - Y[X=0]", using = "posteriors"),
        deep1check = update_model(M, dfM1[1:20,], keep_transformed = TRUE, iter = 5000) %>% 
      query_distribution("Y[X=1]  - Y[X=0]", using = "posteriors")
  )
  
  write_rds(out, "saved/ch13_wide_deep_histograms.rds")
  }

out <- read_rds("saved/ch13_wide_deep_histograms.rds")
par(mfrow = c(2,2))
hist(out[[1]], main = "(1) 2000 XY observations", xlab = "ATE", xlim = 0:1, freq = FALSE)
hist(out[[2]], main = "(2) Add another 2000 XY observations", xlab = "ATE", xlim = 0:1, freq = FALSE)
hist(out[[3]], main = "(3) Add 20 M observations (uncorrelated)", xlab = "ATE", xlim = 0:1, freq = FALSE)
hist(out[[4]], main = "(4) Add 20 M observations (correlated)", xlab = "ATE", xlim = 0:1, freq = FALSE)

```


Panel 1 in  Figure \@ref(fig:widedeepXYMX) shows our posterior distribution over the average causal effect from observation of the base data: 2000 cases with $X$ and $Y$ perfectly correlated. The distribution is quite wide, despite the strong correlation, because the posterior includes our uncertainty over the nature of confounding.  Our estimate for the $ATE$ is `r round(mean(out$base), 2)` but with a posterior standard deviation of  `r round(sd(out$base), 4)`. There is positive weight on all positive value of the $ATE$.

How can we improve on this estimate?

One possibility would be to go wide and collect $X,Y$ data on an additional 2000 cases. Panel 2 displays our posterior on the average causal effect with the addition of these 2000 cases. We assume that the new data also display a perfect $X,Y$ correlation, like the first set of data. Again, we could not imagine data that more strongly confirms a positive relation, and now we have twice as much of it. What we see, however, is that investing in gathering data on 2000 additional cases does not help us very much. The mean of our posterior on the $ATE$ is now `r round(mean(out$wide), 2)`, with a standard deviation of  `r round(sd(out$wide), 4)`. So the updating is very slight.

Suppose that, for the cost of gathering $X,Y$ data on an additional 2000 cases, we could drill down on a random subset of 20 of the original 2000 cases and observe $M$ in those cases. What might we learn? 

Because we start out with a flat prior on how $M$ will relate to $X$ and $Y$, we display inferences for two possible realizations of that pattern. In Panel 3, we show the updating if $M$ turns out to be uncorrelated with both $X$ and $Y$. The mean of our posterior on the $ATE$ now rises to `r round(mean(out$deep1), 2)`, and the posterior standard deviation shrinks dramatically, to  `r round(sd(out$deep1), 4)`. Greater depth in a relatively small number of cases is enough to convince us that the $X,Y$ relationship is not spurious.  

Panel 4 shows inferences from the same "going deep" strategy but where $M$ turns out to be perfectly correlated with $X$ and $Y$. Now our estimate for the $ATE$ shifts downward to `r round(mean(out$deep2), 2)`, with a posterior standard deviation of  `r round(sd(out$deep2), 4)`. 

In other words, in this setup, what we observe from our "going deep" strategy can have a big impact on our inferences. One reason we stand to learn so much from process-tracing so few cases is that the process-tracing speaks to relationships about which we start out knowing so little: $M$'s effect on $X$ and $M$'s effect on $Y$, effects on which the $X,Y$ data themselves shed no light. 

It is also interesting to note that we cannot learn as much by updating *only* using information from the 20 cases for which we have full $X$, $M$, $Y$ data. Were we to use only the subset with this complete data --- ignoring the other 1880 cases --- and observe $M$ to be uncorrelated with $X$ and $Y$, the mean of our posterior on the $ATE$ would be `r round(mean(out$deep1check), 2)` with a posterior standard deviation of  `r round(sd(out$deep1check), 4)` (not graphed). The breadth provided by those 1880 $X,Y$-only cases thus adds a great deal. While observing an uncorrelated $M$ in 20 cases allows us to largely rule out $M$ as a cause of any $X,Y$ correlation, observing a strong $X,Y$ correlation over a large number of cases provides evidence that $X$ in fact affects $Y$.

We use this example to illustrate a simple but stark point: there will be situations in which the expected gains from collecting more data on the same cases and from collecting the same data on more cases will be different, sometimes very different. The model and the prior data shape the tradeoff. In this particular setup, it is the confounding together with the large number of prior $X,Y$ observations that makes depth the better strategy. Once we have learned from 2000 $X,Y$ observations, data of the same form from more cases will not change beliefs. Yet going deep---even if only in a few cases---provides information on parameters we know nothing about, helping us interpret the $X,Y$ correlation in causal terms.


## A simulation-based approach to choosing mixes

While the results in the last section are striking, they depend upon particular realizations of the data under each strategy. When selecting strategies we, of course, do not know how the data will turn out. Our problem becomes, as in the case-selection analyses, one of figuring out the *expected posterior variance* from different strategies. 

<!-- ```{r morn1, fig.width = 6, fig.height = 5, echo = FALSE, message = FALSE, fig.cap = "Distributions of posterior variances from many simulated datasets"} -->
<!-- path = "rep/wide_deep_2" -->
<!-- results <-  -->
<!--   list( -->
<!--     Chain = read_rds(paste0(path, "/chain.rds")), -->
<!--     Confounded = read_rds(paste0(path, "/confounded.rds")), -->
<!--     Confounded_large_n = read_rds(paste0(path, "/confounded_large.rds")), -->
<!--     Restricted = read_rds(paste0(path, "/restricted.rds")), -->
<!--     Base = read_rds(paste0(path, "/base.rds")), -->
<!--     Chain_homogeneous = read_rds(paste0(path, "/chain_homog.rds")), -->
<!--     Chain_with_priors = read_rds(paste0(path, "/chain_with_prior.rds")) -->
<!-- ) %>% bind_rows(.id = "Model") %>% -->
<!--     mutate(Model = ifelse(Model == "Confounded_large_n", "Confounded", Model)) -->

<!-- r2 <- results %>% filter(Model == "Confounded" & wide_n %in% c(800, 1600) & deep_n %in% c(0, 100) & Case.estimand == "FALSE" & Given == "-") -->

<!-- r2 %>% -->
<!--   ggplot(aes(sd^2)) + -->
<!--   geom_histogram() +  -->
<!--   facet_grid(deep_n ~ wide_n, labeller = label_both) + xlab("Posterior variance on the ATE") -->
<!-- ``` -->


<!-- ```{r morn2, echo = FALSE, message = FALSE} -->
<!-- r2 %>% group_by(wide_n, deep_n) %>% summarize(expected = mean(sd^2))  %>% kable(caption = "Expected posterior variance on the ATE", digits = 4) -->

<!-- ``` -->


### Approach

The more general, simulation-based approach that we introduce here is parallel to the approach for case-selection. The steps of this procedure are:

1. **Model**. We posit a causal model, with any priors or restrictions.
2. **Prior data**. We specify the data that we already have in hand. For the simulations below, we assume no prior data.
3. **Strategies**. We then specify a set of mixing strategies to assess. A strategy, in this context, is defined as any combination of collecting data on the same nodes for a given number of additional cases (randomly drawn from the population) and collecting data on new nodes for a random sample of the original set of cases.
4. **Data possibilities.** For each strategy, we define the set of possible data-realizations. Whereas for case-selection the structure of the possible data-realizations will be the same for all strategies of a given $N$, possible data patterns in wide-versus-deep analyses involve much greater complexity and will vary in structure across strategies. This is because the number of cases itself varies across strategies. Also, whereas we fix the $X,Y$ pattern for the purposes of case-selection, here we allow the $X,Y$ patterns we discover to vary across each simulation draw.
5. **Data probabilities**. As for case-selection, we use the model and prior data to calculate the probability of each data possibility under each strategy.
6. **Inference.** Again, as for case-selection, we update the model using each possible data pattern to derive a posterior distribution. 
7. **Expected posterior variance.** We then average the posterior distributions across the possible data patterns under a given strategy, weighted by the probability of each data pattern.

<!-- We illustrate the last two steps in Figure \@ref(fig:morn1), using a similar setup to the one above. We again posit the same $X \rightarrow Y \leftarrow M \rightarrow X$. We assume that we have started with $X,Y$ observations for 800 cases. We then choose among drawing another 800 cases for $X,Y$ analysis, going deeper in 100 of our cases, or both. What Figure \@ref(fig:morn1) shows is the distribution of *posterior variances* that we obtain from many possible data draws under each strategy.  In the top row, we are comparing two levels of breadth, with no depth. Moving to the bottom row, we introduce depth in 100 cases, for each level of breadth. We see here a distinct shift in the distributon to the left --- meaning, a reduction in the expected posterior variance -- when we drill down to gather data on $M$. In contrast, we see no such shift in the expected posterior variance when we increase the number of cases for which we gather data on $X$ and $Y$. For greater precision, we report the expected value for each distribution of posterior variances in Table \@ref(tab:morn2). The pattern here is consistent with the analysis above, although here we have averaged across all *possible* data patterns arising from the strategies, rather than focusing on just a couple of data possibilities.  -->

<!-- This approach can be applied to *any* model, with *any* query, and *any* prior set of data.  -->

### Simulation results

We now explore alternative mixes of going wide and going deep for a range of models and queries, the same set that we examined for case-selection in Chapter \@ref(caseselection). We present the results in compact form in Figure \@ref(morn3). The structure of the panels is the same as those in the figure in Chapter \@ref(caseselection), with models being crossed with queries. Within each panel, each line represents going wide to a differing degree: collecting $X,Y$ data for $N=100$, for $N=400$, and for $N=1600$. As we move rightward along each line, we are adding depth: we show results for a strategy with no process-tracing, for process tracing 50 of the $X,Y$ cases, and for process-tracing 100 of the $X,Y$ cases. On the $y-$axis of each graph, we plot the expected posterior variance from each wide-deep combination.^[Note that the expected posterior variance is always $0$ for queries that are already answered with certainty by the model itself, such as the probability of a negative effect in a model with negative effects excluded.]

Looking at the figure as a whole, one pattern that leaps out is that there are, for almost all model-query combinations, gains from going wider. We can also see, unsurprisingly, that the marginal gains to going wide are diminishing. It is also interesting to note where going wider appears to add little or nothing: when we want to estimate the probability that a positive effect runs through the indirect path, we learn nothing from observing $X$ and $Y$ for a larger set of cases. 

Focusing just on the gains to depth, we see that these are much more concentrated in specific model-query combinations. Going deep to learn about the $ATE$ or the probability of positive or negative causation appears at best marginally advantageous --- at least up to $N=100$ --- for the unconstrained confounded and moderator models and for both unconstrained and monotonic two-path models. Notably, we *do* learn from going deep in both the unconstrained and monotonic chain models. As for going wide, the marginal gains from going deep are also diminishing, with the biggest gains arising from the first 50 cases in which we observe $M$.

Part of what is likely going on here is that unconstrained models are ones in which we start out with no information about $M$'s probative value. We can *learn* about $M$'s effects from observing $M$ (as discussed in Chapters \@ref(mixing) and \@ref(mixingapp)); this is why we can learn from process tracing in even the unconstrained version of the chain model. However, that learning gets much harder in unconstrained models with confounding or in models with two paths. In a confounded model, for instance, suppose we see $M$ positively correlated with $X$ and negatively correlated with $Y$, with $X$ and $Y$ themselves negatively correlated. This pattern is consistent with $M$ having a positive effect on $X$ and a negative effect on $Y$, and thus a high degree of confounding. But it is equally consistent with $M$ having only a positive effect on $X$ and $X$ having a negative effect on $Y$, with no $M \rightarrow Y$ effect and thus no confounding. A monotonicity assumption makes $M$ *a priori* informative: for instance, when we see $X=1, Y=1$, confounding is only a possibility if we also see $M=1$; confounding is ruled out if we observe $M=0$. 

In a two-path model, observing $M$ correlated with $X$ and with $Y$ across a set of cases should allow us to learn about $M$'s informativeness about the operation of an *indirect* effect, just as we can learn about $M$'s probative value in a chain model. The problem is that knowing about the indirect effect in a two-path model contributes only marginally to the first three queries in the figure since these are about total effects. Thus, even adding monotonicity restrictions, which makes $M$ *a priori* informative, does not significantly improve learning from $M$ about total effects.

The important exception, when it comes to learning about the two-path model, is when it is the *pathway itself* that we seek to learn about. As we can see in the figure, we can learn a great deal about whether effects operate via $M$ by observing $M$, even in an unconstrained two-path model. Interestingly, the gains from depth for causal-effect queries in an unconstrained chain model closely resemble the gains from depth for the indirect-effect query in the unconstrained two-path model. This similarity suggests that both kinds of model-query combinations allow for learning *about* $M$ that, in turn, permits learning *from* $M$. 

We also see that the context in which depth delivers the steepest gains of all is when we seek to learn about the probability of an indirect-effect in a monotonic two-path model. Part of the reason is likely that $M$ is *a priori* informative about the operation of the mediated pathway (as it is about the operation of effects in the monotonic chain model). Additionally, however, it appears that we start out with relatively high uncertainty about the pathway query because the model itself is quite uninformative about it. Thus, for instance, we learn much more from depth here than we do for a total effect query in a monotonic chain model: the monotonicity assumptions themselves already tell us a great deal about total effects, whereas they imply nothing about the path through which effects unfold. There is simply more to be learned from $M$ about pathways than about total effects.

Most interesting perhaps is using the graphs to examine different wide versus deep tradeoffs we might face. Suppose, for instance, that we wish to learn about the probability of negative causation in an unconstrained chain model for $X=0, Y=1$ cases. We start out with $X,Y$ data for 100 cases and have additional resources with which to collect more data. Let us further assume that the cost of collecting $X,Y$ data for an additional 300 cases is equal to the cost of collecting $M$ on 50 of the original 100 cases. Where whould we invest?

We can read a fairly clear answer off of the graph. As we can see in the relevant graph, we can expect to do better from adding depth than by adding breadth. In fact, even expanding our $X,Y$ sample to 1600 cases only gets us about as much leverage as get from process-tracing 50 cases.

We can also see how the optimal choice depends on what data-collection we have already committed to. Consider the probability of positive causation query for a monotonic confounded model. If we start with $X,Y$ data for 100 cases, we expect to gain more from going deep in 50 of these cases than from going wide to an additional 300. However, once we have decided to process-trace 50 cases, if we have additional resources, we then expect do be much better by investing in an expansion to 300 $X,Y$ cases than by process-tracing the other 50 $X,Y cases.

A further question we can ask is: where is mixing methods advantageous? And where are maximally wide or maximally deep strategies best? We can examine this question by comparing a strategy with maximal breadth and no process tracing; a strategy with maximal process-tracing and minimal breadth; and a strategy in which we invest in a mixture of new data, by examining $X,Y$ in 400 cases while process-tracing 50 cases. 

We see some places where we are best off going as wide as possible, at least for the ranges we explore in these simulations. For instance, if we wish to estimate the $ATE$ in a chain model (unconstrained or monotonic), a pure "going wide" strategy is optimal. At the other extreme, when we seek to learn about the probability of an indirect effect from an unconstrained two-path model, we are best off process-tracing our original 100 cases and gain nothing by expanding our sample.

In many contexts, however, it is the mixed strategy that performs best. The advantage of mixing appears starkest for the confounded monotonic model: examining $X$ and $Y$ for 400 cases and process-tracing 50 of these delivers much greater gains, for estimation of the $ATE$ and probability of positive causation, than either expanding the sample to 1600 $X,Y$ cases or than going deeper into the original 100 cases.

<!-- Interestingly, it does not appear to be generically the case that depth is of greater value for some estimands than for others, at least for the models explored here. The gains to depth for a given query depend on the model. However, going wide *does* generally seem more advantageous for learning about the $ATE$ than for learning about other estimands, even for the models with confounding.  -->





<!-- For the purposes of this illustration, we focus on the $ATE$ and the probability of positive causation as queries, and the results of the exercise would likely differ for different queries. On each graph, we plot 3 lines, each representing a different number of "wide" ($X,Y$-data only) cases, ranging from 100 to 1600. Each line then shows how the expected posterior variance changes as we add "depth," going from observing $M$ in 0 cases to observing it in 50 and then 100 cases. -->

<!-- Starting with the chain model ($X \rightarrow Y$), we see that there are gains to be reaped from both breadth and depth, for both queries. We can also get a sense of the nature of the tradeoffs. One thing we observe is that, for the $ATE$, the gains to breadth are greater the less depth we have. Once we have process-traced 100 cases, the difference in the posterior variance we expect between having $X,Y$ data only on those 100 cases and having $X,Y$ data on an additional 300 cases is very modest compared to the situation in which we have done no process tracing. Likewise, the gains to depth diminish with breadth: the slope of the depth line flattens as we add $X,Y$ cases. Further, there appear to be diminishing returns to each strategy on its own terms: note, for instance, how the move from 50 to 100 process-tracing cases deliver less expected learning than does the move from 0 to 50. -->

<!-- We can also how the tradeoff varies by estimand. For the $ATE$, the gains to going deep in an additional 50 cases look roughly equivalent to those of expanding the dataset by 100 cases. However, for the probability of positive causation, the scales tip quite dramatically in favor of depth: The gains to depth in 50 cases are more than double the gains of broadening by as many as 300 cases.  -->

```{r morn3, fig.width = 10, fig.height = 7, echo = FALSE, message = FALSE, fig.cap = "Expected posterior variance over multiple models with multiple data strategies."}

read_rds("saved/wide_deep_fig_book.rds")

```


## Factoring in the cost of data

We can also use these results to think through optimal allocations of resources with varying prices of breadth and depth. To illustrate, consider the unconstrained chain model, where we see similar expected posterior variance for the probability of positive causation query  from the following three combinations of wide and deep (first column, third row in Figure \@ref(morn3)): 

1. Maximally wide: 1600 wide + 0 deep
2. Maximally deep: 100 wide + 100 deep
3. Mixed: 400 wide + 50 deep

However, which strategy is optimal will depend on the relative cost of collecting $X,Y$ data for a new case (which we normalize to a cost of 1) and collecting $M$ for an existing case (at cost $d$ per observation).

Then, for this model-query combination, the widest strategy is better than the deepest strategy if and only if $1600 < 100 + 100d$, that is, when $d > 15$. The mixed strategy is better than the maximally deep strategy if and only if $400 + 50d < 100 + 100d$, that is when $d > 6$. And the maximally wide strategy is better than the mixed strategy if and only if $1600 < 400 + 50d$, or $d > 24$. Thus, roughly speaking, if $d < 6$, then our ordering is deepest > mixed > widest, if $6 < d < 15$, our ordering is mixed > deepest > widest, if $15 < d < 24$, our ordering is mixed > widest > deepest, and if $d > 24$, our preference-ordering is widest > mixed > deepest. We can, thus, see that the mixed strategy is optimal across a broad range of $d$, though for sufficiently cheap or expensive within case data gathering it may be optimal to go purely wide or purely deep. 

<!-- For difference prices of $m$ and $d$ any of these may be best. In particular if the cost of w is $1:  -->

<!-- * strategy 2, mixed is better if deep costs between $2 and $4 -->
<!-- * if deep costs more than $4 then go all wide -->
<!-- * If deep costs < $2 then go all deep -->


<!-- Common set -->

<!-- * Chain -->
<!-- * Chain monotonic -->
<!-- * Moderator -->
<!-- * Moderator monotonic -->
<!-- * Two path -->
<!-- * Two path monotonic -->
<!-- * Confounded -->
<!-- * Confounded monotonic -->

<!-- Mediation estimand for two path models -->

<!-- ## Principles -->

<!-- *  Qualitative and quantitative data can act as partial substitutes for assessing causal effects. -->
<!-- *  The *relative* marginal gains from going wider and going deeper vary with the study design.  -->
<!-- * Optimal strategies might involve going deep in a subsample of cases only. -->





<!--chapter:end:14-wideordeep.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# (PART) Models in Question  {-}


# Justifying models {#justifying}

```{r, include = FALSE}
drop_sd <- function(tab, name = "Doubly decisive") {

  a <- tab[, 1:4]
  b <- tab[, c(1:3, 5)] 
  names(a) <- names(b) <- c("Query",	"Given",	"Using", name)
  x <- rbind(a,b)[c(1,4,2,5,3,6),]
  x[c(2,4,6), 1:3] <- ""
  x
}
```

:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
Where do our model assumptions come from? Model assumptions matter most for single-case process tracing because single-case analysis does not permit updating the model itself on the basis of the data: it is the model assumptions that generate beliefs about the probative value of clues and thus our case-level inferences. In this chapter, we outline strategies for justifying our models on the basis of data and, in particular, for *empirically grounding* our beliefs about the probative value of clues.
::::
<br>



```{r packagesused14, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```



The approaches to inference that we have described always involve updating beliefs given data. But to get off the ground researchers need to be able to state priors on all parameters. 

We see two broad responses to this problem.

One is to simply emphasize the model-contingent nature of claims. Some causal models might reasonably reflect actual beliefs about the world---for example, one might be convinced that a treatment was randomly assigned, that there is no interference, and that units are independently sampled. All of these beliefs may be unwise, of course. But if held, then simple models such as that represented by an $X \rightarrow Y$ DAG is a representation of coarse beliefs about the world and less of a model of the world, in the sense of a simplified representation.^[Even in this simple case there are ways in which the representation is a model, not least the coding of events as a variable involves a form of modeling.] Recognizing that we are generally dealing with models that are not true, results in a reposing of the question: the question becomes not whether the assumptions are correct but whether the model is useful for some purpose [@clarke2012model]. 


<!-- For an even modestly more complex situation, it seems inevitable that the model being used really is a model and hard to think of as a faithful summary of beliefs.  -->


<!-- That is the subject of Chapter \@ref(evaluation). -->

A second approach is to seek justify a model empirically. In this chapter, we explore some of the constraints on and possibilities for empirical model-justification. We take up the problem in two steps. First, we focus on process tracing and ask whether and when, given a causal structure, can we empirically derive the probative value of clues. Second, we briefly summarize an approach to discovering causal structures, which are of course a critical input into both process tracing and mixed-method inference.

<!-- by claiming exchangeability with units for which we have a lower-level  model. In a sense, this approach  pushes the question down a level, since the lower level model itself needs to be justified. There are two further responses to this concern. One is to justify the lower-level model with  data or a combination of data and theory. We discuss this approach here.  Another is to assess the importance of the structural assumptions made in our DAG, an approach that we address in Chapter \@ref(evaluation). -->


## Justifying probative value for process-tracing

The problem of justifying assumptions is most acute for case-level process-tracing inferences, for two reasons. First, the  beliefs that come into play in generating probative value for our clues are beliefs over the distribution of *individual-level* effects, not just beliefs over average effects. We need beliefs, for instance, about the probability of seeing some clue, $K$, in a given case if $X=1$ causes $Y=1$ in the cases. This puts us up against the fundamental problem of causal inference [@holland1986statistics]. Second, in single-case inference, we do not have opportunities to learn about our model from the case at hand; so the beliefs we go in with are critical. Indeed for case-level queries, inferences might be little more than conditional applications of a model.

The question we pursue in this chapter is whether and under what conditions we can empirically justify those beliefs that yield probative value for our clues.


### Nothing from nothing


```{r, echo = FALSE, include = FALSE}

agnostic_data <- make_model("X->M->Y") %>% 
  set_parameters(node = c("M", "Y"), parameters = list(c(.1, 0, .8, .1),
                                                       c(.1, 0, .8, .1))) %>%
  simulate_data(n = 1000)

if(do_diagnosis){
  
agnostic_XMY <- make_model("X->M->Y<-X; X<->M; X<->Y; M <->Y")%>%
  update_model(agnostic_data)

write_rds(agnostic_XMY, "ch14_agnostic.rds")


agnostic_query <-  query_model(agnostic_XMY,
              query = "Y[X=1] > Y[X=0]", 
              given = c(TRUE, "M==0", "M==1"),
              using = "posteriors")

write_rds(agnostic_query, "ch14_agnostic_query.rds")


}

agnostic_query <- read_rds("ch14_agnostic_query.rds")

agnostic_query %>% kable(caption = "Conditional inferences from an update agnostic model given a true model in which $X$ causes $M$ and $M$ causes $Y$")
```


We start with a fairly discouraging result. Many of the models we have looked at---especially for process tracing---include a good deal of structure, viz:  

* conditional-independence assumptions
* assumptions of no confounding
* monotonicity assumptions or other restrictions 

What happens if we make none of these assumptions? One way to think about this question is: can we start with a DAG that makes none of these assumptions and then use observational data---i.e., learn from those data---to render clues informative about causal effects?

Suppose that we would like to be able to learn from a mediator node. We work through this problem under favorable conditions: a world in which in fact (though unknown *ex ante* to the researcher): 

* $X$ causes $Y$ through $M$
* $X$ is a necessary condition for $M$, and $M$ is a sufficient condition for $Y$ -- and so $Y$ is monotonic in $X$ and 
* There is no confounding

We also assume that we have access to large amounts of observational data on $X$, $M$, and $Y$.


```{r appnana, include = FALSE}

data <- make_model("X -> M -> Y") %>% 
  
        make_data(n = 2000, 
                  parameters = c(.5, .5, .2, 0, .8, 0, 0, 0, .8, .2))


kable(cor(data), digits = 2, caption = "Data contain strong correlations.")

```


We work through inferences for two types of model in which $X$ can have both indirect and direct effects on $Y$. We impose no restrictions on nodal types in either model. Even though there are only three nodes, this model has 128 causal types ($2\times 4 \times 16$). In addition: 

* In `model_1` we allow confounding between all pairs of nodes. This results in 127 free parameters. 

* In `model_2` we assume that $X$ is known to be randomized but allow for confounding between $M$ and $Y$. There are now only 64 free parameters.


```{r appnana3, include = FALSE}
model_1 <- 
  make_model("X -> M -> Y <- X; X <-> M; M <->Y; X <-> Y") 

model_2 <- 
  make_model("X -> M -> Y <- X; M <->Y") 

```

```{r, echo = FALSE, fig.cap= "Two models. The model on the right might be justified if $X$ is known to be randomized."}
par(mfrow = 1:2)
plot(model_1)
# title(main = list("X not known to be randomized", cex = .5))
plot(model_2)
# title("X  known to be randomized")
# text(0, -1.3, "X  known to be randomized")
```


After updating we query the models to see how inferences depend on $M$ like this:



```{r, echo = FALSE}

if(do_diagnosis){

model_1 <- update_model(model_1, data, iter = 6000)

model_2 <- update_model(model_2, data, iter = 6000)

q1 <- query_model(model_1, 
            queries = "Y[X=1] > Y[X=0]",
            given = c("X==1 & Y==1", "X==1 & M==1 & Y==1", "X==1 & M==0 & Y==1"),
            using = c("priors", "posteriors"), 
            expand_grid = TRUE)

q2 <- query_model(model_2, 
            queries = "Y[X=1] > Y[X=0]",
            given = c("X==1 & Y==1", "X==1 & M==1 & Y==1", "X==1 & M==0 & Y==1"),
            using = c("priors", "posteriors"), 
            expand_grid = TRUE)

write_rds(model_1, "saved/App_somethingnothing_model_1.rds")
write_rds(model_2, "saved/App_somethingnothing_model_2.rds")
write_rds(q1, "saved/App_somethingnothing1.rds")
write_rds(q2, "saved/App_somethingnothing2.rds")
}

q1 <- read_rds("saved/App_somethingnothing1.rds")
q2 <- read_rds("saved/App_somethingnothing2.rds")

# q1 %>% arrange(Using) %>% kable( 
#       caption = "Can observation of large N data render mediator $M$ informative for case level inference? Observational data.")
# 
# kable(q2 %>% arrange(Using), 
#       caption = "Can observation of large N data render mediator $M$ informative for case level inference? X randomized.")

tab <- data.frame(drop_sd(q1[c(2,4,6),], name = "M1") ,  drop_sd(q2[c(2,4,6),], name = "M1")[,4]) 
for(j in 4:5){
tab[c(2, 4, 6), j] <- paste0("(", tab[c(2, 4, 6), j], ")")}

tab %>% kable(caption = "Can observation of large N data render mediator $M$ informative for case level inference? Model 1: No knowledge of structure; Model 2: $X$ known to be randomized. Posterior SD in parentheses.", col.names = c("Query",	"Given",	"Using", "Model 1", "Model 2"), row.names = FALSE)

```

We find that even with an auspicious monotonic data-generating process in which $M$ is a total mediator, $M$ gives no traction on causal inference under Model 1. In contrast, it gives considerable leverage under Model 2: $M$ is informative, especially if $M=0$ (in which case we downgrade confidence that $X$ caused $Y$), when $X$ is known to be randomized.

```{r appnana6, eval = FALSE, echo = FALSE}
model1 <- read_rds("saved/App_somethingnothing_model_1.rds")

kable(cbind(prior = get_parameters(model1, param_type = "flat"),
            posterior = get_parameters(model1, param_type = "posterior_mean")), digits = 2)
```


This example nicely illustrates the @cartwright1994nature's idea of "no causes in, no causes out." It also poses, we think, a challenge to any process-tracing exercise that aspires to than model-independence: observational data alone is not sufficient to generate a justification for process-tracing inferences for 3-node problems *even when in reality* causal structures are simple.

### Justifying the classic process-tracing tests

Now,  on a more encouraging note, we show the  possibility of justification of each of the four classical "qualitative tests" described by @collier2011understanding and drawing on @Van-Evera:1997, at least when treatment assignment is randomized. 

Recall that the four tests are "smoking gun" tests, "hoop" tests, "doubly-decisive" tests, and "straw-in-the-wind" tests. A hoop test is one which, if failed, bodes especially badly for a claim; a smoking gun test is one that bodes especially well for a hypothesis if passed; a doubly decisive test is strongly conclusive no matter what is found; and a straw-in-the-wind test is suggestive, though not conclusive, either way. Of course, Bayesian inference involves continuous probabilities, not discrete test categories, but we speak to these categories for heuristic purposes. The more general point is that probative value can be derived from data in which randomization of a causal variable can be assumed.

In some treatments of Bayesian process tracing (such as our own work in @humphreys2015mixing), formalization involves specifying (a) a prior that a hypothesis is true and, *independently* of that (b) a set of beliefs about the probability of seeing a given data pattern if the hypothesis is true and if it is false. Updating then proceeds using Bayes' rule. 

This simple approach suffers from two related weaknesses however. First, there is no good reason to expect these probabilities to be independent. Our prior beliefs about the hypotheses constitute beliefs about *how the world works*, and beliefs about how the world works should have implications for the conditions under which clues are likely to be observed. Second, there is nothing in the setup to indicate how beliefs about the probative value of clues should be established or justified. 

Both of these problems are resolvable in the context of inference from fully specified causal models. We illustrate first by using an idealized example to show that a case-level "doubly decisive" test can be justified by population-level data from factorial designs, then generalize to all four tests.

Suppose that we have observed just $X$ and $Y$ across a large set of cases, allowing us to infer that $\Pr(Y=1|X=1) = \Pr(Y=1|X=0) = .5$. Here we have an average treatment effect of 0. But suppose, further, that our query is whether $X=1$ caused $Y=1$ in a particular case with $X=1, Y=1$. The marginal distributions we have observed so far are consistent with a world in which $X$ never affects $Y$; one in which $X$ always affects $Y$ (sometimes negatively, sometimes positively); and with many possibilities in between. 

Now, let's say that we have data on a third variable, $K$, and find (a) that $K=1$ arises with 50% probability and (b) that the marginal distributions of $Y$ given $X$ and $K$ are as follows:

* $\Pr(Y=1|X=0, K = 0) = 1$
* $\Pr(Y=1|X=1, K = 0) = .5$
* $\Pr(Y=1|X=0, K = 1) = 0$
* $\Pr(Y=1|X=1, K = 1) = .5$

We thus see that, in cases in which $K=1$, $X=0$ is a necessary condition for $Y=1$. So if $K=1$, then $X=1$ certainly caused $Y=1$ (since, in that case, were $X$ zero then $Y$ would certainly be 0.) On the other hand, were $K=0$, then $X=0$ would be a sufficient condition for $Y=1$, which means that in this case $X=1$ most certainly did *not* cause $Y=1$.  We have then that, if $K=1$, then certainly $X=1$ caused $Y=1$ whereas if  $K=0$, then certainly $X=1$ did not cause $Y=1$
 
This example demonstrates that, with infinite data, it is in principle possible to justify a doubly decisive test on the basis of experimental data---provided that the case about which we seek to make inferences can be considered exchangeable with the cases in the experimental data.  

Table \@ref(tab:tests15) shows how this logic generalizes to different types of tests. For each test we first show the data available to us (first 5 rows); we then show the inferences on whether $X=1$ causes $Y=1$ in cases where $X=1$ and $Y=1$ as a function of $K$.

``` {r tests15, echo = FALSE}

bounds <- function(p0, p1){
  tau = p1 - p0
  rho = p1 - (1-p0)
  if(tau>=0)
  x <- c(lower = (2*tau / (1+ tau + rho)), upper = ifelse(rho<0, 1, (1+tau -abs(rho))/(1+tau + abs(rho)))) 
  x
}

# bounds(.25, .75)
# bounds(.5, .75)
# bounds(1/3, .75)




table <- data.frame(Value = c(
  "$\\Pr(K = 1)$", 
  "$\\Pr(Y=1 \\vert X=0, K = 0)$", 
  "$\\Pr(Y=1 \\vert X=1, K = 0)$",
  "$\\Pr(Y=1 \\vert X=0, K = 1)$", 
  "$\\Pr(Y=1 \\vert X=1, K = 1)$", 
  "$\\Pr(X \\text{  causes }Y \\vert K=1)$",
  "$\\Pr(X \\text{  causes }Y \\vert K=0)$"),
  
  DD =      c(0.5, 1,     "1/2",   0,    "1/2",  1, 0),
  Hoop =    c(0.9, 1,     "1/2", "1/3", "2/3", "[1/2, 1]", 0),
  Smoking = c( 0.1, "1/3", "2/3",   0, "1/2",     1, "[.5, 1]"), 
  Straw =   c(0.5,    "1/2", "3/4",  "1/4", "3/4",    "[1/3,2/3]", "[2/3,1]") 
) 

kable(table, col.names = c("", "Doubly decisive", "Hoop", "Smoking gun", "Straw in the wind"),
             caption = "Known probabilities from a model with  $X \\rightarrow Y \\leftarrow K$  justifying classic test types for clue $K$ given $X=Y=1$.", align=c("l", rep('c', 4)))

```

<!-- ```{r tests15, echo = FALSE} -->
<!-- data.frame(Value = c( -->
<!-- "$\\Pr(K = 1)$",  -->
<!-- "$\\Pr(Y=1 \\vert X=0, K = 0)$",  -->
<!-- "$\\Pr(Y=1 \\vert X=1, K = 0)$", -->
<!-- "$\\Pr(Y=1 \\vert X=0, K = 1)$",  -->
<!-- "$\\Pr(Y=1 \\vert X=1, K = 1)$",  -->
<!-- "$\\Pr(X \\text{  causes }Y \\vert K=1)$", -->
<!-- "$\\Pr(X \\text{  causes }Y \\vert K=0)$"), -->

<!-- DD =      c(.5, 1,     .5,   0,    .5,  1, 0), -->
<!-- Hoop =    c(.9, 1,     .5, .44, "4/9", "", 0), -->
<!-- Smoking = c( 1, "4/9", .5,   0, .5,     1, ""),  -->
<!-- Straw =   c(.5,    .6, .5,  .4, .5,    "", "")  -->
<!-- )  %>% kable(col.names = c("", "Doubly decisive", "Hoop", "Smoking gun", "Straw in the wind"), -->
<!--              caption = "Known probabilities from a model with  $X \\rightarrow Y \\leftarrow K$  justifying classic test types for clue $K$ given $X=Y=1$.", align=c("l", rep('c', 4))) -->


<!-- ``` -->

<!-- `r flag()` Put bounds in empty cells. -->

Note that some entries in Table \@ref(tab:tests15)  appear as ranges. This reflects that fact that, unless we are at edge cases, the estimand here is not identified even with infinite experimental data. In practice, we expect never to be at these edges. However, despite not being identified, bounds can be placed on causal quantities. For instance, for the hoop test, when $K=1$ the bounds are $[.5,1]$. The reason that the probability of causation here can not be *less* than 0.5, is that we can see from the data when $X=1, X=0$ that *at most* one third of cases can by cases for which $Y=1$ regardless of $X$ ($\theta^Y_{11}$) meaning that at *least* half of cases that produce $Y=1$ when $X=1$ and $K=1$ are cases for which $Y=1$ *because* $X=1$. In these examples the fact that queries are not point identified with infinite data does not detract from the fact that $K$ is informative in the ways associated with the different types of tests. 

The procedures given in Chapter \@ref(mixing) let us form posteriors over inferences with finite data. We briefly illustrate in Table \@ref(testsfinite). The results displayed here are drawn from simulations in which we start by making four different $X \rightarrow Y \leftarrow K$ models. For each model, we set particular restrictions on $Y$'s nodal types and a particular value for $Pr(\theta^K)$ such that $K$ provides different kinds of probative value about $X$'s effect. (We have set these restrictions and assignment propensities for $K$ such that the four resulting data-generating processes will, in the next steps, generate learning about $K$ that turns our clue into one of the four classic tests.)  We then simulate 1000 observations from each model and next update an agnostic $X \rightarrow Y \leftarrow K$ model. 

In Table \@ref(testsfinite), we show the inferences we would now draw from observing $K=1$ versus $K=0$ in a single case, based on the *updated* model. Each column---which we have labelled with the classic test names---represents our beliefs about $K$'s probative value as derived from a different simulated data pattern. Thus, for instance, in the "hoop test" column, we see that (having updated based on data from one of the four data-generating models) we have formed beliefs such that observing $K=1$ in a case slightly boosts our confidence that $X$ has a positive effect on $Y$, while observing $K=0$ dramatically undermines our confidence in such an effect. In the smoking gun column, we see that (having updated based on data from another one of the four data-generating models) we have formed beliefs such that observing $K=1$ in a case greatly boosts our confidence that $X$ has a positive effect on $Y$, while observing $K=0$ slightly undermines our confidence in such an effect.

We underline that we have here *derived* the probative value of the clue from observed data and a model that was entirely agnostic about the clue's probative value (but which did assume *some* priors on causal types). In particular, the model that we start with has no restrictions on $Y$'s nodal types, has flat beliefs over the distribution of $K$, and imposes no assumption that $K$ is informative for how $Y$ responds to $X$. It does, however, assume that $X$ and $K$ are exogenous, as might be the case if these were experimentally manipulated.


<!-- ^[We retain the $Y$ types in which $X$ and only $X$ has a positive effect on $U$, only $K$ matters, or you have to have both.] -->

<!-- We then use a function that draws inferences, given different values of a clue $K$, from a model that has been updated using available data.  -->


```{r, echo = FALSE}
van_evera_data <- function(y_types, k_types)
  
  make_model("X -> Y <- K") %>%
  
  set_restrictions(labels = list(Y = y_types), keep = TRUE) %>%
  
  set_parameters(param_type = "define", node = "K", parameters = c(1 - k_types, k_types)) %>%
  
  make_data(n = 1000)

van_evera_inference <- function(data)
  
  make_model("X -> Y <- K") %>%
  
  update_model(data = data) %>%  
  
  query_model(query = "Y[X=1] > Y[X=0]", 
              given = c(TRUE, "K==0", "K==1"),
              using = "posteriors")
```

<!-- We can now generate posterior beliefs, given $K$, for different types of tests where the tests are now justified by different types of data, coupled with a common prior causal model. -->



```{r, testsfinite, echo = FALSE}
if(do_diagnosis){
doubly_decisive <- van_evera_data("0001", .5) %>% van_evera_inference

hoop            <- van_evera_data(c("0001", "0101"), .9) %>% van_evera_inference

smoking_gun     <- van_evera_data(c("0001", "0011"), .1) %>% van_evera_inference

straw_in_wind   <- van_evera_data(c("0001", "0101", "0011"), .5) %>% van_evera_inference

write_rds(list(doubly_decisive, hoop, smoking_gun, straw_in_wind), "saved/ch14_vanevera.rds")
}

tests <- read_rds("saved/ch14_vanevera.rds")


teststable <- 
  cbind(drop_sd(tests[[1]]),  
      Hoop = drop_sd(tests[[2]])[,4],
      `Smoking gun` = drop_sd(tests[[3]])[,4],
      `Straw in the Wind` = drop_sd(tests[[4]])[,4]
      
      )
for(j in 4:7){
teststable[c(2, 4, 6), j] <- paste0("(", teststable[c(2, 4, 6), j], ")")}

  teststable %>% 
  kable(caption= "Classic tests with probative value inferred from (simulated) data, for query, Does $X$ have a positive effect on $Y$ in this case?", row.names = FALSE)
```

For both of these examples we have focused on moderators as clues. For results on mediators as clues see @dawid2019bounding which establishes that mediators in chain models can produce hoop tests but are generally unable to generate smoking gun tests.

This approach to thinking about process tracing tests is quite different to that described in existing (including Bayesian) treatments such as @collier2011understanding, @BennettAppendix, @fairfield2017explicit, or @humphreys2015mixing. Rather than having a belief about the probative value of a clue, and a prior over a hypothesis, inferences are drawn directly from a causal model that embeds a clue in a network of possible causal effects. Critically, with this approach, the inferences made from observing clues can be justified by reference to a more fundamental,  relatively agnostic model, that has been updated in light of data. The updated model yields a prior over the proposition, beliefs about probative values, and guidance for what conclusions to draw given knowledge of $K$. 


<!-- ## Bounds on probative value -->

<!-- Classic treatments of process tracing make use of Causal Process Observations --- observations that are taken to be indicative of a particular causal process in operation. We introduced in Chapter 5 (as well as in FLAG CITE humphreysjacobs)  quantities such as $\phi_{b}$---the probability that $K=1$ given $X$ caused $Y$ and $X=Y=1$, or $\phi_{d}$-----the probability that $K=1$ given $X$ did not cause $Y$ and $X=Y=1$.  -->

<!-- These accounts do not guide much guidance however regarding where these quantities come from --- given that causal types are unobservable how can one justify a belief about the probability of some observation *given* a causal type. Is it even possible to justify such beliefs? -->

<!-- The grounded approach we described provides an answer to this puzzle. In short, knowledge of the structure of a causal model, together with data on exchangeable units, can be enough to place bounds on possible values of $\phi_{b}, \phi_{d}$.  -->

<!-- We illustrate the basic idea and then review some results in this area. -->

<!-- Imagine a fortunate situation in which (a) it is known that the true causal model has the form $X \rightarrow M \rightarrow Y$ and (b) we have a lot of experimental data on the conditional distribution of $M$ given $X$ and of $Y$ given $M$ for exchangeable units (meaning that we can treat our unit of interest as if it were a draw from this set).  -->

<!-- Let us define: -->

<!-- * $\tau_1 = \Pr(M=1 | X=1) - \Pr(M=1 | X=0)$ -->
<!-- * $\rho_1 = \Pr(M=1 | X=1) - \Pr(M=0 | X=0)$ -->
<!-- * $\tau_2 = \Pr(Y=1 | M=1) - \Pr(Y=1 | M=0)$ -->
<!-- * $\rho_2 = \Pr(Y=1 | M=1) - \Pr(Y=0 | M=0)$ -->

<!-- These are all quantities that can be calculated from the data. The $\tau$s are average treatment effects and the $\rho$s are indicators for how common the $Y=1$ outcome is. -->

<!-- We are interested in the probability of observing $M=1$ given $X=Y=1$: -->

<!-- $$\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + \lambda_{a}^K\lambda_{a}^Y}$$ -->


<!-- Noting that $\tau_j = \lambda_{b_j} - \lambda_{a_j}$: -->

<!-- $$\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + (\lambda_{b}^K-\tau_1)(\lambda_{b}^Y - \tau_2)}$$ -->
<!-- which we can see is decreasing in $\lambda_{b}^j$ (this may seem counterintuitive, but the reason is that with $\tau^j$ fixed, lower $\lambda_{b}^j$ also means lower $\lambda_{a}^j$ which means less ambiguity about *how* $X$ affects $Y$ (i.e. through positive or negative effects on $K$). -->

<!--  <!-- $$\phi_{1} = \frac{\lambda_{b}^Y}{2\lambda_{b}^Y -\tau_2 - \tau_1(\lambda_{b}^Y - \tau_2)/\lambda_{b}}^K$$ -->

<!-- The lowest permissible value of  $\lambda_{b_j}$  is $\tau_j$, yielding $\phi_{b1} = 1$.  -->

<!-- The highest value obtainable by $\lambda_{b_j}$ is when $\lambda_{a_j} = \frac{1-\tau_j+\rho_j}2$ and so $\lambda_{b_j} = \frac{1+\tau_j+\rho_j}2$.  -->

<!-- In this case: -->
<!-- $$\phi_{b1} = \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2) + (1-\tau_1+\rho_1)(1-\tau_2+\rho_2)}= \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau}$$ -->

<!-- And so: -->

<!-- $$\frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau_2} \leq \phi_{b1} \leq 1$$ -->

<!-- These are the bounds on $\phi_{b1}$. We can calculate bounds on $\phi_{d1}$ in a similar way (though of course the bounds on  $\phi_{b1}$ and $\phi_{d1}$ are not independent).  -->


<!-- $$\phi_{d1} = \frac{\lambda_{b}^K\lambda_{d}^Y}{(\lambda_{a}^K + \lambda_{b}^K + \lambda_{c}^K)\lambda_{d}^Y+ \lambda_{c}^K\lambda_{a}^Y}$$ -->

<!-- Figure \@ref(fig:probval1) illustrates how "smoking gun" and "hoop" tests might each be justified with knowledge of $\tau_j, \rho_j$.  -->

<!-- ```{r phis85, echo = FALSE, include = FALSE} -->

<!-- make_phis <- function(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5, .3, 0, .6, .1, .1, 0, .6, .3), -->
<!--                         n = 60000){ -->

<!--   data <- simulate_data(model,  -->
<!--                         n = n, -->
<!--                         parameters = par,  -->
<!--                         using = "parameters")  -->

<!--   m1 <- with(data, c(mean(M[X==0]), mean(M[X==1]))) -->
<!--   m1[2] - m1[1]; m1[2] - 1 + m1[1] -->
<!--   m2<- with(data, c(mean(Y[M==0]), mean(Y[M==1]))) -->
<!--   m2[2] - m2[1]; m2[2] - 1 + m2[1] -->

<!--   if(!exists("fit")) fit <- fitted_model() -->

<!--   updated <- CausalQueries(model, data, stan_model = fit) -->

<!--   # check <- rstan::extract(updated$posterior, pars= "lambdas")$lambdas -->

<!--   phi_b_num <- query_distribution(updated, query = "(M==1) & (Y[X=0]==0)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_b_denom <- query_distribution(updated, query = "(Y[X=0]==0)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_b <- phi_b_num/phi_b_denom -->

<!--   phi_d_num <- query_distribution(updated, query = "(M==1) & (Y[X=0]==1)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_d_denom <- query_distribution(updated, query = "(Y[X=0]==1)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_d <- phi_d_num/phi_d_denom -->

<!--   out <- data.frame(phi_b, phi_d) -->

<!--   out -->
<!--   } -->

<!-- plot_phi <- function(out, main = "bounds"){ -->
<!--     plot(out$phi_d, out$phi_b, xlim = c(0,1), ylim = c(0,1), cex = .5, main = main,  -->
<!--        xlab = expression(phi[d]), ylab = expression(phi[b])) -->
<!--   abline(0,1) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r writephis, include = FALSE} -->
<!-- if(do_diagnosis){ -->
<!--   phis1  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5, .3, 0, .6, .1, .1, 0, .6, .3), -->
<!--                         n = 80000) -->
<!--   write_rds(phis1, "saved/phis_1.rds") -->

<!--   phis2  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5,  -->
<!--                                 .95, 0, 0, .05,  -->
<!--                                 .95, 0, 0, .05), -->
<!--                         n = 80000) -->
<!--   write_rds(phis2, "saved/phis_2.rds") -->

<!-- } -->
<!-- ``` -->

<!-- ```{r plotphis, echo = FALSE} -->
<!-- phis1 <- read_rds("saved/phis_1.rds") -->
<!-- phis2 <- read_rds("saved/phis_2.rds") -->

<!-- par(mfrow = c(1,2)) -->

<!-- plot_phi(phis1, -->
<!--          main = expression(paste("Hoop: ", tau[1], "= .6, ", -->
<!--                                            rho[1], "= -.2, ", -->
<!--                                            tau[2], "= .6, ", -->
<!--                                            rho[2], "= .2")) -->
<!--          ) -->

<!-- plot_phi(phis2, -->
<!--           main = expression(paste("Smoking gun: ",  -->
<!--                                   tau[1], "= 0, ", -->
<!--                                   rho[1], "= -.9, ", -->
<!--                                   tau[2], "= 0, ", -->
<!--                                   rho[2], "= -.9")) -->
<!--          ) -->

<!-- ``` -->


<!-- <!-- plot_bounds(.6, -.2, .6, .2, 20, main = expression(paste("Hoop: ", tau[1], "= .6, ", -->
<!-- <!--                                                             rho[1], "= -.2, ", -->
<!-- <!--                                                             tau[2], "= .6, ", -->
<!-- <!--                                                             rho[2], "= .2"))) -->

<!-- <!-- plot_bounds(0.0, -.9, .0, -.9, 100, main = expression(paste("Smoking gun: ", tau[1], "= 0, ", -->
<!-- <!--                                                             rho[1], "= -.9, ", -->
<!-- <!--                                                             tau[2], "= 0, ", -->
<!-- <!--                                                             rho[2], "= -.9, "))) -->

<!-- <!-- ``` -->


<!-- <!-- ```{r, eval = FALSE, echo = FALSE} -->
<!-- <!-- # An odd one! -->
<!-- <!-- plot_bounds(.02, -.5, -.10, -.5,600) -->
<!-- <!-- ``` -->

<!-- For the smoking gun,  $\phi_{b1}$ is .5 because $\lambda_a^j = \lambda_b^j$ so half of the upper level $b$ types work through a positive effect on $M$ and half through a negative effect on $M$. $\phi_{d1}$, on the other hand, is low here $d$ types mostly arise because of $c$ types in the first step and $a$ types in the second, and hence most commonly with $M=1$.  -->

<!-- Whether the bounds map into useful probative value depends in part on whether causal effects are better identified in the first or the second stage. We can see this in Figure \@ref(fig:probval2). -->

<!-- The key difference between the panels is that $\phi_d$ is constrained to be low in the first panel but not in the second.  -->

<!-- For intuition note that a higher level $d$ type will exhibit $M=1$ if it is formed via $db$, $bd$,or $dd$ and it will exhibit $M=0$ if it is formed via $ca$, $cd$, $ad$. The weak second stage makes it possible that there are no second stage d types, only a and b types. The stronger first stage makes it possible that there are no first stage $c$ types. In that case the higher level d types are formed uniquely of $db$ types -- which always exhibit $M=1$ if $X=1$. -->

<!-- This is not possible however for the data assume in the first panel. In the first panel the the higher value on $\rho_2$ means that there must be at least .25 d types. And the weak first stage means that there must at least .5 a and c types combined. Thus there *must* be a set of cases in which $M$ is not observed even though we have an upper level d type. -->

<!-- ```{r probval2, echo = FALSE, fig.width = 10, fig.cap = "Probative value with different first and second stage relations"} -->

<!-- # par(mfrow = c(1,2)) -->
<!-- #  -->
<!-- # plot_bounds(0, 0, .25, .25, 20, main = expression(paste("Weak first stage: ", tau[1], "= 0, ", -->
<!-- #                                                             rho[1], "= 0, ", -->
<!-- #                                                             tau[2], "= .25 ", -->
<!-- #                                                             rho[2], "= .25"))) -->
<!-- #  -->
<!-- # plot_bounds(.25, .25, 0, 0, 20, main = expression(paste("Weak second stage: ", tau[1], "= .25, ", -->
<!-- #                                                             rho[1], "= .25, ", -->
<!-- #                                                             tau[2], "= 0, ", -->
<!-- #                                                            rho[2], "= 0"))) -->

<!-- if(do_diagnosis){ -->
<!--   phis3  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5,  -->
<!--                                 .25, .25, .25, .25,  -->
<!--                                 .25, 0, .25, .5), -->
<!--                         n = 50000) -->
<!--   write_rds(phis3, "saved/phis_3.rds") -->

<!--   phis4  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5,  -->
<!--                                 .25, 0, .25, .5, -->
<!--                                 .25, .25, .25, .25), -->
<!--                         n = 50000) -->
<!--   write_rds(phis4, "saved/phis_4.rds") -->

<!-- } -->

<!-- phis3 <- read_rds("saved/phis_3.rds") -->
<!-- phis4 <- read_rds("saved/phis_4.rds") -->

<!-- par(mfrow = c(1,2)) -->

<!-- plot_phi(phis3, -->
<!--          main = expression(paste("Hoop: ", tau[1], "= 0, ", -->
<!--                                                             rho[1], "= 0, ", -->
<!--                                                             tau[2], "= .25, ", -->
<!--                                                             rho[2], "= .25"))) -->

<!-- plot_phi(phis4, -->
<!--           main = expression(paste("Smoking gun: ",  -->
<!--                                   tau[1], "= .25, ", -->
<!--                                   rho[1], "= .25, ", -->
<!--                                   tau[2], "= 0, ", -->
<!--                                   rho[2], "= 0, "))) -->

<!-- ``` -->


<!-- In short we emphasize that difficult as it might seem at first it is possible to put relatively tight bounds on probative value for causal types with access to experimental data on exchangeable units.  -->


<!-- ### Learning about probative value from process tracing -->

<!-- <!-- FLAG: The below is imported from Chap on case selection. May want to resurrect. But would need to bring in part of the relevant table (commented out) with conditioning on M. --> -->

<!-- It is also easy to show how conducting process tracing on a set of cases can incrementally add probative value to a clue. Consider the setup we employed to assess case-selection strategies in Chapter \@ref(caseselection), where we began with positively correlated $X,Y$ data in 6 cases. We showed there that, if we start with Jeffreys priors for all  parameters in a chain model, nothing can be learned about the $ATE$ or the probability of causation from observing $M$ in a single case. However, observing $M=1$ in a single case *can* tell us something about effects in *another* case in which $M=1$.  -->

<!-- The $X,Y$ pattern in the initial 6 cases---a weak positive relationship---tells us that (with respect to the $X \rightarrow Y$ relationship) there are relatively more $b$'s than other types in the population. This means, in turn, that an $X=1, Y=1$ case is more likely to be a $b$ than a $d$. If we now observe $M=1$ in an $X=1, Y=1$---a case we think is probably a $b$---we now have reason to believe that positive $X \rightarrow Y$ effects operate through a positive $X \rightarrow M$ effect followed by a positive $M \rightarrow Y$ effect. (And had we seen $M=0$ in this case, we'd believe positive effects more likely operate through a negative effect followed by a positive effect.) Put differently, we now believe that seeing $M=1$ in an $X=1, Y=1$ case is a clue that that case is likely a $b$ type---and our belief about the causal effect in an $X=1, M=1, Y=1$ case shifts upward. -->

<!-- Thus, while we do not learn about the overall population from an $N=1$ process-tracing exercise, process-tracing in that single case does make $M$ informative about *other* cases. We can see this by comparing our beliefs about an $X=1, M=1, Y=1$ case in the row in which we have only observed the $X,Y$ correlation to our beliefs about that same kind of case in the row in which we have process-traced in an on-the-line case. While in the first instance conditioning on $M=1$ has no impact on the estimated causal effect (because we haven't previously learned about $M$), it does have an impact in the latter instance. -->

<!-- We also see that our belief about the causal effect in an $X=0, M=1, Y=1$ case shifts upward. Why? Updating toward the belief that positive effects --- which we believe to be relatively common --- more often operate via linked positive positive effects implies that $X$ more often has a positive than a negative effect on $M$. Yet for an $X=0, M=1, Y=1$ to be a case with a negative causal effect, $X$'s effect on $M$ would have to be negative. Thus, interestingly, evidence from *on* the regression line can inform inferences about cases *off* the regression line via information about mediating processes. -->

<!-- <!-- FLAG: Haven't figured out why seeing M=1 in an X=1, Y=0 case reduces the estimated causal effect for an X=1, M=1, Y=1 case relative to just seeing the X,Y pattern. If we see M=1 in this off-line case, we have evidence in favor of X having a positive effect on M, and M having a negative effect on Y. So this should generally speak against positive effects in  X=1, M=1, Y=1 cases. But if we'd seen M=0, wouldn't this have the same effect? It would suggest negative X->M effects and positive M->Y effects, which would also speak against a positive effect in an  X=1, M=1, Y=1 case. So how does observing M help? --> 

<!-- We see even stronger learning about an off-the-line $X=0, M=1, Y=1$, however, if we process trace off-the-line. The observation of $M=1$ in an $X=1, Y=0$ case counts as evidence against the operation of *both* steps in the causal chain through which $X$ could have a negative effect on $Y$ in an $X=0, M=1, Y=1$ case: it suggests that $X$'s effect on $M$ is more likely positive than negative *and* that $M$'s effect on $Y$ is more likely negative---precisely the opposite of what would have to be true for a negative effect to emerge in this kind of case. Thus, we now think this kind of case is less likely to involve a causal effect. -->


<!-- ## Justification from experimental designs -->

<!-- idea: show inferences given for example parallel designs for mediation -->

<!-- ### Mediator -->

<!-- Say now that *in addition* we know from experimental data, that $K$ mediates the relationship between $X$ and $Y$; indeed we will assume that we have a case of complete mediation, such that, conditional on $K$, $Y$ does not depend on $X$.  -->

<!-- Say the transition matrices from $X$ to $K$ and $K$ to $Y$ are: -->

<!-- $$P^{xk}=\left( \begin{array}{cc} 1 & 0 \\ 1/2 & 1/2\end{array}\right), P^{ky}=\left( \begin{array}{cc} 1/2 & 1/2 \\ 0 & 1\end{array}\right)$$  -->
<!-- Even without observing $K$, this information is sufficient to place a prior on PC of $p=\frac13$.  -->

<!-- To see this, note that we can calculate: -->

<!-- * $\lambda_a^K =0$, $\lambda_b^K = \frac{1}{2}$, $\lambda_c^K = \frac{1}{2}$, $\lambda_d^K = 0$ -->
<!-- * $\lambda_a^Y =0$, $\lambda_b^Y=\frac{1}{2}$, $\lambda_c^Y=0$,  $\lambda_d^Y=\frac{1}{2}$ -->

<!-- and so: -->

<!-- * $\lambda_b^u = \lambda_b^K\lambda_b^Y = \frac{1}4$ -->
<!-- * $\lambda_d^u = \lambda_d^Y$  -->
<!-- * $p = \frac{\lambda_b^u}{\lambda_b^u + \lambda_d^u} = \frac{1}3$. -->

<!-- whence: -->

<!-- * $\phi_{b1} = 1$ -->
<!-- * $\phi_{d1} = \lambda_d^K + \lambda_b^K = \frac{1}{2}$ -->

<!-- More generally we can calculate the lower bound on the probability that $X$ caused $Y$ as the product of the lower bounds that $X$ caused $M$ and that $M$ caused $Y$, and similarly for the upper bound, using the same formula as before. Signing things so that $\tau^j\geq 0$, $j \in {1,2}$: -->

<!-- $$\frac{2\tau_1}{1+\tau_1+\rho_1}\frac{2\tau_2}{1+\tau_2+\rho_2}  \leq PC \leq \frac{1+\tau_1-|\rho_1|}{1+\tau_1+\rho_1}\frac{1+\tau_2-|\rho_2|}{1+\tau_2+\rho_2} $$ -->


<!-- We have undertaken essentially the same operations as above except that now we are placing bounds on a substantive estimand of interest rather than first placing bounds on probative value of a clue and then turning to Bayes rule to place bounds on the estimand. -->


<!-- ### Moderator -->
<!-- Consider now  a situation  in which our case is drawn from a set of cases for which $X$ and $K$ were each randomly assigned. Say then that the transition matrices, conditional on $K$ look as follows: -->

<!-- $$P^{K=0}=\left( \begin{array}{cc} 0 & 1 \\ 0.5 & 0.5 \end{array}\right), P^{K=1}=\left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right)$$ -->
<!-- In this case we can now identify PC, even before observing $K$. If $K=0$, PC is 0---there are no cases with positive effects in this condition. If $K=1$ PC = 1.  We have a prior  that $K=1$ of  .5 and after observing $X=Y=1$ we raise this to $2/3$. Thus our prior belief on $PC$ --- before seeing $K$--- is $2/3 * 1 + 1/3 * 0 = 2/3$.  -->

<!-- How about $\phi_{b1}$ and $\phi_{d1}$? -->

<!-- Here positive effects only arise when $K=1$ and so $\phi_{b1} = 1$. $Y=1$ without being cause by $X$ only if $K=0$ and so  $\phi_{b0} = 0$. Thus we have a double decisive clue. -->




## Empirical discovery of causal structure

In the preceding discussion of learning about probative value for process tracing, we have taken causal structure---the DAG itself---as given. Moreover, even when we are engaged in mixed-data inference on multiple cases---where we can start with *only* the DAG---we need to start with some causal structure. Where does knowledge of causal structure come from?

The empirical discovery of causal structure is itself a very large field of inquiry and we cannot do it justice here. For a review see, for instance, @glymour2019review.

Here we seek simply to illustrate the possibility of casual discovery. We demonstrate with three situations in which there is a true---but unknown model---relating $X,M,$ and $Y$ to each other. Critically, we assume that there are no unobserved confounding relations (this is a requirement for the "PC" algorithm but not for the "Fast Causal Inference" algorithm). In each situation, we show the true relationship and the "skeleton" of the model as discovered by a prominent technique that uses a "constraint-based algorithm"---examining whether observed data correlations are consistent with one or another set of conditional-independence relations.   


```{r skeleton, include = FALSE, eval = TRUE, message = FALSE}

install_discovery_packages <- FALSE

if(install_discovery_packages){
# source("http://bioconductor.org/biocLite.R")
# biocLite("RBGL")

 install.packages("BiocManager")
 BiocManager::install("Rgraphviz")
 BiocManager::install(c("graph", "RBGL"))
 install.packages(pcalg)
}

library(Rgraphviz)
library(pcalg)

recover_model <- function(model) {
    
  df <- make_data(model, n = 20000)
  V  <- colnames(df)
  
  suffStat <- list(dm = df, nlev = c(2,2,2), adaptDF = FALSE)
  skel.fit <- fci(suffStat,
                       indepTest = disCItest, 
                       alpha = 0.01, labels = V, 
                       verbose = TRUE)
  plot(skel.fit)
  }
```

In Figure \@ref(fig:plots), the top row of models represent the true models from which simulated data are generated. The objective is then to see how much of this true causal structure the discovery algorithm can recover from the data. In the first true model, $X$ affects $Y$ directly and indirectly through $M$. In the second model, $Y$ has two causes that do not influence each other. Finally, in the third model, $X$ causes $Y$ through $M$ but not directly. When we simulate data from these models, we assume monotonicity but otherwise a flat distribution over nodal types. We also assume no confounding: there is nothing not represented on the graph that affects more than one node on the graph.


```{r model1ch14, include = FALSE}
model1 <- make_model("X -> M -> Y <- X") %>% 
  set_restrictions(c("(M[X=1]<M[X=0])", 
                     "(Y[M=1, X=.]<Y[M=0, X=.]) | (Y[X=1, M=.]<Y[X=0, M=.])"))
```




```{r model3ch14, include = FALSE}
model2 <- make_model("X1 -> Y <- X2") %>% 
  set_restrictions("(Y[X1=1]<Y[X1=0]) | (Y[X2=1]<Y[X2=0])")
```


```{r model2ch14, include = FALSE}
model3 <- make_model("X -> M -> Y") %>% 
  set_restrictions(c("(M[X=1]<M[X=0])", "(Y[M=1]<Y[M=0])"))
```



```{r model4ch14, include = FALSE}
model4 <- make_model("X -> Y <-> Z -> W") %>% 
  set_restrictions(c("(Y[X=1]<Y[X=0])", "(W[Z=1]<W[Z=0])")) %>%
  set_parameters(param_set = "Y_1", nodal_type = "11", parameters = .1) %>%
  set_parameters(param_set = "Y_0", nodal_type = "00", parameters = .1) 
```


```{r plots, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "(Partially) DAGs from Data: True DAGs in upper row with partially directed graphs in lower row. Circles indicate uncertainty whether a aroow starts or ends at a given point.", fig.width = 8, fig.height = 8,results='hide',fig.keep='all', eval = TRUE, fig.align = "center", out.width = "80%"}
par(mfrow = c(2,3))
#par(mar = rep(3, 4))
plot(model1); title("I", line = -2)
plot(model2); title("II", line = -2)
plot(model3); title("III", line = -2)
recover_model(model1)
recover_model(model2)
recover_model(model3)
```

In the bottom row of Figure \@ref(fig:plots), we show the structures that we recover. In all situations, we correctly recover the skeleton: which nodes are connected to which nodes. Note that, for any link where we have an "o" on both ends of the link, we do not know in which direction causal effects flow. 

In the first situation, the skeleton is unrestricted: we have correctly *not* excluded links between any two nodes, but we have not learned about the directions of effects. In the second situation, however, we have fully recovered the causal structure. Thus, the algorithm has figured out that  $X_1$ and $X_2$ are not children of $Y$. The algorithm sees, in essence, data patterns that are distinctively associated with colliders: $X_1$, $X_2$ correlated conditional on $Y$ but not otherwise.  In the last setup, we have not figured out the direction of the causal arrows, but the inference is still rather impressive. Although $X$, $M$ and $Y$ are all correlated with each other, the algorithm has figured out that there should be no direct link between $X$ and $Y$---by observing that $X$ and $M$ are not correlated with each other *conditional* on $M$.

Note also that, in all three situations, if we have access to all relevant variables the true graph can be recovered with additional knowledge of the temporal ordering of the variables.

The assumption that we have captured all nodes that might generate confounding is critical to these results. Yet these examples provide grounds for hope that causal models can be discovered and not simply assumed. If all relevant nodes are known and measured---a tall order for sure---causal structures can be identified from data.  


<!-- ### A model of models -->

<!-- In the following mode there is an unknown, $Q$, which determines the relevant causal model.  -->
<!-- If $Q=1$ then we have $A \rightarrow B \rightarrow C2$; if $Q=0$ then $A \rightarrow B \leftarrow C1$. In this case the temporal order of $C1$ and $C2$ is observable, so there is not confusion there; what is not clear however is which is the important node to include in the model.  -->

<!-- ```{r ch14Qornot} -->

<!-- model <- make_model("A -> B -> C2 <- C1 -> B <- Q -> C2") %>% -->
<!--   # These restrictions capture the role of Q in turning parentage on or off  -->
<!--   set_restrictions(c( -->
<!--     "(B[C1=1, Q=1] != B[C1=0, Q=1])", -->
<!--     "(C2[B=1, Q=0] != C2[B=0, Q=0])")) %>% -->

<!--   # These restrictions are for simplification: monotonicity and complementarity  -->
<!--   set_restrictions(c( -->
<!--     "(C2[B=1] < C2[B=0])", -->
<!--     "(C2[C1=1] < C2[C1=0])", -->
<!--     "(B[A=1]  < B[A=0])", -->
<!--     "(B[C1=1] < B[C1=0])", -->
<!--     "((B[A=1, C1=1] - B[A=0, C1=1]) < (B[A=1, C1=0] - B[A=0, C1=0]))")) -->

<!-- model -->

<!-- plot(model) -->

<!-- if(do_diagnosis){ -->
<!--   model_Q0 <- set_parameters(model, node = "Q", alphas = c(1,0)) -->
<!--   model_Q1 <- set_parameters(model, node = "Q", alphas = c(0,1)) -->

<!--   data_0 <- make_data(model_Q0, 100, vars = c("A", "B", "C1", "C2")) -->
<!--   data_1 <- make_data(model_Q1, 100, vars = c("A", "B", "C1", "C2")) -->

<!--   model_Q0 <- update_model(model, data_0) -->
<!--   model_Q1 <- update_model(model, data_1) -->

<!-- Qu0 <- query_model(model_Q0, "Q==1", using = "posteriors") -->
<!-- Qu1 <- query_model(model_Q1, "Q==1", using = "posteriors") -->

<!--   write_rds(list(model_Q0, model_Q1, Qu0, Qu1), "saved/ch14_Qornot.rds") -->
<!-- } -->

<!-- Qornot <- read_rds("saved/ch14_Qornot.rds") -->

<!-- kable(Qornot[[3]]) -->
<!-- kable(Qornot[[4]]) -->




<!-- ``` -->

<!-- [Ideally here however $\lambda^Q$ is either 0 or 1 --- we want to know which world we are in] -->


<!-- ## Exercise -->

<!-- Imagine a model in which in fact $X \rightarrow Y \leftarrow K$ but in which the researcher knows only the temporal ordering of variables. Say in fact that on average $X$ and $K$ both have strong positive effects on $Y$ with positive interactions. Can access to observational data provide a justification for using $K$ as a clue for the effect of $X$ on $Y$ in an $X=Y=1$ case?   -->

<!--chapter:end:15-Justifying-Models.Rmd-->

# Evaluating models {#evaluation}


:::: {.headerbox data-latex=""}
::: {.center data-latex=""}
:::
Model based inference takes the model seriously. But deep down we know that all of these models are wrong, in myriad ways. We examine strategies for figuring out whether a model is likely doing more harm than good.
::::
<br>

```{r packagesused15b, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
library(DeclareDesign)
```


Throughout this book we have maintained the conceit that you believe your model. But it is also obvious that even the most non-parametric-seeming models depend on substantive assumptions and that these are almost certainly wrong. The question then is not how much you believe your model (or whether you really believe what you say you believe) but whether your model is useful is some sense. How can we evaluate the usefulness of our models?


## Four Strategies

Sometimes a model is just a poor representation of underlying causal processes. Fortunately it can sometimes be possible to figure out your model is sending you in the wrong direction. We describe five strategies.

Imagine a situation in which researchers believe that the effect of $X$ on $Y$ runs entirely through $M$, positing a model of the form $X \rightarrow M \rightarrow Y$. Imagine, however, that the true causal process is one in which $X$ affects $Y$ *directly*;  $X$ has no effect on $M$, so that there is no indirect effect of $X$ on $Y$; and $M$ never has a negative effect on $Y$. 

The problem with the posited model, then, is that it represents overly strong beliefs about independence relations: it does not allow for a direct effect that is in fact operating.

<!-- AJ: "Restrictedness" seems a bit off here insofar as the "unrestricted" model is also restricted in essentially assuming no operative $X -> M$ arrow, as well as no negative direct effects.  -->

We are perfectly able to update using this too-strong $X \rightarrow M \rightarrow Y$ model and the data --- but the updated model can produce wildly inaccurate causal inferences. We show this using set of 200 observations simulated from a "true" model with direct effects only and an average effect of $X$ on $Y$ of $1/3$.^[Specifically, the data-generating model is $X \rightarrow M \rightarrow Y \leftarrow X$, with no effect of $X$ on $M$ and thus no indirect effect, and no negative effect of $X$ on $Y$, and an otherwise flat parameter space.] In the left panel of Figure \@ref(fig:badmodels15), we show the estimated average treatment effect of $X$ on $Y$ when using these data to update the $X \rightarrow M \rightarrow Y$ model.

In the righthand panel of the figure, we show the inferences using the same data but a model that makes weaker assumptions by allowing for direct effects: a $X \rightarrow M \rightarrow Y \leftarrow X$ model. With both models, we start with flat priors over nodal types.^[Note that the "true" model here involves restrictions on nodal types (no $X \rightarrow M$ effects and no negative $X \rightarrow Y$ effects) while the analysis models are both unrestricted.] 

We represent the true average effect with the vertical line in each graph.

As we can see, the weaker (i.e., more permissive) model performs OK: the true effect falls well within the posterior predictive distribution on the ATE. However, the stronger model, which excludes direct effects, generates a posterior predictive distribution that essentially excludes the right answer. So, if we go into the analysis with the stronger model, we have a problem. 

But will we notice?

In the remainder of this section, we explore a range of diagnostics that researchers can undertake to evaluate the usefulness of their models or to compare models with one another: checking assumptions of conditional independence built into a model; checking the model's fit; using "leave-one-out" cross-validation; and assessing model sensitivity.

```{r modelsch15, echo = FALSE}
n = 200

if(do_diagnosis){
  
 unrestricted_model <- 
    make_model("X -> M -> Y <- X") %>%
    set_parameters(
      statement = c("M[X=1] != M[X=0]", "(Y[X=1] < Y[X=0])"),
      parameters = c(0, 0)
      ) 
 
 restricted_model <- make_model("X -> M -> Y")%>%
    set_parameters(
      statement = c("M[X=1] < M[X=0]", "(Y[M=1] <= Y[M=0])"),
      parameters = c(0, 0)
      ) 
 
 data <- simulate_data(unrestricted_model, using = "parameters", n = n) 
     write_rds(data, "saved/ch15_data.rds") 
  
 XMYdata <- simulate_data(restricted_model, using = "parameters", n = n) 
     write_rds(data, "saved/ch15_XMYdata.rds") 
  
 restricted_model %>%
   update_model(data, keep_fit = TRUE) %>%
   write_rds("saved/ch15_restricted_model.rds") 

 unrestricted_model %>%
   update_model(data, keep_fit = TRUE) %>%
   write_rds("saved/ch15_unrestricted_model.rds") 

}

data               <-   read_rds("saved/ch15_data.rds") 
XMYdata            <-   read_rds("saved/ch15_XMYdata.rds") 
restricted_model   <-   read_rds("saved/ch15_restricted_model.rds") 
unrestricted_model <-   read_rds("saved/ch15_unrestricted_model.rds") 

if(do_diagnosis){
  
  ateXY1 <- query_distribution(restricted_model, "Y[X=1] - Y[X=0]", using = "posteriors")
  ateXY2 <- query_distribution(unrestricted_model, "Y[X=1] - Y[X=0]", using = "posteriors")

 write_rds(data.frame(restricted_ate = ateXY1, unrestricted_ate = ateXY2),
           "saved/ch15_ates.rds")
}

ates <- read_rds("saved/ch15_ates.rds") 

```

```{r badmodels15, echo = FALSE, fig.cap = "A restricted model yields a credibility interval that does not contain the actual average effect.", message = FALSE}

ates %>% gather("var", "val") %>% 
  mutate(var = factor(var, c("restricted_ate", "unrestricted_ate"), c("Stronger Model (Restricted)", "Weaker Model  (Unrestricted)"))) %>% 
  ggplot(aes(val)) + geom_histogram() + facet_grid(~var) + theme_bw() + xlab("Effect of X on Y") + geom_vline(xintercept = 1/3, color = "red")
```


<!-- * $X$ correlates with $Y$ -->
<!-- * $X$ does not correlate with $M$  -->
<!-- * $M$ does not correlate with $Y$.  -->

<!-- These data are inconsistent with the model: under this model, if $X$ doesn't cause $M$ and $M$ doesn't cause $Y$, then there is no other way for $X$ to cause $Y$.  -->

### Check conditional independence

First, even before engaging in updating, we can look to see whether the data pattern is consistent with our causal model. In particular, we can check whether there are inconsistencies with the Markov condition that we introduced in Chapter 2:  that every node is *conditionally independent* of its non-descendants, given its parents. In this case, if the stronger model is right, then given $M$, $Y$ should be independent of $X$. 

Is it? 

One way to check is to assess the covariance of $X$ and $Y$ given $M$ in the data. Specifically, we regress $Y$ on $X$ for each value of $M$, once for $M=1$ and again for $M=0$; a correlation between $X$ and $Y$ at either value of $M$ would be problematic for the conditional independence assumption embedded in the stronger model. 

Note that this form of diagnostic test is a classical one in the frequentist sense: we start by hypothesizing that our model is correct and then ask whether the data were unlikely given the model.


```{r, echo = FALSE}
rbind(
  estimatr::lm_robust(Y~X, data = filter(data, M==0)) %>% tidy %>% filter(term == "X"), 
  estimatr::lm_robust(Y~X, data = filter(data, M==1)) %>% tidy %>% filter(term == "X")) %>%
  mutate(M = 0:1) %>%
  select(M, estimate, std.error, p.value) %>% 
  kable(digits = 3, caption =  "Regression coefficient on $X$ given $M=0$ and $M=1$")


```

We report the regression coefficients on $X$ in the table below. It is immediately apparent that we have a problem. At both values of $M$, there is a strong correlation between $X$ and $Y$, evidence of a violation of the Markov condition implied by the stronger model.^[In applying the Markov condition, we also need to take into account any unobserved confounding. For instance, suppose that there was an unobserved confounder of the relationship between $M$ and $Y$ in the $X \rightarrow M \rightarrow Y$ model. Then we would *not* expect $Y$ to be independent of $X$ conditional on $M$. Intuitively, the data pattern we described could be consistent with such a model in which on average $X$ does not cause $M$ but still *in those cases in which $X=M$* we have $Y=X$. Another way to think about this is that $M$ now acts as a collider between $X$ and another unobserved cause of $Y$; so conditioning on $M$ introduces a correlation between $X$ and this unobserved cause, and thus between $X$ and $Y$.] 

Identifying the full set of conditional independence assumptions in  a causal model can be difficult. There are however well developed algorithms for identifying what sets, if any, you need to conditional on to ensure conditional Independence between two nodes given a DAG. `R` users can quickly access such results  using the `impliedConditionalIndependencies` function in dagitty package.

<!-- AJ: I do not follow the sentence starting with "Intuitively" in the above. -->




<!-- ROUGH TEXT: -->

<!-- approach 2 -- say actual confound is q~=0; but model  assumes q = 0. Draw data from priors, draw data; given data type (001, 100 etc) plot (a) the posterior distribution under no confounding nad (b) the distribution of estimands that gave rise to the data.   -->


<!-- (Verma and Pearl, 1990) identify conditions under which we can check some independence assumptions. -->


<!--  Pearl (1995) gives conditions for assessing for discrete data whether $Z$ has a  direct effect on $Y$. (Involves inequalities) -->

<!-- Evans (Graphical methods for inequality constraints in marginalized DAGs) generalizes the instrumental inequality. -->
 


<!-- ### Computational clues -->

<!-- Second, we may be lucky and run into computation issues. In this example there is a good chance that when you fit the stronger  model, `stan` will throw an error:  -->

<!-- > `Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.` -->


<!-- AJ: Needs elaboration: why is this error a function of problems with the model? -->
<!-- MH: I agree; do feel free to look into it; I will try -->
<!-- AJ: I have done some digging and not found anything helpful on this -->
<!-- MH: Well that's just great -->
<!-- AJ: Looked a little more - still nothing! Whence did you get a sense there might be something to this? -->


### Bayesian $p-$value: Are the data unexpected given your model?

A third approach asks whether features of the data you observe are in some sense unusual given your model, or more unusual given your model than another model. For instance, if one model assumed no adverse effects of $X$ on $Y$ and no confounding, then a strong negative correlation between $X$ and $Y$ would be unusual, even for the model updated with this data; and this negative correlation would *more* unusual for this model than for a model that allowed for adverse effects.

In fact, this approach is quite classical: we are looking to see whether we should "reject" our model in light of inconistent data.  

<!-- AJ: But it can also be done in a comparative sense as we do below, right? Which is then not about "rejection", right? I've actually adjusted the text two paragraphs up to make the idea of comparison more prominent. This works?-->

An approach for doing this using simulated data from the posterior predictive distribution is described in @gabry2019visualization.^[Tools in the `bayesplot` package can be used to show how typical the data we observe is for different models] 

We consider two test statistics, comparing our stronger to our weaker model. First, we look just at the distribution of the outcome $Y$ to see how the actual distribution in the data compares to the predicteddistribution from the updated model. Second, we look at the actual correlation between $X$ and $Y$ and see how this compares to the predicted distribution. In both cases we calculate a two sided $p$-value by assessing and doubling the share of the mass of the predictive posterior distribution that lies on the more extreme side of the observed data. If the observed data were at the mean of the predicive distribution, then we would have a $p$-value of 1. If it were at the 95th percentile we would have a $p$-value of 0.10. (We note that in this straightforward calculation we assess the probability of the data given the same model that generated the data; approaches could also be used that seek out-of-sample estimates of the  probability of observing the observed data.)

<!-- AJ: Last sentence confuses me. Aren't we using data only from unrestricted_model? -->

For the first test, we see that the predicted distribution of the outcome $Y$ is similar for both updated models; and the actual mean outcome is within the distribution of predicted mean outcomes. The $p$-values for the stronger (0.11) and weaker models (0.14) suggest that the observed mean $Y$ values are about equally likely for both models. 

When it comes to the correlation between $X$ and $Y$, however, the two models perform very differently. The posterior predictive distribution from the stronger model is centered around a $0$ correlation and does not even extend out as far as the observed correlation. The resulting $p$-value is 0, meaning that from the perspective of the stronger model the $X,Y$ correlation in the data is entirely unexpected. A frequentist looking at the observed correlation between $X$ and $Y$ should feel comfortable rejecting the stronger model. The updated weaker model, in contrast, predicts a strong correlation, and the observed correlation is comfortably within the posterior predictive distribution, with a $p$-value of 0.08. 


<!-- AJ: Need to code in the p-values -->

At first blush, the abysmal performance of the stronger model may seem paradoxical. Even after this model has *seen* the $X,Y$ correlations in the data, the model still finds those correlations highly surprising. What keeps the  $X \rightarrow M \rightarrow Y$ model  from learning, however, is the strength of the assumptions it contains. The problem is that $M$ is uncorrelated with $X$ in the true data-generating process, so the stronger model learns that there is no indirect effect. But, at the same time, this model does not *allow* for a direct effect. Despite what would seem to be overwhelming evidence of a systematic $X,Y$ correlation, a causal relationship connecting $X$ to $Y$ remains extremely unlikely given the $X,M$ data pattern and the impossibility of direct effects. The stronger model just can't handle the truth. The weaker model, on the other hand, readily learns about the direct $X \rightarrow Y$ effect.


```{r ch15simulations, echo = FALSE, message = FALSE}

if(do_diagnosis){
  
 sims <- 500
 n <- nrow(data)
 
 # Outcome prediction
 ############################################# 
 t(replicate(sims, {
   dd <- make_data(restricted_model, n = n, param_type = "posterior_draw")
   c(Y=mean(dd$Y), C = cor(dd$Y, dd$X))})) %>%  data.frame %>%
  write_rds("saved/ch15_replicates_restricted.rds") 

 t(replicate(sims, {
   dd <- make_data(unrestricted_model, n = n, param_type = "posterior_draw")
   c(Y=mean(dd$Y), C = cor(dd$Y, dd$X))})) %>% data.frame %>%
  write_rds("saved/ch15_replicates_unrestricted.rds") 
  
  }

df <- 
  list(
    Stronger = read_rds("saved/ch15_replicates_restricted.rds"),
    Weaker =    read_rds("saved/ch15_replicates_unrestricted.rds")) %>%
  bind_rows(.id = "Model") %>% gather("test", "Val", -Model) 


p_two_sided <- function(T, t, digits = 2){
  if(t <  mean(T, na.rm = TRUE)) return(round(2* mean(t > T, na.rm = TRUE), digits))
  if(t >= mean(T, na.rm = TRUE)) return(round(2* mean(t <= T, na.rm = TRUE), digits))
}

tests <- c("Test stat: Mean of Y", "Test stat: X, Y covariance")

wegot <- data.frame(test = c("Y", "C"), 
                    wegot = c(mean(data$Y), cor(data$Y, data$X)))
wegot_labelled <-  wegot  %>%
  mutate(test = factor(test, c("Y", "C"), tests))
                       
ps <- df %>% group_by(Model, test) %>% left_join(wegot) %>% 
  summarize(p = p_two_sided(Val, wegot[1])) %>% 
  mutate(Val = 1, x = 0, y = 120, label = paste0("p = ", p))

ps_labelled <- ps %>%
  mutate(test = factor(test, c("Y", "C"), tests)) 
  
df %>%
  mutate(test = factor(test, c("Y", "C"), tests)) %>% 
  ggplot(aes(Val)) + geom_histogram() + facet_grid(Model ~ test) + xlab("predictions") +
  geom_vline(data = wegot_labelled, aes(xintercept = wegot), color = "red") + theme_bw() + 
  ggtext::geom_textbox(data  = ps_labelled, aes(x = x, y = y, label = label), width = unit(0.4, "npc"))

```


<!-- ## Model likelihoods -->

<!-- Bayesian information criterion (BIC)^[$BIC = \ln(n)k - 2\ln(\hat{L})$ where $\hat{L}$ is the maximized likelhood, $k$ is the number of parameters, and $n$ the number of data points.] -->
<!-- BIC involves a penalty for more parameters -->


<!-- ```{r, echo = FALSE} -->

<!-- if(do_diagnosis) { -->
<!-- observed_data <- collapse_data(data, restricted_model) -->

<!-- likely <- function(model, s, posterior = TRUE) { -->
<!--   if(posterior)  pars <- model$posterior_distribution[1:s,] -->
<!--   if(!posterior) pars <- get_prior_distribution(model, s)[1:s,] -->
<!--   L <- apply(pars, 1,  function(par) make_data_probabilities(model, pars = par, observed_data, normalize = FALSE)) -->
<!--   mean(L) -->
<!--   } -->

<!-- s <- 500 -->
<!-- L_prior         <- likely(restricted_model, s, posterior = FALSE) -->
<!-- L_restricted    <- likely(restricted_model, s) -->
<!-- L_unrestricted  <- likely(unrestricted_model, s) -->

<!-- df <- data.frame(L = (c(L_unrestricted/L_prior, L_restricted/L_prior, L_unrestricted/L_restricted))) -->

<!-- rownames(df) <- c("Unrestricted / Prior", "Restricted / Prior", "Unrestricted / Restricted") -->

<!-- write_rds(df, "saved/ch15_likelihoodsdf.rds")  -->
<!-- } -->

<!-- df <- read_rds("saved/ch15_likelihoodsdf.rds")  -->

<!-- kable(df, col.names = "Bayes factors", digits = 2, caption = "Posterior odds: the relative likelihood of one model over another") -->
<!-- ``` -->

<!-- Compare likelihoods of the data under different models -->
<!-- Check look package for rstan -->


### Leave-one-out (LOO) cross-validation

A further class of model-validation methods involves cross-validation. Rather than asking how well the updated model predicts the data used to update it, cross-validation uses the data at hand to estimate how well the model is likely to predict new data that have not yet been seen. One way to do this is to split the available data, using one subsample to update and then assessing predictions using the other subsample. We focus here, however, on approaches that use *all* of the available data to estimate out-of-sample predictive performance.

One such approach is the "leave-one-out" (LOO) algorithm. In a LOO approach, we update the model using all data points except for one and then ask how well the model performs in predicting the left-out observation. We repeat this for every data point in the dataset to assess how well we can predict the entire dataset. 

Often, the LOO approach is used to predict a particular outcome variable. In a \CausalQuery\ framework, however, we are interested in predictions over the joint realization of all nodes, not just a single "outcome." Thus, we calculate the posterior probability of each data point, using the model updated with all of the other observations. 

<!-- AJ: Is the "predict all nodes" using a sledghammer here?Should we be setting this up in a more query-dependent way, I wonder? Could the same model be good for some kind of queries but not for others? Not quite sure how to think about that -- about *what* we'd be predicting conditional on what the query is, and maybe this isn't a coherent notion. -->

<!-- AJ: Change figure labels to stronger/weaker models. Also, need to make x-axes "data count" (without referring to a model) -->



```{r loolik1, echo = FALSE}
#Leave one out


if(do_diagnosis){
  
  get_loo_likelihood(data, restricted_model, 1000) %>% write_rds("saved/ch15_loo_restricted.rds")
    
  get_loo_likelihood(data, unrestricted_model, 1000) %>% write_rds("saved/ch15_loo_unrestricted.rds")

}

loo_restricted   <- read_rds("saved/ch15_loo_restricted.rds") 
loo_unrestricted <- read_rds("saved/ch15_loo_unrestricted.rds")  

data.frame(restricted = loo_restricted$case_likelihood %>% unlist,
           unrestricted = loo_unrestricted$case_likelihood %>% unlist,
           label = loo_restricted$data$event) %>%
  ggplot(aes(restricted, unrestricted, label = label)) +
  geom_point(color = "red")+
  ggrepel::geom_text_repel() + theme_bw() + ggtitle("LOO likelihoods for restricted and unrestricted models")

```

```{r loolik2}
plot_loo <- function(models)
  
  lapply(models, function(m)
         data.frame(
           case_likelihood = m$case_likelihood %>% unlist,
           label = m$data$event,
           count = m$data$count)
         ) %>% bind_rows(.id = "model") %>%
    filter(count > 0) %>%
    ggplot(aes(case_likelihood, count, label = label)) + geom_point()+ 
       xlab("Case likelihood") + 
       ylab("Actual number of cases") + facet_grid(~model) + theme_bw() +
    geom_abline(slope = models[[1]]$data$count %>% sum,intercept = 0) +
    ggrepel::geom_text_repel() + theme_bw() + 
    ggtitle("LOO likelihoods and case shares")

list(restricted = loo_restricted, unrestricted  = loo_unrestricted) %>% plot_loo

```

The LOO estimate of out-of-sample predictive fit, for a dataset with $n$ observations, is then:

$$\prod_1^np(y_i|y_{-i}, \text{model})$$
where $y_{-i}$ is the data pattern with observation $y_i$ left out, and $y_i$ represents the values of all nodes of interest for observation $i$. 

<!-- AJ: Any reason not to use a log in the above equation? Would this make some of the outputs easier to read? -->
<!-- MH it would get a more humnan scale but think raw numbers are more intuitive? -->

We implement LOO cross-validation of the stronger and weaker models using 200 observations generated from the same data-generating model employed above. We find that the LOO likelihood of the data under the stronger model is `r  formatC(loo_restricted$loo_likelihood, format = "e", digits = 2)` while
the likelihood is  `r  formatC(loo_unrestricted$loo_likelihood, format = "e", digits = 2)` under the weaker model. Thus, the weaker model represents an estimated improvement in out-of-sample prediction on the order of   `r  formatC(loo_unrestricted$loo_likelihood/loo_restricted$loo_likelihood, format = "e", digits = 2)`. 

We can visualize the pattern in Figure \@ref(fig:loolik1), where we plot the likelihood of each possible data type under the stronger model against the likelihood of that data type under the weaker model. Looking at the scales of the two axes---which is much more compressed on the horizontal than on the vertical---one can see that the stronger model is not able to differentiate as much across the data types as the weaker. 

<!-- AJ: What are these likelihoods exactly? They are informed by updating, I assume? But updating using what data exactly -- what's left out? Generally unsure if my text above is right.-->

Notably, the stronger model is not able to "learn" from the data about the (*in fact*, operative) relationship between $X$ and $Y$. We can see that, for any given $X, M$ combination, the two possible values of $Y$ are predicted with essentially the same likelihood. The stronger model also seems to have "learned" from chance correlations in the data that different values $X,M$ combinations are differentially likely---even though they are uncorrelated under the true model. The weaker model, on the other hand, basically divides the data types into two groups: those with a positive $X,Y$ correlation and those with a negative $X,Y$ correlation and has correctly (given the true model) learned that the former is more likely than the latter.

In  Figure \@ref(fig:loolik2), we then see how the likelihoods of each data type line up with the actual count of each data type. As we can see, the weaker model updates to likelihoods that fit the actual data pattern well while the stronger model does not, in particular the stronger model under predicts cases that are on the diagnonal and over predicts cases that are off it.


We can also turn the tables and imagine that the *stronger* model represents the true data-generating process. We  implement LOO cross-validation of the two models using 200 data points generated from the stronger model. In Figure \@ref(fig:loostronger), we see a comparison of the likelihoods of the data types under the two updated models and note that they are extremely similar. This represents an important asymmetry: the model that makes weaker assumptions performs far better in handling data generated by a "stronger" true model than does the stronger model in learning about a process that violates one of its assumptions. Since the weaker model allows for both direct and indirect effects, the weaker *can* learn about the parameters of the true process in the first situation; but the strong model cannot do so in the second situation because it has by assumption ruled out a key feature of that process (the direct effect). 


```{r loostronger, echo = FALSE, fig.cap = "Data prediction of a restricted and unrestricted model when in fact the data is generated by the unrestricted (stronger) model"}

if(do_diagnosis){
  
  get_loo_likelihood(XMYdata, restricted_model, 1000) %>% write_rds("saved/ch15_loo_restricted_2.rds")
    
  get_loo_likelihood(XMYdata, unrestricted_model, 1000) %>% write_rds("saved/ch15_loo_unrestricted_2.rds")

}

loo_restricted_2   <- read_rds("saved/ch15_loo_restricted_2.rds") 
loo_unrestricted_2 <- read_rds("saved/ch15_loo_unrestricted_2.rds")  


list(restricted = loo_restricted_2, unrestricted  = loo_unrestricted_2) %>% plot_loo + 
  ggtitle("Comparative performance II")

```

<!-- FLAG: BETTER: PLOT CASES ON BOTTOM AND TWO LIKELIHOODS AS POINTS; HAVE TYPES AS LABELS X AI IF POSSIBLE -->

While it is difficult to see this in Figure XXXX, the stronger model still performs better here than the weaker model. The likelihood of the data under the stronger model is now `r  formatC(loo_restricted_2$loo_likelihood, format = "e", digits = 2)`, compared to 
the likelihood of  `r  formatC(loo_unrestricted_2$loo_likelihood, format = "e", digits = 2)` under the weaker model. Thus, the weaker model represents an estimated loss to out-of-sample prediction on the order of   `r  formatC(loo_unrestricted_2$loo_likelihood/loo_restricted_2$loo_likelihood, format = "e", digits = 2)`. This is not surprising insofar as the stronger model *precisely* models the data-generating process while the extra parameters in the weaker model allow for "learning" from chance features of the data.

These examples display features of estimation of out-of-sample prediction accuracy familiar from a regression context. In a regression framework, adding parameters to a model may improve fit to sample---generating gains to out-of-sample prediction accuracy when the new parameters pick up systematic features of the data-generating process---but run a risk of over-fitting to chance pattenrs in the data. Similarly, in a structural-causal-model framework, for a model with weaker assumptions and more parameters. We saw that the weaker model performed much better when the true process involved direct effects since the extra parameters, allowing for direct effects, captured something "real" going on. But the same model performed slightly worse than the stronger model when there were no direct effects to pick up, such that the extra parameter could only model noise.

<!-- AJ: Last graph here doesn't seem right, with the data-type labels all stacked vertically. And no graph for unrestricted model. -->

<!-- Note on parsimony. It is interesting to note that more complex models have to pay their way: if they do not predict better than simpler models, then simpler models  are favored. For the intuition imagine in fact $X$ and $Y$ are unrelated. Say we had two data points, one with... -->

### Sensitivity

The last approach we consider brackets the question of which model is better and asks, instead: how much do your conclusions depend on the model? You can worry less about your assumptions if the conclusions are not strongly dependent on them.

To illustrate using a process tracing example, consider a situation in which we are unsure about posited parameter values: that is, about the probability of particular effects at particular nodes. It is likely to be the case in many research situations that we are considerably uncertain about how to quantify intuitive or theoretically informed beliefs about the relative likelihood of different effects. 

Suppose, for instance, that we begin with an $X \rightarrow M \rightarrow Y$ model. And suppose, further, that we believe that it is unlikely that $M$ has an adverse effect on $Y$. But we are not sure *how* unlikely that adverse effect is.  (We assume all other modal types are equally likely.) Finally, say that we want to use the observation of $M$ to draw an inference about whether $X=1$ caused $Y=1$ in an $X=Y=1$ case. 

How much does our inference on $X$'s effect on $Y$---when we see $M=0$ or $M=1$---depend on this second stage assumption about the probability of a negative $M \rightarrow Y$ effect?

We answer the question by looking at posterior beliefs for a range of possible values for the relevant parameter, $\lambda^Y_{10}$. In Table  REF, we examine a range of values for $\lambda^Y_{10}$, from 0 to 0.25 (full parity with other types). For each parameter value, we first show the resulting prior belief about the probability that $X=1$ caused $Y=1$. We can see that, before we observe $M$, we think that a positive $X \rightarrow Y$ effect is more likely as a negative $M \rightarrow Y$ effect is more likely. This stands to reason since a negative second-stage effect is one possible process in which a positive $X \rightarrow Y$ effect might occur. And higher values for $\lambda^Y_{10}$ come disproportionately at the expense of types under which $X$ cannot affect $Y$.^[Increasing weight on $\lambda^Y_{10}$ is drawn equally from $\lambda^Y_{00}$, $\lambda^Y_{11}$, and $\lambda^Y_{10}$, with the first two of these three representing null effects.]

In the next two columns, we show the posterior belief we arrive at when we observe $M=0$ and then $M=1$, for each $\lambda^Y_{10}$ assumption. Looking at the last column first, we see that our inference from $M=1$ does *not* depend at all on our beliefs about adverse $M \rightarrow Y$ effects. The reason is that, if we see $M=1$, we already know that $M$ did not have a negative effect on $Y$, given that we also know $Y=1$. Our beliefs are purely a function of the probability that there are positive effects at both stages as compared to the probability of other causal types that could yield $X=M=Y=1$, a comparison unaffected by the probability of a negative $M \rightarrow Y$ effect.  

Our inferences when $M=0$, on the other hand, do depend on $\lambda^Y_{10}$: when we see $M=0$, our belief about a positive $X \rightarrow Y$ effect depends on the likelihood of *negative* effects at both stages. We see, then, that the likelier we think negative effects are at the second stage, the higher our posterior confidence in a positive $X \rightarrow Y$ effect when we see $M=0$. 


```{r, fig.cap = "Possible inferences on `X caused Y` given observation of M=0 and M=1 given different possible models. The highest inference when M=0 is lower than the lowest inference when M=1", echo = FALSE}

model <- make_model("X -> M -> Y") %>%
    set_parameters(
      param_names = c("M.10"), 
      parameters = .25)


adverse2 <- seq(0, .25, .05)

out <-  
  sapply(adverse2, function(a) {
  
  query_model(
  set_parameters(model, 
                 param_names = c("Y.10"), 
                 parameters = c(a)),
  query = c("Y[X=1] > Y[X=0]"),
  given = c("X==1 & Y==1", "X==1 & Y==1 & M==0", "X==1 & Y==1 & M==1"),
  using = "parameters",
  expand_grid = TRUE)$mean})
out2 <- t(out)  %>% data.frame
names(out2) <- c("No data", "M0", "M1")
out2 <- arrange(out2,M0, M1)

kable(cbind(adverse2, out2), col.names = c("$\\lambda^Y_{10}$", "Prior", "$M=0$", "$M=1$"), caption = "Inferences on the probability that $X$ caused $Y$ upon seeing $M=0$ or $M=1$ for a range of possible values of $\\lambda^Y_{10}$")

```


Even though our inferences given $M=1$ do not depend on $\lambda^Y_{10}$, the amount that we *update* if we see $M=1$ *does* depend on $\lambda^Y_{10}$. This is because $\lambda^Y_{10}$ affects our belief, prior to seeing $M$, that $X=1$ caused $Y=1$. Working with a low $\lambda^Y_{10}$ value, we start out less confident that $X=1$ caused $Y=1$, and thus our beliefs make a bigger jump if we do see $M=1$ than if we had worked with a $\lambda^Y_{10}$ higher value. 

However, to the extent that we want to know how our assumptions affect our *conclusions*, the interesting feature of this illustration is that sensitivity depends on what we find. The answer to our query is sensitive to the $\lambda^Y_{10}$ assumption if we find $M=0$, but not if we find $M=1$.  It is also worth noting that, even if we observe $M=0$, the sensitivity is limited across the range of parameter values tested. In particular, for all $\lambda^Y_{10}$ values below parity (0.25), seeing $M=0$ moves our beliefs *in the same direction.* 

We can use the same basic approach to examine how our conclusions change if we relax assumptions about nodal-type restrictions, about confounds, or about causal structure. 

We also note that in cases in which you cannot quantify uncertainty about parameters you might still be able to engage in a form of "qualitative inference." There is a literature on probabilistic causal models that assesses the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see @parsons2001qualitative. Under this kind of approach, a researcher might willing to say that they think some probability $p$ is not plausibly  greater than .5,  but unwilling to make a statement about their beliefs about where in the $0$ to $0.5$ range it lies. Such incomplete statements can be enough to rule our classes of conclusion.
 


## Evaluating the Democracy-Inequality model

<!-- ** FLAG: GENERATE A TABLE SHOWING HOW PIMD DOES ON 5 CRITERIA **  -->


```{r pimdmodelsch15, echo = FALSE}

pimd <- make_model("I -> M -> D <- P; I -> D") %>%
  
         set_restrictions(c( 
           "(M[I=1] < M[I=0])",
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))

# make an alternative model that permits negative I->M effects

pimd_negmed <- make_model("I -> M -> D <- P; I -> D") %>%
  
         set_restrictions(c( 
           "(D[I=1] > D[I=0]) | (D[M=1] < D[M=0]) | (D[P=1] < D[P=0])"))

pimd_nonmon <- make_model("I -> M -> D <- P; I -> D")

data <- CausalQueries::democracy_data

data_2 <- read.csv("data/pimd2.csv") %>% #new data we compiled from other sources
  mutate(I2 = I2*1) %>% select(country, P2, I2, M2, D2) %>%
  rename(P = P2, I = I2, M= M2, D = D2)

#data <- read.csv("data/pimd2.csv") %>% 
 # mutate(I2 = I2*1) %>% select(country, P2, I2, M2, D2) %>%
  #rename(P = P2, I = I2, M= M2, D = D2) 


if(do_diagnosis){
  
pimd %>%
   update_model(data, keep_fit = TRUE, iter = 8000) %>%
   write_rds("saved/ch15_pimd_model.rds") 

pimd_negmed %>%
   update_model(data, keep_fit = TRUE, iter = 8000) %>%
   write_rds("saved/ch15_pimd_negmed_model.rds") 

pimd_nonmon %>%
   update_model(data, keep_fit = TRUE, iter = 4000) %>%
   write_rds("saved/ch15_pimd_nonmon_model.rds") 
#unrestricted model saved here

}

if(do_diagnosis)
  pimd_nonmon %>% set_priors(distribution = "jeffreys") %>% 
   update_model(data_2, chains = 4, iter = 6000, keep_transformed = TRUE) %>%
   write_rds("saved/ch15_pimd_nonmon_model_2.rds") 
#unrestricted model saved here


pimd <-   read_rds("saved/ch15_pimd_model.rds") 
pimd_negmed <-   read_rds("saved/ch15_pimd_negmed_model.rds")
pimd_nonmon <-   read_rds("saved/ch15_pimd_nonmon_model.rds")

pimd_nonmon_2 <-   read_rds("saved/ch15_pimd_nonmon_model_2.rds")

```

### Check assumptions of conditional independence

Our model presupposes that $P$ and $I$ are independent and that $P$ and $M$ are independent. Note that the model is consistent with the possibility that, conditional on $D$, there is a correlation betwen $M$ and $P$ or between $I$ and $P$, as $D$ acts as a collider for these pairs of nodes. 

To test these assumptions, we in fact need to depart from the dataset drawn from @haggard2012distributive because these authors only examined cases in which $D=1$, those that democratized. Thus, we cannot use these data to assess the relationships not conditional on $D$ or conditional on $D=0$. We generate observations on all four nodes for a broader set of cases by pulling together measures from the following sources, with the aim of modeling democratization that occurred between 1990 and 2000^[The data that we use to measure mobilization, from @clark2016mobilization, covers only the 1990s.]:

* **Inequality**: We measure inequality, $I$, using the Gini estimates from the University of Texas Inequality Project (@galbraith2019inequality). As we want to measure inequality at the beginning of the period, we take the Gini measure for each country that is closest in time to the year 1989. We then dichotomize the variable using the median value for the period as a cutoff. 

* **Mobilization**: We measure $M$ using the Mass Mobilization Protest Data from @clark2016mobilization. To capture the kinds of mobilization on which redistributive theories of democratization focus, we restrict our focus to protests in the demand categories "land farm issue," "labor wage dispute," "price increases, tax policy," and "political behavior, process." We also include only those gatherings with a size of at least 1000 protesters. We code a country case as $M=1$ if and only if, during the 1990s, it experienced at least one protest that meets both the demand-type and size criteria.

* **Pressure**: We draw on the GIGA Sanctions Dataset to measure international pressure, $P$. Specifically, we code a country case as $P=1$ if and only if the country was the target of democratization-focused sanctions during the 1990-2000 period. 

* **Democratization**: We use dichotomous democracy measures from @cheibub2010revisited, in two ways. First, we filter countries such that our sample includes only those that were not democracies in 1990 ($N=77$). We then use the democracy measure for the year 2000 to determine which countries democratized, coding as $D=1$ those and only those cases that Cheibub and Vreeland code as democracies in that year.


We check the model's assumptions in relation to conditional independencies through a set of simple regression models, displayed in Table \@ref(tab:ch15cipimd). In the first two rows, we examine the simple correlation between $P$ and $I$ and between $P$ and $M$, respectively. We can see from the estimates in the first row that the data pattern is consistent with our assumption of unconditional independence of $I$ and $P$. However, we also see that there *is* an unconditional correlation between $I$ and $M$, something that is excluded by our model.

<!-- MH: Realize we actually have a big iussue here: we only have M,P data in cases with D = 1 so we are in fact condtiioining on D and so we do *not* expect independence  -->

```{r ch15cipimd, echo=FALSE}
independencies <- function(model){

  dagitty::dagitty(paste("dag{", model$statement, "}" )) %>%
  dagitty::impliedConditionalIndependencies()
}
# independencies(pimd)


# dd <- CausalQueries::democracy_data 

rbind(
  lm_robust(P~I, data = data_2) %>% tidy %>% filter(term == "I"), 
  lm_robust(P~M, data = data_2) %>% tidy %>% filter(term == "M"), 
  lm_robust(P~I, data = filter(data_2, M==0)) %>% tidy %>% filter(term == "I"), 
  lm_robust(P~I, data = filter(data_2, M==1)) %>% tidy %>% filter(term == "I"))  %>% 
  #lm_robust(P~M, data = filter(data_2, I==0)) %>% tidy %>% filter(term == "M"), 
  #lm_robust(P~M, data = filter(data_2, I==1)) %>% tidy %>% filter(term == "M"))    
 mutate(Given = c("-", "-", "M = 0", "M = 1")) %>%
  select(Given, estimate, std.error, p.value) %>% 
  mutate(Correlation = c("P,I", "P,M", "P,I", "P,I")) %>%
  select(Correlation, everything()) %>%
  kable(digits = 3, caption =  "Regression coefficients to assess conditional independence")


```



We can dig a little deeper, however. The model *also* implies that $P$ should be independent of $I$ given $M$ --- since $D$ blocks all paths between $P$ and either $I$ or $M$. We test these assumptions in rows 3 and 4 of the table, wgere we examine the conditional independence of $P$ and $I$ given $M=0$ and given $M=1$. Here the evidence is also troubling for our model, as we see a relatively strong negative correlation between $P$ and $I$ when $M=0$, and positive correlation when $M=1$. 

While we cannot identify the correct model from this data pattern, one possible explanation could be that pressure has a direct effect on mobilization, making mobilization the product of inequality and pressure jointly.^[There is, in fact, also a strong positive interaction between $I$ and $P$ in a linear model of $M$.] A model with an arrow running from $P$ to $M$ would make the model consistent with the unconditional correlation between these two variables, the conditional correlation between $P$ and $I$ given $M$ (since $M$ would now be a collider for $I$ and $P$), as well as the unconditional independence of $I$ and $P$. A possible way forward --- which we do not pursue here --- would be to now amend the model and evaluate the revised model against an independent set of data. 


<!-- Substantively, in states with high inequality, mobilization is more common in cases without international pressure ($P=0$) (arising in 0 of 13 cases) than in cases that experience pressure arising in just 1 of 5 cases. Again, our model may be missing a direct link from pressure to mobilization, which would generate the possibility that inequality and pressure interactively affect mobilization. -->

<!-- Unclear what the case numbers above are referring to. For instance, I see 13 cases with I=0, P=0. But what's the 0?-->

<!-- I am thinking we should now bring in models that relax the assumptions that appear not to hold in the CI analysis. For instance, introduce an arrow from $P$ to $M$. -->


### Bayesian $p$-value

We turn next to evaluating the democratization model using the Bayesian $p-$value approach, and for this purpose can return to the data that we coded from @haggard2012distributive's qualitative vignettes. In the two panels of Figure \@ref(fig:ch15pvalue_pimd), we plot the posterior predictive distributions from our updated model for two quantities of interest: the outcome $D$ and the correlation between $I$ and $M$. In each graph, we indicate with a vertical line the mean value for these quantities for the data at hand and report the $p-$value: the probability of the observed data conditional on our model. 


```{r ch15pvalue_pimd, echo = FALSE, eval = FALSE}
if(do_diagnosis){
  
 sims <- 500
 n <- nrow(data)
 
 # Outcome prediction
 ############################################# 
 t(replicate(sims, {
   pimd_sim <- make_data(pimd, n = n, param_type = "posterior_draw")
   c(D=mean(pimd_sim$D), C = cor(pimd_sim$I, pimd_sim$M))})) %>%  data.frame %>%
  write_rds("saved/ch15_replicates_pimd.rds") 

  
  }


df <-  read_rds("saved/ch15_replicates_pimd.rds")  

p_two_sided <- function(T, t, digits = 2){
   if(t <  mean(T, na.rm = TRUE)) return(round(2* mean(t > T, na.rm = TRUE), digits))
   if(t >= mean(T, na.rm = TRUE)) return(round(2* mean(t <= T, na.rm = TRUE), digits))
 }
 
tests <- c("Test stat: Mean of D", "Test stat: I, M covariance")
 
replicates_pimd <- df %>% gather("test", "Val")

pimd_data <- CausalQueries::democracy_data

wegot <- data.frame(
  test = c("D", "C"),
  wegot = c(mean(pimd_data$D),
            cor(pimd_data$M, pimd_data$I, use = "complete.obs")))
wegot_labelled <-  wegot  %>%
  mutate(test = factor(test, c("D", "C"), tests))
                        
ps <- replicates_pimd %>% group_by(test) %>% left_join(wegot) %>% 
   summarize(p = p_two_sided(Val, wegot[1])) %>% 
   mutate(Val = 1, x = 0, y = 120, label = paste0("p = ", p))

ps_labelled <- ps %>%
  mutate(test = factor(test, c("D", "C"), tests)) 

replicates_pimd %>%
   mutate(test = factor(test, c("D", "C"), tests)) %>% 
   ggplot(aes(Val)) + geom_histogram() + facet_grid( ~ test) + xlab("predictions") +
   geom_vline(data = wegot_labelled, aes(xintercept = wegot), color = "red") + theme_bw() + 
   ggtext::geom_textbox(data  = ps_labelled, aes(x = x, y = y, label = label), width = unit(0.4, "npc"))

```


As we can see both visually and from the $p-$values, the model performs well here as the data that we observe are not at all unexpected under the model.

### LOO validation

Turning to "leave one out" model assessment, we now consider comparing our base model to models that make weaker assumptions. In one alternative model, we drop the assumption of monotonicity of $M$ in $I$. We also test a maximally flexible model (given the DAG) in which we make no montonocity assumptions for any of the causal effects.

```{r loo_pimdplots, echo = FALSE, fig.cap = "C"}
# Leave one out



if(do_diagnosis){
  
  get_loo_likelihood(CausalQueries::democracy_data, pimd, sims = 25, iter = 4000) %>% write_rds("saved/ch15_loo_pimd.rds")
    
  get_loo_likelihood(CausalQueries::democracy_data, pimd_negmed, sims = 25, iter = 4000) %>% write_rds("saved/ch15_loo_pimdnegmed.rds")
  
  get_loo_likelihood(CausalQueries::democracy_data, pimd_nonmon, sims = 25, iter = 4000) %>% write_rds("saved/ch15_loo_pimdnonmon.rds")

}

loo_pimd       <- read_rds("saved/ch15_loo_pimd.rds") 
loo_pimdnegmed <- read_rds("saved/ch15_loo_pimdnegmed.rds")
loo_pimdnonmon <- read_rds("saved/ch15_loo_pimdnonmon.rds")


data.frame(restricted = loo_pimd$case_likelihood %>% unlist,
           unrestricted = loo_pimdnonmon$case_likelihood %>% unlist,
           label = loo_pimd$data$event,
           count = loo_pimd$data$count) %>%
  dplyr::filter(count > 0) %>%
  ggplot(aes(restricted, unrestricted, label = label)) +
  geom_point(color = "red")+
  ggrepel::geom_text_repel() + theme_bw() + ggtitle("LOO likelihoods for restricted and unrestricted models (PIMD)")

```


```{r pimdloocomp, echo = FALSE, caption = "LOO data predictions for three PIMD models"}

list(`restricted model` = loo_pimd,  `unrestricted model` = loo_pimdnegmed,  `fully unrestricted model` =  loo_pimdnonmon ) %>% plot_loo()

```

Figures \@ref(fig:loo_pimdplots) shows that in fact the likelihoods of the most restricted and least restricted model are not very different from each other. Both models are able to re-create the data structure reasonably well. Figure  \@ref(fig:loo_pimdloocomp), confirms this, showing the relationship, for each model, of the likelihood of each data type against the number of cases of that data type in the data. A data type is here defined as a possible combination of realized values on all nodes ($I, P, M$ and $D$).  In each plot, the diagonal line represents equality between the proportion of expected cases under the model and the proportion of actual cases. Just eyeballing the relationships, it appears as though the two stronger models might be performing better than the fully unrestricted model.

More formally, we calculate the LOO likelihood for each model as `r formatC(loo_pimd$loo_likelihood, format = "e", digits = 2)` for the model with monotonicity assumptions throughout, `r formatC(loo_pimdnegmed$loo_likelihood, format = "e", digits = 2)` for the model that relaxes the monotonicity assumption for $I \rightarrow M$, and `r formatC(loo_pimdnonmon$loo_likelihood, format = "e", digits = 2)` for the model with no monotonicity assumptions at all. In other words, we see that the most restricted model (with all montonicity assumptions) performs best, by about an order of magnitude, followed by the completely unrestricted model (no restrictions and flat priors), and then the moderately restricted one (removing the monotonicity restriction just for $\I rightarrow M$).


### Sensitivity to priors

In our base model we assume a set of monotonicity relations among nodes. How much do conclusions depend on these restrictions?  We answer the question by comparing our conclusion with these restrictions to what we would conclude without this assumption. As above, we compare the fully restricted model ($M1$) to a model that is unrestricted at $M$ ($M2$) and a model that is fully unrestricted ($M3$). 

We first show results for population inference from a mixed methods analysis. As seen in Table \@ref(tab:15pimdgraphs), our inferences regarding the overall effect of $I$ on $D$ are not very sensitive to the monotonicity assumption at $M$. However, they are extremely sensitive to the other monotonicity assumptions made in the model: as we can see, the effect goes from around $-0.25$ to $0$ when we remove all restrictions.  Our conditional inferences about the probability of negative causation are not sensitive to these assumptions, however. In particular, in cases with $I=0, D=1$ we are about equally likely to think that democratization was due to low inequality given any of the models. When we see that in fact there was no mobilization, our attribution increases in the restricted model but decreases in the unrestricted model. In the fully unrestricted model our inferences are not affected at all by observation of $M=0$. 

<!-- FLAG: AJ: Check intuition here! -->

In the non-monotonic model we entertain the possibility that low inequality mattered not just directly but also, perhaps, by inducing protests. However, when you observe no protests, you rule out this possible pathway. In the monotonic model you do not think that democratization could have been produced by low inequality via demonstrations---but nevertheless entertain the possibility of mobilization that is *not* due to inequality, which could nevertheless be the cause of democratization. In this case, observing no mobilization removes a rival, cause of democratization, not a second channel. 

In all, we judge the conditional inferences as very sensitive to the monotonicity assumptions we put in place.


```{r pimdgraphs215, echo = FALSE, fig.cap = "ATE of I on D."}

if(do_diagnosis)
  list(pimd = pimd, pimd_negmed = pimd_negmed, pimd_nonmon = pimd_nonmon) %>% 
    lapply(function(m) query_model(m, "D[I=1] - D[I=0]", using = "posteriors", 
                           given = c(TRUE, "I==0 & D==1", "I==1 & D==1", "I==0 & M==0 & D==1", "I==1 & M==0 & D==1"))) %>%
    bind_rows(.id = "model") %>%
    write_rds("saved/ch15_ateID2.rds")

read_rds("saved/ch15_ateID2.rds")  %>% 
  mutate(model = factor(model, c("pimd", "pimd_negmed", "pimd_nonmon"), 
                        c("M and D restricted", "D restricted", "Unrestricted"))) %>%
  ggplot(aes(mean, Given, color = model)) + theme_bw() + 
  geom_point(position=position_dodge(.5)) + 
  theme_bw() +
  geom_errorbarh(aes(xmin=mean - sd, xmax=mean + sd), 
                size=.2, height = .1,
                position=position_dodge(.5)) +
  xlab("Posterior on average effect of I on D")
  
```


We now consider case-level analysis. For this setup, we compare our restricted model to one in which we allow for negative effects of $I$ on $M$, but consider them to be *unlikely* rather than impossible (with null and positive effects somewhat likely). We refer to these priors as "quantitative priors" in the sense that they place a numerical value on beliefs rather than a logical restriction. Specifically, we set prior probabilities on the elements of $\theta^M$ as: $p(\theta^M=\theta^M_{10})=0.1$, $p(\theta^M=\theta^M_{00})=0.3$, $p(\theta^M=\theta^M_{11})=0.3$, and $p(\theta^M=\theta^M_{01})=0.3$. This is in comparison to the 0, 1/3,1/3,1/3 distribution implied by the restricted model. 

In Tables \@ref(tab:HK8cases1quant) and \@ref(tab:HK8cases2quant), we simply call these our restricted and unrestricted models and compare findings for a set of cases with different data realizations.

```{r HK8cases1quant, echo = FALSE, eval = TRUE}


pimd_negmed_1 <- set_parameters(pimd_negmed, label = "10", node = "M", parameters = 0)
pimd_negmed_2 <- set_parameters(pimd_negmed, label = "10", node = "M", parameters = .1)

f_given <- function(i, d) {
  a = paste("I ==", i,  " & D == ", d)
  list(a, paste(a, "& M == 0 & P == 0"), paste(a, "& M == 0 & P == 1"), paste(a, "& M == 1 & P == 0"),   paste(a, "& M == 1 & P == 1"))}

q_01_1 <- query_model(
  pimd_negmed_1, 
  query = "D[I=1] != D[I=0]", 
  using = "parameters", 
  given = f_given(0,1)
) 

q_01_2 <- query_model(
  pimd_negmed_2, 
  query = "D[I=1] != D[I=0]", 
  using = "parameters", 
  given = f_given(0,1)
) 

# Do same for nonmon model

q_01 <- select(q_01_1, Given) %>% 
  mutate(
    Restricted   = q_01_1$mean,
    Unrestricted = q_01_2$mean)


q_01 %>%
  mutate(Example = c(" ", "Mexico (2000)", "Taiwan (1996)", 
                  "Albania (1991)", "Nicaragua (1984)")) %>%
kable(caption = "\\label{tab:HK8cases1quant} Four cases with low inequality and  democratization. Question of interest: Was low inequality a cause of democracy? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012).")



```

```{r HK8cases2quant, echo= FALSE}

q_11_1 <- query_model(
  pimd_negmed_1, 
  query = "D[I=1] != D[I=0]", 
  using = "parameters", 
  given = f_given(1,1)
) 

q_11_2 <- query_model(
  pimd_negmed_2, 
  query = "D[I=1] != D[I=0]", 
  using = "parameters", 
  given = f_given(1,1)
) 

q_11 <- select(q_11_1, Given) %>% 
  mutate(
    Restricted    = q_11_1$mean,
    Unrestricted  = q_11_2$mean)
q_11 %>%
  mutate(Example = c(" ", "Mongolia (1990)", "Paraguay (1989)", "Sierra Leone (1996)", "Malawi (1994)")) %>%
  kable(caption = "\\label{tab:HK8cases2} Four cases with high inequality and  democratization. Question of interest: Was high inequality a cause of democratization? Table shows posterior beliefs for different data for 4 cases given information on $M$ or $P$. Data from Haggard and Kaufman (2012).")

```


The results differ in various modest ways. In Table \@ref(tab:HK8cases1quant), we examine $I=0, D=1$ cases and ask whether the low inequality caused democratization. There are significant differences when we are looking for negative effects of inequality, though the ordering of inferences does not change. The differences appear in the cases of Albania and Nicaragua, where $M=1$. Under priors fully constrained to monotonic causal effects, we see that observing $M=1$ makes us think low inequality was less likely to have caused democracy because $M=1$ represents an alternative cause and because low inequality cannot cause democratization via $M$ if $I \rightarrow M$ effects cannot be negative. However, if we allow for a negative effect of $I$ on $M$, even while believing it to be unlikely, we know believe a negative effect of inequality on democratization, conditional on mobilization, to be more likely since now that effect *can* run from $I=0$ to $M=1$ to $D=1$. Thus, our estimate for Albania and Nicaragua goes up relative to the restricted model. We see, likewise, that $M$ becomes less *informative* about the effect, as the estimates for Albania ($M=1, P=0$) converge on those for Mexico ($M=0, P=0$), and those for Nicaragua ($M=1, P=1$) on those for Taiwan ($M=0, P=1$).

<!-- As before, in cases where $M=0$, the monotonicity of $M$ in $D$ is not relevant since there is no question of a negative effect anyway. However if $M=1$, then it is possible that $I=0$ caused $D=1$ via mobilization in the model that allows negative effects and so we put more weight on the possibility that $I=0$ was the cause.  -->

Turning to Table \@ref(tab:HK8cases2quant) and cases with high inequality and democratization, inferences about the probability of positive causation are unaffected by the assumption about the effect of $I$ on $M$. The reason is that, since we still maintain a monotonicity assumption for the direct effect of $I$ on $D$ (no positive effects), the only question is whether there was an indirect effect. Since we maintain the assumption of a monotonic effect of $M$ on $D$, it remains the case in both models that observing $M=0$ rules out a positive indirect effect. If however $M=1$, then $I$ did not have a negative effect on $M$ and the only question is whether $M=1$ because of $I$ or independent of it --- which depends only on the relative sizes of $\theta^M_{11}$ and $\theta^M_{01}$, which remain the same (and equal to one another) in both models.

Overall the evaluation of the democracy and inequality model paints a mixed picture. Although the model is able to recreate data patterns consistent with observations, the inferences from within case observations discussed in Chapter \@ref(ptapp) depended on assumptions about  processes that, while theoretically compelling, can*not* be justified from observation of broader  data patterns even under relatively heroic assumptions on causal identification.   

<!--chapter:end:16-Evaluating-Models5.Rmd-->

# Final Words {#conclusionchapter}

The central idea of this book is that we can usefully learn about the world by combining new evidence with prior causal models to produce updated models of how the world works. When asking specific questions---such as whether this caused that or whether one or other channel is important---we look up answers in our updated model of causal processes rather than seeking to the question directly from data. 

<!-- many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account for how and under what conditions causal relations play out.  -->

This way of thinking about learning is very different to many standard approaches in the social sciences. It promises benefits, but it also comes with risks. We try to describe both in this closing chapter. 

The approach stands in particularly stark contrast to the  design based approach to causal inference, which has gained prominence in recent years. Design based approaches have shown that  it is possible to greatly diminish the role of background assumptions for some research questions and contexts. This is a remarkable achievement that has put the testing of some hypotheses and estimation of some causal quantities on a firmer footing. It allows researchers to maintain agnostic positions and base their inferences more solidly on what they know to be true---such as how units were sampled and how treatments were assigned---and less on speculations about background data generating processes. Nothing here argues against these strengths.

At the same time, there are limits to model-free social science that affect the kinds of questions we can ask and the conditions we need to be in place to able to generate an answer. Most simply we often don;t understand the design very well, we cannot control the processes we want to study, or the quantities we care can not be identified with design based inferential tools.    
<!-- , . In particular, design-based inference relies on (as-good-as) random assignment by the researcher or by nature, placing bounds on the kinds of causes and contexts we can investigate. The approach is also generally limited to estimating a single causal quantity: the average causal effect. -->

Building on pioneering work by scholars in computer science, statistics, and philosophy, we have outlined a principled approach to mobilizing prior knowledge to learn from new data in situations where randomization is unavailable and to answer questions for which randomization is unhelpful. In this approach, causal models are *guides* to research design, *machines* for inference, and *objects* of inquiry. As guides, the models yield expectations about the learning that can be derived from a given case or set of cases and from a given type of evidence, conditional on the question being asked. As inferential machines, models allow updating on that query once the data are in hand. Finally, when we confront a model with data, we learn about the parameters of the model itself, which can be used to answer a range of other causal questions and allowing cumulation of knowledge across studies.  To complement the conceptual infrastructure we have provided software tools that let researchers build, update, and query binary causal models.

## The benefits

<!-- Intellectual currents in empirical social science have been rushing against model-based inquiry for the last decade or more. Uneasiness with the assumptions required to draw causal inferences from observational regression estimates has prompted a flight to the epistemic safety of random assignment. With a minimal set of assumptions, a randomized experiment allows for an unbiased estimate of an important quantity, the average treatment effect. Randomized experiments offer answers that are easy to defend. Model-based analysis offers answers that will always be model-dependent. To the extent that what we our after is confidence in our conclusions, the move from models to strong designs represents unquestionable progress. -->

<!-- What we hope that readers will take from this book, however, is that it is a fallacy to think of explicitly model-based inquiry as a second-best alternative to model-free experimentation. It is  a different way of learning about the world altogether. Whether our data are generated from an experiment or arise observationally, organizing inquiry around a causal model has a number of distinct advantages: -->

Strategies centered on building, updating, and querying causal models come with a set of striking advantages.

**Many questions.** When we update a causal model, we do not estimate a single causal quantity of interest: we learn about *the model*. Most concretely, when we encounter new data, we update our beliefs about *all* parameters in the model at the same time. We can then use the updated parameters to answer very broad classes of causal questions, well beyond the population-level average effect. These include case-level questions (*Does $X$ explain $Y$ in this case?*), process questions  (*Through which channel does $X$ affect  $Y$?*), and transportability questions (*What are the implications of results derived in one context for processes and effects in other contexts?*). 

**Common answer strategy.** Strikingly, these diverse types of questions are all asked and answered in this approach using the same procedure: forming, updating, and querying a causal model. Likewise, once we update a model given a set of data, we can then pose the full range of causal queries to the updated model. In this respect, the causal models approach differs markedly from common statistical frameworks in which distinct estimators are constructed to estimate particular estimands.

**Answers without identification.** The approach can be used to generate answers even when queries are not *identified.* The ability to "identify" causal effects has been a central pursuit of much social science research in recent years. But identification is in some ways a curious goal. A causal quantity is identified, if, with infinite data, the correct value can be ascertained with certainty---informally, the distribution that will emerge is consistent with only one parameter value. Oddly, however knowing that a model, or quantity, is identified in this way does not tell you that estimation with finite data is any good [@maclaren2019can]. Nor is estimation of a non-identified model with finite data necessarily bad. While there is a tendency to discount models for which quantities of interest are not identified, in fact it is relatively easy to see that conisderable learning is possible even without identification, using the same procedure of updating and querying models. Updating non-identified models can lead to a tightening of posteriors, even if some quantities can never be distinguished from each other.  

<!-- Using causal models also provides a clear *procedure* for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw.  -->

**Integration** Embedding inference within an explicit causal model brings about an integration across forms of data and beliefs that may otherwise develop in isolation from one another. For one thing, the approach allows us to combine arbitrary mixes of forms of evidence, including data on causes and outcomes and evidence on causal processes (whether from the same or different sets of cases). Further, the causal-model approach ensures that our findings about *cases* (given evidence about those cases) are informed by what we know about the *population* to which those cases belong, and vice versa. And, as we discuss further below, approach generates integration between inputs and outputs into the inferential process: it ensures that the way in which we update from the data is logically consistent with our prior beliefs about the world. 

**A framework for knowledge cumulation.** Closely related to integration is cumulation: a causal-model framework provides a ready-made apparatus for combining information across studies. Thinking in meta-analytic terms, the framework provides a mechanism for combining the evidence from multiple, independent studies. Thinking sequentially, the model updated from one set of data can become the starting point for the next study of the same causal domain. 

Yet organizing inquiry around a causal model allows for cumulation in a deeper sense as well. Compared with most prevailing approaches to observational inference---where the background model is typically left implicit or conveyed informally or incompletely---the approach ensures *transparency* about the beliefs on which inferences rest. Explicitness about starting assumptions allows us to assess the degree of sensitivity of conclusions to our prior beliefs. Sensitivity analyses cannot, of course, tell us which beliefs are right. But they can tell us which assumptions are most in need of defending, pinpointing *where more learning would be of greatest value.* Those features of our model about which we are most uncertain and that matter most to our conclusions --- be it the absence of an arrow, a restriction, a prior over nodal types, the absence of confounding --- represent the questions most in need of answers down the road. 

**A framework for learning about strategies.** As we showed in chapters \@ref(clue) and \@ref(caseselection) access to a model provides an explicit formulation of how and what inferences will be drawn from future data patterns provides a formal framework for justifying design decisions. Of course this feature is not unique to model based inference---one can certainly have a model that describes expectations over future data patterns and imagine what inferences you will make using design based inference or  any otehr procedure.

<!-- **Limits of qualitative data under ignorable assignments.** A key payoff deploying causal models is the prospect of combining in-depth observations of a small number of cases with less-intensive investigation of a larger number of cases. Yet one of the lessons of the foregoing analysis is that the gains to such mixing may be limited. For instance, where the causal effect of $X$ on $Y$ can be identified (assignment of $X$ is as-good-as random), one will learn little about this effect from process observations in a small number of cases. Put differently, case studies simply cannot add much to experimental estimates of the ATE, though there may be other queries of interest on which small-$N$ process evidence can be informative. [To be expanded] -->


<!-- While we have outlined a set of strategies for validating and selecting our models, the model-contingency of conclusions is an important and inescapable limitation of the framework.  -->

**Conceptual clarifications.** Finally, we have found, this framework has been useful for providing conceptual clarification for how to think about qualitative, quantitative, and mixed-method inference. Consider two. 

The first is with respect to the difference between "within-case" and "between-case" inference.  In @humphreys2015mixing, for instance, we drew on a common operationalization of "quantitative" and "qualitative" data as akin to "dataset" and "causal process" observations, respectively, as defined by @collier2010sources  ( see also @mahoney2000strategies). In a typical mixed-method setup, we might think of combining a "quantitative" dataset as containing $X$ and $Y$ (and covariate) observations for many cases with "qualitative" observations on causal processes, such as a mediator $M$, for a subset of these cases. But this apparent distinction has no meaning in the formal setup and analysis of models. There is no need to think of $X$ and $Y$ observations as being tied to a large-$N$ analysis or to observations of mediating or other processes as being tied to small-$N$ analysis. One could, for instance, have data on $M$ for a large set of cases but data on $Y$ or $X$ for only a small number. Updating the model to learn about the causal query of interest will proceed in the same basic manner. The cross-case/within-case dichotomy plays no role in the way inferences are drawn: given any pattern of data we observe in the cases at hand, we are always assessing the likelihood of that data pattern under different values of the model's parameters. In this framework, what we have conventionally thought of as qualitative and quantitative inference strategies are not just integrated; the distinction between them breaks down completely. 

A second is with regards to the relationship between beliefs about queries and beliefs about the informativeness of evidence. In many accouns of process tracing, researchers posit a set of prior beliefs about the values of estimands and other---independent---beliefs about the informativeness of within-case  information (see also @FairfieldBayes2015, @BennettAppendix). Viewd theough a cusal models lens however  *both* sets of beliefs --- about the hypothesis being examined and about the probative value of the data --- represent substantive probabilistic claims about the world, and in particular about *causal relationships* in the domain under investigation. They, thus, cannot not be treated as generally independent of one another: our beliefs about causal relations *imply* our beliefs about the probative value of the evidence. These implications flow naturally in a causal-model framework. When both sets of beliefs are themselves derived from an underlying model representing prior knowledge about the domain of interest, then the same conjectures that inform our beliefs about the hypotheses also inform our beliefs about the informativeness of additional data. Seen in this way the researcher is under pressure to provide reasons to support beliefs about probative value, but more constructively, they have availabel to them a strategy to do so.

<!-- **Pretending to have priors.** As we explored the world of causal models, we started out thinking that, in providing priors over causal relations, one is directly stating beliefs about how the world works. For instance, one might believe that either $X$ caused $Y$ or that it did not; and one might believe either that $M=1$ should be observed in the event that $X$ caused $Y$ or it should not be. These statements are, in fact, clearly model-dependent. Beyond the model required to describe events in such crisp terms, the statements involve counterfactuals on counterfactuals---models of causal processes. Once a model involves assertions of conditional independence, we are clearly in the business of dealing in simplifications. Our priors become less statements of how we believe the world works and become statements about what set of models are least bad within a class of tractable abstractions. -->

<!-- AJ: There's a clear tension between the above point and the ways in which we talk about models as reflecting "prior beliefs" or "prior knowledge" about the world. Need to resolve this. -->




## The worries

While we have found the syntax of Directed Acyclic Graphs to provide a flexible framework for setting up causal models, we have also become more keenly aware of some of the limitations of DAGs in representing causal processes. We discuss a few of these here.

**Well-defined nodes?** A DAG presupposes a set of well-defined nodes that come with location and time stamps. Income in time $t$ affects democracy in time $t+1$ which affects income in time $t$. Yet it is not always easy to figure out how to partition the world into such near event bundles. Income in 1985 is not an "event" exactly but a state, and the temporal ordering with respect to "Democracy 1985" is none too clear. Moreover, even if events are coded into well ordered nodes values on these nodes may poorly capture actual processes, even in simple systems. Consider the simplest set up with a line of dominos. You are interested in whether the fall of the first domino cases the fall of the last one. But the observations of the states of the dominos do not fully capture the causal process even  in this simplest of systems. The data might report that (a) domino 1 fell and (b) domino 2 fell. But the observer will notice that domino 2 fell *just as* domino 1 hit it. 

<!-- AJ: I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->

**Acyclic, really?** DAGs are by definition acyclic. And it is not hard to argue that, since cause precedes effect, causal relations *should* be acyclic for any well-defined nodes. In practice, however, our variables often come with coarse periodizations: there was or was not mobilization in the 1990s; there was or was not democratization in the 1990s. We cannot extract the direction of arrows from the definition of nodes this coarse. 

<!-- AJ: Here too, I am not exactly sure what point you mean to make here. If you spell ouit a bit more in point form, I can clean up. -->

**Coherent underlying causal accounts.** The approach we describe is one in which researchers are asked to provide a coherent model---albeit with uncertainty---regarding the ways in which nodes are causally related to each other.  For instance, a researcher interested in using information on $K$ to ascertain whether $X$ caused $Y$ is expected to have a theory of whether $K$ acts as a moderator or a mediator for $X$, and whether it is realized before or after $Y$. Yet it is possible that a researcher has well formed beliefs about the informativeness of $K$ *without* an underlying model of how $K$ is causally related to $X$ or $Y$. Granted, one might wonder where these beliefs come from or how they can be defended. We nonetheless note that one limitation of the approach we have described is that one cannot make use of an observation without a coherent account of that observation's causal position relative to other variables and relationships of interest. 

**Complexity.** To maintain simplicity, we have largely focused in this book on models with binary nodes. At first blush, this class of causal models indeed appears very simple. Yet even with binary nodes, complexity rises rapidly as the number of nodes and connections among them increases. As a node goes from having 1 parent to 2 parents to 3 parents to 4 parents, for instance, the number of nodal types --- at that node alone --- goes from 4 to 16 to 256 to 65,536, with knock-on effects for the number of possible causal types (combinations of nodal types across the model). A move in the direction of continuous variables --- say, from binary nodes to nodes with 3 ordinal values --- would also involve a dramatic increase in the complexity to the type-space.^[If, for instance, we moved to nodes with 3 ordered categories, then each of $Y$'s nodal types in an $X \rightarrow Y$ model would have to register 3 potential outcomes, corresponding to the 3 values that $X$ takes on. And $Y$ would have $3 \times 3 \times 3 = 27$ nodal types (as $Y$ can take on 3 possible values for each possible value of $X$).] There are practical and substantive implications of this. A practical implications is that one can hit computational constraints very quickly for even moderately sized models. Substantively, models can quickly involve more complexity than humans can comfortably understand.  

One solution is to  move away from a fully non-parametric setting and impose structure on permissible function forms---for example by imposiing motonicity or no high level interactions. 

A second approach might be to give up on the commitment to a complete specification of causal relations and seek lower dimensional representations of models that are sufficient for questions we care about. For instance, we could imagine representing an $X \rightarrow Y$ model with just two parameters rather than for (for $Y$): define $\tau := \theta^Y_{10}-\theta^Y_{01}$ and $\rho := \theta^Y_{11}-\theta^Y_{00}$, both of which are identified with experimental data. These give us enough to learn about how common different types of outcomes are as well as average effects, though not enough to infer the probability that $X$ caused $Y$ in a $X=Y=1$ case. 

**Unintended structure.**  The complexity of causal models means that it is easy to generate a fully specified causal model with features that you do not fully udnerstand. In the same way it is possible to make choices between models unaware of differences in assuptions that they have built in. 

Consider three examples: 

* We specify a model $X  \rightarrow Y$ and assume flat priors over nodal types. The implied prior that $X$ has a positive effect on $Y$ is then 0.25. We then add detail by specifying $X \rightarrow M \rightarrow Y$ but continue to hold flat priors. In our more detailed model, however, the probability of a positive effect of $X$ on $Y$ is now just 0.125. Adding the detail requires either moving from flat priors on nodal types or? changing priors on aggregate causal relations.


<!-- * You specify a model with $X\rightarrow Y$ and update given some data on $X$ and $Y$. You then add detail and specify $X \rightarrow M \rightarrow Y$ with teh same prior beliefs over how $X$ affects $Y$. You update with teh same data but come to different results. -->

```{r, echo = FALSE, eval = FALSE}

M1 <- make_model("X->Y")

k <- (1/8)^.5
kk <- 3.5
M2 <- make_model("X->M->Y") %>% set_priors(alpha = c(1,1, c((1-2*k)/2, k, k, ((1-2*k)/2))*kk, c((1-2*k)/2, k, k, ((1-2*k)/2))*kk))

Q1 <- query_model(M1, c("Y[X=1]==0 & Y[X=0]==0 ", "Y[X=1] > Y[X=0]"), using = "priors" )
Q2 <- query_model(M2, c("Y[X=1]==0 & Y[X=0]==0 ", "Y[X=1] > Y[X=0]"), using = "priors" )

```

* We specify a model $X  \rightarrow Y \leftarrow W$ and build in that $W$ is a smoking gun for the effect of $X$ on $Y$. You add detail by specifying $X \rightarrow M \rightarrow Y \leftarrow W$. This means however that $W$ cannot be a smoking gun for $Y$ unless the $X \rightarrow M$ relation is certain. Why? To be a smoking gun it must be the case that, if $W=1$, we are sure that $X$ causes $M$ and that $M$ causes $Y$ which requires an arrow from $W$ to $M$ and not just from $W$ to $X$.




**Model-dependence of conclusions** One striking finding of some of the analyses presented here is see how sensitive conclusions can be to what would seem to be quite modest changes to models. We see two ways of thinking about the implications of this fact for a causal-models framework. 

One lesson to draw would be that there are tight limits to building inference upon causal models. If results in this approach depend heavily on prior beliefs, which could be wrong, then we might doubt the utility of the framework. Perhaps all that we have done in this book is to make the case for pure design-based inference.

An alternative lesson also offers itself, however. To the extent that our inferences depend on our background causal beliefs, a transparent and systematic engagement with models becomes all the more important. If inferences are not built explicitly on models, we have no way of knowing how fragile they are, how they would change under an alternative set of premises, or what kind of learning we need to undertake if we want to generate more secure conclusions. 

We do not see causal models as the only way forward or as a panacea, and we are conscious of the limitations and complexities of the approach we have outlined, as well as the need for extension and elaboration along numerous fronts. Yet we think there is value in further development of forms of empirical social science that can operate with analytic transparency outside the safe inferential confines of random assignment.



## The future

<!-- - What do we want: to be able to know how to update on a set of general questions as new evidence comes in. New info comes in, new event occurs, we want to be able to add that to the store of knowledge in a way that is integrated with what we previously new and updates over general principles. And go the other way: have our inferences on an individual case be informed by what we know generally.  (Make clear this is not a hegemonic project.) -->
<!-- - Framework for knowledge integration and cumulation -->
<!-- - Coordination required -->
<!-- - Some challenges of coordinating -->
<!--     - Enough overlapping nodes? -->
<!--     - Same ways of measuring? -->
<!-- - Coordination could take a variety of forms -->
<!--     - Could just nail down the nodes and how they’re defined. Nothing on priors or arrows. Anyone can make their own model from those nodes — and update using all the available data. Would need to take into account obs vs. exp. -->
<!--     - Common queries — everyone comes up with their own model and they compete for predictive performance. -->
<!-- - Qualities we’d want models to have -->
<!--     - Well-defined -->
<!--     - Scope conditions: what’s implicit? -->
<!--     - Can talk to other models — interoperable, similar nodes -->
<!--     - Would want models to be justified in some way -->
<!--         - Grounded in theory -->
<!--         - Consistent with past data -->
<!--         - Reflecting beliefs of community of expert -->
<!-- - I will give you: Integrated inferences -->

The future are we see it lies in improvements in model grounding, coordination, and cumulation. 

**Cumulation** The cumulation of knowledge requires integration. As we acquire new evidence -- perhaps from observation of additional cases or new events -- we want to be able to update our general beliefs about how the world works by integrating new information with the existing store of knowledge. At the same time, we want our inferences about individual cases to be informed by our beliefs about how the world works in general. 
Causal models provide a natural framework for cumulating knowledge on these terms.  We have spelled out in this book how the individual researcher can use models to join up new data with prior beliefs and ground case-level inferences in beliefs about population-level parameters. In a scientific discipline, however, cumulation must operate *across* researchers. For a field to make progress, I need to update *my* beliefs in light of *your* new evidence and vice versa. 

The challenge is one of fragmentation. Not only do I need access to your evidence, but we have to be operating to some degree with a common causal model of the domain of interest; otherwise, the data that you generate might fail to map onto my model. Moreover, if we are all updating our own separate models, the prospects are dim for agreement about what has been learned. There is also little reason to be optimistic that individual researchers will naturally tend to generate models that align with one another. Not only might our substantive causal beliefs diverge, but we are likely to make differing choices about matters of model-construction --- from which nodes to include and the appropriate level of detail to the manner of operationalizing variables. 

**Coordination.** Turning causal models into vehicles for knowledge cumulation in a field will thus require coordination around models. We are under no illusion that such coordination would be easy. But productive coordination would not require prior agreement about how the world works. It is unlikely that experts in any social scientific domain would converge on a single model representing shared beliefs. Rather, the most likely way forward would involve coordination around a *set* of models. 

One possibility would be to fix (provisionally, at least) the set of nodes relevant in a given domain --- including outcomes of interest, potential causes, and mediators and moderators implicated in prevailing theories --- and how those nodes are defined. Individual researchers would then be free to develop their own models by draw arrows and setting priors and restrictions in line with their beliefs. Coordination around model nodes would then guide data-collection, as teams running new studies would seek to collect data on at least some subset of the common nodes --- allowing, in turn, for all models to be updated as the new data come in. 

Another possibility would be to allow for modularity, with different researchers or projects modeling different *parts* of a causal system. For instance, in the field of democratization, one set of researchers might model and collect data on links between inequality on democratization; others might focus on the role of external pressures; while still others might focus on inter-elite bargaining. Coordination would operate at the level of inter-operability. Modules would have to have at least some overlapping nodes for updating from new data to operate across them.  Ideally, each module would also take into account any confounding among its nodes that is implied by other modules.

A further, more minimalist mode of coordination would be for researchers to develop models that agree only on the inclusion of one or more outcome nodes. As new data comes in, models would then be set in competition over predictive performance.

Coordination would also require agreement on some minimal set of qualities that included models must have. For instance we would surely want all models to be well defined, following the basic rules of DAG-construction and with clear rules for operationalizing all nodes. 

<!-- Integration could also be greatly facilitated to the extent that researchers explicitly specify any scope conditions that apply to their model. Researchers are likely to create models with particular contexts or domains of applicability in mind. Specifying those conditions can allow for the assembling of multiple context-specific models into an overarching model in which scope conditions operate as moderators.  -->

**Grounding.** Most of our models are on stilts. We want them grounded. One kind of grounding is theoretical. As we show in Chapter \@ref(theory), for instance, game-theoretic models can be readily translated into causal models. Clear informal theoretical logics could underwrite causal models also. A second approach would be subjective-Bayesian: models could be founded in aggregated expert beliefs. For instance, it would not be hard to imagine researchers gathering and aggregating prior causal beliefs from a set of experts within a field of inquiry. A third---our preferred---form of grounding is empirical. As we discuss in Chapter \@ref(justifying), experimental data can be used to anchor model assumptions, which can in turn provide grounds for drawing case-level inferences. Ultimately, we hope, the benefits from mixing methods will flow in both directions.



<!-- *********************************** -->
<!-- We began this intellectual journey developing an approach to Bayesian mixed-method analysis in which the researcher stipulates priors beliefs about estimands and the probative value of new evidence, and selection into assignment. Over time we became increasingly dissatisfied with the fact that this approach lacked foundations. These translate into the inputs you need to apply Bayes rule and you can apply Bayes rule without any justification for any of these inputs. Where do prior beliefs about estimands, probative value, and assignment come from? On what basis should readers accept your outputs if you cannot justify your inputs? -->

<!-- This book represents our answer to that problem: a causal model allows the researcher to write down a set of more foundational causal beliefs, from which priors, probative value, and selection effects can be derived. Doing so forces a coherence that is otherwise not required when you turn to Bayes: from the same model you can derive both the prior probability that $X$ affects $Y$ and that $M$ is informative for $X$ affects $Y$ and for the probability that $X=1$ depending on whether or not $X$ affects $Y$. This model may itself be justified by reference to a more fundamental model that has been updated with prior data or to a model that reflects the beliefs of a community of research consumers.  -->

<!-- Yet this answer raises its own question about foundations: where do our foundational causal models come from?  While we have provided some guidance on how to construct models and evaluate them, we have largely bracketed the question of how researchers might form, extract, or elicit the substantive causal beliefs that inform models. We see the development of transparent, systematic approaches to model construction as an important next step in the elaboration of the framework.  -->

<!-- ### The goal: A compendium of models -->

<!-- * Shared models -->
<!-- * Models we can defend -->
<!-- * Models we can reject -->
<!-- * We know what we learn when we learn something new -->


<!-- ### The properties of  models -->

<!-- One could imagine multiple approaches to model-generation, and we seek here to outline a set of desiderata toward which any model-generating technique might plausibly be oriented. -->

<!-- **1. Theory based causal models.**  -->

<!-- A good causal model will usually be interpretable by reference to a plausible theoretical logic. For instance, if our model allows for high inequality to cause mass mobilization, we might place a value on that feature of the model being underwritten by a set of defensible claims about the mechanism through which that effect might unfold.   -->
<!-- <!-- Conversely an incoherent model may be suspect. Consider the model: --> 
<!-- <!-- $$\text{Inequality} \rightarrow \text{Mobilization} \rightarrow \text{Democratization} \leftarrow \text{Price Inflation}$$ -->
<!-- <!-- Is it consistent to allow inequality to have an effect on mobilization but exclude inflation from operating through mobilization? If economic grievances can provoke mass mobilization, as in inequality's effect, should we not also allow for rising food prices to have a similar effect?  --> 
<!-- A theoretical logic is not likely to be *sufficient* to justify a particular causal model, as there will generally be too many potential theoretical logics around a given domain to narrow down the possibilities sufficiently (especially taking the complexity desideratum, below, into account). Nor will a theoretical logic be strictly necessary for a useful causal model. One could imagine allowing for an effect on $X$ on $M$ in a model based on empirical -- perhaps experimental -- evidence of such an effect, absent a theorization of the mechanism. Nevertheless underlying theory can clarify what one has to be believe in order to buy the structure of a model.  -->

<!-- **Would be nice to have an example where theory suggests that X woud be random or soemthing like that** -->

<!-- **2. History subsuming causal models.**  -->

<!-- A second justification for causal models would be a maximally complete, accurate incorporation of the information available prior to analysis that speaks to causal relationships within the domain of interest. In general, this desideratum is likely to act as a conservative constraint on the strength of beliefs embedded within a model. For instance, one set of prior information might point toward an $X \rightarrow M \rightarrow Y$ chain of effects, yet this is a relatively restrictive model. Incorporating *more* prior information is unlikely to definitively discredit the possibility of this causal chain, but it might well yield evidence of pathways from $X$ to $Y$ that do not include $M$. And if it does, then consistency with prior information would require us to add a direct link from $X$ to $Y$, generating a less restrictive model. IS THIS POINT FRAMED RIGHT? OR SHOULD IT INSTEAD BE ABOUT NOT MAKING STRONGER CLAIMS THAN THE PRIOR INFO JUSTIFIES (WHEREAS THE NEXT POINT IS ABOUT NOT MAKING WEAKER CLAIMS THAN YOU CAN)? -->

<!-- **3. Leverage.**  -->

<!-- As we have shown throughout this book, some models will provide more empirical leverage than others, in the sense that observations of their nodes will have greater probative value. Leverage typically derives from the strength of the prior beliefs embedded in a model. For instance, the causal model $X \rightarrow M \rightarrow Y$ will tend to provide strong opportunities for updating than the model $X \rightarrow M \rightarrow Y$ *if* we also have non-uniform beliefs over $X$'s effects on $M$ and $M$'s effects on $Y$. One way to put this is that we don't want to leave precision on the table. While we do not want our models to build in stronger beliefs than prior information can justify, we also would not want a model-generating procedure to yield models that are weaker than they could be. -->

<!-- **4. Tractable models.** -->

<!-- More complex models are harder to work with. As the number of a node's parents increases, for instance, the number of nodal types increases steeply. It becomes accordingly harder for researchers to conceptualize and form meaningful priors over these nodal types. Moreover, the computational demands of operating even moderately complex models can be very high.  -->

<!-- **5. Transparent.**  -->

<!-- Ideally, we would want any model-generating procedure to make use of prior information via an explicit, transparent set of procedures. If one researcher derives a model, a second researcher should be able to see clearly how the model was generated and, in turn, to assess how the model might have changed if additional or different information had been incorporated.  -->

<!-- **6. Reliability** The procedure should ideally yield the same model, with the same restrictions and priors, when repeated and regardless of which researcher is implementing the procedure. COMBINE WITH TRANSPARENCY?? -->

<!-- **7. Dynamic adaptability.** Closely related to transparency is dynamic adaptibility. The base of "prior" information on any topic is constantly changing, and we would value a procedure that allows our models to adapt accordingly. It would be helpful if a procedure that generated a model at time $t$ could be incrementally updated in light of new information for use at time $t+k$.  -->

<!-- There will tend to be tradeoffs across these desiderata. For instance, tractability will sometimes be in tension with a full, accurate use of prior information. As we broaden the base of prior information, we are likely to add nodes as well as arrows (representing possible direct causal effects) between existing nodes. Likewise, taking more prior information into account may mean giving up leverage if, for instance, additional information makes us less certain about the prior weights over nodal types. Some desiderata will be complementary, however: models with more leverage will often include restrictions on nodal types, which will also reduce complexity. In any case, we expect that no single procedure will be able to maximize all desiderata at the same time.  -->

<!-- ### Procedures to get there -->

<!-- Consider a few operational possibilities: -->

<!-- **Crowdsourcing** -->

<!-- One could imagine a platform that allowed researchers to gather and aggregate prior causal beliefs from a set of experts within a field of inquiry. Experts could be asked to weigh in on relationships among a fixed set of variables: which variables have direct effects on which? Experts could also be invited to add nodes, for instance, as mediators between the original set of nodes, as confounders, or as colliders. To generate restrictions and priors, experts could also be asked what kinds of effects (positive, negative, null) are relatively more and less likely.  -->
<!-- Instruments for eliciting such beliefs might vary widely, but one approach could be a conjoint design: for each node, respondents are presented, successively, with paired sets of assigned values for the node's parents and then asked for which pair of parental values they expect the node itself to take on a higher value. -->

<!-- Crowdsourcing would likely perform well in achieving comprehensiveness of the prior information base insofar as experts will be, at least implicitly, drawing on a wide range of observations, findings in the literature, and theoretical insights. The procedure is transparent insofar as the elicitation methods, expert sampling frame, and expert responses can be shared; on the other hand, we would have little insight into what information experts themselves have drawn on in arriving at their beliefs. The reliability of a crowdsourced procedure would depend in part on how large the "crowd" is: how many experts can be polled on a given causal domain. A key challenge would be implementing crowdsourcing in a way that yielded a clear grounding in internally consistent theoretical logics. It is also our hunch that crowdsourced models will tend to be more complex, given that they will likely represent a broader range of causal possibilities and a more diffuse set of beliefs over causal effects. Crowdsourcing could also be implemented adaptively, with some additional effort. One could imagine a platform that would allow "new" experts to register their views at any time or for "old" experts to update their responses as their beliefs change.   -->

<!-- * Formal models: As we show in the appendix to Chapter \@ref(theory), we can readily translate the implications of a game-theoretic model into a causal model. Building a causal model atop a formal model ensures clear theoretical grounding, internal consistency, transparency, and reliability  (insofar as a single formal model implies a single causal model). Formal models that yield clear predictions are also likely to generate relatively strong beliefs about causal effects and, thus, a good deal of leverage. Complexity could be more or less arbitrarily determined by the modeler. A key major weakness of a formal-model-based approach would be the flip side of leverage: rather than incorporating a wide array of beliefs, a formal model would instantiate the implications of a fixed set of assumptions set out by the modeler. Moreover, there is no obvious mechanism for adapting a formal model to new information. -->

<!-- * Data-driven approach: Perhaps the most transparent, reliable, adaptive, and conservative approach to building causal models is to rest them directly and maximally on prior data. We discuss in Chapter \@ref(justifying) how one can use experimental data to justify models. At its most conservative, the data-driven approach might involve *only* building into a model beliefs that can be grounded in an explicit meta-analysis of experimental findings -- yet we might also incorporate observational findings. Of course, the most natural way to implement a data-driven approach would be to start with a causal model with minimal structure and then use all available prior data to update it. And, as we show in Chapter \@ref{mm}, with the right initial model we can readily update on a mixture of experimental and observational data. It would be extremely straightforward, moreover, to update a data-driven model as new findings come in.  -->

<!-- The limitations of a data-driven approach are, effectively, the flip side of its virtues. Conservatively grounded in what the data can tell us, a data-driven procedure will tend to yield models that have less structure, with higher complexity and less leverage. Involving minimal assumptions, data-derived models will also be unconstrained by theoretical logic or consistency. -->

<!-- Even a maximally data-driven approach cannot be assumption-free. One inescapable assumption is portability: we need to believe that the model that describes the context in which the prior data were generated also applies to the context in which new data are to be generated. -->


<!-- Crodsourced models; -->
<!-- discpline wide agreement on models -->

<!-- building structures so research can contribute to models -->

<!-- connecting more thoroughly to formal theorey -->
<!-- logical coherence -->

<!-- modularization -->



<!--chapter:end:17-conclusion.Rmd-->

# (PART) Appendices {-}

# `CausalQueries`  {#examplesappendix}

Examples of canonical models, together with a guide to the `CausalQueries` package is provided at:

https://macartan.github.io/causalmodels/


`r if (knitr:::is_html_output()) '# References {-}'`


<!--chapter:end:18-appendix.Rmd-->

