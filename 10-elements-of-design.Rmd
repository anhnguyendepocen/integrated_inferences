# (PART) Design Choices {-}

# Elements of Design


```{r, include = FALSE}
source("_packages_used.R")
```


***

A fully specified causal model includes the information needed to assess the properties of a research design that seeks to learn from or learn about the model. We talk through how to go from defining causal models to "declaring" research designs and use this framework in later chapters to inform decisions about details of design choices.  

***

So far we have described a way to think about causal models, a way to specify causal estimands, and a Bayesian approach to inference, given models and estimands. Together with a strategy for data gathering these elements are enough to fully characterize a research design. If in addition we provide criterea for evaluating a design we have enough to be able to simulate the behavior of a research design and assess whether a design is up to the task fo answering the questions we want to answer. 

Once we have a  method to assess the performance of a given design we can can start asking what kind of design is optimal, given some beliefs about the world (see @blair2016declaring for more on this general approach to design declaration and diagnosis). In the next chapters we use this approach to assess a set of design choices including choices regarding the clues about which data is sought, the types of cases for which data is sought, and the number of cases for which different types of data is sought.  

In the remainder of this chapter we discuss a simple evaluative criterion for a design and give examples for design declaration for a simple single case process tracing design and a mixed methods design.



## Model, inquiry, data strategy, answer strategy

We use the MIDA approach (model, inquiry, data strategy, answer strategy) approach to declare a simple process tracing design with an arbitrary model.


*  **Model.** We will define a model as introduced in Chapter 2. We outline approaches for this in section X below. 

* **Inquiry** As discussed in Chapter 4, an inquiry is a question asked of a model. This is typically a question about the distribution of a variable in some controlled or natural condition, or some summary of such distributions. We refer to the quantity being targeted by a query as the estimand.

* **Data strategy.** The data strategy describes how data will be gathered. In typical  `DeclareDesign` applications this includes both randomization and data gathering (sampling) strategies. We focus on data gathering though we highlight that randomization strategies can be implemented via a modification of the confounds allowed by the model.   A sampling strategy might indicate a sequence of conditional data gathering schemes, for instance: gather data on $X$ and $Y$ for 100 cases, then gather data on $M$ for all cases in which $X=Y$.
Note that in some cases we might want to think of the estimand as being defined *after* the data strategy. This would be the case for instance if we chose a case and we seek to work out some feature *about that case* rather than about the population.

* **Answer strategy**. The answer strategy combines the observed data with a causal model to generate an updated model from which inferences can be drawn. Importantly, the model used in the answer strategy does not need to be the same model as assumed at the model step since we could imagine analysts coming to the data with quite different models in mind. Of course any model used in the answer strategy should generally involve the same variables as in the model itself.

A design is a concatenation of these four steps. 

The concatenated lets us examine instances of the application of a design. A single instance would involve 

1. a single draw of a true parameter vector from the distribution given in the model definition
2. a calculation of the value of an estimand given this true parameter draw
3. the generation of a dataset given the model implied by step 1 and the data strategy
4. the generation of an answer to the inquiry generated from the realized data from 3. and the answer strategy

With the observation of multiple instances we get to assess the distribution of our answers --- and our uncertainty around these -- over repeated draws, and each time we get to see how well the answer we get maps onto the assumed truth in that draw.




## Defining a model

For this we need to define:

* A directed acyclic graph. In doing so we declare the set of variables we are interested and the relations of independence between them. In defining the variables we generally also define the ranges of the  variables---indicating, for example whether  they are binary, categorical, or continuous. In defining the edges we identify the set of parents of any node. 
    1. Which nodes?
    2. How much detail? Be aware of implications of uneveness in the level of detail
    3. Justifying conditional independence claims
    
* Any restrictions on functional forms relating parents to children. In the binary set up, imposing  functional form assumptions is the same as restructuring causal types. 

* A declaration of structures of unobserved confounding.  

* Priors. Beliefs about the distribution of shocks. When defined as part of the model we think of these priors as being the priors from the vantage point of someone assessing a design and they need not be the same as the priors used in the analysis. 
    1. REFELCTING THE LITERATURE
    2. GATHERING DISCPILINARY PRIORS
    3. EMPIRICAL APPROACH


## Choosing estimands

## Selecting a data strategy

```{r, echo = FALSE}
# detach(package:DeclareDesign, unload=TRUE)
```



## Evaluating inferences

The  observation that theories vary in their precision points to a method for describing the learning that is attributable to a lower-level theory relative to a higher level theory. When a lower-level theory represents a disaggregation, the lower-level theory identifies a set of potentially observable variables that are not listed by the the higher-level theory. This allows one to assess the gains in precision (for some collection of unobserved variables) that can arise from  learning the values of additional observables in the lower-level theory. 

Suppose that the contribution of a lower-level theory is to allow for inferences from new data $K$ about some set of query variables $Q$, after we have already observed variables $W$ from the higher-level model.  

Then we can use the expected squared error from the mean posterior estimate as a measure of precision for collection  $Q$, as a measure of loss:

$$E_{k, q} \left(\left( \int q' P(q' | k, w)dq' - q\right)^2\right)$$
where the expectation is taken over the joint distribution of $K$ and $Q$, given $W$. This is an expected loss---or the *Bayes risk*. The inner term $P(q'|k, w)$ is the posterior distribution on $q'$ given observation of $k$ and $w$. 

Another way to think of the gains is as the expected reduction in the variance of the Bayesian posterior: how certain do you expect you will be after you make use of this new information?

In fact these two quantities are equivalent  (see for example @scharf1991statistical). Moreover, it is easy to see that whenever inferences are sensitive to $K$, the expected variance of the posterior will be lower than  the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of $K$, given what is already known from observation of $W$.^[A similar expression can be given for the expected posterior variance from learning $K$ in addition to $W$ when $W$ is not yet known. See, for example, Proposition 3 in @geweke2014analysis.]  
$$Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))$$

The contribution of a theory can then be defined as the mean reduction in Bayes risk:

$$\text{Gains from theory} = 1- \frac{E_{K|W}(Var(Q|K,W))}{Var(Q|W)}$$

This is a kind of $R^2$ measure (see also @gelman2006bayesian). 

### Other loss functions

Other loss functions could be used, including functions that take account of the costs of collecting additional data,^[Further, one might call into question the value of a theory if the gains in precision depend upon data that are practically impossible to gather.] or to the risks associated with false diagnoses.^[For instance, in @heckerman1991toward, an objective function is generated using  expected utility gains from diagnoses generated based on new information over diagnoses based on what is believed already. In their treatment [@heckerman1991toward, Equation 6],  the expected value of new information $K$, given existing information $W$ is: $\sum{K}P(K|W)( EU(d(Q,W,K)|W, K) - EU(d(Q, W)|W, K))$ where $EU$ is expected utility and $d$ is the optimal inference (diagnosis) given available data. Note that the diagnosis can take account of $K$ when it is observed, but the expected utility depends on $K$ whether or not it is observed, as $K$ carries information about the state of interest.] 

For illustration say that it is known that $X=1, Y=1$ and that, given this information (playing the role of $W$), the posterior probability that a unit is of type $b$ (and not type $d$) is $p$. Say then that a theory specifies that $K$ will take a value 1  with probability $\phi_j$ if the unit is of type $j$. Then what is the value added of this theory? Define $Q$ here as the query regarding whether the unit is a $b$ type. Then the prior variance, $Var(Q|W)$, is simply $p(1-p)^2 +(1-p)p^2 = p(1-p)$. 

<!-- Would be best to  write down the theory as a structural equation that has phi_j as p(K=1|j) -->

To calculate $E_{K|W}(Var(Q|K,W))$, note that the posterior if $K$ is observed is $\frac{\phi_bp}{\phi_bp+\phi_d(1-p)}$. Let us call this $\hat{q}_K$, and the belief when $K$ is not observed $\hat{q}_{\overline{K}}$.
In that case the  *expected error* is: 

$$\text{Expected Error} = p\phi_b\left(1-\hat{q}_K\right)^2+(1-p)\phi_d\hat{q}_K^2+p(1-\phi_b)\left(1-\hat{q}_{\overline{K}}\right)^2+(1-p)(1-\phi_d)\hat{q}_{\overline{K}}^2$$

where the four terms are the errors when $K$ is seen for a $b$ type, when $K$ is seen for a $d$ type, when $K$ is not seen for a $b$ type, and when $K$ is not see for a $d$ type.


Defining $\rho_K = (p\phi_b+(1-p)\phi_d)$ as the probability of observing $K$ given the prior, we can write the posterior variance as:

$$\text{Expected Posterior Variance} = \rho_K\hat{q}_K(1-\hat{q}_K)+(1-\rho_K)\hat{q}_{\overline{K}}(1-\hat{q}_{\overline{K}})$$


<!-- Making use of the fact that $\rho_K\hat{q}_K = ({\phi_bp+\phi_d(1-p)})\frac{\phi_bp}{\phi_bp+\phi_d(1-p)} = \phi_bp$ and similarly  -->
<!-- $(1-\rho_K)\hat{q}_{\overline{K}} = (1-\phi_b)p$, this can be written in terms of primitives as: -->

With a little manipulation, both of these expressions simplify to:

$$\text{Expected Posterior Variance} =p(1-p)\left(\frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} + \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\right)$$


The gains are then:

$$\text{Gains} =1- \frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} - \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}$$

Let's consider the same question using a particular model and calculate these quantities given this model.

```{r}
model <- make_model("X -> Y; K -> Y") %>%
         set_parameters(c(
           .5, .5, 
           .5, .5, 
           0,.5,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,.5))
```

The probability that $K=1$ given $X=Y=1$ is:

```{r}
prob_K <- query_distribution(model, 
                   query = "K[Y=1]==1",
                   using = "parameters", 
                   subset = "X==1 & Y==1")
```

The inferences conditional on the observation of $K$ are:

```{r}
inferences <- gbiqq::query_model(
                   model, 
                   queries = "Y[X=1]>Y[X=0]",
                   using = "parameters", 
                   subsets = list("X==1 & Y==1", "X==1 & Y==1 & K==1", "X==1 & Y==1 & K==0"))


```

```{r, include = FALSE}
z <- as.numeric(inferences$mean)
```

The expected error is then reduced from `r round(z[1]*(1-z[1]), 2)` to 
`r round(prob_K*z[2]*(1-z[2]) + (1-prob_K)*z[3]*(1-z[3]), 2)`

### Other measures of a gain from a theory

Other natural measures of gains from theory might include the simple correlation between $K$ and $Q$, or entropy-based measures (see @zhang2003properties for many more possibilities). 

For this problem the correlation is given by (see appendix):

$$\rho_{KQ} = \frac{(\phi_b+\phi_d)(1-2p)(p(1-p))^{.5}}{
(p\phi_b+(1-p)\phi_d)(1-(p\phi_b+(1-p)\phi_d)))^{.5}}$$

One might also use a measure of "mutual information" from information theory:

$$I(Q,K) = \sum_q \sum_k P(q,k)\log\left(\frac{P(q,k)}{P(q)P(k)}\right)$$

<!-- here: -->


<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- I(Q,K) ={} & p\phi_b\log\left(\frac{\phi_b}{p\phi_b+(1-p)\phi_d}\right)+ (1-p)\phi_d\log\left(\frac{\phi_d}{p\phi_b+(1-p)\phi_d}\right) \\ -->
<!--       & +p(1-\phi_b)\log\left(\frac{1-\phi_b}{1-p\phi_b-(1-p)\phi_d}\right)+ -->
<!-- (1-p)(1-\phi_d)\log\left(\frac{1-\phi_d}{1-p\phi_b-(1-p)\phi_d}\right) -->
<!-- \end{aligned} -->
<!-- \end{equation} -->

To express this mutual information as a share of variation explained, we could divide $I(Q,K)$ by the entropy of $Q$, $H(Q)$ where $H(Q) = -\sum_qP(q)\log(P(q))$. The resulting ratio can  be interpreted as 1 minus the ratio of the entropy of $Q$ conditional (on $K$) to the unconditional entropy of $Q$.

For this example, Figure \ref{fig:probative_value} shows gains as a function of $\phi_b$ given a fixed value of $\phi_d$. The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely. 

```{r, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:probative_value} The solid line shows gains in precision (reduced posterior variance) for different values of $\\phi_b$ given $\\phi_d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\\phi_b - \\phi_d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$."}



gains = function(p, phi_b, phi_d){
  1- (phi_b*phi_d)/(phi_b*p +phi_d*(1-p)) - (1-phi_b)*(1-phi_d)/((1-phi_b)*p+(1-phi_d)*(1-p))
}

corr_qk <- function(p, phi_b, phi_d){
  ((phi_b-phi_d)*(p*(1-p))^{.5})/
  (((p*phi_b+(1-p)*phi_d)*(1-(p*phi_b+(1-p)*phi_d)))^{.5})
  }
# Mutual Information
mi_qk <- function(p, phi_b, phi_d, base = 2){
 p*phi_b*        log({phi_b}   / {p*phi_b+(1-p)*phi_d}, base = base)+
(1-p)*phi_d*     log({phi_d}   / {p*phi_b+(1-p)*phi_d}, base = base)+
p*(1-phi_b)*     log({1-phi_b} / {1-p*phi_b-(1-p)*phi_d}, base = base)+
(1-p)*(1-phi_d)* log({1-phi_d}/ {1-p*phi_b-(1-p)*phi_d}, base = base)
  }

norm_mi_qk <-  function(p, phi_b, phi_d, base = 2){
    -mi_qk(p, phi_b, phi_d, base = base)/(p*log(p, base = base)+(1-p)*log(1-p, base = base))}

phi_b = seq(0,1,.01)

plot(phi_b, gains(.75, phi_b, .25), type = "l", xlab = expression(paste(phi[b])), ylab = "Probative Value")
  points(phi_b, abs(corr_qk(.75, phi_b, .25)), type = "l", lty=2)
  points(phi_b, (norm_mi_qk(.75, phi_b, .25)), type = "l", lty = 3)
  points(phi_b, abs(phi_b - .25), type = "l", lty = 4)
  title("Reduced posterior variance, correlation, mutual information")
text(.8, c(.15, .25, .3, .62, .41), c("I(K,Q)/H(Q)","(Reduced posterior variance)", "Gains",  expression(paste(abs(phi[b]-phi[d]))), "Cor(K,Q)"))

#plot(abs(corr_qk(.75, phi_b, .25)), gains(.75, phi_b, .25), type = "l", xlab = "Probative value")
#lines(abs(phi_b - .25), gains(.75, phi_b, .25), type = "l", lty=2)
#lines(norm_mi_qk(.75, phi_b, .25), gains(.75, phi_b, .25), type = "l", lty=2)


```


## Illustration in code

In the `gbiqq` package there is a single function that lets you declare a full design in one go by letting you supply arguments to declare a model, an inquiry, a data strategy, and an answer strategy

```{r, include = FALSE}
library(DeclareDesign)
library(gbiqq)
library(dplyr)
```

```{r ch10designer, message = FALSE, warning = FALSE, eval = FALSE}
# A single function can be used to declare a model, an inquiry, a data strategy, and an answer strategy

my_design <- gbiqq_designer(
  
   model           = make_model("X -> M -> Y"),
   inquiry         = list(ATE = "Y[X=1] - Y[X=0]"),
   data_strat      = list(n_obs = 5, 
                          vars = list(c("X", "Y"), "M"), 
                          probs = list(1, .5), 
                          subsets = list(NULL, "X==1 & Y==0"),
                          n = NULL),
   answer_strat  = NULL
)

```


With this model in hand you can use it to draw likely data, run analyses, an drun diagnostics. 

```{r, eval = FALSE}
sample_data      <- draw_data(my_design)
sample_estimands <- draw_estimands(my_design)
sample_estimates <- get_estimates(my_design, df)
```

