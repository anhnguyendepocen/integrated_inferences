# Final Words

The central idea of this book is that we can learn about the world by combining new evidence with causal models that represent our prior state of knowledge about causal relations in the domain of interest. 

<!-- many of the claims we want to make as social scientists require causal models that have sufficient complexity to be able to account for how and under what conditions causal relations play out.  -->

The growth of randomized experiments and other design-based approaches over the last two decades has made it possible to dispense with or diminish the role of background assumptions for some research questions and contexts. This is a remarkable achievement that has put the testing of some hypotheses and estimation of some causal quantities on a firmer footing. At the same time, there are limits to model-free social science. In particular, design-based inference relies on (as-good-as) random assignment by the researcher or by nature, placing bounds on the kinds of causes and contexts we can investigate. The approach is also generally limited to estimaing a single causal quantity: the average causal effect.

Building on pioneering work by scholars in computer science, statistics, and philosophy, we have outlined a principled approach to, and provided software tools for, mobilizing prior knowledge to learn from new data in situations where randomization is unavailable or to answer questions for which randomization is unhelpful. In this approach, causal models are *guides* to research design, *machines* for inference, and *objects* of inquiry. As guides, the models yield expectations about the learning that can be derived from a given case or set of cases and from a given type of evidence, conditional on the question being asked. As inferential machines, models generating updating on that query once the data are in hand. Finally, when we confront a model with data, we learn about the parameters of the model itself, which can be used to answer a range of other causal questions and allowing cumulation of knowledge across studies.  

Moreover, when we embed our prior beliefs in a causal model, we are able to address a broad range of causal questions, beyond estimation of population-level average effects. These include:

* **Case level questions**: Does $X$ explain $Y$ in this case?
* **Process questions**: Through which channel did $X$ matter for $Y$ in this case? Which channels operate most often in the population?
* **Transportability questions**: What are the implications of results derived in one context for processes and effects in other contexts?

<!-- Using causal models also provides a clear *procedure* for drawing inferences. They clarify when different kinds of information will be informative for different estimands and they clarify what inferences you can draw.  -->

The answers we get from the approach are necessarily model-dependent. But compared with most prevailing approaches to observational inference---where the background model is typically left implicit or conveyed informally or incompletely---the approach ensures both *consistency* between inferences and prior beliefs, given the data, and *transparency* about the beliefs on which inferences rest. These features allow us then to assess the degree of sensitivity of conclusions to our prior beliefs. 

<!-- While we have outlined a set of strategies for validating and selecting our models, the model-contingency of conclusions is an important and inescapable limitation of the framework.  -->

As we have developed the approach, our thinking about qualitative, quantitative, and mixed-method inference has shifted. In using causal models and seeing their benefits, we have have also developed a keener sense of the risks they entail. We outline these lessons and these risks next.   

## Lessons learned along the way

We note three ways in which our thinking about inference evolved in the course of this project.

**1. "Within" vs. "Between" Case Evidence.** We embarked on this project motivated by an interest in how qualitative and quantitative data could be formally combined to draw case- and population-level causal inferences. In @humphreys2015mixing, we drew on a common operationalization of "quantitative" and "qualitative" data as akin to "dataset" and "causal process" observations, respectively, as defined by @collier2010sources; this is a distinction that roughly maps onto somewhat older notions of "cross-case" and "within-case" forms of analysis @mahoney2000strategies. In a typical setup, we would think of data on $X$ and $Y$ data on many cases and $M$ data on process for some. In fact however this distinction has no meaning in the formal set up and analysis of models. One could just as easily be interested in the effect of $X$ on $Y$ and have plentiful data on $M$ but limited data on $Y$ or $X$. In this framework the qualitative and quantitative inference strategies are not just integrated, the distinction between them breaks down completely.

We started off thinking of beliefs about the values of estimands and beliefs about the informativeness of within case information as being essentially independent. This was a feature of the  models we explored in @humphreys2015mixing and implicit in many accounts of process tracing: you articulate a belief about some hypothesis  and you articulate a belief about how informative evidence will be about your hypothesis. When both of these beliefs are tehmselves derived from an integrated model however then the same conjectures that infrom your beliefs about the hypotheses also inform your beliefs about the informativeness of additional data, you just cannot think of them as independent from each other.

We started off thinking that in providing priors over causal relations you were directly stating beliefs about how the world works. In the simple case one might think that either $X$ cuased $Y$ or it did not and either $M$ should be seen in the event that $X$ caused $Y$ or it shouldn't be.  But these statements are in fact clearly model dependent. Beyond the model required to describe events in such crisp terms, the statements involve counterfactuals on counterfactuals---models of causal processes. Once a model involves assertions of conditional independence we are clearly in the business of dealing in simplifications and our priors become less statements of how we believe the world works to become somewhat statements about what set of models are least bad within a class of abstractions.

## Worries about what you have to put in

Trading in models also brought into focus some of the limitations of DAGs in representing causal processes. 

**Well defined nodes?** Do DAGs actually capture causal processes that qualitative researchers see -- qualitative researchers see that the domino 2 fell *the moment it was hit* by domino 1. How do we express this in a DAG?

**Acyclic really?** The first assumption made in the construction of causal models in this book is the underlying DAG. One can specify a DAG without making any substantive claims about function forms or patterns of confounding. Yet even the  DAG presents worries. 

**Theoretically deeper models.** 
<!-- AJ: What's this about? -->


## Limits on what you can get out

**Complexity.** We have sought to use non-parametric models.. To maintain simplicity we have largely focused on models with binary nodes. At first blush this class of causal models appears very simple. In fact however we quickly learn that even with a small set of nodes produces a dizzying variety of causal types.   

<!-- AJ: These next two points seem more like advantages to me. They are fundamental features of causal inference we can see more clearly if we use models, not limitations of the approach itself. No? Maybe we should have a section on that sort of thing: limits to causal inference uncovered by the approach. (I've added the model-dependence point.)-->

**Identification.** In modelling complete structures we see clearly how much easier it is to define problems than it is to solve them. Many of the quantities we care about are easily shown to not be identified by the causal models we employ. The causes of effect estimand is perhaps the most obvious of these. 

**Limits of qualitative data under ignorable assignments.** You generally cannot conclude all that much about population quantities from only a small number of cases when causal effects are identified. 

**Model-dependence of conclusions** We have been interested to see how sensitive conclusions can be to relatively modest changes to models. We see two ways of thinking about the implications of this fact for a causal-models framework. One is to focus on the limits to building inference upon causal models. If results depend on prior beliefs, which could be wrong, how valuable are the results? At the same time, the close relationship between assumptions and inferences makes a transparent and systematic engagement with models all the more important: if inferences are not built explicitly on models, we have no way of knowing how fragile they are or how they would change under an alternative set of premises.


## A world of models: Practical steps forward for collective cumulation
